[
    {
        "filename": "HIVE-10992.json",
        "creation_time": "2015-06-12T19:52:35.000+0000",
        "bug_report": {
            "Title": "WebHCat should not create delegation tokens when Kerberos is not enabled",
            "Description": "TempletonControllerJob.run() method does\n{noformat}\n      Token<DelegationTokenIdentifier> mrdt = jc.getDelegationToken(new Text(\"mr token\"));\n      job.getCredentials().addToken(new Text(\"mr token\"), mrdt);\n{noformat}\n\nit should only do this if UserGroupInformation.isSecurityEnabled().\n\nFor long running jobs submitted via WebHCat (> 24 hours), this token is cancelled automatically (see YARN-2964) for the LaunchMapper while the child job may still be running.\n\nThen errors like this may happen\n{noformat}\n2015-05-25 20:49:38,026 WARN [main] org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache2015-05-25 20:49:38,058 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : \njava.lang.RuntimeException: Exception occurred while finding child jobs \n at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:204) \n at org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(WebHCatJTShim23.java:158) \n at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(LaunchMapper.java:156) \n at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(LaunchMapper.java:124) \n at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(LaunchMapper.java:261) \n at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784) \n at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) \n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) \n at java.security.AccessController.doPrivileged(Native Method) \n at javax.security.auth.Subject.doAs(Subject.java:415) \n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) \n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache \n at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) \n at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) \n at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) \n at java.lang.reflect.Constructor.newInstance(Constructor.java:526) \n at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53) \n at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104) \n at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:250) \n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \n at java.lang.reflect.Method.invoke(Method.java:606) \n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) \n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) \n at com.sun.proxy.$Proxy26.getApplications(Unknown Source) \n at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:198) ... 11 more\n \n Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache \n at org.apache.hadoop.ipc.Client.call(Client.java:1469) \n at org.apache.hadoop.ipc.Client.call(Client.java:1400) \n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) \n at com.sun.proxy.$Proxy25.getApplications(Unknown Source) \n at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:247) ... 19 more\n{noformat}\n\nThanks [~jianhe] for the analysis"
        }
    },
    {
        "filename": "HIVE-16450.json",
        "creation_time": "2017-04-14T13:59:12.000+0000",
        "bug_report": {
            "Title": "Some metastore operations are not retried even with desired underlining exceptions",
            "Description": "In RetryingHMSHandler class, we are expecting the operations should retry when the cause of MetaException is JDOException or NucleusException.\n{noformat}\n        if (e.getCause() instanceof MetaException && e.getCause().getCause() != null) {\n          if (e.getCause().getCause() instanceof javax.jdo.JDOException ||\n              e.getCause().getCause() instanceof NucleusException) {\n            // The JDOException or the Nucleus Exception may be wrapped further in a MetaException\n            caughtException = e.getCause().getCause();\n       }\n{noformat}\n\nWhile in ObjectStore, many places we are only throwing new MetaException(msg) without the cause, so we are missing retrying for some cases. e.g., with the following JDOException, we should retry but it's ignored.\n\n{noformat}\n2017-04-04 17:28:21,602 ERROR metastore.ObjectStore (ObjectStore.java:getMTableColumnStatistics(6555)) - Error retrieving statistics via jdo\njavax.jdo.JDOException: Exception thrown when executing query\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)\n        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(ObjectStore.java:6546)\n        at org.apache.hadoop.hive.metastore.ObjectStore.access$1200(ObjectStore.java:171)\n        at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6606)\n        at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6595)\n        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2633)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(ObjectStore.java:6594)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatistics(ObjectStore.java:6588)\n        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:103)\n        at com.sun.proxy.$Proxy0.getTableColumnStatistics(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:787)\n        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:247)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3809)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3779)\n        at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)\n        at com.sun.proxy.$Proxy3.alter_table_with_environment_context(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9617)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9601)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-6389.json",
        "creation_time": "2014-02-07T01:33:32.000+0000",
        "bug_report": {
            "Title": "LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.",
            "Description": "RCFile tables that use the LazyBinaryColumnarSerDe don't seem to handle look-ups into map-columns when the value of the column is null.\n\nWhen an RCFile table is created with LazyBinaryColumnarSerDe (as is default in 0.12), and queried as follows:\n\n{code}\nselect mymap['1024'] from mytable;\n{code}\n\nand if the mymap column has nulls, then one is treated to the following guttural utterance:\n\n{code}\n2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":null,\"mymap\":null,\"isnull\":null}\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)\n  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)\n  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text\n  at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)\n  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)\n  at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)\n  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)\n  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)\n  ... 10 more\n{code}\n\nA patch is on the way, but the short of it is that the LazyBinaryMapOI needs to return nulls if either the map or the lookup-key is null.\n\nThis is handled correctly for Text data, and for RCFiles using ColumnarSerDe."
        }
    },
    {
        "filename": "HIVE-2372.json",
        "creation_time": "2011-08-12T09:07:34.000+0000",
        "bug_report": {
            "Title": "java.io.IOException: error=7, Argument list too long",
            "Description": "I execute a huge query on a table with a lot of 2-level partitions. There is a perl reducer in my query. Maps worked ok, but every reducer fails with the following exception:\n\n2011-08-11 04:58:29,865 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: Executing [/usr/bin/perl, <reducer.pl>, <my_argument>]\n2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: tablename=null\n2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: partname=null\n2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: alias=null\n2011-08-11 04:58:29,935 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":129390185139228,\"reducesinkkey1\":\"00008AF10000000063CA6F\"},\"value\":{\"_col0\":\"00008AF10000000063CA6F\",\"_col1\":\"2011-07-27 22:48:52\",\"_col2\":129390185139228,\"_col3\":2006,\"_col4\":4100,\"_col5\":\"10017388=6\",\"_col6\":1063,\"_col7\":\"NULL\",\"_col8\":\"address.com\",\"_col9\":\"NULL\",\"_col10\":\"NULL\"},\"alias\":0}\n\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator\n\tat org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)\n\t... 7 more\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/perl\": java.io.IOException: error=7, Argument list too long\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:460)\n\tat org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)\n\t... 15 more\nCaused by: java.io.IOException: java.io.IOException: error=7, Argument list too long\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:148)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:65)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:453)\n\t... 16 more\n\nIt seems to me, I found the cause. ScriptOperator.java puts a lot of configs as environment variables to the child reduce process. One of variables is mapred.input.dir, which in my case more than 150KB. There are a huge amount of input directories in this variable. In short, the problem is that Linux (up to 2.6.23 kernel version) limits summary size of environment variables for child processes to 132KB. This problem could be solved by upgrading the kernel. But strings limitations still be 132KB per string in environment variable. So such huge variable doesn't work even on my home computer (2.6.32). You can read more information on (http://www.kernel.org/doc/man-pages/online/pages/man2/execve.2.html).\n\nFor now all our work has been stopped because of this problem and I can't find the solution. The only solution, which seems to me more reasonable is to get rid of this variable in reducers.\n\n\n"
        }
    },
    {
        "filename": "HIVE-2958.json",
        "creation_time": "2012-04-17T15:02:38.000+0000",
        "bug_report": {
            "Title": "GROUP BY causing ClassCastException [LazyDioInteger cannot be cast LazyInteger]",
            "Description": "This relates to https://issues.apache.org/jira/browse/HIVE-1634.\n\nThe following work fine:\n\n{code}\nCREATE EXTERNAL TABLE tim_hbase_occurrence ( \n  id int,\n  scientific_name string,\n  data_resource_id int\n) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\n  \"hbase.columns.mapping\" = \":key#b,v:scientific_name#s,v:data_resource_id#b\"\n) TBLPROPERTIES(\n  \"hbase.table.name\" = \"mini_occurrences\", \n  \"hbase.table.default.storage.type\" = \"binary\"\n);\nSELECT * FROM tim_hbase_occurrence LIMIT 3;\nSELECT * FROM tim_hbase_occurrence WHERE data_resource_id=1081 LIMIT 3;\n{code}\n\nHowever, the following fails:\n{code}\nSELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id;\n{code}\n\nThe error given:\n{code}\n0 TS\n2012-04-17 16:58:45,693 INFO org.apache.hadoop.hive.ql.exec.MapOperator: Initialization Done 7 MAP\n2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.MapOperator: Processing alias tim_hbase_occurrence for file hdfs://c1n2.gbif.org/user/hive/warehouse/tim_hbase_occurrence\n2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.MapOperator: 7 forwarding 1 rows\n2012-04-17 16:58:45,714 INFO org.apache.hadoop.hive.ql.exec.TableScanOperator: 0 forwarding 1 rows\n2012-04-17 16:58:45,716 INFO org.apache.hadoop.hive.ql.exec.SelectOperator: 1 forwarding 1 rows\n2012-04-17 16:58:45,723 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:264)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:83)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)\n\t... 9 more\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger\n\tat org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(LazyIntObjectInspector.java:43)\n\tat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:239)\n\tat org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:150)\n\tat org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:142)\n\tat org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.copyKey(KeyWrapperFactory.java:119)\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:750)\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:722)\n\t... 18 more\n{code}\n\n"
        }
    },
    {
        "filename": "HIVE-13392.json",
        "creation_time": "2016-03-30T22:32:50.000+0000",
        "bug_report": {
            "Title": "disable speculative execution for ACID Compactor",
            "Description": "https://developer.yahoo.com/hadoop/tutorial/module4.html\nSpeculative execution is enabled by default. You can disable speculative execution for the mappers and reducers by setting the mapred.map.tasks.speculative.execution and mapred.reduce.tasks.speculative.execution JobConf options to false, respectively.\n\nCompactorMR is currently not set up to handle speculative execution and may lead to something like\n\n{code}\n2016-02-08 22:56:38,256 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2451)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)\n{code}\n\nShort term: disable speculative execution for this job\nLonger term perhaps make each task write to dir with UUID...\n\n"
        }
    },
    {
        "filename": "HIVE-11301.json",
        "creation_time": "2015-07-18T00:41:40.000+0000",
        "bug_report": {
            "Title": "thrift metastore issue when getting stats results in disconnect",
            "Description": "On metastore side it looks like this:\n{noformat}\n2015-07-17 20:32:27,795 ERROR [pool-3-thread-150]: server.TThreadPoolServer (TThreadPoolServer.java:run(294)) - Thrift error occurred during processing of message.\norg.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)\n        at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nand then\n{noformat}\n2015-07-17 20:32:27,796 WARN  [pool-3-thread-150]: transport.TIOStreamTransport (TIOStreamTransport.java:close(112)) - Error closing output stream.\njava.net.SocketException: Socket closed\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:153)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n        at java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n        at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)\n        at org.apache.thrift.transport.TSocket.close(TSocket.java:196)\n        at org.apache.hadoop.hive.thrift.TFilterTransport.close(TFilterTransport.java:52)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:304)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\n\nWhich on client manifests as\n{noformat}\n2015-07-17 20:32:27,796 WARN  [main()]: metastore.RetryingMetaStoreClient (RetryingMetaStoreClient.java:invoke(187)) - MetaStoreClient lost connection. Attempting to reconnect.\norg.apache.thrift.transport.TTransportException\n        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_aggr_stats_for(ThriftHiveMetastore.java:3029)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_aggr_stats_for(ThriftHiveMetastore.java:3016)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2067)\n        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n        at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1991)\n        at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3124)\n        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:252)\n        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:137)\n        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:125)\n        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(Stat\nsRulesProcFactory.java:111)\n{noformat}\n\nand CLI hangs for a really long time while this thing is retrying."
        }
    },
    {
        "filename": "HIVE-11028.json",
        "creation_time": "2015-06-16T23:03:38.000+0000",
        "bug_report": {
            "Title": "Tez: table self join and join with another table fails with IndexOutOfBoundsException",
            "Description": "{noformat}\ncreate table tez_self_join1(id1 int, id2 string, id3 string);\ninsert into table tez_self_join1 values(1, 'aa','bb'), (2, 'ab','ab'), (3,'ba','ba');\n\ncreate table tez_self_join2(id1 int);\ninsert into table tez_self_join2 values(1),(2),(3);\n\nexplain\nselect s.id2, s.id3\nfrom\n(\n select self1.id1, self1.id2, self1.id3\n from tez_self_join1 self1 join tez_self_join1 self2\n on self1.id2=self2.id3 ) s\njoin tez_self_join2\non s.id1=tez_self_join2.id1\nwhere s.id2='ab';\n{noformat}\n\nfails with error:\n\n{noformat}\n2015-06-16 15:41:55,759 ERROR [main]: ql.Driver (SessionState.java:printError(979)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 3, vertexId=vertex_1434494327112_0002_4_04, diagnostics=[Task failed, taskId=task_1434494327112_0002_4_04_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)\n        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635)\n        at java.util.ArrayList.get(ArrayList.java:411)\n        at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118)\n        at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.<init>(StandardStructObjectInspector.java:109)\n        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:290)\n        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:275)\n        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:175)\n        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:313)\n        at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:71)\n        at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:99)\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:362)\n        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)\n        ... 13 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-14380.json",
        "creation_time": "2016-07-29T00:14:58.000+0000",
        "bug_report": {
            "Title": "Queries on tables with remote HDFS paths fail in \"encryption\" checks.",
            "Description": "If a table has table/partition locations set to remote HDFS paths, querying them will cause the following IAException:\r\n\r\n{noformat}\r\n2016-07-26 01:16:27,471 ERROR parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1867)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to determine if hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table, expected: hdfs://bar.ygrid.yahoo.com:8020\r\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2204)\r\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStrongestEncryptedTablePath(SemanticAnalyzer.java:2274)\r\n...\r\n{noformat}\r\n\r\nThis is because of the following code in {{SessionState}}:\r\n{code:title=SessionState.java|borderStyle=solid}\r\n public HadoopShims.HdfsEncryptionShim getHdfsEncryptionShim() throws HiveException {\r\n    if (hdfsEncryptionShim == null) {\r\n      try {\r\n        FileSystem fs = FileSystem.get(sessionConf);\r\n        if (\"hdfs\".equals(fs.getUri().getScheme())) {\r\n          hdfsEncryptionShim = ShimLoader.getHadoopShims().createHdfsEncryptionShim(fs, sessionConf);\r\n        } else {\r\n          LOG.debug(\"Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.\");\r\n        }\r\n      } catch (Exception e) {\r\n        throw new HiveException(e);\r\n      }\r\n    }\r\n\r\n    return hdfsEncryptionShim;\r\n  }\r\n{code}\r\n\r\nWhen the {{FileSystem}} instance is created, using the {{sessionConf}} implies that the current HDFS is going to be used. This call should instead fetch the {{FileSystem}} instance corresponding to the path being checked.\r\n\r\nA fix is forthcoming...\r\n\r\n(Note to self: YHIVE-860)"
        }
    },
    {
        "filename": "HIVE-7799.json",
        "creation_time": "2014-08-20T09:45:21.000+0000",
        "bug_report": {
            "Title": "TRANSFORM failed in transform_ppr1.q[Spark Branch]",
            "Description": "Here is the exception:\n{noformat}\n2014-08-20 01:14:36,594 ERROR executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.next(HiveKVResultCache.java:113)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:124)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:82)\n        at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:65)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n        at org.apache.spark.scheduler.Task.run(Task.scala:54)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n{noformat}\n\nBasically, the cause is that RowContainer is misused(it's not allowed to write once someone read row from it), i'm trying to figure out whether it's a hive issue or just in hive on spark mode."
        }
    },
    {
        "filename": "HIVE-6537.json",
        "creation_time": "2014-03-03T18:57:44.000+0000",
        "bug_report": {
            "Title": "NullPointerException when loading hashtable for MapJoin directly",
            "Description": "We see the following error:\n{noformat}\n2014-02-20 23:33:15,743 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)\n        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)\n        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)\n        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)\n        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)\n        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)\n        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: java.lang.NullPointerException\n        at java.util.Arrays.fill(Arrays.java:2685)\n        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)\n        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:81)\n        ... 15 more\n{noformat}\n\nIt appears that the tables in Arrays.fill call is nulls. I don't really have full understanding of this path, but what I gleaned so far is this...\nFrom what I see, tables would be set unconditionally in initializeOp of the sink, and in no other place, so I assume for this code to ever  work that startForward calls it at least some time.\nHere, it doesn't call it, so it's null. \nPrevious loop also uses tables, and should have NPE-d before fill was ever called; it didn't, so I'd assume it never executed. \nThere's a little bit of inconsistency in the above code where directWorks are added to parents unconditionally but sink is only added as child conditionally. I think it may be that some of the direct works are not table scans; in fact given that loop never executes they may be null (which is rather strange). \nRegardless, it seems that the logic should be fixed, it may be the root cause"
        }
    },
    {
        "filename": "HIVE-13691.json",
        "creation_time": "2016-05-04T23:40:03.000+0000",
        "bug_report": {
            "Title": "No record with CQ_ID=0 found in COMPACTION_QUEUE",
            "Description": "{noformat}\n2016-04-29 18:49:31,594 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(141)) - Caught exception while trying to determine if we should compact id:0,dbname:default,tableName:service_logs_v2,par\ntName:ds=2016-04-21,state:^@,type:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking clean to avoid repeated failures, MetaException(message:Timeout when executing method: getTable)\n        at org.apache.hadoop.hive.metastore.Deadline.newMetaException(Deadline.java:187)\n        at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:177)\n        at org.apache.hadoop.hive.metastore.Deadline.checkTimeout(Deadline.java:160)\n        at org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1839)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2255)\n        at org.apache.hadoop.hive.metastore.ObjectStore.access$300(ObjectStore.java:165)\n        at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2051)\n        at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2043)\n        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2400)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal(ObjectStore.java:2043)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNames(ObjectStore.java:2037)\n        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)\n        at com.sun.proxy.$Proxy0.getPartitionsByNames(Unknown Source)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolvePartition(CompactorThread.java:111)\n        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:129)\nCaused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable\n        at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)\n        ... 16 more\n\n2016-04-29 18:49:31,595 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(154)) - Initiator loop caught unexpected exception this time through the loop: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE\n        at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)\n        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)\n\n{noformat}\n\n{noformat}\n2016-04-29 18:49:31,595 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(154)) - Initiator loop caught unexpected exception this time through the loop: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE\n        at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)\n        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)\n{noformat}\n\nis triggered by _DeadlineException: Timeout when executing method_ but is nonetheless an issue.\n\nWe should be able to record an entry in completed_compaction_queue to represent a failed compaction even if an entry in compaction_queue was never made, as is the case here."
        }
    },
    {
        "filename": "HIVE-17758.json",
        "creation_time": "2017-10-10T12:33:52.000+0000",
        "bug_report": {
            "Title": "NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL.defaultLongVal is -1",
            "Description": "HIVE-16886 introduced a retry logic; which has a configurable retry interval. unfortunately {{HiveConf}} has some public fields which at first glance seems to be usefull to pass as arguments to other methods - but in this case the default value is not even loaded into the field read by the code.. and because of that the innocent client code [here|https://github.com/apache/hive/blob/a974a9e6c4659f511e0b5edb97ce340a023a2e26/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L8554]  have used a {{-1}} value incorrectly which eventually caused an exception [here|https://github.com/apache/hive/blob/a974a9e6c4659f511e0b5edb97ce340a023a2e26/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java#L8581]:\r\n{code}\r\n2017-10-10 11:22:37,638 ERROR [load-dynamic-partitions-12]: metastore.ObjectStore (ObjectStore.java:addNotificationEvent(7444)) - could not get lock for update\r\njava.lang.IllegalArgumentException: timeout value is negative\r\n        at java.lang.Thread.sleep(Native Method)\r\n        at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:7407)\r\n        at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:7361)\r\n        at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7424)\r\n        at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n[...]\r\n{code}\r\n"
        }
    },
    {
        "filename": "HIVE-14898.json",
        "creation_time": "2016-10-06T00:02:36.000+0000",
        "bug_report": {
            "Title": "HS2 shouldn't log callstack for an empty auth header error",
            "Description": "Currently when the auth header is not sent by the client (Knox seems to do this every time - it only adds auth header after receiving 401), HS2 logs the following twice, for two principals.\nThe callstack is useless because this is an expected condition and 401 is returned to the client.\n{noformat}\n2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(169)) - Failed to authenticate with hive/_HOST kerberos principal\n2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(104)) - Error: \norg.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:170)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:83)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:349)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:952)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:167)\n\t... 23 more\nCaused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(ThriftHttpServlet.java:311)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.access$100(ThriftHttpServlet.java:59)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:212)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:175)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\t... 24 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-5546.json",
        "creation_time": "2013-10-15T15:06:59.000+0000",
        "bug_report": {
            "Title": "A change in ORCInputFormat made by HIVE-4113 was reverted by HIVE-5391",
            "Description": "{code}\n2013-10-15 10:49:49,386 INFO org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: included column ids = \n2013-10-15 10:49:49,386 INFO org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: included columns names = \n2013-10-15 10:49:49,386 INFO org.apache.hadoop.hive.ql.io.orc.OrcInputFormat: No ORC pushdown predicate\n2013-10-15 10:49:49,834 INFO org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader: Processing file hdfs://localhost:54310/user/hive/warehouse/web_sales_orc/000000_0\n2013-10-15 10:49:49,834 INFO org.apache.hadoop.mapred.MapTask: numReduceTasks: 1\n2013-10-15 10:49:49,840 INFO org.apache.hadoop.mapred.MapTask: io.sort.mb = 100\n2013-10-15 10:49:49,968 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1\n2013-10-15 10:49:49,994 INFO org.apache.hadoop.io.nativeio.NativeIO: Initialized cache for UID to User mapping with a cache timeout of 14400 seconds.\n2013-10-15 10:49:49,994 INFO org.apache.hadoop.io.nativeio.NativeIO: Got UserName yhuai for UID 1000 from the native implementation\n2013-10-15 10:49:49,996 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n{code}\n\nIf includedColumnIds is an empty list, we do not need to read any column. But, right now, in OrcInputFormat.findIncludedColumns, we have ...\n{code}\nif (ColumnProjectionUtils.isReadAllColumns(conf) ||\n      includedStr == null || includedStr.trim().length() == 0) {\n      return null;\n    } \n{code}\nIf includedStr is an empty string, the code assumes that we need all columns, which is not correct."
        }
    },
    {
        "filename": "HIVE-7557.json",
        "creation_time": "2014-07-30T19:25:12.000+0000",
        "bug_report": {
            "Title": "When reduce is vectorized, dynpart_sort_opt_vectorization.q under Tez fails",
            "Description": "\nTurned off dynpart_sort_opt_vectorization.q (Tez) since it fails when reduce is vectorized to get HIVE-7029 checked in.\n\nStack trace:\n{code}\nContainer released by application, AttemptID:attempt_1406747677386_0003_2_00_000000_2 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector\n\tat org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\n ]\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector\n\tat org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\n ]\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:382)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\t... 6 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector\n\tat org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\n ]\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:486)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\t... 8 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.serialize(VectorizedOrcSerde.java:75)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcSerde.serializeVector(OrcSerde.java:148)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.processOp(VectorFileSinkOperator.java:79)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.processOp(VectorExtractOperator.java:99)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:470)\n\t... 9 more\n{code}"
        }
    },
    {
        "filename": "HIVE-1712.json",
        "creation_time": "2010-10-14T17:17:44.000+0000",
        "bug_report": {
            "Title": "Migrating metadata from derby to mysql thrown NullPointerException",
            "Description": "Exported derby data to csv, loaded data into mysql and ran hive query which worked in derby and got the following exception\n\n2010-10-16 08:57:29,080 INFO  metastore.ObjectStore (ObjectStore.java:setConf(106)) - Initialized ObjectStore\n2010-10-16 08:57:29,552 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logStartFunction(171)) - 0: get_table : db=default tbl=testimport\n2010-10-16 08:57:30,140 ERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException\n        at java.util.Hashtable.put(Hashtable.java:394)\n        at java.util.Hashtable.putAll(Hashtable.java:466)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)\n        at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
        }
    },
    {
        "filename": "HIVE-12608.json",
        "creation_time": "2015-12-07T21:26:01.000+0000",
        "bug_report": {
            "Title": "Parquet Schema Evolution doesn't work when a column is dropped from array<struct<>>",
            "Description": "When a column is dropped from array<struct<>>, I got the following exception.\n\nI used the following sql to test it.\n\n{quote}\nCREATE TABLE arrays_of_struct_to_map (locations1 array<struct<c1:int,c2:int>>, locations2 array<struct<f1:int,\nf2:int,f3:int>>) STORED AS PARQUET;\nINSERT INTO TABLE arrays_of_struct_to_map select array(named_struct(\"c1\",1,\"c2\",2)), array(named_struct(\"f1\",\n77,\"f2\",88,\"f3\",99)) FROM parquet_type_promotion LIMIT 1;\nSELECT * FROM arrays_of_struct_to_map;\n-- Testing schema evolution of dropping column from array<struct<>>\nALTER TABLE arrays_of_struct_to_map REPLACE COLUMNS (locations1 array<struct<c1:int>>, locations2\narray<struct<f2:int>>);\nSELECT * FROM arrays_of_struct_to_map;\n{quote}\n\n{quote}\n2015-12-07 11:47:28,503 ERROR [main]: CliDriver (SessionState.java:printError(921)) - Failed with exception java.io.IOException:java.lang.RuntimeException: cannot find field c2 in [c1]\njava.io.IOException: java.lang.RuntimeException: cannot find field c2 in [c1]\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)\n        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1029)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1003)\n        at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:139)\n        at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_type_promotion(TestCliDriver.java:123)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at junit.framework.TestCase.runTest(TestCase.java:176)\n        at junit.framework.TestCase.runBare(TestCase.java:141)\n        at junit.framework.TestResult$1.protect(TestResult.java:122)\n        at junit.framework.TestResult.runProtected(TestResult.java:142)\n        at junit.framework.TestResult.run(TestResult.java:125)\n        at junit.framework.TestCase.run(TestCase.java:129)\n        at junit.framework.TestSuite.runTest(TestSuite.java:255)\n        at junit.framework.TestSuite.run(TestSuite.java:250)\n        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: java.lang.RuntimeException: cannot find field c2 in [c1]\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getStructFieldTypeInfo(HiveStructConverter.java:130)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getFieldTypeIgnoreCase(HiveStructConverter.java:103)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.init(HiveStructConverter.java:90)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:67)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:59)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:63)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:75)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter$ElementConverter.<init>(HiveCollectionConverter.java:141)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter.<init>(HiveCollectionConverter.java:52)\n{quote}"
        }
    },
    {
        "filename": "HIVE-17774.json",
        "creation_time": "2017-10-11T20:02:01.000+0000",
        "bug_report": {
            "Title": "compaction may start with 0 splits and fail",
            "Description": "{noformat}\r\n2017-09-26 10:36:01,979 INFO  [...]: compactor.CompactorMR (CompactorMR.java:launchCompactionJob(295)) - \r\nSubmitting MINOR compaction job ....\r\n (current delta dirs count=0, obsolete delta dirs count=0. TxnIdRange[9223372036854775807,-9223372036854775808]\r\n...\r\n2017-09-26 10:36:02,350 INFO  [...]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:0\r\n...\r\n2017-09-26 10:36:08,637 INFO  [...]: mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - \r\nJob job_1503950256860_15982 failed with state FAILED due to: No of maps and reduces are 0 job_1503950256860_15982\r\nJob commit failed: java.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)\r\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)\r\n\tat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n{noformat}\r\n\r\nLooks like the MR job should not have been attempted in this case."
        }
    },
    {
        "filename": "HIVE-14564.json",
        "creation_time": "2016-08-18T00:11:34.000+0000",
        "bug_report": {
            "Title": "Column Pruning generates out of order columns in SelectOperator which cause ArrayIndexOutOfBoundsException.",
            "Description": "Column Pruning generates out of order columns in SelectOperator which cause ArrayIndexOutOfBoundsException.\n\n{code}\n2016-07-26 21:49:24,390 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)\n\t... 9 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.hadoop.io.Text.set(Text.java:225)\n\tat org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(LazyBinaryString.java:48)\n\tat org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)\n\tat org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)\n\tat org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:94)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(ReduceSinkOperator.java:550)\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:377)\n\t... 13 more\n{code}\n\nThe exception is because the serialization and deserialization doesn't match.\nThe serialization by LazyBinarySerDe from previous MapReduce job used different order of columns. When the current MapReduce job deserialized the intermediate sequence file generated by previous MapReduce job, it will get corrupted data from the deserialization using wrong order of columns by LazyBinaryStruct. The unmatched columns between  serialization and deserialization is caused by SelectOperator's Column Pruning {{ColumnPrunerSelectProc}}."
        }
    },
    {
        "filename": "HIVE-3651.json",
        "creation_time": "2012-11-01T23:31:20.000+0000",
        "bug_report": {
            "Title": "bucketmapjoin?.q  tests fail with hadoop 0.23",
            "Description": "The hive.log show error in MR job -\nTask failed!\nTask ID:\n  Stage-1\n\nThe job log has following error -\n2012-11-01 15:51:20,253 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(479)) - job_local_0001\njava.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)\n        at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:679)\n\n"
        }
    },
    {
        "filename": "HIVE-5199.json",
        "creation_time": "2013-09-03T20:40:29.000+0000",
        "bug_report": {
            "Title": "Custom SerDe containing a nonSettable complex data type row object inspector throws cast exception with HIVE 0.11",
            "Description": "The issue happens because of the changes in HIVE-3833.\n\nConsider a partitioned table with different custom serdes for the partition and tables. The serde at table level, say, customSerDe1's object inspector is of settableDataType where as the serde at partition level, say, customSerDe2's object inspector is of nonSettableDataType. The current implementation introduced by HIVE-3833 does not convert nested Complex Data Types which extend nonSettableObjectInspector to a settableObjectInspector type inside ObjectInspectorConverters.getConvertedOI(). However, it tries to typecast the nonSettableObjectInspector to a settableObjectInspector inside  ObjectInspectorConverters.getConverter(ObjectInspector inputOI, ObjectInspector outputOI).\n\nThe attached patch HIVE-5199.2.patch.txt contains a stand-alone test case.\n\nThe below exception can happen via FetchOperator as well as MapOperator. \nFor example, consider the FetchOperator.\nInside FetchOperator consider the following call:\ngetRecordReader()->ObjectInspectorConverters. getConverter()\n\nThe stack trace as follows:\n2013-08-28 17:57:25,307 ERROR CliDriver (SessionState.java:printError(432)) - Failed with exception java.io.IOException:java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector\njava.io.IOException: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488)\nat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)\nat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1412)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:601)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:144)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:307)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:138)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:406)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)"
        }
    },
    {
        "filename": "HIVE-17368.json",
        "creation_time": "2017-08-22T01:27:32.000+0000",
        "bug_report": {
            "Title": "DBTokenStore fails to connect in Kerberos enabled remote HMS environment",
            "Description": "In setups where HMS is running as a remote process secured using Kerberos, and when {{DBTokenStore}} is configured as the token store, the HS2 Thrift API call {{GetDelegationToken}} fail with exception trace seen below. HS2 is not able to invoke HMS APIs needed to add/remove/renew tokens from the DB since it is possible that the user which is issue the {{GetDelegationToken}} is not kerberos enabled.\n\nEg. Oozie submits a job on behalf of user \"Joe\". When Oozie opens a session with HS2 it uses Oozie's principal and creates a proxy UGI with Hive. This principal can establish a transport authenticated using Kerberos. It stores the HMS delegation token string in the sessionConf and sessionToken. Now, lets say Oozie issues a {{GetDelegationToken}} which has {{Joe}} as the owner and {{oozie}} as the renewer in {{GetDelegationTokenReq}}. This API call cannot instantiate a HMSClient and open transport to HMS using the HMSToken string available in the sessionConf, since DBTokenStore uses server HiveConf instead of sessionConf. It tries to establish transport using Kerberos and it fails since user Joe is not Kerberos enabled.\n\n\nI see the following exception trace in HS2 logs.\n{noformat}\n2017-08-21T18:07:19,644 ERROR [HiveServer2-Handler-Pool: Thread-61] transport.TSaslTransport: SASL negotiation failure\njavax.security.sasl.SaslException: GSS initiate failed\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_121]\n        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[libthrift-0.9.3.jar:0.9.3]\n        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]\n        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:488) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:255) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_121]\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [?:1.8.0_121]\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.8.0_121]\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [?:1.8.0_121]\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3595) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3647) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3627) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]\n        at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnTokenStore(DBTokenStore.java:157) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.DBTokenStore.addToken(DBTokenStore.java:74) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:142) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:56) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.security.token.Token.<init>(Token.java:59) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getDelegationToken(DelegationTokenSecretManager.java:109) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:123) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]\n        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationToken(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationTokenWithService(HiveDelegationTokenManager.java:130) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.auth.HiveAuthFactory.getDelegationToken(HiveAuthFactory.java:261) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken(HiveSessionImplwithUGI.java:174) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]\n        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at com.sun.proxy.$Proxy36.getDelegationToken(Unknown Source) [?:?]\n        at org.apache.hive.service.cli.CLIService.getDelegationToken(CLIService.java:589) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetDelegationToken(ThriftCLIService.java:254) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1737) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1722) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:621) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [libthrift-0.9.3.jar:0.9.3]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\nCaused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) ~[?:1.8.0_121]\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) ~[?:1.8.0_121]\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) ~[?:1.8.0_121]\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) ~[?:1.8.0_121]\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) ~[?:1.8.0_121]\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_121]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_121]\n        ... 65 more\n{noformat}\n\nOn HMS side I see a exception saying \n\n{noformat}\n2017-08-17 11:45:13,655 ERROR org.apache.thrift.server.TThreadPoolServer: [pool-7-thread-34]: Error occurred during processing of message.\njava.lang.RuntimeException: org.apache.thrift.transport.TTransportException: DIGEST-MD5: IO error acquiring password\n{noformat}"
        }
    },
    {
        "filename": "HIVE-4233.json",
        "creation_time": "2013-03-26T13:02:20.000+0000",
        "bug_report": {
            "Title": "The TGT gotten from class 'CLIService'  should be renewed on time",
            "Description": "When the HIveServer2 have started more than 7 days, I use beeline  shell  to  connect the HiveServer2,all operation failed.\n\nThe log of HiveServer2 shows it was caused by the Kerberos auth failure,the exception stack trace is:\n\n2013-03-26 11:55:20,932 ERROR hive.ql.metadata.Hive: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1084)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:51)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:61)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2140)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2151)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(Hive.java:2275)\n        at org.apache.hive.service.cli.CLIService.getDelegationTokenFromMetaStore(CLIService.java:358)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:127)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1073)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1058)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:565)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.GeneratedConstructorAccessor52.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1082)\n        ... 16 more\nCaused by: java.lang.IllegalStateException: This ticket is no longer valid\n        at javax.security.auth.kerberos.KerberosTicket.toString(KerberosTicket.java:601)\n        at java.lang.String.valueOf(String.java:2826)\n        at java.lang.StringBuilder.append(StringBuilder.java:115)\n        at sun.security.jgss.krb5.SubjectComber.findAux(SubjectComber.java:120)\n        at sun.security.jgss.krb5.SubjectComber.find(SubjectComber.java:41)\n        at sun.security.jgss.krb5.Krb5Util.getTicket(Krb5Util.java:130)\n        at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:328)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at sun.security.jgss.krb5.Krb5InitCredential.getTgt(Krb5InitCredential.java:325)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:128)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)\n        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)\n        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)\n        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:277)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:163)\n        ... 20 more\n\nI check the code of HiveAuthFactory.loginFromKeytab,it does not schedule a timer to renew the TGT. So I suspect this is the reason of the kerberos auth failure?\n\nThanks.\n\n"
        }
    },
    {
        "filename": "HIVE-14303.json",
        "creation_time": "2016-07-21T03:16:20.000+0000",
        "bug_report": {
            "Title": "CommonJoinOperator.checkAndGenObject should return directly to avoid NPE if ExecReducer.close is called twice.",
            "Description": "CommonJoinOperator.checkAndGenObject should return directly (after {{CommonJoinOperator.closeOp}} was called ) to avoid NPE if ExecReducer.close is called twice. ExecReducer.close implements Closeable interface and ExecReducer.close can be called multiple time. We saw the following NPE which hide the real exception due to this bug.\r\n{code:java}\r\nError: java.lang.RuntimeException: Hive Runtime Error while closing operators: null\r\n\r\n        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)\r\n\r\n        at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\r\n\r\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\r\n\r\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\r\n\r\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\r\n\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n\r\n        at javax.security.auth.Subject.doAs(Subject.java:415)\r\n\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\r\n\r\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\r\n\r\nCaused by: java.lang.NullPointerException\r\n\r\n        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)\r\n\r\n        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)\r\n\r\n        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)\r\n\r\n        ... 8 more\r\n{code}\r\nThe code from ReduceTask.runOldReducer:\r\n{code:java}\r\n      reducer.close(); //line 453\r\n      reducer = null;\r\n      \r\n      out.close(reporter);\r\n      out = null;\r\n    } finally {\r\n      IOUtils.cleanup(LOG, reducer);// line 459\r\n      closeQuietly(out, reporter);\r\n    }\r\n{code}\r\nBased on the above stack trace and code, reducer.close() is called twice because the exception happened when reducer.close() is called for the first time at line 453, the code exit before reducer was set to null. NullPointerException is triggered when reducer.close() is called for the second time in IOUtils.cleanup at line 459. NullPointerException hide the real exception which happened when reducer.close() is called for the first time at line 453.\r\n The reason for NPE is:\r\n The first reducer.close called CommonJoinOperator.closeOp which clear {{storage}}\r\n{code:java}\r\nArrays.fill(storage, null);\r\n{code}\r\nthe second reduce.close generated NPE due to null {{storage[alias]}} which is set to null by first reducer.close.\r\n The following reducer log can give more proof:\r\n{code:java}\r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.JoinOperator: 0 finished. closing... \r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.JoinOperator: 0 finished. closing... \r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.JoinOperator: SKEWJOINFOLLOWUPJOBS:0\r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.SelectOperator: 1 finished. closing... \r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.SelectOperator: 2 finished. closing... \r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.SelectOperator: 3 finished. closing... \r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: 4 finished. closing... \r\n2016-07-14 22:24:51,016 INFO [main] org.apache.hadoop.hive.ql.exec.FileSinkOperator: FS[4]: records written - 53466\r\n2016-07-14 22:25:11,555 ERROR [main] ExecReducer: Hit error while closing operators - failing tree\r\n2016-07-14 22:25:11,649 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Hive Runtime Error while closing operators: null\r\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)\r\n\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\r\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\r\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\r\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\r\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)\r\n\tat org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)\r\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)\r\n\t... 8 more\r\n{code}\r\n\u00a0"
        }
    },
    {
        "filename": "HIVE-19248.json",
        "creation_time": "2018-04-19T17:45:21.000+0000",
        "bug_report": {
            "Title": "REPL LOAD couldn't copy file from source CM path and also doesn't throw error if file copy fails.",
            "Description": "Hive replication uses Hadoop distcp to copy files from primary to replica warehouse. If the HDFS block size is different across clusters, it cause file copy failures.\r\n{code:java}\r\n2018-04-09 14:32:06,690 ERROR [main] org.apache.hadoop.tools.mapred.CopyMapper: Failure in copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0\r\njava.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0\r\n at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:299)\r\n at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:266)\r\n at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)\r\n at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\r\n at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\r\n at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\r\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\r\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\r\nCaused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0\r\n at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)\r\n at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:296)\r\n ... 10 more\r\nCaused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)\r\n at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:212)\r\n at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)\r\n at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)\r\n at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\r\n ... 11 more\r\n{code}\r\nDistcp failed as the CM path for the file doesn't point to source file system. So, it is needed to get the qualified cm root URI as part of files listed in dump.\r\n\r\nAlso,\u00a0REPL LOAD returns success even if distcp jobs failed.\r\n\r\nCopyUtils.doCopyRetry doesn't throw error if copy failed even after maximum attempts.\u00a0\r\n\r\nSo, need to perform 2 things.\r\n # If\u00a0copy of multiple files fail for some reason,\u00a0then retry with same set of files again but need to set CM path if original source file is missing or modified based on checksum. Let distcp to skip the properly copied files. FileUtil.copy will always overwrite the files.\r\n # If source path is moved to CM path, then delete the incorrectly copied files.\r\n # If copy fails for maximum attempt, then throw error.\r\n\r\n\u00a0"
        }
    },
    {
        "filename": "HIVE-7167.json",
        "creation_time": "2014-06-02T18:13:36.000+0000",
        "bug_report": {
            "Title": "Hive Metastore fails to start with SQLServerException",
            "Description": "In the case that hiveserver2 uses embedded metastore and hiveserver uses remote metastore, this exception comes up when hiveserver2 and hiveserver are started simultaneously.\n\nmetastore service status is running but when I launch hive cli, I get following metastore connection error:\n\nC:\\apps\\dist\\hive-0.13.0.2.1.2.0-1660\\bin>hive.cmd\n\n{noformat}\n14/05/09 17:40:03 WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no l\nonger has any effect.  Use hive.hmshandler.retry.* instead\n\nLogging initialized using configuration in file:/C:/apps/dist/hive-0.13.0.2.1.2.\n0-1660/conf/hive-log4j.properties\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeExceptio\nn: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.jav\na:347)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.\njava:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.h\nive.metastore.HiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStore\nUtils.java:1413)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(Retry\ningMetaStoreClient.java:62)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(Ret\nryingMetaStoreClient.java:72)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.ja\nva:2444)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2456)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.jav\na:341)\n        ... 7 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstruct\norAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingC\nonstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStore\nUtils.java:1411)\n        ... 12 more\nCaused by: MetaException(message:Could not connect to meta store using any of th\ne URIs provided. Most recent failure: org.apache.thrift.transport.TTransportExce\nption: java.net.ConnectException: Connection refused: connect\n        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaSto\nreClient.java:336)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaS\ntoreClient.java:214)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstruct\norAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingC\nonstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStore\nUtils.java:1411)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(Retry\ningMetaStoreClient.java:62)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(Ret\nryingMetaStoreClient.java:72)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.ja\nva:2444)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2456)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.jav\na:341)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.\njava:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.net.ConnectException: Connection refused: connect\n        at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method)\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.ja\nva:339)\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocket\nImpl.java:200)\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java\n:182)\n        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:157)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)\n        at java.net.Socket.connect(Socket.java:579)\n        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)\n        ... 19 more\n)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaSto\nreClient.java:382)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaS\ntoreClient.java:214)\n        ... 17 more\n{noformat}\n\nBecause of this issue, we are seeing many failures in hive related system tests."
        }
    },
    {
        "filename": "HIVE-12360.json",
        "creation_time": "2015-11-06T18:04:00.000+0000",
        "bug_report": {
            "Title": "Bad seek in uncompressed ORC with predicate pushdown",
            "Description": "Reading from an ORC file bombs in HDP-2.3.2 when pushing down predicate:\n\n{noformat:title=Error message in CLI}\nFailed with exception java.io.IOException:java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data\n{noformat}\n\n{noformat:title=Stack trace in log4j file}\n2015-11-06 09:48:11,873 ERROR [main]: CliDriver (SessionState.java:printError(960)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data\njava.io.IOException: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)\n\tat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)\n\tat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.seek(InStream.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.read(InStream.java:87)\n\tat java.io.InputStream.read(InputStream.java:102)\n\tat com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)\n\tat com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)\n\tat com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7429)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7393)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7482)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7477)\n\tat com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200)\n\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)\n\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:223)\n\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.parseFrom(OrcProto.java:7593)\n\tat org.apache.hadoop.hive.ql.io.orc.MetadataReader.readRowIndex(MetadataReader.java:88)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1166)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1151)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:750)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1235)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1117)\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:324)\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:446)\n\t... 15 more\n{noformat}\n\nThis is very similar to HIVE-9471, except that:\n# HDP2.3.2 says it already incorporates HIVE-9471, HIVE-10303;\n# the test in HIVE-9471 does not fail in my HDP2.3.2 setup.\n\nJust as in HIVE-9471, the ORC file is not compressed and the failure only occurs with hive.optimize.index.filter=true.\n\n"
        }
    },
    {
        "filename": "HIVE-13160.json",
        "creation_time": "2016-02-26T00:02:11.000+0000",
        "bug_report": {
            "Title": "HS2 unable to load UDFs on startup when HMS is not ready",
            "Description": "The error looks like this:\n\n{code}\n2016-02-18 14:43:54,251 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083\n2016-02-18 14:48:54,692 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...\n2016-02-18 14:48:54,692 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.\n2016-02-18 14:48:55,692 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083\n2016-02-18 14:53:55,800 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...\n2016-02-18 14:53:55,800 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.\n2016-02-18 14:53:56,801 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083\n2016-02-18 14:58:56,967 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...\n2016-02-18 14:58:56,967 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.\n2016-02-18 14:58:57,994 WARN  hive.ql.metadata.Hive: [main]: Failed to register all functions.\njava.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)\n.......\n016-02-18 14:58:57,997 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083\n2016-02-18 15:03:58,094 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...\n2016-02-18 15:03:58,095 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.\n2016-02-18 15:03:59,095 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083\n2016-02-18 15:08:59,203 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...\n2016-02-18 15:08:59,203 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.\n2016-02-18 15:09:00,203 INFO  hive.metastore: [main]: Trying to connect to metastore with URI thrift://host-10-17-81-201.coe.cloudera.com:9083\n2016-02-18 15:14:00,304 WARN  hive.metastore: [main]: Failed to connect to the MetaStore Server...\n2016-02-18 15:14:00,304 INFO  hive.metastore: [main]: Waiting 1 seconds before next connection attempt.\n2016-02-18 15:14:01,306 INFO  org.apache.hive.service.server.HiveServer2: [main]: Shutting down HiveServer2\n2016-02-18 15:14:01,308 INFO  org.apache.hive.service.server.HiveServer2: [main]: Exception caught when calling stop of HiveServer2 before retrying start\njava.lang.NullPointerException\n        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)\n        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)\n        at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:69)\n        at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:545)\n{code}\n\nAnd then none of the functions will be available for use as HS2 does not re-register them after HMS is up and ready.\n\nThis is not desired behaviour, we shouldn't allow HS2 to be in a servicing state if function list is not ready. Or, maybe instead of initialize the function list when HS2 starts, try to load the function list when each Hive session is created. Of course we can have a cache of function list somewhere for better performance, but we would better decouple it from class Hive."
        }
    },
    {
        "filename": "HIVE-12008.json",
        "creation_time": "2015-10-01T19:26:46.000+0000",
        "bug_report": {
            "Title": "Hive queries failing when using count(*) on column in view",
            "Description": "count(*) on view with get_json_object() UDF and lateral views and unions fails in the master with error:\n\n2015-10-27 17:51:33,742 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Error in configuring object\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n        ... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)\n        ... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n        ... 17 more\nCaused by: java.lang.RuntimeException: Map operator initialization failed\n        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:147)\n        ... 22 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635)\n        at java.util.ArrayList.get(ArrayList.java:411)\nThis query works fine in 1.1 version. \nThe last two qfile unit tests added by HIVE-11384 fail when hive.in.test is false. It may relate how we handle prunelist for select. When select include every column in a table, the prunelist for the select is empty. It may cause issues to calculate its parent's prunelist.. "
        }
    },
    {
        "filename": "HIVE-6205.json",
        "creation_time": "2014-01-15T07:34:15.000+0000",
        "bug_report": {
            "Title": "alter <table> partition column throws NPE in authorization",
            "Description": "alter table alter_coltype partition column (dt int);\n{noformat}\n2014-01-15 15:53:40,364 ERROR ql.Driver (SessionState.java:printError(457)) - FAILED: NullPointerException null\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:197)\n{noformat}\n\nOperation for TOK_ALTERTABLE_ALTERPARTS is not defined."
        }
    },
    {
        "filename": "HIVE-15309.json",
        "creation_time": "2016-11-29T21:56:28.000+0000",
        "bug_report": {
            "Title": "RemoteException(java.io.FileNotFoundException): File does not exist... _flush_length",
            "Description": "OrcAcidUtils.getLastFlushLength() should check for file existence first.  Currently causes unnecessary/confusing logging:\n{noformat}\norg.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/r\\\nrslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1831)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1744)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolSe\\\nrverSideTranslatorPB.java:373)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientName\\\nnodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)\n\n        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1496)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1396)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n        at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB\\\n.java:270)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)\n        at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1211)\n        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)\n        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)\n        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)\n        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1536)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:330)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:326)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:782)\n        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)\n        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:460)\nat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1525)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:631)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:610)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n{noformat}\n\nAlso,\n{noformat}\n2016-08-02 01:05:01,107 INFO  [org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService-0]: txn.TxnHandler (TxnHandler.java:timeOutLocks(2836)) - Deleted 9 ext locks from HIVE_LOCKS due to timeout (vs. 1 found. List: [738]) maxHeartbeatTime=1470099601000\n{noformat}\n\nNote that the msg says \"Deleted 9 ext locks...\"  It actually delete 1 ext which has 9 internal components.  Need to follow up on this.\n\nAlso,\nTxnHandler has\n{noformat}\n        LOG.info(quoteString(key) + \" locked by \" + quoteString(TxnHandler.hostname));\n{noformat}\nand a corresponding \"unlock\" msg which flood the metastore log.\n"
        }
    },
    {
        "filename": "HIVE-10808.json",
        "creation_time": "2015-05-23T02:24:16.000+0000",
        "bug_report": {
            "Title": "Inner join on Null throwing Cast Exception",
            "Description": "select\n    > a.col1,\n    > a.col2,\n    > a.col3,\n    > a.col4\n    > from\n    > tab1 a\n    > inner join\n    > (\n    > select\n    > max(x) as x\n    > from\n    > tab1\n    > where\n    > x < 20130327\n    > ) r\n    > on\n    > a.x = r.x\n    > where\n    > a.col1 = 'F'\n    > and a.col3 in ('A', 'S', 'G');\n\nFailed Task log snippet:\n\n2015-05-18 19:22:17,372 INFO [main] org.apache.hadoop.hive.ql.exec.mr.ObjectCache: Ignoring retrieval request: __MAP_PLAN__\n2015-05-18 19:22:17,372 INFO [main] org.apache.hadoop.hive.ql.exec.mr.ObjectCache: Ignoring cache key: __MAP_PLAN__\n2015-05-18 19:22:17,457 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Error in configuring object\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\nat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\nat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: java.lang.reflect.InvocationTargetException\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\nat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\nat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\nat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)\n... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n... 17 more\nCaused by: java.lang.RuntimeException: Map operator initialization failed\nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:157)\n... 22 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector\nat org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:334)\nat org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:352)\nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)\n... 22 more\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspectorUtils.java:1111)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1149)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:219)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:183)\nat org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:316)\n"
        }
    },
    {
        "filename": "HIVE-18429.json",
        "creation_time": "2018-01-10T20:45:15.000+0000",
        "bug_report": {
            "Title": "Compaction should handle a case when it produces no output",
            "Description": "Suppose we start with empty delta_8_8 and delta_9_9 and compaction runs.\r\nIt will currently produce an MR job with 0 splits and so {{CompactorMR.TMP_LOCATION}} never gets created.  This causes {{CompactorOutputCommitted.commitJob()}} to fail when it tries to do \r\n {{FileStatus[] contents = fs.listStatus(tmpLocation);}} since tmpLocation doesn't exist.\r\n\r\nIf compactor fails to produce delta_8_9 here it will fail to do further compaction unless new delta with data is created.  \r\n\r\nIf the number of empty deltas is > than HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA, compaction will not be able to proceed at all.\r\n\r\nIt should produce a delta_8_9 in this case even if it's empty.\r\n\r\nThe error (in the log of standalone metastore process) would look like this\r\n{noformat}\r\n2017-12-27 17:19:28,850 ERROR CommitterEvent Processor #1 org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job\r\njava.io.FileNotFoundException: File hdfs://OTCHaaS/apps/hive/warehouse/momi.db/sensor_data/babyid=5911806ebf69640100004257/_tmp_b4c5a3f3-44e5-4d45-86af-5b773bf0fc96 does not exist.\r\nat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:923)\r\nat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:114)\r\nat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:985)\r\nat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:981)\r\nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\nat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:992)\r\nat rg.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:785)\r\nat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\nat  org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)\r\nat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\nat java.lang.Thread.run(Thread.java:745)\r\n{noformat}\r\n\r\n\r\n\r\n"
        }
    },
    {
        "filename": "HIVE-10776.json",
        "creation_time": "2015-05-21T00:56:28.000+0000",
        "bug_report": {
            "Title": "Schema on insert for bucketed tables throwing NullPointerException",
            "Description": "Hive schema on insert queries, with select * , are failing with below exception\n\n{noformat}\n2015-05-15 19:29:01,278 ERROR [main]: ql.Driver (SessionState.java:printError(957)) - FAILED: NullPointerException null\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:7257)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6100)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6271)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8972)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8863)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9708)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9601)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10037)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:323)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n{noformat}\nSteps to reproduce\n{noformat}\nset hive.support.concurrency=true;\nset hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\nset hive.enforce.bucketing=true;\n\ndrop table if exists studenttab10k;\ncreate table studenttab10k (age int, name varchar(50),gpa decimal(3,2));\ninsert into studenttab10k values(1,'foo', 1.1), (2,'bar', 2.3),(3,'baz', 3.1);\n\n\ndrop table if exists student_acid;\ncreate table student_acid (age int, name varchar(50),gpa decimal(3,2), grade int) \nclustered by (age) into 2 buckets\nstored as orc\ntblproperties ('transactional'='true');\n\ninsert into student_acid(name,age,gpa) select * from studenttab10k;\n{noformat}"
        }
    },
    {
        "filename": "HIVE-6301.json",
        "creation_time": "2014-01-24T01:42:18.000+0000",
        "bug_report": {
            "Title": "get_json_object throw java.lang.IllegalStateException: No match found exception.",
            "Description": "In fact, you can find the bug in code.\n\nhttps://github.com/apache/hive/blob/branch-0.10/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFJson.java#L190\n\nMiss call _mKey.matches()_ before use _mKey.group(1)_\n\nThe bug still exists in the newest version.\n\nSo, we met such exception in some query:\n\n\n{quote}\n\n2014-01-23 11:08:19,869 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String)  on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2\nCaused by: java.lang.IllegalStateException: No match found\n\tat java.util.regex.Matcher.group(Matcher.java:468)\n\tat org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)\n\tat org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)\n\t... 24 more\n\n{quote}"
        }
    },
    {
        "filename": "HIVE-8295.json",
        "creation_time": "2014-09-29T21:16:32.000+0000",
        "bug_report": {
            "Title": "Add batch retrieve partition objects for metastore direct sql ",
            "Description": "Currently in MetastoreDirectSql partition objects are constructed in a way that fetching partition ids first. However, if the partition ids that match the filter is larger than 1000, direct sql will fail with the following stack trace:\n\n{code}\n2014-09-29 19:30:02,942 DEBUG [pool-1-thread-1] metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(604)) - Direct SQL query in 122.085893ms + 13.048901ms, the query is [select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\"  inner join \"TBLS\" on \"PARTITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\"     and \"TBLS\".\"TBL_NAME\" = ?   inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\"      and \"DBS\".\"NAME\" = ? inner join \"PARTITION_KEY_VALS\" \"FILTER2\" on \"FILTER2\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER2\".\"INTEGER_IDX\" = 2 where ((\"FILTER2\".\"PART_KEY_VAL\" = ?))]\n2014-09-29 19:30:02,949 ERROR [pool-1-thread-1] metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2248)) - Direct SQL failed, falling back to ORM\njavax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\", \"SDS\".\"SD_ID\", \"SDS\".\"CD_ID\", \"SERDES\".\"SERDE_ID\", \"PARTITIONS\".\"CREATE_TIME\", \"PARTITIONS\".\"LAST_ACCESS_TIME\", \"SDS\".\"INPUT_FORMAT\", \"SDS\".\"IS_COMPRESSED\", \"SDS\".\"IS_STOREDASSUBDIRECTORIES\", \"SDS\".\"LOCATION\", \"SDS\".\"NUM_BUCKETS\", \"SDS\".\"OUTPUT_FORMAT\", \"SERDES\".\"NAME\", \"SERDES\".\"SLIB\" from \"PARTITIONS\"  left outer join \"SDS\" on \"PARTITIONS\".\"SD_ID\" = \"SDS\".\"SD_ID\"   left outer join \"SERDES\" on \"SDS\".\"SERDE_ID\" = \"SERDES\".\"SERDE_ID\" where \"PART_ID\" in (136,140,143,147,152,156,160,163,167,171,174,180,185,191,196,198,203,208,212,217...\n) order by \"PART_NAME\" asc\".\n    at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:422)\n    at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)\n    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:331)\n    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)\n    at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1920)\n    at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1914)\n    at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2213)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1914)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1887)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)\n    at com.sun.proxy.$Proxy8.getPartitionsByExpr(Unknown Source)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3800)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9366)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9350)\n    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)\n    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)\n    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:206)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nNestedThrowablesStackTrace:\njava.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000\n{code}\n\nAdd retrieve partition objects in batch for direct sql will solve this Oracle specific problem. And it also bring some performance benefit and will reduce the memory footprint. "
        }
    },
    {
        "filename": "HIVE-8915.json",
        "creation_time": "2014-11-19T19:40:17.000+0000",
        "bug_report": {
            "Title": "Log file explosion due to non-existence of COMPACTION_QUEUE table",
            "Description": "I hit an issue with a fresh set up of hive in a vm, where I did not have db tables as specified by hive-txn-schema-0.14.0.mysql.sql created.\n\nOn metastore startup, I got an endless loop of errors being populated to the log file, which caused the log file to grow to 1.7GB in 5 minutes, with 950k copies of the same error stack trace in it before I realized what was happening and killed it. We should either have a delay of sorts to make sure we don't endlessly respin on that error so quickly, or we should error out and fail if we're not able to start.\n\nThe stack trace in question is as follows:\n\n{noformat}\n2014-11-19 01:44:57,654 ERROR compactor.Cleaner\n(Cleaner.java:run(143)) - Caught an exception in the main loop of\ncompactor cleaner, MetaException(message:Unable to connect to\ntransaction database\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table\n'hive.COMPACTION_QUEUE' doesn't exist\nat sun.reflect.GeneratedConstructorAccessor20.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\nat com.mysql.jdbc.Util.getInstance(Util.java:386)\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2569)\nat com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1524)\nat com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)\nat org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:266)\nat org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)\n)\nat org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:291)\nat org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-7249.json",
        "creation_time": "2014-06-18T00:09:42.000+0000",
        "bug_report": {
            "Title": "HiveTxnManager.closeTxnManger() throws if called after commitTxn()",
            "Description": " I openTxn() and acquireLocks() for a query that looks like \"INSERT INTO T PARTITION(p) SELECT * FROM T\".\nThen I call commitTxn().  Then I call closeTxnManger() I get an exception saying lock not found (the only lock in this txn).  So it seems TxnMgr doesn't know that commit released the locks.\n\nHere is the stack trace and some log output which maybe useful:\n{noformat}\n2014-06-17 15:54:40,771 DEBUG mapreduce.TransactionContext (TransactionContext.java:onCommitJob(128)) - onCommitJob(job_local557130041_0001). this=46719652\n2014-06-17 15:54:40,771 DEBUG lockmgr.DbTxnManager (DbTxnManager.java:commitTxn(205)) - Committing txn 1\n2014-06-17 15:54:40,771 DEBUG txn.TxnHandler (TxnHandler.java:getDbTime(872)) - Going to execute query <values current_timestamp>\n2014-06-17 15:54:40,772 DEBUG txn.TxnHandler (TxnHandler.java:heartbeatTxn(1423)) - Going to execute query <select txn_state from TXNS where txn_id = 1 for\\\n update>\n2014-06-17 15:54:40,773 DEBUG txn.TxnHandler (TxnHandler.java:heartbeatTxn(1438)) - Going to execute update <update TXNS set txn_last_heartbeat = 140304568\\\n0772 where txn_id = 1>\n2014-06-17 15:54:40,778 DEBUG txn.TxnHandler (TxnHandler.java:heartbeatTxn(1440)) - Going to commit\n2014-06-17 15:54:40,779 DEBUG txn.TxnHandler (TxnHandler.java:commitTxn(344)) - Going to execute insert <insert into COMPLETED_TXN_COMPONENTS select tc_txn\\\nid, tc_database, tc_table, tc_partition from TXN_COMPONENTS where tc_txnid = 1>\n2014-06-17 15:54:40,784 DEBUG txn.TxnHandler (TxnHandler.java:commitTxn(352)) - Going to execute update <delete from TXN_COMPONENTS where tc_txnid = 1>\n2014-06-17 15:54:40,788 DEBUG txn.TxnHandler (TxnHandler.java:commitTxn(356)) - Going to execute update <delete from HIVE_LOCKS where hl_txnid = 1>\n2014-06-17 15:54:40,791 DEBUG txn.TxnHandler (TxnHandler.java:commitTxn(359)) - Going to execute update <delete from TXNS where txn_id = 1>\n2014-06-17 15:54:40,794 DEBUG txn.TxnHandler (TxnHandler.java:commitTxn(361)) - Going to commit\n2014-06-17 15:54:40,795 WARN  mapreduce.TransactionContext (TransactionContext.java:cleanup(317)) - cleanupJob(JobID=job_local557130041_0001)this=46719652\n2014-06-17 15:54:40,795 DEBUG lockmgr.DbLockManager (DbLockManager.java:unlock(109)) - Unlocking id:1\n2014-06-17 15:54:40,796 DEBUG txn.TxnHandler (TxnHandler.java:getDbTime(872)) - Going to execute query <values current_timestamp>\n2014-06-17 15:54:40,796 DEBUG txn.TxnHandler (TxnHandler.java:heartbeatLock(1402)) - Going to execute update <update HIVE_LOCKS set hl_last_heartbeat = 140\\\n3045680796 where hl_lock_ext_id = 1>\n2014-06-17 15:54:40,800 DEBUG txn.TxnHandler (TxnHandler.java:heartbeatLock(1405)) - Going to rollback\n2014-06-17 15:54:40,804 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - NoSuchLockException(message:No such lock: 1)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(TxnHandler.java:1407)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(TxnHandler.java:477)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.unlock(HiveMetaStore.java:4817)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)\n        at com.sun.proxy.$Proxy14.unlock(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.unlock(HiveMetaStoreClient.java:1598)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(DbLockManager.java:110)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close(DbLockManager.java:162)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct(DbTxnManager.java:300)\n        at org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager(HiveTxnManagerImpl.java:39)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.closeTxnManager(DbTxnManager.java:43)\n        at org.apache.hive.hcatalog.mapreduce.TransactionContext.cleanup(TransactionContext.java:327)\n        at org.apache.hive.hcatalog.mapreduce.TransactionContext.onCommitJob(TransactionContext.java:142)\n        at org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.commitJob(OutputCommitterContainer.java:61)\n        at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(FileOutputCommitterContainer.java:251)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:537)\n\n2014-06-17 15:54:40,804 ERROR lockmgr.DbLockManager (DbLockManager.java:unlock(114)) - Metastore could find no record of lock 1\n2014-06-17 15:54:40,810 INFO  mapreduce.FileOutputCommitterContainer (FileOutputCommitterContainer.java:cancelDelegationTokens(976)) - Cancelling delegation token for the job.\n{noformat}"
        }
    },
    {
        "filename": "HIVE-11540.json",
        "creation_time": "2015-08-12T23:12:18.000+0000",
        "bug_report": {
            "Title": "Too many delta files during Compaction - OOM",
            "Description": "Hello,\n\nI am streaming weblogs to Kafka and then to Flume 1.6 using a Hive sink, with an average of 20 million records a day. I have 5 compactors running at various times (30m/5m/5s), no matter what time I give, the compactors seem to run out of memory cleaning up a couple thousand delta files and ultimately falls behind compacting/cleaning delta files. Any suggestions on what I can do to improve performance? Or can Hive streaming not handle this kind of load?\n\nI used this post as reference: http://henning.kropponline.de/2015/05/19/hivesink-for-flume/\n\n{noformat}\n2015-08-12 15:05:01,197 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Direct buffer memory\n\nMax block location exceeded for split: CompactorInputSplit{base: hdfs://Dev01HWNameService/user/hive/warehouse/weblogs.db/dt=15-08-12/base_1056406, bucket: 0, length: 6493042, deltas: [delta_1056407_1056408, delta_1056409_1056410, delta_1056411_1056412, delta_1056413_1056414, delta_1056415_1056416, delta_1056417_1056418,\u2026\n, delta_1074039_1074040, delta_1074041_1074042, delta_1074043_1074044, delta_1074045_1074046, delta_1074047_1074048, delta_1074049_1074050, delta_1074051_1074052]} splitsize: 8772 maxsize: 10\n2015-08-12 15:34:25,271 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:3\n2015-08-12 15:34:25,367 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.JobSubmitter (JobSubmitter.java:printTokens(287)) - Submitting tokens for job: job_1439397150426_0068\n2015-08-12 15:34:25,603 INFO  [upladevhwd04v.researchnow.com-18]: impl.YarnClientImpl (YarnClientImpl.java:submitApplication(274)) - Submitted application application_1439397150426_0068\n2015-08-12 15:34:25,610 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:submit(1294)) - The url to track the job: http://upladevhwd02v.researchnow.com:8088/proxy/application_1439397150426_0068/\n2015-08-12 15:34:25,611 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1339)) - Running job: job_1439397150426_0068\n2015-08-12 15:34:30,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:33,756 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1360)) - Job job_1439397150426_0068 running in uber mode : false\n2015-08-12 15:34:33,757 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%\n2015-08-12 15:34:35,147 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:40,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:45,184 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:50,201 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:34:55,256 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:00,205 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:02,975 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 33% reduce 0%\n2015-08-12 15:35:02,982 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_0, Status : FAILED\n2015-08-12 15:35:03,000 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_0, Status : FAILED\n2015-08-12 15:35:04,008 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 0% reduce 0%\n2015-08-12 15:35:05,132 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:10,206 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:15,228 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:20,207 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:25,148 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:28,154 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_1, Status : FAILED\n2015-08-12 15:35:29,161 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_1, Status : FAILED\n2015-08-12 15:35:30,142 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:35,140 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:40,170 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:45,153 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:50,150 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:35:52,268 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000000_2, Status : FAILED\n2015-08-12 15:35:53,274 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:printTaskEvents(1406)) - Task Id : attempt_1439397150426_0068_m_000001_2, Status : FAILED\n2015-08-12 15:35:55,149 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:00,160 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:05,145 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:10,155 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:15,158 INFO  [Thread-7]: compactor.Initiator (Initiator.java:run(88)) - Checking to see if we should compact weblogs.vop_hs.dt=15-08-12\n2015-08-12 15:36:17,397 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1367)) -  map 100% reduce 0%\n2015-08-12 15:36:18,409 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - Job job_1439397150426_0068 failed with state FAILED due to: Task failed task_1439397150426_0068_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n\n2015-08-12 15:36:18,443 INFO  [upladevhwd04v.researchnow.com-18]: mapreduce.Job (Job.java:monitorAndPrintJob(1385)) - Counters: 10\n\tJob Counters \n\t\tFailed map tasks=7\n\t\tKilled map tasks=1\n\t\tLaunched map tasks=8\n\t\tOther local map tasks=6\n\t\tData-local map tasks=2\n\t\tTotal time spent by all maps in occupied slots (ms)=191960\n\t\tTotal time spent by all reduces in occupied slots (ms)=0\n\t\tTotal time spent by all map tasks (ms)=191960\n\t\tTotal vcore-seconds taken by all map tasks=191960\n\t\tTotal megabyte-seconds taken by all map tasks=884551680\n2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)\n\n2015-08-12 15:36:18,444 ERROR [upladevhwd04v.researchnow.com-18]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(327)) - Expected to remove at least one row from completed_txn_components when marking compaction entry as clean!\n^C\n{noformat}\n[ngmathew@upladevhwd04v ~]$ tail -f /var/log/hive/hivemetastore.log\n2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)\n\n\n\nSettings:\nhive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager\nhive.compactor.initiator.on = true\nhive.compactor.worker.threads = 5\nTable stored as ORC\nhive.vectorized.execution.enabled = false\nhive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat"
        }
    },
    {
        "filename": "HIVE-15755.json",
        "creation_time": "2017-01-30T20:48:25.000+0000",
        "bug_report": {
            "Title": "NullPointerException on invalid table name in ON clause of Merge statement",
            "Description": "Ran into this error message - \"Error while compiling statement: FAILED: NullPointerException null \" when I specified an incorrect tablename in the merge statement.\n \n{code:java}\n> create table src (col1 int,col2 int);\nNo rows affected (0.231 seconds)\n> create table trgt (tcol1 int,tcol2 int);\nNo rows affected (0.182 seconds)\n> insert into src values (1,232);\n{code}\n\n{code:java}\n> merge into trgt using (select * from src) sub on sub.col1 = *invalidtablename.tcol1* when not matched then insert values (sub.col1,sub.col2);\nError: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)\n\n> merge into trgt using (select * from src) sub on sub.col1 = *trgt.tcol1* when not matched then insert values (sub.col1,sub.col2);\n\nINFO  : Session is already open\nINFO  : Dag name: merge into trgt using ...(sub.col1,sub.col2)(Stage-1)\nINFO  : Setting tez.task.scale.memory.reserve-fraction to 0.30000001192092896\nINFO  : \n\nINFO  : Status: Running (Executing on YARN cluster with App id application_1485398058799_0129)\n\nINFO  : Map 1: 0/1\tMap 2: -/-\t\nINFO  : Map 1: 0(+1)/1\tMap 2: -/-\t\nINFO  : Map 1: 0(+1)/1\tMap 2: -/-\t\nINFO  : Map 1: 1/1\tMap 2: -/-\t\nINFO  : Loading data to table tpch.trgt from hdfs://tesths2-merge-ks-5.openstacklocal:8020/apps/hive/warehouse/tpch.db/trgt/.hive-staging_hive_2017-01-30_06-54-50_743_6276941178188398287-1/-ext-10000\nINFO  : Table tpch.trgt stats: [numFiles=1, numRows=1, totalSize=4, rawDataSize=3]\nNo rows affected (7.709 seconds)\n{code}\nHiveserver2 logs:\n{code:java}\n2017-01-30 19:34:09,972 INFO  [HiveServer2-Handler-Pool: Thread-70]: parse.ParseDriver (ParseDriver.java:parse(185)) - Parsing command: merge into trgt using (select * from src) sub on sub.col1 = target.tcol1 when not matched then insert values (sub.col1,sub.col2)\n2017-01-30 19:34:09,975 INFO  [HiveServer2-Handler-Pool: Thread-70]: parse.ParseDriver (ParseDriver.java:parse(209)) - Parse Completed\n2017-01-30 19:34:09,976 INFO  [HiveServer2-Handler-Pool: Thread-70]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=parse start=1485804849971 end=1485804849976 duration=5 from=org.apache.hadoop.hive.ql.Driver>\n2017-01-30 19:34:09,976 INFO  [HiveServer2-Handler-Pool: Thread-70]: log.PerfLogger (PerfLogger.java:PerfLogBegin(149)) - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>\n2017-01-30 19:34:09,977 INFO  [HiveServer2-Handler-Pool: Thread-70]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(824)) - 13: get_table : db=tpch tbl=trgt\n2017-01-30 19:34:09,977 INFO  [HiveServer2-Handler-Pool: Thread-70]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(393)) - ugi=hive     ip=unknown-ip-addr      cmd=get_table : db=tpch tbl=trgt\n2017-01-30 19:34:10,031 ERROR [HiveServer2-Handler-Pool: Thread-70]: ql.Driver (SessionState.java:printError(980)) - FAILED: NullPointerException null\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)\n        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)\n        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)\n        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)\n        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)\n        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:298)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:506)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n\n"
        }
    },
    {
        "filename": "HIVE-9390.json",
        "creation_time": "2015-01-15T18:50:32.000+0000",
        "bug_report": {
            "Title": "Enhance retry logic wrt DB access in TxnHandler",
            "Description": "example of error\n{noformat}\n2015-01-13 16:09:21,148 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(141)) - org.apache.thrift.TException: MetaException(message:Unable to get jdbc connection from pool, Read timed out)\n\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5324)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)\n\n\tat com.sun.proxy.$Proxy11.get_open_txns(Unknown Source)\n\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidTxns(HiveMetaStoreClient.java:1696)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)\n\n\tat com.sun.proxy.$Proxy12.getValidTxns(Unknown Source)\n\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidTxns(DbTxnManager.java:289)\n\n\tat org.apache.hadoop.hive.ql.Driver.recordValidTxns(Driver.java:882)\n\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:399)\n\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)\n\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)\n\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)\n\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)\n\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)\n\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)\n\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)\n\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)\n\n\tat sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)\n\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)\n\n\tat java.security.AccessController.doPrivileged(Native Method)\n\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)\n\n\tat com.sun.proxy.$Proxy21.executeStatementAsync(Unknown Source)\n\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)\n\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:401)\n\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\n\tat org.apache.thrift.server.TServlet.doPost(TServlet.java:83)\n\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:101)\n\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\n\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)\n\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)\n\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)\n\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)\n\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)\n\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)\n\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)\n\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)\n\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)\n\n\tat org.eclipse.jetty.server.Server.handle(Server.java:349)\n\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)\n\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)\n\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857)\n\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)\n\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)\n\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)\n\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\n\tat java.lang.Thread.run(Thread.java:745)\n\nCaused by: MetaException(message:Unable to get jdbc connection from pool, Read timed out)\n\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:850)\n\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:196)\n\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5322)\n\n\t... 66 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-7623.json",
        "creation_time": "2014-08-05T23:58:27.000+0000",
        "bug_report": {
            "Title": "hive partition rename fails if filesystem cache is disabled",
            "Description": "Seems to be similar issue https://issues.apache.org/jira/browse/HIVE-3815 when calling alterPartition (when renaming partitions)\n\nSetting fs.hdfs.impl.disable.cache=false  and  fs.file.impl.disable.cache=falseworks around this problem\n\n\n\nError:\n=====\n2014-08-05 21:46:14,522 ERROR [pool-3-thread-1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - InvalidOperationException(message:table new location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=123 is on a different file system than the old location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=456. This operation is not supported)\nat org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(HiveAlterHandler.java:361)\nat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2629)\nat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2602)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:622)\nat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)\nat com.sun.proxy.$Proxy5.rename_partition(Unknown Source)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9057)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9041)\nat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:416)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n\n\n\nLooking at the code apache-hive-0.13.1-src/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java on line 361 see that its using != to compare filesystem objects \n\n// check that src and dest are on the same file system\n          if (srcFs != destFs) {\n            throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n          }"
        }
    },
    {
        "filename": "HIVE-15997.json",
        "creation_time": "2017-02-21T16:49:46.000+0000",
        "bug_report": {
            "Title": "Resource leaks when query is cancelled ",
            "Description": "There may some resource leaks when query is cancelled.\nWe see following stacks in the log:\n\nPossible files and folder leak: \n{noformat} \n2017-02-02 06:23:25,410 WARN hive.ql.Context: [HiveServer2-Background-Pool: Thread-61]: Error Removing Scratch: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: \"ychencdh511t-1.vpc.cloudera.com/172.26.11.50\"; destination host is: \"ychencdh511t-1.vpc.cloudera.com\":8020; \nat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772) \nat org.apache.hadoop.ipc.Client.call(Client.java:1476) \nat org.apache.hadoop.ipc.Client.call(Client.java:1409) \nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) \nat com.sun.proxy.$Proxy25.delete(Unknown Source) \nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:535)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \nat java.lang.reflect.Method.invoke(Method.java:606) \nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256) \nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104) \nat com.sun.proxy.$Proxy26.delete(Unknown Source) \nat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:2059) \nat org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:675) \nat org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:671) \nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) \nat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:671) \nat org.apache.hadoop.hive.ql.Context.removeScratchDir(Context.java:405) \nat org.apache.hadoop.hive.ql.Context.clear(Context.java:541) \nat org.apache.hadoop.hive.ql.Driver.releaseContext(Driver.java:2109) \nat org.apache.hadoop.hive.ql.Driver.closeInProcess(Driver.java:2150) \nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1472) \nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212) \nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207) \nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237) \nat org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88) \nat org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:415) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796) \nat org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306) \nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) \nat java.util.concurrent.FutureTask.run(FutureTask.java:262) \nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) \nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) \nat java.lang.Thread.run(Thread.java:745) \nCaused by: java.nio.channels.ClosedByInterruptException \nat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) \nat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:681) \nat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) \nat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530) \nat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494) \nat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615) \nat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:714) \nat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376) \nat org.apache.hadoop.ipc.Client.getConnection(Client.java:1525) \nat org.apache.hadoop.ipc.Client.call(Client.java:1448) \n... 35 more \n\n2017-02-02 12:26:52,706 INFO org.apache.hive.service.cli.operation.OperationManager: [HiveServer2-Background-Pool: Thread-23]: Operation is timed out,operation=OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=2af82100-94cf-4f26-abaa-c4b57c57b23c],state=CANCELED \n{format} \n\nPossible lock leak:\n\nLocks leak:\n{format}\n2017-02-02 06:21:05,054 ERROR ZooKeeperHiveLockManager: [HiveServer2-Background-Pool: Thread-61]: Failed to release ZooKeeper lock: \njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Object.wait(Object.java:503)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:871)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive(ZooKeeperHiveLockManager.java:488)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockWithRetry(ZooKeeperHiveLockManager.java:466)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlock(ZooKeeperHiveLockManager.java:454)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.releaseLocks(ZooKeeperHiveLockManager.java:236)\n\tat org.apache.hadoop.hive.ql.Driver.releaseLocksAndCommitOrRollback(Driver.java:1175)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1432)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)\n\tat org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)\n\tat org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-7009.json",
        "creation_time": "2014-05-02T20:50:24.000+0000",
        "bug_report": {
            "Title": "HIVE_USER_INSTALL_DIR could not bet set to non-HDFS filesystem",
            "Description": "In {{hive/ql/exec/tez/DagUtils.java}}, we enforce the user path get from {{HIVE_USER_INSTALL_DIR}} to be HDFS. This makes it impossible to run Hive+Tez jobs on non-HDFS filesystem, e.g. WASB. Relevant code are as follows:\n{noformat}\n  public Path getDefaultDestDir(Configuration conf) throws LoginException, IOException {\n    UserGroupInformation ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);\n    String userName = ShimLoader.getHadoopShims().getShortUserName(ugi);\n    String userPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_USER_INSTALL_DIR);\n    Path userPath = new Path(userPathStr);\n    FileSystem fs = userPath.getFileSystem(conf);\n    if (!(fs instanceof DistributedFileSystem)) {\n      throw new IOException(ErrorMsg.INVALID_HDFS_URI.format(userPathStr));\n    }\n{noformat}\n\nExceptions running jobs with defaultFs configured to WASB.\n{noformat}\n2014-05-01 00:21:39,847 ERROR exec.Task (TezTask.java:execute(192)) - Failed to execute tez graph.\njava.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri\n\tat org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)\n\tat org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:912)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-2031.json",
        "creation_time": "2011-03-08T11:38:53.000+0000",
        "bug_report": {
            "Title": "Correct the exception message for the better traceability for the scenario load into the partitioned table having 2  partitions by specifying only one partition in the load statement. ",
            "Description": " Load into the partitioned table having 2 partitions by specifying only one partition in the load statement is failing and logging the following exception message.\n\n{noformat}\n org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)\n\tat org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)\n\tat org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)\n\tat org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)\n\tat org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n{noformat}\n\nThis needs to be corrected in such a way what is the actual root cause for this."
        }
    },
    {
        "filename": "HIVE-4018.json",
        "creation_time": "2013-02-13T09:02:20.000+0000",
        "bug_report": {
            "Title": "MapJoin failing with Distributed Cache error",
            "Description": "When I'm a running a star join query after HIVE-3784, it is failing with following error:\n\n2013-02-13 08:36:04,584 ERROR org.apache.hadoop.hive.ql.exec.MapJoinOperator: Load Distributed Cache Error\n2013-02-13 08:36:04,585 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException\n\tat org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:189)\n\tat org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:203)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1421)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:614)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:266)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:416)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:260)\n"
        }
    },
    {
        "filename": "HIVE-11255.json",
        "creation_time": "2015-07-14T15:39:11.000+0000",
        "bug_report": {
            "Title": "get_table_objects_by_name() in HiveMetaStore.java needs to retrieve table objects in multiple batches ",
            "Description": "get_table_objects_by_name() function in HiveMetaStore.java right now will pass all the tables of one database to ObjectStore to retrieve the table objects, which will cause {{java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000}} in Oracle database. We should break the table list into multiple sublists similar as the drop database op.\n\n{noformat}\n2015-06-29 13:36:00,093 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: Retrying HMSHandler after 1000 ms (attempt 1 of 1) with error: javax.jdo.JDOException: Exception thrown when executing query\nat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)\nat org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)\nat org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:945)\nat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)\nat com.sun.proxy.$Proxy0.getTableObjectsByName(Unknown Source)\nat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1618)\nat sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:106)\nat com.sun.proxy.$Proxy5.get_table_objects_by_name(Unknown Source)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8172)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8156)\nat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\nat org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\nat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nNestedThrowablesStackTrace:\njava.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000\n{noformat}"
        }
    },
    {
        "filename": "HIVE-10151.json",
        "creation_time": "2015-03-31T00:17:22.000+0000",
        "bug_report": {
            "Title": "insert into A select from B is broken when both A and B are Acid tables and bucketed the same way",
            "Description": "BucketingSortingReduceSinkOptimizer makes \ninsert into AcidTable select * from otherAcidTable\nuse BucketizedHiveInputFormat which bypasses ORC merge logic on read and tries to send bucket files (rather than table dir) down to OrcInputFormat.\n(this is true only if both AcidTable and otherAcidTable are bucketed the same way).  Then ORC dies.\n\nMore specifically:\n{noformat}\ncreate table acidTbl(a int, b int) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')\ncreate table acidTblPart(a int, b int) partitioned by (p string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')\ninsert into acidTblPart partition(p=1) (a,b) values(1,2)\ninsert into acidTbl(a,b) select a,b from acidTblPart where p = 1\n{noformat}\nresults in \n{noformat}\n2015-04-29 13:57:35,807 ERROR [main]: exec.Task (SessionState.java:printError(956)) - Job Submission failed with exception 'java.lang.RuntimeException(serious problem)'\njava.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(BucketizedHiveInputFormat.java:141)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:624)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:616)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:492)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:430)\n        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.ql.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:225)\n        at org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn2(TestTxnCommands2.java:148)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\n        at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:254)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:149)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:188)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:998)\n        ... 56 more\nCaused by: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_\n        at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)\n        at org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(AcidUtils.java:172)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:655)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:620)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n\n2015-04-29 13:57:35,809 ERROR [main]: ql.Driver (SessionState.java:printError(956)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n{noformat}\n"
        }
    },
    {
        "filename": "HIVE-13546.json",
        "creation_time": "2016-04-19T07:43:42.000+0000",
        "bug_report": {
            "Title": "Patch for HIVE-12893 is broken in branch-1 ",
            "Description": "The following sql fails:\n{noformat}\nset hive.map.aggr=true;\nset mapreduce.reduce.speculative=false;\nset hive.auto.convert.join=true;\nset hive.optimize.reducededuplication = false;\nset hive.optimize.reducededuplication.min.reducer=1;\nset hive.optimize.mapjoin.mapreduce=true;\nset hive.stats.autogather=true;\n\nset mapred.reduce.parallel.copies=30;\nset mapred.job.shuffle.input.buffer.percent=0.5;\nset mapred.job.reduce.input.buffer.percent=0.2;\nset mapred.map.child.java.opts=-server -Xmx2800m -Djava.net.preferIPv4Stack=true;\nset mapred.reduce.child.java.opts=-server -Xmx3800m -Djava.net.preferIPv4Stack=true;\nset mapreduce.map.memory.mb=3072;\nset mapreduce.reduce.memory.mb=4096;\nset hive.enforce.bucketing=true;\nset hive.enforce.sorting=true;\nset hive.exec.dynamic.partition.mode=nonstrict;\nset hive.exec.max.dynamic.partitions.pernode=100000;\nset hive.exec.max.dynamic.partitions=100000;\nset hive.exec.max.created.files=1000000;\nset hive.exec.parallel=true;\nset hive.exec.reducers.max=2000;\nset hive.stats.autogather=true;\nset hive.optimize.sort.dynamic.partition=true;\n\nset mapred.job.reduce.input.buffer.percent=0.0;\nset mapreduce.input.fileinputformat.split.minsizee=240000000;\nset mapreduce.input.fileinputformat.split.minsize.per.node=240000000;\nset mapreduce.input.fileinputformat.split.minsize.per.rack=240000000;\nset hive.optimize.sort.dynamic.partition=true;\nuse tpcds_bin_partitioned_orc_4;\ninsert overwrite table store_sales partition (ss_sold_date_sk)\nselect\n        ss.ss_sold_time_sk,\n        ss.ss_item_sk,\n        ss.ss_customer_sk,\n        ss.ss_cdemo_sk,\n        ss.ss_hdemo_sk,\n        ss.ss_addr_sk,\n        ss.ss_store_sk,\n        ss.ss_promo_sk,\n        ss.ss_ticket_number,\n        ss.ss_quantity,\n        ss.ss_wholesale_cost,\n        ss.ss_list_price,\n        ss.ss_sales_price,\n        ss.ss_ext_discount_amt,\n        ss.ss_ext_sales_price,\n        ss.ss_ext_wholesale_cost,\n        ss.ss_ext_list_price,\n        ss.ss_ext_tax,\n        ss.ss_coupon_amt,\n        ss.ss_net_paid,\n        ss.ss_net_paid_inc_tax,\n        ss.ss_net_profit,\n        ss.ss_sold_date_sk\n      from tpcds_text_4.store_sales ss;\n{noformat}\n\nError log is as follows\n{noformat}\n2016-04-19 15:15:35,252 FATAL [main] ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":null},\"value\":{\"_col0\":null,\"_col1\":5588,\"_col2\":170300,\"_col3\":null,\"_col4\":756,\"_col5\":91384,\"_col6\":16,\"_col7\":null,\"_col8\":855582,\"_col9\":28,\"_col10\":null,\"_col11\":48.83,\"_col12\":null,\"_col13\":0.0,\"_col14\":null,\"_col15\":899.64,\"_col16\":null,\"_col17\":6.14,\"_col18\":0.0,\"_col19\":null,\"_col20\":null,\"_col21\":null,\"_col22\":null}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:180)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:174)\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n\tat java.util.ArrayList.rangeCheck(ArrayList.java:653)\n\tat java.util.ArrayList.get(ArrayList.java:429)\n\tat org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:151)\n\tat org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:131)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:1003)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:919)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:713)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)\n\t... 7 more\n{noformat}\n\n"
        }
    },
    {
        "filename": "HIVE-7049.json",
        "creation_time": "2014-05-12T21:46:48.000+0000",
        "bug_report": {
            "Title": "Unable to deserialize AVRO data when file schema and record schema are different and nullable",
            "Description": "It mainly happens when \n1 )file schema and record schema are not same\n2 ) Record schema is nullable  but file schema is not.\n\nThe potential code location is at class AvroDeserialize\n \n{noformat}\n if(AvroSerdeUtils.isNullableType(recordSchema)) {\n      return deserializeNullableUnion(datum, fileSchema, recordSchema, columnType);\n    }\n{noformat}\n\nIn the above code snippet, recordSchema is verified if it is nullable. But the file schema is not checked.\n\nI tested with these values:\n{noformat}\nrecordSchema= [\"null\",\"string\"]\nfielSchema= \"string\"\n{noformat}\n\nAnd i got the following exception <line numbers might not be the same due to mu debugged code version>.\n\n{noformat}\norg.apache.avro.AvroRuntimeException: Not a union: \"string\" \n        at org.apache.avro.Schema.getTypes(Schema.java:272)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)\n        at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)\n        at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)\n\n{noformat}\n"
        }
    },
    {
        "filename": "HIVE-9755.json",
        "creation_time": "2015-02-23T20:50:43.000+0000",
        "bug_report": {
            "Title": "Hive built-in \"ngram\" UDAF fails when a mapper has no matches.",
            "Description": "hive> describe ngramtest;\nOK\ncol1                \tint                 \t                    \ncol3                \tstring              \t                    \nTime taken: 0.192 seconds, Fetched: 2 row(s)\n\nSELECT explode(ngrams(sentences(lower(t.col3)), 3, 10)) as x FROM (SELECT col3  FROM ngramtest WHERE col1=0) t;\n\nwhen any result has value equal null, returned the error. \n\n2015-01-08 09:15:00,262 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":[\"0\",\"0\",\"0\",\"0\"]},\"alias\":0} \nat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:258) \nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:506) \nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:447) \nat org.apache.hadoop.mapred.Child$4.run(Child.java:268) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:396) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408) \nat org.apache.hadoop.mapred.Child.main(Child.java:262) \nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: GenericUDAFnGramEvaluator: mismatch in value for 'n', which usually is caused by a non-constant expression. Found '0' and '1'. \nat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams$GenericUDAFnGramEvaluator.merge(GenericUDAFnGrams.java:242) \nat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:142) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:658) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:911) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:753) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:819) \nat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:474) \nat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:249) \n\n"
        }
    },
    {
        "filename": "HIVE-19130.json",
        "creation_time": "2018-04-09T10:18:33.000+0000",
        "bug_report": {
            "Title": "NPE is thrown when REPL LOAD applied drop partition event.",
            "Description": "During incremental replication, if we split the events batch as follows, then the REPL LOAD on second batch throws NPE.\r\n\r\nBatch-1: CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION (t1.p1)\r\n\r\nBatch-2: DROP_TABLE(t1) ->\u00a0 CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION (t1.p1)\r\n\r\n\r\n\r\n{code}\r\n2018-04-05 16:20:36,531 ERROR [HiveServer2-Background-Pool: Thread-107044]: metadata.Hive (Hive.java:getTable(1219)) - Table catalog_sales_new not found: new5_tpcds_real_bin_partitioned_orc_1000.catalog_sales_new table not found\r\n2018-04-05 16:20:36,538 ERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4016)\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3983)\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:341)\r\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:162)\r\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\r\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1765)\r\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1506)\r\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1303)\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1165)\r\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)\r\n        at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)\r\n        at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\r\n        at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:266)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\r\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.NullPointerException\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)\r\n        ... 23 more\r\n{code}"
        }
    },
    {
        "filename": "HIVE-13090.json",
        "creation_time": "2016-02-18T21:58:48.000+0000",
        "bug_report": {
            "Title": "Hive metastore crashes on NPE with ZooKeeperTokenStore",
            "Description": "Observed that hive metastore shutdown with NPE from ZookeeperTokenStore.\n\n{code}\nINFO  [pool-5-thread-192]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(714)) - 191: Metastore shutdown complete.\n INFO  [pool-5-thread-192]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(340)) - ugi=cvdpqap\tip=/19.1.2.129\tcmd=Metastore shutdown complete.\t\n ERROR [Thread[Thread-6,5,main]]: thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(331)) - ExpiredTokenRemover thread received unexpected exception. org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token\norg.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token\n\tat org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)\n\tat org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)\n\tat org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.NullPointerException\n\tat java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)\n\tat org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)\n\tat org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)\n\t... 3 more\n INFO  [Thread-3]: metastore.HiveMetaStore (HiveMetaStore.java:run(5639)) - Shutting down hive metastore.\n{code}"
        }
    },
    {
        "filename": "HIVE-5664.json",
        "creation_time": "2013-10-28T03:50:29.000+0000",
        "bug_report": {
            "Title": "Drop cascade database fails when the db has any tables with indexes",
            "Description": "\n{code}\nCREATE DATABASE db2; \nUSE db2; \nCREATE TABLE tab1 (id int, name string); \nCREATE INDEX idx1 ON TABLE tab1(id) as 'COMPACT' with DEFERRED REBUILD IN TABLE tab1_indx; \nDROP DATABASE db2 CASCADE;\n{code}\n\nLast DDL fails with the following error:\n{code}\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database does not exist: db2\n\nHive.log has following exception\n2013-10-27 20:46:16,629 ERROR exec.DDLTask (DDLTask.java:execute(434)) - org.apache.hadoop.hive.ql.metadata.HiveException: Database does not exist: db2\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3473)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1441)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1219)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1047)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:915)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: NoSuchObjectException(message:db2.tab1_indx table not found)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1376)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)\n        at com.sun.proxy.$Proxy7.get_table(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:890)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:660)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:652)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:546)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)\n        at com.sun.proxy.$Proxy8.dropDatabase(Unknown Source)\n        at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:284)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3470)\n        ... 18 more\n\n{code}"
        }
    },
    {
        "filename": "HIVE-15778.json",
        "creation_time": "2017-02-01T04:20:12.000+0000",
        "bug_report": {
            "Title": "DROP INDEX (non-existent) throws NPE when using DbNotificationListener ",
            "Description": "Trying to execute a DROP INDEX operation on a non-existant index throws NPE.  \n{code}\n0: jdbc:hive2://nightly-unsecure-1.gce.cloude> DROP INDEX IF EXISTS vamsee1 ON sample_07;\nINFO  : Compiling command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4): DROP INDEX IF EXISTS vamsee1 ON sample_07\nINFO  : Semantic Analysis Completed\nINFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\nINFO  : Completed compiling command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4); Time taken: 0.238 seconds\nINFO  : Executing command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4): DROP INDEX IF EXISTS vamsee1 ON sample_07\nINFO  : Starting task [Stage-0:DDL] in serial mode\nERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.NullPointerException\nINFO  : Completed executing command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4); Time taken: 0.061 seconds\nError: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.NullPointerException (state=08S01,code=1)\n{code}\n\nHMS log:\n{code}\n2017-01-31 16:27:29,421 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-5-thread-3]: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5823)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rethrowException(HiveMetaStore.java:4892)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4403)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)\n\tat com.sun.proxy.$Proxy16.drop_index_by_name(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10803)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10787)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)\n\tat org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropIndexMessage(JSONMessageFactory.java:159)\n\tat org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name_core(HiveMetaStore.java:4469)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4396)\n\t... 20 more\n{code}\n\nLooks like if an exception is raised at [HiveMetaStore#4572|https://github.com/apache/hive/blob/4becd689d59ee3f75a36119fbb950c44e16c65df/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L4572] (which gets triggered if we try to drop an index which doesn't exist) , the control directly jumps to finally block where we are trying to call drop index event on various metastore event listeners [HiveMetaStore#4619|https://github.com/apache/hive/blob/4becd689d59ee3f75a36119fbb950c44e16c65df/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L4619].\n\nIf one of the event listeners is DbNotificationListener, then it calls the code under JSONDropIndexMessage.java and this fails with NPE during instantiation at [JSONDropIndexMessage.java#46|https://github.com/apache/hive/blob/4becd689d59ee3f75a36119fbb950c44e16c65df/hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONDropIndexMessage.java#L46] as the exception raised in HiveMetaStore.java wouldn't set the index variable."
        }
    },
    {
        "filename": "HIVE-8386.json",
        "creation_time": "2014-10-07T22:30:12.000+0000",
        "bug_report": {
            "Title": "HCAT api call is case sensitive on fields in struct column",
            "Description": "Falcon using hcat api to verify the target table schema and getting the error:\n\n{noformat}\n2014-10-07 00:30:23,255 ERROR - [1972803970@qtp-1214921164-3:gfoetl:POST//entities/submitAndSchedule/feed a0c221e3-efa8-4235-a403-b1047f23ec05] ~ Failure reason (FalconWebException:40)\njava.lang.RuntimeException: cannot find field givenName(lowercase form: givenname) in [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase]\n\tat org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(StructTypeInfo.java:109)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(HCatSchemaUtils.java:154)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(HCatSchemaUtils.java:165)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:127)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:115)\n\tat org.apache.hive.hcatalog.api.HCatTable.<init>(HCatTable.java:59)\n\tat org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(HCatClientHMSImpl.java:157)\n\tat org.apache.falcon.catalog.HiveCatalogService.tableExists(HiveCatalogService.java:143)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validateStorageExists(FeedEntityParser.java:367)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validateFeedStorage(FeedEntityParser.java:309)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:79)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:54)\n\tat org.apache.falcon.resource.AbstractEntityManager.validate(AbstractEntityManager.java:364)\n\tat org.apache.falcon.resource.AbstractEntityManager.submitInternal(AbstractEntityManager.java:331)\n\tat org.apache.falcon.resource.AbstractEntityManager.submit(AbstractEntityManager.java:153)\n\tat org.apache.falcon.resource.ConfigSyncService.submit(ConfigSyncService.java:44)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.falcon.resource.channel.IPCChannel.invoke(IPCChannel.java:48)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$1.doExecute(SchedulableEntityManagerProxy.java:118)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$EntityProxy.execute(SchedulableEntityManagerProxy.java:410)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody0(SchedulableEntityManagerProxy.java:120)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure1.run(SchedulableEntityManagerProxy.java:1)\n\tat org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n\tat org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit(SchedulableEntityManagerProxy.java:107)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody12(SchedulableEntityManagerProxy.java:341)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure13.run(SchedulableEntityManagerProxy.java:1)\n\tat org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n\tat org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule_aroundBody16(SchedulableEntityManagerProxy.java:341)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure17.run(SchedulableEntityManagerProxy.java:1)\n\tat org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n\tat org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule(SchedulableEntityManagerProxy.java:335)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n\tat org.apache.falcon.security.BasicAuthFilter$2.doFilter(BasicAuthFilter.java:183)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:392)\n\tat org.apache.falcon.security.BasicAuthFilter.doFilter(BasicAuthFilter.java:221)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-14714.json",
        "creation_time": "2016-09-07T15:46:07.000+0000",
        "bug_report": {
            "Title": "Avoid misleading \"java.io.IOException: Stream closed\" when shutting down HoS",
            "Description": "After execute hive command with Spark, finishing the beeline session or\neven switch the engine causes IOException. The following executed Ctrl-D to\nfinish the session but \"!quit\" or even \"set hive.execution.engine=mr;\" causes\nthe issue.\n\nFrom HS2 log:\n{code}\n2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [HiveServer2-Handler-Pool: Thread-106]: Timed out shutting down remote driver, interrupting...\n2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [Driver]: Waiting thread interrupted, killing child process.\n2016-09-06 16:15:12,296 WARN  org.apache.hive.spark.client.SparkClientImpl: [stderr-redir-1]: Error in redirector thread.\njava.io.IOException: Stream closed\n        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)\n        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)\n        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)\n        at java.io.InputStreamReader.read(InputStreamReader.java:184)\n        at java.io.BufferedReader.fill(BufferedReader.java:154)\n        at java.io.BufferedReader.readLine(BufferedReader.java:317)\n        at java.io.BufferedReader.readLine(BufferedReader.java:382)\n        at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{code}"
        }
    },
    {
        "filename": "HIVE-5428.json",
        "creation_time": "2013-10-02T20:46:10.000+0000",
        "bug_report": {
            "Title": "Direct SQL check fails during tests",
            "Description": "Noticed this while work on mavenization. If you run the following command\n\n{noformat}\nant test -Dtestcase=TestCliDriver -Dqfile=udf_case.q -Dtest.silent=false\n{noformat}\n\nand look at the top of the logs you see the exception below. It looks like something needs to be changed in the initialization order.\n\n{noformat}\n2013-10-02 13:42:21,596 INFO  metastore.ObjectStore (ObjectStore.java:initialize(243)) - ObjectStore, initialize called\n2013-10-02 13:42:22,048 DEBUG bonecp.BoneCPDataSource (BoneCPDataSource.java:maybeInit(148)) - JDBC URL = jdbc:derby:;databaseName=../build/test/junit_metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 0, min (per partition) = 0, helper threads = 3, idle max age = 60 min, idle test period = 240 min\n2013-10-02 13:42:22,051 WARN  bonecp.BoneCPConfig (BoneCPConfig.java:sanitize(1537)) - Max Connections < 1. Setting to 20\n2013-10-02 13:42:30,218 INFO  metastore.ObjectStore (ObjectStore.java:getPMF(312)) - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n2013-10-02 13:42:30,253 DEBUG bonecp.BoneCPDataSource (BoneCPDataSource.java:maybeInit(148)) - JDBC URL = jdbc:derby:;databaseName=../build/test/junit_metastore_db;create=true, Username = APP, partitions = 1, max (per partition) = 0, min (per partition) = 0, helper threads = 3, idle max age = 60 min, idle test period = 240 min\n2013-10-02 13:42:30,253 WARN  bonecp.BoneCPConfig (BoneCPConfig.java:sanitize(1537)) - Max Connections < 1. Setting to 20\n2013-10-02 13:42:30,262 INFO  metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(99)) - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: \"@\" (64), after : \"\".\n2013-10-02 13:42:30,298 ERROR metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(112)) - Self-test query [select \"DB_ID\" from \"DBS\"] failed; direct SQL is disabled\njavax.jdo.JDODataStoreException: Error executing SQL query \"select \"DB_ID\" from \"DBS\"\".\n  at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)\n  at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:230)\n  at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:108)\n  at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:249)\n  at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\n  at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)\n  at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n  at org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\n  at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:418)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:405)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:444)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:329)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:289)\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4084)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1211)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n  at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2404)\n  at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2415)\n  at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:871)\n  at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:853)\n  at org.apache.hadoop.hive.ql.QTestUtil.cleanUp(QTestUtil.java:534)\n  at org.apache.hadoop.hive.cli.TestCliDriver.<clinit>(TestCliDriver.java:44)\n  at java.lang.Class.forName0(Native Method)\n  at java.lang.Class.forName(Class.java:190)\n  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:374)\n  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)\n  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)\nNestedThrowablesStackTrace:\njava.sql.SQLSyntaxErrorException: Table/View 'DBS' does not exist.\n  at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n  at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement40.<init>(Unknown Source)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-12567.json",
        "creation_time": "2015-12-02T16:38:52.000+0000",
        "bug_report": {
            "Title": "Enhance TxnHandler retry logic to handle ORA-08176",
            "Description": "{noformat}\nFAILED: Error in acquiring locks: Error communicating with the metastore\n2015-12-01 09:19:32,459 ERROR [HiveServer2-Background-Pool: Thread-55]: ql.Driver (SessionState.java:printError(932)) - FAILED: Error in acquiring locks: Error communicating with the metastore\norg.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:132)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)\n        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-08176: consistent read failure; rollback data not available\n\n        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)\n        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)\n        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)\n        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)\n        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)\n        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:30)\n        at oracle.jdbc.driver.T4CStatement.executeForDescribe(T4CStatement.java:762)\n        at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)\n        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)\n        at oracle.jdbc.driver.OracleStatement.executeQuery(OracleStatement.java:1309)\n        at oracle.jdbc.driver.OracleStatementWrapper.executeQuery(OracleStatementWrapper.java:422)\n        at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getLockInfoFromLockId(TxnHandler.java:1951)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(TxnHandler.java:1600)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:1576)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:480)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n        at com.sun.proxy.$Proxy8.lock(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:1869)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n        at com.sun.proxy.$Proxy9.lock(Unknown Source)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:93)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)\n        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:485)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n        at com.sun.proxy.$Proxy8.lock(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:1869)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n        at com.sun.proxy.$Proxy9.lock(Unknown Source)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:93)\n        ... 18 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-6984.json",
        "creation_time": "2014-04-28T23:08:43.000+0000",
        "bug_report": {
            "Title": "Analyzing partitioned table with NULL values for the partition column failed with NPE",
            "Description": "The following describes how to produce the bug:\n{code}\nhive> desc test2;\nname                \tstring              \t                    \nage                 \tint                 \t                    \n\nhive> select * from test2;\n6666666666666666666\tNULL\n5555555555555555555\tNULL\ntom\t15\njohn\tNULL\nmayr\t40\n\t30\n\tNULL\n\nhive> create table test3(name string) partitioned by (age int);\n\nhive> from test2 insert overwrite table test3 partition(age) select test2.name, test2.age;\nLoading data to table default.test3 partition (age=null)\n\tLoading partition {age=40}\n\tLoading partition {age=__HIVE_DEFAULT_PARTITION__}\n\tLoading partition {age=30}\n\tLoading partition {age=15}\nPartition default.test3{age=15} stats: [numFiles=1, numRows=1, totalSize=4, rawDataSize=3]\nPartition default.test3{age=30} stats: [numFiles=1, numRows=1, totalSize=1, rawDataSize=0]\nPartition default.test3{age=40} stats: [numFiles=1, numRows=1, totalSize=5, rawDataSize=4]\nPartition default.test3{age=__HIVE_DEFAULT_PARTITION__} stats: [numFiles=1, numRows=4, totalSize=46, rawDataSize=42]\n\nhive> analyze table test3 partition(age) compute statistics;\n...\nTask with the most failures(4): \n-----\nDiagnostic Messages for this Task:\njava.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)\n\tat org.apache.hado\n\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\n{code}\n\nThe following is the stack trace in mapper log:\n{code}\n2014-04-28 15:39:25,073 FATAL org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)\n\t... 9 more\n{code}"
        }
    },
    {
        "filename": "HIVE-10736.json",
        "creation_time": "2015-05-18T03:25:45.000+0000",
        "bug_report": {
            "Title": "HiveServer2 shutdown of cached tez app-masters is not clean",
            "Description": "The shutdown process throws concurrent modification exceptions and fails to clean up the app masters per queue.\n\n{code}\n2015-05-17 20:24:00,464 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:OperationManager is stopped.\n2015-05-17 20:24:00,464 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:SessionManager is stopped.\n2015-05-17 20:24:00,464 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-9()]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(175)) - Closing tez session default? true\n2015-05-17 20:24:00,465 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:CLIService is stopped.\n2015-05-17 20:24:00,465 INFO  [Thread-6()]: service.AbstractService (AbstractService.java:stop(125)) - Service:HiveServer2 is stopped.\n2015-05-17 20:24:00,465 INFO  [Thread-6()]: tez.TezSessionState (TezSessionState.java:close(332)) - Closing Tez Session\n2015-05-17 20:24:00,466 INFO  [Thread-6()]: client.TezClient (TezClient.java:stop(495)) - Shutting down Tez Session, sessionName=HIVE-94cc629d-63bc-490a-a135-af85c0cc0f2e, applicationId=application_1431919257083_0012\n2015-05-17 20:24:00,570 ERROR [Thread-6()]: server.HiveServer2 (HiveServer2.java:stop(322)) - Tez session pool manager stop had an error during stop of HiveServer2. Shutting down HiveServer2 anyway.\njava.util.ConcurrentModificationException\n        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)\n        at java.util.LinkedList$ListItr.next(LinkedList.java:888)\n        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)\n        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)\n        at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)\n{code}"
        }
    },
    {
        "filename": "HIVE-7710.json",
        "creation_time": "2014-08-13T10:46:32.000+0000",
        "bug_report": {
            "Title": "Rename table across database might fail",
            "Description": "If there is already a table d1.t2, the following rename statement would fail. \n{code}\nalter table  d1.t1 rename to d2.t2; \n\n//Exception\n2014-08-13 03:32:40,512 ERROR Datastore.Persist (Log4JLogger.java:error(115)) - Update of object \"org.apache.hadoop.hive.metastore.model.MTable@729c5167\" using statement \"UPDATE TBLS SET TBL_NAME=? WHERE TBL_ID=?\" failed : java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)\n\tat com.jolbox.bonecp.PreparedStatementHandle.executeUpdate(PreparedStatementHandle.java:205)\n\tat org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:399)\n\tat org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:439)\n\tat org.datanucleus.store.rdbms.request.UpdateRequest.execute(UpdateRequest.java:374)\n\tat org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateTable(RDBMSPersistenceHandler.java:417)\n\tat org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateObject(RDBMSPersistenceHandler.java:390)\n\tat org.datanucleus.state.JDOStateManager.flush(JDOStateManager.java:5027)\n\tat org.datanucleus.flush.FlushOrdered.execute(FlushOrdered.java:106)\n\tat org.datanucleus.ExecutionContextImpl.flushInternal(ExecutionContextImpl.java:4119)\n\tat org.datanucleus.ExecutionContextThreadedImpl.flushInternal(ExecutionContextThreadedImpl.java:450)\n\tat org.datanucleus.ExecutionContextImpl.markDirty(ExecutionContextImpl.java:3879)\n\tat org.datanucleus.ExecutionContextThreadedImpl.markDirty(ExecutionContextThreadedImpl.java:422)\n\tat org.datanucleus.state.JDOStateManager.postWriteField(JDOStateManager.java:4815)\n\tat org.datanucleus.state.JDOStateManager.replaceField(JDOStateManager.java:3356)\n\tat org.datanucleus.state.JDOStateManager.updateField(JDOStateManager.java:2018)\n\tat org.datanucleus.state.JDOStateManager.setStringField(JDOStateManager.java:1791)\n\tat org.apache.hadoop.hive.metastore.model.MStorageDescriptor.jdoSetlocation(MStorageDescriptor.java)\n\tat org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setLocation(MStorageDescriptor.java:88)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.copyMSD(ObjectStore.java:2699)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2572)\n\tat sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)\n\tat com.sun.proxy.$Proxy6.alterTable(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:205)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2771)\n\tat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)\n\tat com.sun.proxy.$Proxy8.alter_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:293)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(SessionHiveMetaStoreClient.java:201)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:288)\n\tat sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)\n\tat com.sun.proxy.$Proxy9.alter_table(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:404)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3542)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:318)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1538)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1305)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1118)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:833)\n\tat org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)\n\tat org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_table(TestCliDriver.java:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:168)\n\tat junit.framework.TestCase.runBare(TestCase.java:134)\n\tat junit.framework.TestResult$1.protect(TestResult.java:110)\n\tat junit.framework.TestResult.runProtected(TestResult.java:128)\n\tat junit.framework.TestResult.run(TestResult.java:113)\n\tat junit.framework.TestCase.run(TestCase.java:124)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:243)\n\tat junit.framework.TestSuite.run(TestSuite.java:238)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: java.sql.SQLException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 86 more\nCaused by: ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.IndexChanger.finish(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.IndexSetChanger.finish(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.RowChangerImpl.finish(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.UpdateResultSet.open(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)\n\t... 80 more\n\n{code}\n\nAnd in HiveAlterHandler#alterTable we should check if rename hdfs directory succeed."
        }
    },
    {
        "filename": "HIVE-8735.json",
        "creation_time": "2014-11-04T22:20:02.000+0000",
        "bug_report": {
            "Title": "statistics update can fail due to long paths",
            "Description": "{noformat}\n2014-11-04 01:34:38,610 ERROR jdbc.JDBCStatsPublisher (JDBCStatsPublisher.java:publishStat(198)) - Error during publishing statistics. \njava.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)\n\tat org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:147)\n\tat org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:144)\n\tat org.apache.hadoop.hive.ql.exec.Utilities.executeWithRetry(Utilities.java:2910)\n\tat org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.publishStat(JDBCStatsPublisher.java:160)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats(FileSinkOperator.java:1153)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:992)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:205)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:722)\nCaused by: java.sql.SQLException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 31 more\nCaused by: ERROR 22001: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.types.SQLChar.hasNonBlankChars(Unknown Source)\n\tat org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)\n\tat org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)\n\tat org.apache.derby.iapi.types.DataTypeDescriptor.normalize(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeColumn(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeRow(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)\n\t... 25 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-13209.json",
        "creation_time": "2016-03-04T21:39:50.000+0000",
        "bug_report": {
            "Title": "metastore get_delegation_token fails with null ip address",
            "Description": "After changes in HIVE-13169, metastore get_delegation_token fails with null ip address.\n\n{code}\n2016-03-03 07:45:31,055 ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/<hostname@realm> from IP null)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n{code}"
        }
    },
    {
        "filename": "HIVE-13065.json",
        "creation_time": "2016-02-16T21:11:31.000+0000",
        "bug_report": {
            "Title": "Hive throws NPE when writing map type data to a HBase backed table",
            "Description": "Hive throws NPE when writing data to a HBase backed table with below conditions:\n\n# There is a map type column\n# The map type column has NULL in its values\n\nBelow are the reproduce steps:\n\n*1) Create a HBase backed Hive table*\n{code:sql}\ncreate table hbase_test (id bigint, data map<string, string>)\nstored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nwith serdeproperties (\"hbase.columns.mapping\" = \":key,cf:map_col\")\ntblproperties (\"hbase.table.name\" = \"hive_test\");\n{code}\n\n*2) insert data into above table*\n{code:sql}\ninsert overwrite table hbase_test select 1 as id, map('abcd', null) as data from src limit 1;\n{code}\n\nThe mapreduce job for insert query fails. Error messages are as below:\n{noformat}\n2016-02-15 02:26:33,225 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)\n\t... 7 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)\n\t... 7 more\nCaused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)\n\t... 14 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)\n\tat org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)\n\t... 15 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-11470.json",
        "creation_time": "2015-08-05T18:45:26.000+0000",
        "bug_report": {
            "Title": "NPE in DynamicPartFileRecordWriterContainer on null part-keys.",
            "Description": "When partitioning data using {{HCatStorer}}, one sees the following NPE, if the dyn-part-key is of null-value:\n\n{noformat}\n2015-07-30 23:59:59,627 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.lang.NullPointerException\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)\nat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\nat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.NullPointerException\nat org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)\nat org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)\nat org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)\nat org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)\nat org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)\nat org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)\nat org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\nat org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)\n... 11 more\n{noformat}\n\nThe reason is that the {{DynamicPartitionFileRecordWriterContainer}} makes an unfortunate assumption when fetching a local file-writer instance:\n\n{code:title=DynamicPartitionFileRecordWriterContainer.java}\n  @Override\n  protected LocalFileWriter getLocalFileWriter(HCatRecord value) \n    throws IOException, HCatException {\n    \n    OutputJobInfo localJobInfo = null;\n    // Calculate which writer to use from the remaining values - this needs to\n    // be done before we delete cols.\n    List<String> dynamicPartValues = new ArrayList<String>();\n    for (Integer colToAppend : dynamicPartCols) {\n      dynamicPartValues.add(value.get(colToAppend).toString()); // <-- YIKES!\n    }\n    ...\n  }\n{code}\n\nMust check for null, and substitute with {{\"\\_\\_HIVE_DEFAULT_PARTITION\\_\\_\"}}, or equivalent."
        }
    },
    {
        "filename": "HIVE-12476.json",
        "creation_time": "2015-11-20T03:30:18.000+0000",
        "bug_report": {
            "Title": "Metastore NPE on Oracle with Direct SQL",
            "Description": "Stack trace looks very similar to HIVE-8485. I believe the metastore's Direct SQL mode requires additional fixes similar to HIVE-8485, around the Partition/StorageDescriptorSerDe parameters.\n\n{noformat}\n2015-11-19 18:08:33,841 ERROR [pool-5-thread-2]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.\njava.lang.NullPointerException\n        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)\n        at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)\n        at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)\n        at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)\n        at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)\n        at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)\n        at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)\n        at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)\n        at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)\n        at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-10559.json",
        "creation_time": "2015-04-30T21:12:40.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException with RemoveDynamicPruningBySize",
            "Description": "The problem can be reproduced by running the script attached.\n\nBacktrace\n{code}\n2015-04-29 10:34:36,390 ERROR [main]: ql.Driver (SessionState.java:printError(956)) - FAILED: IndexOutOfBoundsException Index: 0, Size: 0\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n\tat java.util.ArrayList.rangeCheck(ArrayList.java:635)\n\tat java.util.ArrayList.get(ArrayList.java:411)\n\tat org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process(RemoveDynamicPruningBySize.java:61)\n\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)\n\tat org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:77)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)\n\tat org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:281)\n\tat org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:123)\n\tat org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10092)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9932)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n\tat org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1026)\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1000)\n\tat org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:139)\n\tat org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_q85(TestMiniTezCliDriver.java:123)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat junit.framework.TestCase.runTest(TestCase.java:176)\n\tat junit.framework.TestCase.runBare(TestCase.java:141)\n\tat junit.framework.TestResult$1.protect(TestResult.java:122)\n\tat junit.framework.TestResult.runProtected(TestResult.java:142)\n\tat junit.framework.TestResult.run(TestResult.java:125)\n\tat junit.framework.TestCase.run(TestCase.java:129)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:255)\n\tat junit.framework.TestSuite.run(TestSuite.java:250)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\n{code}"
        }
    },
    {
        "filename": "HIVE-9721.json",
        "creation_time": "2015-02-19T06:56:17.000+0000",
        "bug_report": {
            "Title": "Hadoop23Shims.setFullFileStatus should check for null",
            "Description": "{noformat}\n2015-02-18 22:46:10,209 INFO org.apache.hadoop.hive.shims.HadoopShimsSecure: Skipping ACL inheritance: File system for path file:/tmp/hive/f1a28dee-70e8-4bc3-bd35-9be13834d1fc/hive_2015-02-18_22-46-10_065_3348083202601156561-1 does not support ACLs but dfs.namenode.acls.enabled is set to true: java.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus\njava.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus\n\tat org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)\n\tat org.apache.hadoop.fs.FilterFileSystem.getAclStatus(FilterFileSystem.java:562)\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:645)\n\tat org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:524)\n\tat org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)\n\tat org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-02-18 17:30:58,753 INFO org.apache.hadoop.hive.shims.HadoopShimsSecure: Skipping ACL inheritance: File system for path file:/tmp/hive/e3eb01f0-bb58-45a8-b773-8f4f3420457c/hive_2015-02-18_17-30-58_346_5020255420422913166-1/-mr-10000 does not support ACLs but dfs.namenode.acls.enabled is set to true: java.lang.NullPointerException\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Hadoop23Shims.java:668)\n        at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:527)\n        at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)\n        at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)\n        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)\n        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)\n        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)\n        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)\n        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-4216.json",
        "creation_time": "2013-03-21T20:53:24.000+0000",
        "bug_report": {
            "Title": "TestHBaseMinimrCliDriver throws weird error with HBase 0.94.5 and Hadoop 23 and test is stuck infinitely",
            "Description": "After upgrading to Hadoop 23 and HBase 0.94.5 compiled for Hadoop 23. The TestHBaseMinimrCliDriver, fails after performing the following steps\n\nUpdate \"hbase_bulk.m\" with the following properties\nset mapreduce.totalorderpartitioner.naturalorder=false;\nset mapreduce.totalorderpartitioner.path=/tmp/hbpartition.lst;\nOtherwise I keep seeing: \"_partition.lst\" not found exception in the mappers, even though set total.order.partitioner.path=/tmp/hbpartition.lst is set.\n\nWhen the test runs, the 3 reducer phase of the second query fails with the following error, but the MiniMRCluster keeps spinning up new reducer and the test is stuck infinitely.\n{code}\ninsert overwrite table hbsort\n select distinct value,\n  case when key=103 then cast(null as string) else key end,\n  case when key=103 then ''\n       else cast(key+1 as string) end\n from src\n cluster by value;\n{code}\n\nThe stack trace I see in the syslog for the Node Manager is the following:\n\n==============================================================\n13-03-20 16:26:48,942 FATAL [IPC Server handler 17 on 55996] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1363821864968_0003_r_000002_0 - exited : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}\n        at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:268)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:448)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:399)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:157)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:152)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}\n        at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)\n        ... 7 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:237)\n        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:477)\n        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:525)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)\n        at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n        at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)\n        ... 7 more\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.mapreduce.TaskID$CharTaskTypeMaps.getRepresentingCharacter(TaskID.java:265)\n        at org.apache.hadoop.mapreduce.TaskID.appendTo(TaskID.java:153)\n        at org.apache.hadoop.mapreduce.TaskAttemptID.appendTo(TaskAttemptID.java:119)\n        at org.apache.hadoop.mapreduce.TaskAttemptID.toString(TaskAttemptID.java:151)\n        at java.lang.String.valueOf(String.java:2826)\n        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209)\n        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:69)\n        at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.getRecordWriter(HFileOutputFormat.java:90)\n        at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getFileWriter(HiveHFileOutputFormat.java:67)\n        at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getHiveRecordWriter(HiveHFileOutputFormat.java:104)\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:246)\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:234)\n        ... 14 more\n=============================================================="
        }
    },
    {
        "filename": "HIVE-13836.json",
        "creation_time": "2016-05-24T22:37:59.000+0000",
        "bug_report": {
            "Title": "DbNotifications giving an error = Invalid state. Transaction has already started",
            "Description": "I used pyhs2 python client to create tables/partitions in hive. I was working fine until I moved to multithreaded scripts which created 8 connections and ran DDL queries concurrently.\nI got the error as\n\n{noformat}\n2016-05-04 17:49:26,226 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-4-thread-194]: HMSHandler Fatal error: Invalid state. Transaction has already started\norg.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started\n        at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)\n        at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)\n        at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)\n        at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)\n        at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:463)\n        at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7522)\n        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)\n        at com.sun.proxy.$Proxy10.addNotificationEvent(Unknown Source)\n        at org.apache.hive.hcatalog.listener.DbNotificationListener.enqueue(DbNotificationListener.java:261)\n        at org.apache.hive.hcatalog.listener.DbNotificationListener.onCreateTable(DbNotificationListener.java:123)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1483)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1502)\n        at sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)\n        at com.sun.proxy.$Proxy14.create_table_with_environment_context(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_table_with_environment_context.getResult(ThriftHiveMetastore.java:9267)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-9873.json",
        "creation_time": "2015-03-05T17:35:33.000+0000",
        "bug_report": {
            "Title": "Hive on MR throws DeprecatedParquetHiveInput exception",
            "Description": "The following error is thrown when information about columns is changed on {{projectionPusher.pushProjectionsAndFilters}}. \n\n{noformat}\n2015-02-26 15:56:40,275 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:105)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:224)\n\t... 11 more\nCaused by: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:199)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:52)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)\n\t... 15 more\n{noformat}\n\nThe bug is in {{ParquetRecordReaderWrapper}}. We store metastore such as the list of columns in the {{Configuration/JobConf}}. The issue is that this metadata is incorrect until the call to {{projectionPusher.pushProjectionsAndFilters}}. In the current codebase we don't use the configuration object returned from {{projectionPusher.pushProjectionsAndFilters}} in other sections of code such as creation and initialization of {{realReader}}. The end result is that parquet is given an empty read schema and returns all nulls. Since the join key is null, no records are joined."
        }
    },
    {
        "filename": "HIVE-13174.json",
        "creation_time": "2016-02-26T23:34:36.000+0000",
        "bug_report": {
            "Title": "Remove Vectorizer noise in logs",
            "Description": "If you have a table with a bin column you're hs2/client logs are full of the stack traces below. These should either be made debug or we just log the message not the trace.\n{code}\n2015-10-12 12:34:23,922 INFO  [main]: physical.Vectorizer (Vectorizer.java:validateExprNodeDesc(1249)) - Failed to vectorize\norg.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)\n\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)\n\tat org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)\n\tat org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)\n\tat org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)\n\tat org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)\n\tat org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n\tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n\tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n{code}"
        }
    },
    {
        "filename": "HIVE-5431.json",
        "creation_time": "2013-10-03T03:35:44.000+0000",
        "bug_report": {
            "Title": "PassthroughOutputFormat SH changes causes IllegalArgumentException",
            "Description": "The recent changes with HIVE-4331 introduced a new key \"hive.passthrough.storagehandler.of\", whose value is set only on storage handler writes, but obviously, will not be set on reads. However, PlanUtils.configureJobPropertiesForStorageHandler winds up trying to set the key for both cases into jobProperties, which cause any reads that are not preceeded by writes to fail.\n\nBasically, if you have a .q in which you insert data into a hbase table and then read it, it's okay. If you have a .q in which you only read data, it throws an IllegalArgumentException, like so:\n\n{noformat}\n2013-09-30 16:20:01,989 ERROR CliDriver (SessionState.java:printError(419)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Property value must not be null\njava.io.IOException: java.lang.IllegalArgumentException: Property value must not be null\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)\n        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.IllegalArgumentException: Property value must not be null\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)\n        at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)\n        at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)\n        at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1826)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:380)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)\n        ... 17 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-13115.json",
        "creation_time": "2016-02-22T21:43:32.000+0000",
        "bug_report": {
            "Title": "MetaStore Direct SQL getPartitions call fail when the columns schemas for a partition are null",
            "Description": "We are seeing the following exception in our MetaStore logs\n\n{noformat}\n2016-02-11 00:00:19,002 DEBUG metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(602)) - Direct SQL query in 5.842372ms + 1.066728ms, the query is [select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\"  inner join \"TBLS\" on \"PART\nITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\"     and \"TBLS\".\"TBL_NAME\" = ?   inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\"      and \"DBS\".\"NAME\" = ?  order by \"PART_NAME\" asc]\n2016-02-11 00:00:19,021 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2243)) - Direct SQL failed, falling back to ORM\nMetaException(message:Unexpected null for one of the IDs, SD 6437, column null, serde 6437 for a non- view)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)\n        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)\n        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)\n        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)\n        at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:483)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)\n        at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n{noformat}\n\nThis direct SQL call fails for every {{getPartitions}} call and then falls back to ORM.\n\nThe query which fails is\n{code}\nselect \n  PARTITIONS.PART_ID, SDS.SD_ID, SDS.CD_ID,\n  SERDES.SERDE_ID, PARTITIONS.CREATE_TIME,\n  PARTITIONS.LAST_ACCESS_TIME, SDS.INPUT_FORMAT, SDS.IS_COMPRESSED,\n  SDS.IS_STOREDASSUBDIRECTORIES, SDS.LOCATION, SDS.NUM_BUCKETS,\n  SDS.OUTPUT_FORMAT, SERDES.NAME, SERDES.SLIB \nfrom PARTITIONS\n  left outer join SDS on PARTITIONS.SD_ID = SDS.SD_ID \n  left outer join SERDES on SDS.SERDE_ID = SERDES.SERDE_ID \n  where PART_ID in (  ?  ) order by PART_NAME asc;\n{code}\n\nBy looking at the source {{MetaStoreDirectSql.java}}, the third column in the query ( SDS.CD_ID), the column descriptor ID, is null, which triggers the exception. This exception is not thrown from the ORM layer since it is more forgiving to the null column descriptor. See {{ObjectStore.java:1197}}\n{code}\n List<MFieldSchema> mFieldSchemas = msd.getCD() == null ? null : msd.getCD().getCols();\n{code}\n\nI verified that this exception gets triggered in the first place when we add a new partition without setting column level schemas for the partition, using the MetaStoreClient API. This exception does not occur when adding partitions using the CLI\n\nI see two ways to solve the issue.\n1. Make the MetaStoreClient API more strict and not allow creating partition without having column level schemas set. (This could break clients which use the MetaStoreclient API)\n2. Make the Direct SQL code path and the ORM code path more consistent, where the Direct SQL does not fail on null column descriptor ID.\n\nI feel 2 is more safer and easier to fix.\n"
        }
    },
    {
        "filename": "HIVE-4723.json",
        "creation_time": "2013-06-12T20:37:55.000+0000",
        "bug_report": {
            "Title": "DDLSemanticAnalyzer.addTablePartsOutputs eats several exceptions",
            "Description": "I accidently tried to archive a partition on a non-partitioned table. The error message was bad, hive ate an exception, and NPE'ed.\n\n{noformat}\n2013-06-09 16:36:12,628 ERROR parse.DDLSemanticAnalyzer (DDLSemanticAnalyzer.java:addTablePartsOutputs(2899)) - Got HiveException during obtaining list of partitions\n2013-06-09 16:36:12,628 ERROR ql.Driver (SessionState.java:printError(383)) - FAILED: NullPointerException null\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2912)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2877)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(DDLSemanticAnalyzer.java:2730)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:316)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:277)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-15686.json",
        "creation_time": "2017-01-20T22:29:36.000+0000",
        "bug_report": {
            "Title": "Partitions on Remote HDFS break encryption-zone checks",
            "Description": "This is in relation to HIVE-13243, which fixes encryption-zone checks for external tables.\nUnfortunately, this is still borked for partitions with remote HDFS paths. The code fails as follows:\n\n{noformat}\n2015-12-09 19:26:14,997 ERROR [pool-4-thread-1476] server.TThreadPoolServer (TThreadPoolServer.java:run_aroundBody0(305)) - Error occurred during processing of message.\njava.lang.IllegalArgumentException: Wrong FS: hdfs://remote-cluster-nn1.myth.net:8020/dbs/mythdb/myth_table/dt=20170120, expected: hdfs://local-cluster-n1.myth.net:8020\n        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1985)\n        at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)\n        at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1290)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.checkTrashPurgeCombination(HiveMetaStore.java:1746)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2974)\n        at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:483)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n        at com.sun.proxy.$Proxy5.drop_partitions_req(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:10005)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:9989)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:767)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:763)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:763)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:285)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nI have a really simple fix."
        }
    },
    {
        "filename": "HIVE-4975.json",
        "creation_time": "2013-08-01T16:21:38.000+0000",
        "bug_report": {
            "Title": "Reading orc file throws exception after adding new column",
            "Description": "ORC file read failure after add table column.\ncreate a table which have three column .(a string,b string,c string).\nadd a new column after c by executing \"ALTER TABLE table ADD COLUMNS (d string)\".\nexecute hiveql \"select d from table\",the following exception goes:\njava.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)\n\tat org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n ]\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:162)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)\n\tat org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n ]\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:671)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\t... 8 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating d\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:80)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:654)\n\t... 9 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)\n\tat org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.evaluate(ExprNodeColumnEvaluator.java:98)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:76)\n\t... 15 more\n2013-08-01 23:34:22,883 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task"
        }
    },
    {
        "filename": "HIVE-10538.json",
        "creation_time": "2015-04-29T20:06:38.000+0000",
        "bug_report": {
            "Title": "Fix NPE in FileSinkOperator from hashcode mismatch",
            "Description": "A Null Pointer Exception occurs when in FileSinkOperator when using bucketed tables and distribute by with multiFileSpray enabled. The following snippet query reproduces this issue:\n\n{code}\nset hive.enforce.bucketing = true;\nset hive.exec.reducers.max = 20;\n\ncreate table bucket_a(key int, value_a string) clustered by (key) into 256 buckets;\ncreate table bucket_b(key int, value_b string) clustered by (key) into 256 buckets;\ncreate table bucket_ab(key int, value_a string, value_b string) clustered by (key) into 256 buckets;\n\n-- Insert data into bucket_a and bucket_b\n\ninsert overwrite table bucket_ab\nselect a.key, a.value_a, b.value_b from bucket_a a join bucket_b b on (a.key = b.key) distribute by key;\n{code}\n\nThe following stack trace is logged.\n\n{code}\n2015-04-29 12:54:12,841 FATAL [pool-110-thread-1]: ExecReducer (ExecReducer.java:reduce(255)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"113\",\"_col1\":\"val_113\"}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)\n\t... 8 more\n{code}"
        }
    },
    {
        "filename": "HIVE-11902.json",
        "creation_time": "2015-09-21T16:12:37.000+0000",
        "bug_report": {
            "Title": "Abort txn cleanup thread throws SyntaxErrorException",
            "Description": "When cleaning left over transactions we see the DeadTxnReaper code threw the following exception:\n{noformat}\n2015-09-21 05:23:38,148 WARN  [DeadTxnReaper-0]: txn.TxnHandler (TxnHandler.java:performTimeOuts(1876)) - Aborting timedout transactions failed due to You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1(SQLState=42000,ErrorCode=1064)\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)\n        at com.mysql.jdbc.Util.getInstance(Util.java:360)\n        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)\n        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)\n        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)\n        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)\n        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)\n        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)\n        at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)\n        at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\nThe problem here is that the method {{abortTxns(Connection dbConn, List<Long> txnids)}} in metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java creates the following bad query when txnids list is empty.\n{code}\ndelete from HIVE_LOCKS where hl_txnid in ();\n{code}\n"
        }
    },
    {
        "filename": "HIVE-18918.json",
        "creation_time": "2018-03-09T00:47:55.000+0000",
        "bug_report": {
            "Title": "Bad error message in CompactorMR.lanuchCompactionJob()",
            "Description": "{noformat}\r\n      rj.waitForCompletion();\r\n      if (!rj.isSuccessful()) {\r\n        throw new IOException(compactionType == CompactionType.MAJOR ? \"Major\" : \"Minor\" +\r\n               \" compactor job failed for \" + jobName + \"! Hadoop JobId: \" + rj.getID());\r\n      }\r\n{noformat}\r\n\r\nproduces no useful info in case of Major compaction\r\n\r\n{noformat}\r\n2018-02-28 00:59:16,416 ERROR [gdpr1-61]: compactor.Worker (Worker.java:run(191)) - Caught exception while trying to compact id:38602,dbname:audit,tableName:COMP_ENTRY_AF_A,partN\\\r\name:partition_dt=2017-04-11,state:^@,type:MAJOR,properties:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking failed to avoid repeated failures, java.io.IOException: Ma\\\r\njor\r\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob(CompactorMR.java:314)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:269)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:175)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:172)\r\n\r\n{noformat}\r\n"
        }
    },
    {
        "filename": "HIVE-8107.json",
        "creation_time": "2014-09-15T19:49:09.000+0000",
        "bug_report": {
            "Title": "Bad error message for non-existent table in update and delete",
            "Description": "update no_such_table set x = 3;\nproduces an error message like:\n\n{noformat}\n2014-09-12 19:45:00,138 ERROR [main]: ql.Driver (SessionState.java:printError(824)) - FAILED: SemanticException [Error 10290]: Encountered parse error while parsing rewritten update or delete query\norg.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:97)\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:66)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:217)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:406)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:302)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1051)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1121)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:988)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:978)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)\n\tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:441)\n\tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:457)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found no_such_table\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:978)\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:128)\n\t... 24 more\n{noformat}\n\nIt should give something much cleaner, or at least push the Table not found message to the top rather than bury it in an exception stack.\n"
        }
    },
    {
        "filename": "HIVE-1326.json",
        "creation_time": "2010-04-25T20:50:54.000+0000",
        "bug_report": {
            "Title": "RowContainer uses hard-coded '/tmp/' path for temporary files",
            "Description": "In our production hadoop environment, the \"/tmp/\" is actually pretty small, and we encountered a problem when a query used the RowContainer class and filled up the /tmp/ partition.  I tracked down the cause to the RowContainer class putting temporary files in the '/tmp/' path instead of using the configured Hadoop temporary path.  I've attached a patch to fix this.\n\nHere's the traceback:\n\n2010-04-25 12:05:05,120 INFO org.apache.hadoop.hive.ql.exec.persistence.RowContainer: RowContainer created temp file /tmp/hive-rowcontainer-1244151903/RowContainer7816.tmp\n2010-04-25 12:05:06,326 INFO ExecReducer: ExecReducer: processing 10000000 rows: used memory = 385520312\n2010-04-25 12:05:08,513 INFO ExecReducer: ExecReducer: processing 11000000 rows: used memory = 341780472\n2010-04-25 12:05:10,697 INFO ExecReducer: ExecReducer: processing 12000000 rows: used memory = 301446768\n2010-04-25 12:05:12,837 INFO ExecReducer: ExecReducer: processing 13000000 rows: used memory = 399208768\n2010-04-25 12:05:15,085 INFO ExecReducer: ExecReducer: processing 14000000 rows: used memory = 364507216\n2010-04-25 12:05:17,260 INFO ExecReducer: ExecReducer: processing 15000000 rows: used memory = 332907280\n2010-04-25 12:05:19,580 INFO ExecReducer: ExecReducer: processing 16000000 rows: used memory = 298774096\n2010-04-25 12:05:21,629 INFO ExecReducer: ExecReducer: processing 17000000 rows: used memory = 396505408\n2010-04-25 12:05:23,830 INFO ExecReducer: ExecReducer: processing 18000000 rows: used memory = 362477288\n2010-04-25 12:05:25,914 INFO ExecReducer: ExecReducer: processing 19000000 rows: used memory = 327229744\n2010-04-25 12:05:27,978 INFO ExecReducer: ExecReducer: processing 20000000 rows: used memory = 296051904\n2010-04-25 12:05:28,155 FATAL ExecReducer: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1013)\n\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)\n\tat org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat$1.write(HiveSequenceFileOutputFormat.java:70)\n\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)\n\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)\n\tat org.apache.hadoop.hive.ql.exec.JoinOperator.processOp(JoinOperator.java:118)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:456)\n\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:244)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:158)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:260)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)\n\t... 22 more\n"
        }
    },
    {
        "filename": "HIVE-11369.json",
        "creation_time": "2015-07-24T16:28:47.000+0000",
        "bug_report": {
            "Title": "Mapjoins in HiveServer2 fail when jmxremote is used",
            "Description": "having hive.auto.convert.join set to true works in the CLI with no issue, but fails in HiveServer2 when jmx options are passed to the service on startup. This (in hive-env.sh) is enough to make it fail: \n{noformat}\n-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.port=8009\n{noformat}\nAs soon as I remove the line, it works properly. I have *no*idea...\nHere's the log from the service:\n{noformat}\n2015-07-24 17:19:27,457 INFO  [HiveServer2-Handler-Pool: Thread-22]: ql.Driver (SessionState.java:printInfo(912)) - Query ID = hive_20150724171919_aaa88a89-dc6d-490b-821c-4eec6d4c0421\n2015-07-24 17:19:27,457 INFO  [HiveServer2-Handler-Pool: Thread-22]: ql.Driver (SessionState.java:printInfo(912)) - Total jobs = 1\n2015-07-24 17:19:27,465 INFO  [HiveServer2-Handler-Pool: Thread-22]: ql.Driver (Driver.java:launchTask(1638)) - Starting task [Stage-4:MAPREDLOCAL] in serial mode\n2015-07-24 17:19:27,467 INFO  [HiveServer2-Handler-Pool: Thread-22]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(159)) - Generating plan file file:/tmp/hive/8932c206-5420-4b6f-9f1f-5f1706f30df8/hive_2015-07-24_17-19-26_552_5082133674120283907-1/-local-10005/plan.xml\n2015-07-24 17:19:27,625 WARN  [HiveServer2-Handler-Pool: Thread-22]: conf.HiveConf (HiveConf.java:initialize(2620)) - HiveConf of name hive.files.umask.value does not exist\n2015-07-24 17:19:27,708 INFO  [HiveServer2-Handler-Pool: Thread-22]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(288)) - Executing: /usr/lib/hadoop/bin/hadoop jar /usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.3.jar org.apache.hadoop.hive.ql.exec.mr.ExecDriver -localtask -plan file:/tmp/hive/8932c206-5420-4b6f-9f1f-5f1706f30df8/hive_2015-07-24_17-19-26_552_5082133674120283907-1/-local-10005/plan.xml   -jobconffile file:/tmp/hive/8932c206-5420-4b6f-9f1f-5f1706f30df8/hive_2015-07-24_17-19-26_552_5082133674120283907-1/-local-10006/jobconf.xml\n2015-07-24 17:19:28,499 ERROR [HiveServer2-Handler-Pool: Thread-22]: exec.Task (SessionState.java:printError(921)) - Execution failed with exit status: 1\n2015-07-24 17:19:28,500 ERROR [HiveServer2-Handler-Pool: Thread-22]: exec.Task (SessionState.java:printError(921)) - Obtaining error information\n2015-07-24 17:19:28,500 ERROR [HiveServer2-Handler-Pool: Thread-22]: exec.Task (SessionState.java:printError(921)) -\nTask failed!\nTask ID:\n  Stage-4\n\nLogs:\n\n2015-07-24 17:19:28,501 ERROR [HiveServer2-Handler-Pool: Thread-22]: exec.Task (SessionState.java:printError(921)) - /tmp/hiveserver2_manual/hive-server2.log\n2015-07-24 17:19:28,501 ERROR [HiveServer2-Handler-Pool: Thread-22]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(308)) - Execution failed with exit status: 1\n2015-07-24 17:19:28,518 ERROR [HiveServer2-Handler-Pool: Thread-22]: ql.Driver (SessionState.java:printError(921)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n2015-07-24 17:19:28,599 WARN  [HiveServer2-Handler-Pool: Thread-22]: security.UserGroupInformation (UserGroupInformation.java:doAs(1674)) - PriviledgedActionException as:hive (auth:SIMPLE) cause:org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n2015-07-24 17:19:28,600 WARN  [HiveServer2-Handler-Pool: Thread-22]: thrift.ThriftCLIService (ThriftCLIService.java:ExecuteStatement(496)) - Error executing statement:\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:146)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:173)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:379)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\n\tat com.sun.proxy.$Proxy23.executeStatement(Unknown Source)\n\tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:258)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-9055.json",
        "creation_time": "2014-12-09T19:51:18.000+0000",
        "bug_report": {
            "Title": "Tez: union all followed by group by followed by another union all gives error",
            "Description": "Here is the way to produce it:\n\nin Hive q test setting (with src table)\n\nset hive.execution.engine=tez;\n\nselect key from \n(\nselect key from src\nunion all \nselect key from src\n) tab group by key\nunion all\nselect key from src;\n\nwill give you\n\nERROR\n2014-12-09 11:38:48,316 ERROR ql.Driver (SessionState.java:printError(834)) - FAILED: IndexOutOfBoundsException Index: -1, Size: 1\njava.lang.IndexOutOfBoundsException: Index: -1, Size: 1\n        at java.util.LinkedList.checkElementIndex(LinkedList.java:553)\n        at java.util.LinkedList.get(LinkedList.java:474)\n        at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:354)\n        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:420)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1108)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1035)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:199)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:151)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:362)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:297)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez(TestMiniTezCliDriver.java:120)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\nbtw: there is not problem when it is run with MR"
        }
    },
    {
        "filename": "HIVE-13856.json",
        "creation_time": "2016-05-25T21:50:49.000+0000",
        "bug_report": {
            "Title": "Fetching transaction batches during ACID streaming against Hive Metastore using Oracle DB fails",
            "Description": "{noformat}\n2016-05-25 00:43:49,682 INFO  [pool-4-thread-5]: txn.TxnHandler (TxnHandler.java:checkRetryable(1585)) - Non-retryable error: ORA-00933: SQL command not properly ended\n (SQLState=42000, ErrorCode=933)\n2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended\n\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)\n\tat oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)\n\tat oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)\n\tat oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)\n\tat oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)\n\tat oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)\n\tat com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy15.open_txns(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n)\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:438)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy15.open_txns(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: thrift.ProcessFunction (ProcessFunction.java:process(41)) - Internal error processing open_txns\nMetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended\n\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)\n\tat oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)\n\tat oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)\n\tat oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)\n\tat oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)\n\tat oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)\n\tat com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy15.open_txns(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n)\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:438)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy15.open_txns(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}\nI think the reason here is that\n{code:title=metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java|borderStyle=solid}\n  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {\n...\n        String query;\n        String insertClause = \"insert into TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host) values \";\n        StringBuilder valuesClause = new StringBuilder();\n\n        for (long i = first; i < first + numTxns; i++) {\n          txnIds.add(i);\n\n          if (i > first &&\n              (i - first) % conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE) == 0) {\n            // wrap up the current query, and start a new one\n            query = insertClause + valuesClause.toString();\n            queries.add(query);\n\n            valuesClause.setLength(0);\n            valuesClause.append(\"(\").append(i).append(\", 'o', \").append(now).append(\", \").append(now)\n                .append(\", '\").append(rqst.getUser()).append(\"', '\").append(rqst.getHostname())\n                .append(\"')\");\n\n            continue;\n          }\n\n          if (i > first) {\n            valuesClause.append(\", \");\n          }\n\n          valuesClause.append(\"(\").append(i).append(\", 'o', \").append(now).append(\", \").append(now)\n              .append(\", '\").append(rqst.getUser()).append(\"', '\").append(rqst.getHostname())\n              .append(\"')\");\n        }\n\n        query = insertClause + valuesClause.toString();\n...\n}\n{code}\nends up building a query of the form\n{code:sql}\nINSERT INTO TXNS (...) VALUES (...), (...)\n{code}\nOracle doesn't like this way of inserting multiple rows of data. Couple of ways the following [post|http://www.oratable.com/oracle-insert-all/] describe is either inserting each row individually or use the {{INSERT ALL}} semantics."
        }
    },
    {
        "filename": "HIVE-7374.json",
        "creation_time": "2014-07-09T16:02:15.000+0000",
        "bug_report": {
            "Title": "SHOW COMPACTIONS fail with remote metastore when there are no compations",
            "Description": "Prerequistes :\n1. Remote metastore\n2. No compactions\n\nIn CLI after doing this :\n{{show compactions;}}\nReturn error :\n{noformat}\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.thrift.transport.TTransportException\n{noformat}\n\nIn metatore logs :\n{noformat}\n2014-07-09 17:54:10,537 ERROR [pool-3-thread-20]: server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.\norg.apache.thrift.protocol.TProtocolException: Required field 'compacts' is unset! Struct:ShowCompactResponse(compacts:null)\n        at org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate(ShowCompactResponse.java:310)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.validate(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.write(ThriftHiveMetastore.java)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-12206.json",
        "creation_time": "2015-10-17T00:30:22.000+0000",
        "bug_report": {
            "Title": "ClassNotFound Exception during query compilation with Tez and Union query and GenericUDFs",
            "Description": "{noformat}\n-- union query without UDF\nexplain\nselect * from (select key + key from src limit 1) a\nunion all\nselect * from (select key + key from src limit 1) b;\n\nadd jar /tmp/udf-2.2.0-snapshot.jar;\ncreate temporary function myudf as 'com.aginity.amp.hive.udf.UniqueNumberGenerator';\n\n-- Now try the query with the UDF\nexplain\nselect myudf()from (select key from src limit 1) a\nunion all\nselect myudf() from (select key from src limit 1) a;\n{noformat}\n\nGot error:\n\n{noformat}\n2015-10-16 17:00:55,557 ERROR ql.Driver (SessionState.java:printError(963)) - FAILED: KryoException Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator\nSerialization trace:\ngenericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)\ncolExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)\nparentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)\nchildOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)\nchildOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)\norg.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator\nSerialization trace:\ngenericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)\ncolExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)\nparentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)\nchildOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)\nchildOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)\n        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138)\n        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:99)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672)\n        at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1081)\n        at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:970)\n        at org.apache.hadoop.hive.ql.exec.Utilities.cloneOperatorTree(Utilities.java:928)\n        at org.apache.hadoop.hive.ql.parse.GenTezUtils.removeUnionOperators(GenTezUtils.java:228)\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:373)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:205)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10193)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.lang.ClassNotFoundException: com.aginity.amp.hive.udf.UniqueNumberGenerator\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:270)\n        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136)\n        ... 64 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-10098.json",
        "creation_time": "2015-03-26T17:12:20.000+0000",
        "bug_report": {
            "Title": "HS2 local task for map join fails in KMS encrypted cluster",
            "Description": "Env: KMS was enabled after cluster was kerberos secured. \nProblem: PROBLEM: Any Hive query via beeline that performs a MapJoin fails with a java.lang.reflect.UndeclaredThrowableException  from KMSClientProvider.addDelegationTokens.\n\n{code}\n2015-03-18 08:49:17,948 INFO [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1022)) - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir \n2015-03-18 08:49:19,048 WARN [main]: security.UserGroupInformation (UserGroupInformation.java:doAs(1645)) - PriviledgedActionException as:hive (auth:KERBEROS) cause:org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) \n2015-03-18 08:49:19,050 ERROR [main]: mr.MapredLocalTask (MapredLocalTask.java:executeFromChildJVM(314)) - Hive Runtime Error: Map local work failed \njava.io.IOException: java.io.IOException: java.lang.reflect.UndeclaredThrowableException \nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:634) \nat org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:363) \nat org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:337) \nat org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:303)\nat org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:735) \nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \nat java.lang.reflect.Method.invoke(Method.java:606) \nat org.apache.hadoop.util.RunJar.main(RunJar.java:212) \nCaused by: java.io.IOException: java.lang.reflect.UndeclaredThrowableException \nat org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:826) \nat org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension.addDelegationTokens(KeyProviderDelegationTokenExtension.java:86) \nat org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2017) \nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121) \nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100) \nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80) \nat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205) \nat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313) \nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:413) \nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:559) \n... 9 more \nCaused by: java.lang.reflect.UndeclaredThrowableException \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1655) \nat org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:808) \n... 18 more \nCaused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) \nat org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:306) \nat org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:196) \nat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:127)\n{code}\n\nTo make sure map join happen, test need a small table join with a large one, for example:\n{code}\nCREATE TABLE if not exists jsmall (code string, des string, t int, s int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\nCREATE TABLE if not exists jbig1 (code string, des string, t int, s int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\nload data local inpath '/tmp/jdata' into table jsmall;\nload data local inpath '/tmp/jdata' into table jbig1;\nload data local inpath '/tmp/jdata' into table jbig1;\nload data local inpath '/tmp/jdata' into table jbig1;\nload data local inpath '/tmp/jdata' into table jbig1;\nload data local inpath '/tmp/jdata' into table jbig1;\nload data local inpath '/tmp/jdata' into table jbig1;\nload data local inpath '/tmp/jdata' into table jbig1;\n\nselect count(*) from jsmall small join jbig1 big on (small.code = big.code);\n{code}"
        }
    },
    {
        "filename": "HIVE-7745.json",
        "creation_time": "2014-08-16T01:20:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException when turn on hive.optimize.union.remove, hive.merge.mapfiles and hive.merge.mapredfiles [Spark Branch]",
            "Description": "When the hive.optimize.union.remove, hive.merge.mapfiles and hive.merge.mapredfiles are turned on, it throws NullPointerException when I do the following queries: \n\n{noformat}\ncreate table inputTbl1(key string, val string) stored as textfile;\ncreate table outputTbl1(key string, values bigint) stored as rcfile;\nload data local inpath '../../data/files/T1.txt' into table inputTbl1;\n\nexplain\ninsert overwrite table outputTbl1\nSELECT * FROM\n(\nselect key, count(1) as values from inputTbl1 group by key \nunion all\nselect * FROM (\n  SELECT key, 1 as values from inputTbl1 \n  UNION ALL\n  SELECT key, 2 as values from inputTbl1\n) a\n)b;\n{noformat}\nIf the hive.merge.mapfiles and hive.merge.mapredfiles are turned off, I do not see any error. \n\nHere is the stack trace:\n{noformat}\n2014-08-16 01:32:26,849 ERROR [main]: ql.Driver (SessionState.java:printError(681)) - FAILED: NullPointerException null\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(GenMapRedUtils.java:1738)\n        at org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.processFileSink(GenSparkUtils.java:281)\n        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:187)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:199)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9508)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)\n        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:414)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1005)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1070)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:197)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-11762.json",
        "creation_time": "2015-09-08T20:10:54.000+0000",
        "bug_report": {
            "Title": "TestHCatLoaderEncryption failures when using Hadoop 2.7",
            "Description": "When running TestHCatLoaderEncryption with -Dhadoop23.version=2.7.0, we get the following error during setup():\n\n{noformat}\ntestReadDataFromEncryptedHiveTableByPig[5](org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption)  Time elapsed: 3.648 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.hdfs.DFSClient.setKeyProvider(Lorg/apache/hadoop/crypto/key/KeyProviderCryptoExtension;)V\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Hadoop23Shims.java:534)\n\tat org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.initEncryptionShim(TestHCatLoaderEncryption.java:252)\n\tat org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.setup(TestHCatLoaderEncryption.java:200)\n{noformat}\n\nIt looks like between Hadoop 2.6 and Hadoop 2.7, the argument to DFSClient.setKeyProvider() changed:\n{noformat}\n   @VisibleForTesting\n-  public void setKeyProvider(KeyProviderCryptoExtension provider) {\n-    this.provider = provider;\n+  public void setKeyProvider(KeyProvider provider) {\n{noformat}"
        }
    },
    {
        "filename": "HIVE-6990.json",
        "creation_time": "2014-04-30T04:24:25.000+0000",
        "bug_report": {
            "Title": "Direct SQL fails when the explicit schema setting is different from the default one",
            "Description": "I got the following ERROR in hive.log\n2014-04-23 17:30:23,331 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(1756)) - Direct SQL failed, falling back to ORM\njavax.jdo.JDODataStoreException: Error executing SQL query \"select PARTITIONS.PART_ID from PARTITIONS  inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID   inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))\".\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)\n        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:98)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1806)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n        at java.lang.reflect.Method.invoke(Method.java:619)\n        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)\n        at com.sun.proxy.$Proxy11.getPartitionsByFilter(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:3310)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n        at java.lang.reflect.Method.invoke(Method.java:619)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)\n        at com.sun.proxy.$Proxy12.get_partitions_by_filter(Unknown Source)\n\n\nReproduce steps:\n1. set the following properties in hive-site.xml\n <property>\n  <name>javax.jdo.mapping.Schema</name>\n  <value>HIVE</value>\n </property>\n <property>\n  <name>javax.jdo.option.ConnectionUserName</name>\n  <value>user1</value>\n </property>\n\n2. execute hive queries\nhive> create table mytbl ( key int, value string);\nhive> load data local inpath 'examples/files/kv1.txt' overwrite into table mytbl;\nhive> select * from mytbl;\nhive> create view myview partitioned on (value) as select key, value from mytbl where key=98;\nhive> alter view myview add partition (value='val_98') partition (value='val_xyz');\nhive> alter view myview drop partition (value='val_xyz');"
        }
    },
    {
        "filename": "HIVE-7114.json",
        "creation_time": "2014-05-22T14:58:09.000+0000",
        "bug_report": {
            "Title": "Extra Tez session is started during HiveServer2 startup",
            "Description": "When starting the HiveServer2 we are seeing an extra Tez AM launched.\n\nThis is where it is getting created .\n{noformat}\n2014-05-09 23:11:22,261 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:addAdminUsers(588)) - No user is added in admin role, since config is empty\njava.lang.Exception: Opening session\n        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)\n        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)\n        at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)\n        at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)\n        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)\n        at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)\n        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)\n        at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)\n        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)\n        at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-11991.json",
        "creation_time": "2015-09-29T21:46:06.000+0000",
        "bug_report": {
            "Title": "groupby11.q failing on branch-1.0",
            "Description": "Running groupby11.q on the branch-1.0 branch hits an error:\n{noformat}\n2015-09-29 14:27:51,676 ERROR CliDriver (SessionState.java:printError(833)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text\njava.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)\n        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1621)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:221)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:153)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:364)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:299)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:832)\n        at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11(TestCliDriver.java:120)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at junit.framework.TestCase.runTest(TestCase.java:176)\n        at junit.framework.TestCase.runBare(TestCase.java:141)\n        at junit.framework.TestResult$1.protect(TestResult.java:122)\n        at junit.framework.TestResult.runProtected(TestResult.java:142)\n        at junit.framework.TestResult.run(TestResult.java:125)\n        at junit.framework.TestCase.run(TestCase.java:129)\n        at junit.framework.TestSuite.runTest(TestSuite.java:255)\n        at junit.framework.TestSuite.run(TestSuite.java:250)\n        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text\n        at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:85)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:572)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:564)\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)\n        ... 27 more\nCaused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text\n        at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)\n        at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:225)\n        at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:492)\n        at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:445)\n        at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)\n        at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:429)\n        at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:50)\n        at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:71)\n        at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:40)\n        at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)\n        ... 34 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-17900.json",
        "creation_time": "2017-10-25T17:12:42.000+0000",
        "bug_report": {
            "Title": "analyze stats on columns triggered by Compactor generates malformed SQL with > 1 partition column",
            "Description": "{noformat}\r\n2017-10-16 09:01:51,255 ERROR [haddl0007.mycenterpointenergy.com-51]: ql.Driver (SessionState.java:printError(993)) - FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement\r\norg.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement\r\n        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)\r\n        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)\r\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)\r\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)\r\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)\r\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1158)\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1148)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:294)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)\r\n\r\n2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=compile start=1508162511253 end=1508162511255 duration=2 from=org.apache.hadoop.hive.ql.Driver>\r\n2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: ql.Driver (Driver.java:compile(559)) - We are resetting the hadoop caller context to\r\n2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: log.PerfLogger (PerfLogger.java:PerfLogBegin(149)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\r\n2017-10-16 09:01:51,255 INFO  [haddl0007.mycenterpointenergy.com-51]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=releaseLocks start=1508162511255 end=1508162511255 duration=0 from=org.apache.hadoop.hive.ql.Driver>\r\n2017-10-16 09:01:51,256 INFO  [haddl0007.mycenterpointenergy.com-51]: tez.TezSessionPoolManager (TezSessionPoolManager.java:close(183)) - Closing tez session default? false\r\n2017-10-16 09:01:51,256 INFO  [haddl0007.mycenterpointenergy.com-51]: tez.TezSessionState (TezSessionState.java:close(294)) - Closing Tez Session\r\n2017-10-16 09:01:51,256 INFO  [haddl0007.mycenterpointenergy.com-51]: client.TezClient (TezClient.java:stop(518)) - Shutting down Tez Session, sessionName=HIVE-ae652f03-72c7-4ca8-a2d8-05dcc7392f4f, applicationId=application_1507779664083_0159\r\n2017-10-16 09:01:51,279 ERROR [haddl0007.mycenterpointenergy.com-51]: compactor.Worker (Worker.java:run(191)) - Caught exception while trying to compact id:3723,dbname:mobiusad,tableName:zces_img_data_small_pt,partName:month=201608/dates=9,state:^@,type:MAJOR,properties:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking failed to avoid repeated failures, java.io.IOException: Could not update stats for table mobiusad.zces_img_data_small_pt/month=201608/dates=9 due to: (40000,FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement,42000line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:296)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)\r\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)\r\n{noformat}\r\n"
        }
    },
    {
        "filename": "HIVE-10816.json",
        "creation_time": "2015-05-25T08:08:37.000+0000",
        "bug_report": {
            "Title": "NPE in ExecDriver::handleSampling when submitted via child JVM",
            "Description": "When {{hive.exec.submitviachild = true}}, parallel order by fails with NPE and falls back to single-reducer mode. Stack trace:\n{noformat}\n2015-05-25 08:41:04,446 ERROR [main]: mr.ExecDriver (ExecDriver.java:execute(386)) - Sampling error\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-13017.json",
        "creation_time": "2016-02-05T23:40:09.000+0000",
        "bug_report": {
            "Title": "Child process of HiveServer2 fails to get delegation token from non default FileSystem",
            "Description": "The following query fails, when Azure Filesystem is used as default file system, and HDFS is used for intermediate data.\n\n{noformat}\n>>>  create temporary table s10k stored as orc as select * from studenttab10k;\n>>>  create temporary table v10k as select * from votertab10k;\n>>>  select registration \nfrom s10k s join v10k v \non (s.name = v.name) join studentparttab30k p \non (p.name = v.name) \nwhere s.age < 25 and v.age < 25 and p.age < 25;\nERROR : Execution failed with exit status: 2\nERROR : Obtaining error information\nERROR : \nTask failed!\nTask ID:\n  Stage-5\n\nLogs:\n\nERROR : /var/log/hive/hiveServer2.log\nError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)\nAborting command set because \"force\" is false and command failed: \"select registration \nfrom s10k s join v10k v \non (s.name = v.name) join studentparttab30k p \non (p.name = v.name) \nwhere s.age < 25 and v.age < 25 and p.age < 25;\"\nClosing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice\nhiveServer2.log shows:\n2016-02-02 18:04:34,182 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,199 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,212 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie\n2016-02-02 18:04:34,213 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:checkConcurrency(168)) - Concurrency mode is disabled, not creating a lock manager\n2016-02-02 18:04:34,219 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal\n2016-02-02 18:04:34,219 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,225 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1390)) - Setting caller context to query id hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0\n2016-02-02 18:04:34,226 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1393)) - Starting command(queryId=hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0): select registration\nfrom s10k s join v10k v\non (s.name = v.name) join studentparttab30k p\non (p.name = v.name)\nwhere s.age < 25 and v.age < 25 and p.age < 25\n2016-02-02 18:04:34,228 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:<init>(90)) - Created ATS Hook\n2016-02-02 18:04:34,229 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,237 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa\n2016-02-02 18:04:34,238 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=PreHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436274229 end=1454436274238 duration=9 from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,239 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,240 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=PreHook.org.apache.hadoop.hive.ql.security.authorization.plugin.DisallowTransformHook start=1454436274239 end=1454436274240 duration=1 from=org.apache.hadoop.hive.ql.Driver>\nQuery ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0\n2016-02-02 18:04:34,242 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Query ID = hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0\nTotal jobs = 1\n2016-02-02 18:04:34,243 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printInfo(923)) - Total jobs = 1\n2016-02-02 18:04:34,245 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=TimeToSubmit start=1454436274199 end=1454436274245 duration=46 from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,246 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,247 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=task.MAPREDLOCAL.Stage-5 from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:34,258 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:launchTask(1718)) - Starting task [Stage-5:MAPREDLOCAL] in serial mode\n2016-02-02 18:04:34,280 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(158)) - Generating plan file file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml\n2016-02-02 18:04:34,288 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>\n2016-02-02 18:04:34,289 INFO  [HiveServer2-Background-Pool: Thread-517]: exec.Utilities (Utilities.java:serializePlan(1028)) - Serializing MapredLocalWork via kryo\n2016-02-02 18:04:34,290 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPreHookEvent(158)) - Received pre-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0\n2016-02-02 18:04:34,358 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=serializePlan start=1454436274288 end=1454436274358 duration=70 from=org.apache.hadoop.hive.ql.exec.Utilities>\n2016-02-02 18:04:34,737 INFO  [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(287)) - Executing: /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop jar /usr/hdp/2.4.1.0-170/hive/lib/hive-common-1.2.1000.2.4.1.0-170.jar org.apache.hadoop.hive.ql.exec.mr.ExecDriver -localtask -plan file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10006/plan.xml   -jobconffile file:/tmp/hive/916e3dbb-a10d-4888-a063-52fb058ea421/hive_2016-02-02_18-04-29_153_625340340820843828-4/-local-10007/jobconf.xml\nWARNING: Use \"yarn jar\" to launch YARN applications.\n2016-02-02 18:04:37,450 INFO  [org.apache.ranger.audit.queue.AuditBatchQueue0]: provider.BaseAuditHandler (BaseAuditHandler.java:logStatus(312)) - Audit Status Log: name=hiveServer2.async.summary.batch.solr, interval=01:21.012 minutes, events=2, succcessCount=2, totalEvents=4, totalSuccessCount=4\nExecution log at: /tmp/hive/hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0.log\n2016-02-02 18:04:39,248 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(127)) - Could not validate cookie sent, will try to generate a new cookie\n2016-02-02 18:04:39,254 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(352)) - Failed to authenticate with http/_HOST kerberos principal, trying with hive/_HOST kerberos principal\n2016-02-02 18:04:39,261 INFO  [HiveServer2-HttpHandler-Pool: Thread-55]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(169)) - Cookie added for clientUserName hrt_qa\n2016-02-02 18:04:40     Starting to launch local task to process map join;      maximum memory = 477102080\nExecution failed with exit status: 2\n2016-02-02 18:04:43,728 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Execution failed with exit status: 2\nObtaining error information\n2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - Obtaining error information\n\nTask failed!\nTask ID:\n  Stage-5\n\nLogs:\n\n2016-02-02 18:04:43,730 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) -\nTask failed!\nTask ID:\n  Stage-5\n\nLogs:\n\n/var/log/hive/hiveServer2.log\n2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: exec.Task (SessionState.java:printError(932)) - /var/log/hive/hiveServer2.log\n2016-02-02 18:04:43,732 ERROR [HiveServer2-Background-Pool: Thread-517]: mr.MapredLocalTask (MapredLocalTask.java:executeInChildVM(307)) - Execution failed with exit status: 2\n2016-02-02 18:04:43,733 INFO  [HiveServer2-Background-Pool: Thread-517]: hooks.ATSHook (ATSHook.java:<init>(90)) - Created ATS Hook\n2016-02-02 18:04:43,734 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:43,736 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1454436283734 end=1454436283736 duration=2 from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:43,736 INFO  [ATS Logger 0]: hooks.ATSHook (ATSHook.java:createPostHookEvent(193)) - Received post-hook notification for :hive_20160202180429_7ffff6ab-64d6-4c89-88b0-6355cc5acbd0\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n2016-02-02 18:04:43,757 ERROR [HiveServer2-Background-Pool: Thread-517]: ql.Driver (SessionState.java:printError(932)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n2016-02-02 18:04:43,758 INFO  [HiveServer2-Background-Pool: Thread-517]: ql.Driver (Driver.java:execute(1621)) - Resetting the caller context to HIVE_SSN_ID:916e3dbb-a10d-4888-a063-52fb058ea421\n2016-02-02 18:04:43,759 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=Driver.execute start=1454436274219 end=1454436283759 duration=9540 from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:43,760 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:43,761 INFO  [HiveServer2-Background-Pool: Thread-517]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - </PERFLOG method=releaseLocks start=1454436283760 end=1454436283761 duration=1 from=org.apache.hadoop.hive.ql.Driver>\n2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nhive Configs can be viewed from http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/artifacts/tmpModifyConfDir_1454394513592/\nAttachments\nDrop files to attach, or browse.\nAdd LinkIssue Links\nrelates to\nBug - A problem which impairs or prevents the functions of the product. HIVE-739 webhcat tests failing in HDInsight secure cluster throwing NullPointerException\t Blocker - Blocks development and/or testing work, production could not run. RESOLVED\nActivity\nAll\nComments\nWork Log\nHistory\nActivity\nAscending order - Click to sort in descending order\nPermalink Edit Delete \ntsaito Takahiko Saito added a comment - 3 days ago\nThe test passes via hive CLI and explain shows:\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> explain select registration\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> from s10k s join v10k v\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (s.name = v.name) join studentparttab30k p\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (p.name = v.name)\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> where s.age < 25 and v.age < 25 and p.age < 25;\n+--------------------------------------------------------------------------------------------------------------+--+\n|                                                   Explain                                                    |\n+--------------------------------------------------------------------------------------------------------------+--+\n| STAGE DEPENDENCIES:                                                                                          |\n|   Stage-5 is a root stage                                                                                    |\n|   Stage-4 depends on stages: Stage-5                                                                         |\n|   Stage-0 depends on stages: Stage-4                                                                         |\n|                                                                                                              |\n| STAGE PLANS:                                                                                                 |\n|   Stage: Stage-5                                                                                             |\n|     Map Reduce Local Work                                                                                    |\n|       Alias -> Map Local Tables:                                                                             |\n|         s                                                                                                    |\n|           Fetch Operator                                                                                     |\n|             limit: -1                                                                                        |\n|         v                                                                                                    |\n|           Fetch Operator                                                                                     |\n|             limit: -1                                                                                        |\n|       Alias -> Map Local Operator Tree:                                                                      |\n|         s                                                                                                    |\n|           TableScan                                                                                          |\n|             alias: s                                                                                         |\n|             filterExpr: (name is not null and (age < 25)) (type: boolean)                                    |\n|             Statistics: Num rows: 459 Data size: 47777 Basic stats: COMPLETE Column stats: NONE              |\n|             Filter Operator                                                                                  |\n|               predicate: (name is not null and (age < 25)) (type: boolean)                                   |\n|               Statistics: Num rows: 76 Data size: 7910 Basic stats: COMPLETE Column stats: NONE              |\n|               HashTable Sink Operator                                                                        |\n|                 keys:                                                                                        |\n|                   0 name (type: string)                                                                      |\n|                   1 name (type: string)                                                                      |\n|                   2 name (type: string)                                                                      |\n|         v                                                                                                    |\n|           TableScan                                                                                          |\n|             alias: v                                                                                         |\n|             filterExpr: (name is not null and (age < 25)) (type: boolean)                                    |\n|             Statistics: Num rows: 1653 Data size: 337233 Basic stats: COMPLETE Column stats: NONE            |\n|             Filter Operator                                                                                  |\n|               predicate: (name is not null and (age < 25)) (type: boolean)                                   |\n|               Statistics: Num rows: 275 Data size: 56103 Basic stats: COMPLETE Column stats: NONE            |\n|               HashTable Sink Operator                                                                        |\n|                 keys:                                                                                        |\n|                   0 name (type: string)                                                                      |\n|                   1 name (type: string)                                                                      |\n|                   2 name (type: string)                                                                      |\n|                                                                                                              |\n|   Stage: Stage-4                                                                                             |\n|     Map Reduce                                                                                               |\n|       Map Operator Tree:                                                                                     |\n|           TableScan                                                                                          |\n|             alias: p                                                                                         |\n|             filterExpr: (name is not null and (age < 25)) (type: boolean)                                    |\n|             Statistics: Num rows: 30000 Data size: 627520 Basic stats: COMPLETE Column stats: COMPLETE       |\n|             Filter Operator                                                                                  |\n|               predicate: (name is not null and (age < 25)) (type: boolean)                                   |\n|               Statistics: Num rows: 10000 Data size: 1010000 Basic stats: COMPLETE Column stats: COMPLETE    |\n|               Map Join Operator                                                                              |\n|                 condition map:                                                                               |\n|                      Inner Join 0 to 1                                                                       |\n|                      Inner Join 1 to 2                                                                       |\n|                 keys:                                                                                        |\n|                   0 name (type: string)                                                                      |\n|                   1 name (type: string)                                                                      |\n|                   2 name (type: string)                                                                      |\n|                 outputColumnNames: _col8                                                                     |\n|                 Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE      |\n|                 Select Operator                                                                              |\n|                   expressions: _col8 (type: string)                                                          |\n|                   outputColumnNames: _col0                                                                   |\n|                   Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE    |\n|                   File Output Operator                                                                       |\n|                     compressed: false                                                                        |\n|                     Statistics: Num rows: 22000 Data size: 2222000 Basic stats: COMPLETE Column stats: NONE  |\n|                     table:                                                                                   |\n|                         input format: org.apache.hadoop.mapred.TextInputFormat                               |\n|                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat            |\n|                         serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                            |\n|       Local Work:                                                                                            |\n|         Map Reduce Local Work                                                                                |\n|                                                                                                              |\n|   Stage: Stage-0                                                                                             |\n|     Fetch Operator                                                                                           |\n|       limit: -1                                                                                              |\n|       Processor Tree:                                                                                        |\n|         ListSink                                                                                             |\n|                                                                                                              |\n+--------------------------------------------------------------------------------------------------------------+--+\n83 rows selected (2.473 seconds)\nPermalink Edit Delete \ntsaito Takahiko Saito added a comment - 3 days ago\nemptablemisc_8 also fails with the same error:\n>>>  create temporary table temp1 as select * from votertab10k;\n>>>  select * \nfrom studenttab10k s \nwhere s.name not in \n(select name from temp1);\nINFO  : Number of reduce tasks determined at compile time: 1\nINFO  : In order to change the average load for a reducer (in bytes):\nINFO  :   set hive.exec.reducers.bytes.per.reducer=<number>\nINFO  : In order to limit the maximum number of reducers:\nINFO  :   set hive.exec.reducers.max=<number>\nINFO  : In order to set a constant number of reducers:\nINFO  :   set mapreduce.job.reduces=<number>\nINFO  : number of splits:1\nINFO  : Submitting tokens for job: job_1454394534358_0164\nINFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 246 for hrt_qa)\nINFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/\nINFO  : Starting Job = job_1454394534358_0164, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0164/\nINFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0164\nINFO  : Hadoop job information for Stage-4: number of mappers: 1; number of reducers: 1\nINFO  : 2016-02-02 10:11:02,367 Stage-4 map = 0%,  reduce = 0%\nINFO  : 2016-02-02 10:11:26,060 Stage-4 map = 100%,  reduce = 0%, Cumulative CPU 5.48 sec\nINFO  : 2016-02-02 10:11:39,024 Stage-4 map = 100%,  reduce = 100%, Cumulative CPU 11.18 sec\nINFO  : MapReduce Total cumulative CPU time: 11 seconds 180 msec\nINFO  : Ended Job = job_1454394534358_0164\nINFO  : Stage-9 is selected by condition resolver.\nINFO  : Stage-1 is filtered out by condition resolver.\nERROR : Execution failed with exit status: 2\nERROR : Obtaining error information\nERROR : \nTask failed!\nTask ID:\n  Stage-9\n\nLogs:\n\nERROR : /var/log/hive/hiveServer2.log\nINFO  : Number of reduce tasks determined at compile time: 1\nINFO  : In order to change the average load for a reducer (in bytes):\nINFO  :   set hive.exec.reducers.bytes.per.reducer=<number>\nINFO  : In order to limit the maximum number of reducers:\nINFO  :   set hive.exec.reducers.max=<number>\nINFO  : In order to set a constant number of reducers:\nINFO  :   set mapreduce.job.reduces=<number>\nINFO  : number of splits:2\nINFO  : Submitting tokens for job: job_1454394534358_0169\nINFO  : Kind: HDFS_DELEGATION_TOKEN, Service: 10.0.0.36:8020, Ident: (HDFS_DELEGATION_TOKEN token 252 for hrt_qa)\nINFO  : The url to track the job: http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/\nINFO  : Starting Job = job_1454394534358_0169, Tracking URL = http://hn0-hs21-h.hdinsight.net:8088/proxy/application_1454394534358_0169/\nINFO  : Kill Command = /usr/hdp/2.4.1.0-170/hadoop/bin/hadoop job  -kill job_1454394534358_0169\nINFO  : Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\nINFO  : 2016-02-02 10:16:38,027 Stage-1 map = 0%,  reduce = 0%\nINFO  : 2016-02-02 10:16:52,498 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 3.8 sec\nINFO  : 2016-02-02 10:16:53,566 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.94 sec\nINFO  : 2016-02-02 10:17:16,202 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 17.04 sec\nINFO  : MapReduce Total cumulative CPU time: 17 seconds 40 msec\nINFO  : Ended Job = job_1454394534358_0169\nERROR : Execution failed with exit status: 2\nERROR : Obtaining error information\nERROR : \nTask failed!\nTask ID:\n  Stage-8\n\nLogs:\n\nERROR : /var/log/hive/hiveServer2.log\nError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)\nAborting command set because \"force\" is false and command failed: \"select * \nfrom studenttab10k s \nwhere s.name not in \n(select name from temp1);\"\nClosing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice\nIts app log can be viewed at http://qelog.hortonworks.com/log/hs21-hbs24-1454382123/app-logs/application_1454394534358_0169.log\nPermalink Edit Delete \ntsaito Takahiko Saito added a comment - 3 days ago\nThe query works without 'temporary table':\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> select registration from studenttab10k s join votertab10k v\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (s.name = v.name) join studentparttab30k p\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> on (p.name = v.name)\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> where s.age < 25 and v.age < 25 and p.age < 25;\nPermalink Edit Delete \ntsaito Takahiko Saito added a comment - 3 days ago\nMore info about temporary table:\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> describe formatted s10k;\n+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+\n|           col_name            |                                                                  data_type                                                                  |        comment        |\n+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+\n| # col_name                    | data_type                                                                                                                                   | comment               |\n|                               | NULL                                                                                                                                        | NULL                  |\n| name                          | string                                                                                                                                      |                       |\n| age                           | int                                                                                                                                         |                       |\n| gpa                           | double                                                                                                                                      |                       |\n|                               | NULL                                                                                                                                        | NULL                  |\n| # Detailed Table Information  | NULL                                                                                                                                        | NULL                  |\n| Database:                     | default                                                                                                                                     | NULL                  |\n| Owner:                        | hrt_qa                                                                                                                                      | NULL                  |\n| CreateTime:                   | Tue Feb 02 23:02:31 UTC 2016                                                                                                                | NULL                  |\n| LastAccessTime:               | UNKNOWN                                                                                                                                     | NULL                  |\n| Protect Mode:                 | None                                                                                                                                        | NULL                  |\n| Retention:                    | 0                                                                                                                                           | NULL                  |\n| Location:                     | hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c  | NULL                  |\n| Table Type:                   | MANAGED_TABLE                                                                                                                               | NULL                  |\n|                               | NULL                                                                                                                                        | NULL                  |\n| # Storage Information         | NULL                                                                                                                                        | NULL                  |\n| SerDe Library:                | org.apache.hadoop.hive.ql.io.orc.OrcSerde                                                                                                   | NULL                  |\n| InputFormat:                  | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat                                                                                             | NULL                  |\n| OutputFormat:                 | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat                                                                                            | NULL                  |\n| Compressed:                   | No                                                                                                                                          | NULL                  |\n| Num Buckets:                  | -1                                                                                                                                          | NULL                  |\n| Bucket Columns:               | []                                                                                                                                          | NULL                  |\n| Sort Columns:                 | []                                                                                                                                          | NULL                  |\n| Storage Desc Params:          | NULL                                                                                                                                        | NULL                  |\n|                               | serialization.format                                                                                                                        | 1                     |\n+-------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--+\n26 rows selected (0.22 seconds)\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> set hive.server2.enable.doAs;\n+---------------------------------+--+\n|               set               |\n+---------------------------------+--+\n| hive.server2.enable.doAs=false  |\n+---------------------------------+--+\nPermalink Edit Delete \ntsaito Takahiko Saito added a comment - 3 days ago\nhdfs dir of temporary table is owned by hive as expected since hive.server2.enable.doAs=false:\nhdfs@hn0-hs21-h:~$ hdfs dfs -ls hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c\nFound 1 items\n-rwx------   3 hive hdfs      47777 2016-02-02 23:02 hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c/000000_0\nNot sure if it's related to the JIRA, but one thing I noticed was that dfs cmd throws error via beeline:\n0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181> dfs -ls hdfs://hn0-hs21-h.hdinsight.net:8020/tmp/hive/hive/de2667ea-5bc1-4548-a4c4-97e46d6081f8/_tmp_space.db/8e8482c9-a2a4-4e8c-ad0f-2cd6e8fcdb4c;\nError: Error while processing statement: Permission denied: user [hrt_qa] does not have privilege for [DFS] command (state=,code=1)\nPermalink Edit \nthejas Thejas Nair added a comment - 5 hours ago - edited\nTakahiko Saito \nThe above DFS error would be from Ranger of SQL standard authorization. It is not related to the test failure.\nBut you are right that it looks like a permission issue.\nThe issue seen in HIVE-739 also exists in this part of Hive. Hive in MR mode launches a new child process to process the small table and create a hash table. This child process needs credentials from HDFS. However, in this setup, azure fs is the default file system. We should also get delegation token from all FS URIs listed under mapreduce.job.hdfs-servers config.\nThere are hueristics around when map-join gets used. That is why you don't see it unless temp table is used (that uses ORC format and might have stats as well, while the original table is probaly text format).\nhttps://github.com/hortonworks/hive/blob/2.4-maint/ql/src/java/org/apache/hadoop/hive/ql/exec/SecureCmdDoAs.java needs change similar to webhcat change in HIVE-739\nPermalink Edit \nsush Sushanth Sowmyan added a comment - 7 minutes ago\nTakahiko Saito, we should file an apache jira for this as well. Would you like to do so, so the bug report has attribution to you?\nPermalink Edit Delete \ntsaito Takahiko Saito added a comment - 4 minutes ago\nSushanth Sowmyan I will file one and update with that JIRA.\n Comment\t\nPeople\nAssignee:\t sush Sushanth Sowmyan\nAssign to me\nReporter:\t tsaito Takahiko Saito\nQEAssignee:\tTakahiko Saito\nVotes:\t0\nWatchers:\t3 Stop watching this issue \nDates\nCreated:\t3 days ago\nUpdated:\t4 minutes ago\nWho's Looking?\n\nAgile\nView on Board\n{noformat}\n\nhivesever2 log shows:\n{noformat}\n2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-11303.json",
        "creation_time": "2015-07-18T01:31:02.000+0000",
        "bug_report": {
            "Title": "Getting Tez LimitExceededException after dag execution on large query",
            "Description": "{noformat}\n2015-07-17 18:18:11,830 INFO  [main]: counters.Limits (Limits.java:ensureInitialized(59)) - Counter limits initialized with parameters:  GROUP_NAME_MAX=256, MAX_GROUPS=500, COUNTER_NAME_MAX=64, MAX_COUNTERS=1200\n2015-07-17 18:18:11,841 ERROR [main]: exec.Task (TezTask.java:execute(189)) - Failed to execute tez graph.\norg.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200\n        at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)\n        at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)\n        at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)\n        at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)\n        at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)\n        at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)\n        at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)\n        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-5899.json",
        "creation_time": "2013-11-27T02:48:39.000+0000",
        "bug_report": {
            "Title": "NPE during explain extended with char/varchar columns",
            "Description": "Running analyze table .. for columns with char/varchar columns and subsequently trying to run explain extended will get a NullPointerException when Hive tries to annotate the operator tree with stats:\n2013-11-26 01:53:06,682 ERROR ql.Driver (SessionState.java:printError(440)) - FAILED: NullPointerException null\njava.lang.NullPointerException\nat org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:367)\nat org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats(ColumnStatisticsData.java:444)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:414)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStatsForColumn(StatsUtils.java:369)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:465)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:109)\nat org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:102)\nat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)\nat org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)\nat org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:76)\nat org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:136)\nat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8913)\nat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)\nat org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)\nat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:341)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:994)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:905)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
        }
    },
    {
        "filename": "HIVE-11102.json",
        "creation_time": "2015-06-24T22:54:55.000+0000",
        "bug_report": {
            "Title": "ReaderImpl: getColumnIndicesFromNames does not work for some cases",
            "Description": "ORC reader impl does not estimate the size of ACID data files correctly.\n\n{code}\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0\n\tat java.util.Collections$EmptyList.get(Collections.java:3212)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:938)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:847)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:713)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n{code}"
        }
    },
    {
        "filename": "HIVE-9195.json",
        "creation_time": "2014-12-23T01:08:45.000+0000",
        "bug_report": {
            "Title": "CBO changes constant to column type",
            "Description": "Making testcase for HIVE-8613, I've found CBO changes constant expr to column expr. For example (only in test mode).\n{code}\nCREATE TABLE bucket (key double, value string) CLUSTERED BY (key) SORTED BY (key DESC)  INTO 4 BUCKETS STORED AS TEXTFILE;\nload data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket;\nload data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket;\nload data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket;\nload data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket;\n\nselect percentile_approx(case when key < 100 then cast('NaN' as double) else key end, 0.5) from bucket;\n{code}\n\nIt works in shell but in TestCliDriver, that induces argument type exception creating udaf evaluator, which expects constant OI for second argument.\n{noformat}\n2014-12-22 17:03:31,433 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:analyzeInternal(10102)) - CBO failed, skipping CBO.\norg.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.\n        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:146)\n        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:1160)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:3794)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:4467)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5536)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8884)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9745)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9638)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10086)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:877)\n        at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23(TestCliDriver.java:120)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-11285.json",
        "creation_time": "2015-07-16T20:46:22.000+0000",
        "bug_report": {
            "Title": "ObjectInspector for partition columns in FetchOperator in SMBJoin causes exception",
            "Description": "STEPS TO REPRODUCE:\n{noformat}\n*$ cat data.out \n1|One \n2|Two\n{noformat}\n{code:sql}\nhql> \nCREATE TABLE data_table (key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'; \nLOAD DATA LOCAL INPATH '${system:user.dir}/data.out' INTO TABLE data_table;\nCREATE TABLE smb_table (key INT, value STRING) \nCLUSTERED BY (key) \nSORTED BY (key) INTO 1 BUCKETS \nSTORED AS ORC;\nCREATE TABLE smb_table_part (key INT, value STRING) \nPARTITIONED BY (p1 DECIMAL) \nCLUSTERED BY (key) \nSORTED BY (key) INTO 1 BUCKETS \nSTORED AS ORC;\nINSERT OVERWRITE TABLE smb_table SELECT * FROM data_table; \nINSERT OVERWRITE TABLE smb_table_part PARTITION (p1) SELECT key, value, 100 as p1 FROM data_table;\nSET hive.execution.engine=mr; \nSET hive.enforce.sortmergebucketmapjoin=false; \nSET hive.auto.convert.sortmerge.join=true; \nSET hive.optimize.bucketmapjoin = true; \nSET hive.optimize.bucketmapjoin.sortedmerge = true; \nSET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\nSELECT s1.key, s2.p1 \nFROM smb_table s1 \nINNER JOIN smb_table_part s2 \nON s1.key = s2.key \nORDER BY s1.key;\n{code}\nERROR:\n{noformat}\n2015-07-15 13:39:04,333 WARN main org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row\n{\"key\":1,\"value\":\"One\"}\n\nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185) \nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) \nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450) \nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) \nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:415) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) \nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) \nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row\n{\"key\":1,\"value\":\"One\"}\n\nat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503) \nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176) \n... 8 more \nCaused by: java.lang.RuntimeException: Map local work failed \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:569) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchNextGroup(SMBMapJoinOperator.java:429) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:260) \nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) \nat org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120) \nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) \nat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) \nat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) \nat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493) \n... 9 more \nCaused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Integer \nat org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(JavaIntObjectInspector.java:35) \nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:305) \nat org.apache.hadoop.hive.ql.exec.JoinUtil.computeValues(JoinUtil.java:193) \nat org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getFilteredValue(CommonJoinOperator.java:408) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:270) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:558) \n... 17 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-10288.json",
        "creation_time": "2015-04-09T23:24:16.000+0000",
        "bug_report": {
            "Title": "Cannot call permanent UDFs",
            "Description": "Just pulled the trunk and built the hive binary. If I create a permanent udf and exit the cli, and then open the cli and try calling the udf it fails with the exception below. However, the call succeeds if I call the udf right after registering the permanent udf (without exiting the cli). The call also succeeds with the apache-hive-1.0.0 release.\n\n{code}\n15-04-13 17:04:54,004 INFO  org.apache.hadoop.hive.ql.log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=parse start=1428969893115 end=1428969894004 duration=889 from=org.apache.hadoop.hive.ql.Driver>\n2015-04-13 17:04:54,007 DEBUG org.apache.hadoop.hive.ql.Driver (Driver.java:recordValidTxns(939)) - Encoding valid txns info 9223372036854775807:\n2015-04-13 17:04:54,007 INFO  org.apache.hadoop.hive.ql.log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>\n2015-04-13 17:04:54,052 INFO  org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:analyzeInternal(9997)) - Starting Semantic Analysis\n2015-04-13 17:04:54,053 DEBUG org.apache.hadoop.hive.ql.exec.FunctionRegistry (FunctionRegistry.java:getGenericUDAFResolver(942)) - Looking up GenericUDAF: hour_now\n2015-04-13 17:04:54,053 INFO  org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:genResolvedParseTree(9980)) - Completed phase 1 of Semantic Analysis\n2015-04-13 17:04:54,053 INFO  org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1530)) - Get metadata for source tables\n2015-04-13 17:04:54,054 INFO  org.apache.hadoop.hive.metastore.HiveMetaStore (HiveMetaStore.java:logInfo(744)) - 0: get_table : db=default tbl=test_table\n2015-04-13 17:04:54,054 INFO  org.apache.hadoop.hive.metastore.HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(369)) - ugi=nyigitbasi\tip=unknown-ip-addr\tcmd=get_table : db=default tbl=test_table\n2015-04-13 17:04:54,054 DEBUG org.apache.hadoop.hive.metastore.ObjectStore (ObjectStore.java:debugLog(6776)) - Open transaction: count = 1, isActive = true at:\n\torg.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:927)\n2015-04-13 17:04:54,054 DEBUG org.apache.hadoop.hive.metastore.ObjectStore (ObjectStore.java:debugLog(6776)) - Open transaction: count = 2, isActive = true at:\n\torg.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:990)\n2015-04-13 17:04:54,104 DEBUG org.apache.hadoop.hive.metastore.ObjectStore (ObjectStore.java:debugLog(6776)) - Commit transaction: count = 1, isactive true at:\n\torg.apache.hadoop.hive.metastore.ObjectStore.getMTable(ObjectStore.java:998)\n2015-04-13 17:04:54,232 DEBUG org.apache.hadoop.hive.metastore.ObjectStore (ObjectStore.java:debugLog(6776)) - Commit transaction: count = 0, isactive true at:\n\torg.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:929)\n2015-04-13 17:04:54,242 INFO  org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1682)) - Get metadata for subqueries\n2015-04-13 17:04:54,247 INFO  org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1706)) - Get metadata for destination tables\n2015-04-13 17:04:54,256 INFO  org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:genResolvedParseTree(9984)) - Completed getting MetaData in Semantic Analysis\n2015-04-13 17:04:54,259 INFO  org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer (CalcitePlanner.java:canHandleAstForCbo(369)) - Not invoking CBO because the statement has too few joins\n2015-04-13 17:04:54,344 DEBUG org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe (LazySimpleSerDe.java:initialize(135)) - org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[_c0, _c1] columnTypes=[int, int] separator=[[B@6e6d4780] nullstring=\\N lastColumnTakesRest=false timestampFormats=null\n2015-04-13 17:04:54,406 DEBUG org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:genTablePlan(9458)) - Created Table Plan for test_table TS[0]\n2015-04-13 17:04:54,410 DEBUG org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:genBodyPlan(8815)) - RR before GB test_table{(_c0,_c0: int)(_c1,_c1: int)(block__offset__inside__file,BLOCK__OFFSET__INSIDE__FILE: bigint)(input__file__name,INPUT__FILE__NAME: string)(row__id,ROW__ID: struct<transactionid:bigint,bucketid:int,rowid:bigint>)}  after GB test_table{(_c0,_c0: int)(_c1,_c1: int)(block__offset__inside__file,BLOCK__OFFSET__INSIDE__FILE: bigint)(input__file__name,INPUT__FILE__NAME: string)(row__id,ROW__ID: struct<transactionid:bigint,bucketid:int,rowid:bigint>)}\n2015-04-13 17:04:54,410 DEBUG org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:genSelectPlan(3608)) - tree: (TOK_SELECT (TOK_SELEXPR (TOK_FUNCTION hour_now)))\n2015-04-13 17:04:54,413 DEBUG org.apache.hadoop.hive.ql.parse.CalcitePlanner (SemanticAnalyzer.java:genSelectPlan(3718)) - genSelectPlan: input = test_table{(_c0,_c0: int)(_c1,_c1: int)(block__offset__inside__file,BLOCK__OFFSET__INSIDE__FILE: bigint)(input__file__name,INPUT__FILE__NAME: string)(row__id,ROW__ID: struct<transactionid:bigint,bucketid:int,rowid:bigint>)}  starRr = null\n2015-04-13 17:04:54,435 ERROR org.apache.hadoop.hive.ql.Driver (SessionState.java:printError(958)) - FAILED: NullPointerException null\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:232)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1048)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1265)\n\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:205)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:149)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10383)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10338)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3815)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3594)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8864)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8819)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9663)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9556)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:9992)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:306)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10003)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:195)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n{code}"
        }
    },
    {
        "filename": "HIVE-8771.json",
        "creation_time": "2014-11-07T00:30:25.000+0000",
        "bug_report": {
            "Title": "Abstract merge file operator does not move/rename incompatible files correctly",
            "Description": "AbstractFileMergeOperator moves incompatible files (files which cannot be merged) to final destination. The destination path must be directory instead of file. This causes orc_merge_incompat2.q to fail under CentOS with IOException failing to rename/move files.\nStack trace:\n{code}\n2014-11-05 02:38:56,588 DEBUG fs.FileSystem (RawLocalFileSystem.java:rename(337)) - Falling through to a copy of file:/home/prasanth/hive/itests/qtest/target/warehouse/orc_merge5a/st=80.0/000000_0 to file:/home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0/000000_0\n2014-11-05 02:38:56,589 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.\n2014-11-05 02:38:56,590 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local1144733438_0036\njava.lang.Exception: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator\n        at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:100)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:679)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator\n        at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:233)\n        at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.closeOp(OrcFileMergeOperator.java:220)\n        at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:98)\n        ... 10 more\nCaused by: java.io.FileNotFoundException: Destination exists and is not a directory: /home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0\n        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:423)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:267)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:257)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n        at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:339)\n        at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:507)\n        at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)\n        at org.apache.hadoop.fs.ProxyFileSystem.rename(ProxyFileSystem.java:177)\n        at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)\n        at org.apache.hadoop.hive.ql.exec.Utilities.renameOrMoveFiles(Utilities.java:1589)\n        at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:218)\n        ... 12 more\n\n{code}"
        }
    },
    {
        "filename": "HIVE-8008.json",
        "creation_time": "2014-09-05T23:00:11.000+0000",
        "bug_report": {
            "Title": "NPE while reading null decimal value",
            "Description": "Say you have this table {{dec_test}}:\n{code}\ndec                 \tdecimal(10,0)       \t                    \n{code}\n\nIf the table has a row that is 9999999999.5, and if we do\n\n{code}\nselect * from dec_test;\n{code}\n\nit will crash with NPE:\n\n{code}\n2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\njava.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)\n  at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)\n  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)\n  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)\n  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)\n  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\n  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:606)\n  at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)\n  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)\n  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)\n  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)\n  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)\n  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)\n  ... 12 more\nCaused by: java.lang.NullPointerException\n  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)\n  at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)\n  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)\n  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)\n  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)\n  ... 19 more\n{code}"
        }
    },
    {
        "filename": "HIVE-6915.json",
        "creation_time": "2014-04-15T20:20:15.000+0000",
        "bug_report": {
            "Title": "Hive Hbase queries fail on secure Tez cluster",
            "Description": "Hive queries reading and writing to HBase are currently failing with the following exception in a secure Tez cluster:\n{noformat}\n2014-04-14 13:47:05,644 FATAL [InputInitializer [Map 1] #0] org.apache.hadoop.ipc.RpcClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.\njavax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n\tat com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)\n\tat org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:918)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:915)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:915)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1065)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1032)\n\tat org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1474)\n\tat org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1684)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1737)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:29288)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1562)\n\tat org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:87)\n\tat org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:84)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:121)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:97)\n\tat org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)\n\tat org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:67)\n\tat org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:174)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:172)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)\n\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:334)\n\tat org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:201)\n\tat org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:415)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)\n\tat org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)\n\tat org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:722)\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n\tat sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n\tat sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n\tat sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n\tat sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n\tat sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n\tat sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n\tat com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)\n\t... 55 more\n{noformat}\nkinit was performed by the client. The error appears in the Tez application logs. The queries work fine when i change the hive.execution.engine to MR."
        }
    },
    {
        "filename": "HIVE-12364.json",
        "creation_time": "2015-11-07T02:04:25.000+0000",
        "bug_report": {
            "Title": "Distcp job fails when run under Tez",
            "Description": "PROBLEM:\n\ninsert into/overwrite directory '/path' invokes distcp for moveTask and fails\nquery when execution engine is Tez \n\nset hive.exec.copyfile.maxsize=40000;\ninsert overwrite into '/tmp/testinser' select * from customer;\nfailed at moveTask\nhive client log:\n{code}\n2015-11-05 16:02:53,254 INFO  [main]: exec.FileSinkOperator (Utilities.java:mvFileToFinalPath(1882)) - Moving tmp dir: hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/_tmp.-ext-10000 to: hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000\n2015-11-05 16:02:53,611 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=task.DEPENDENCY_COLLECTION.Stage-2 from=org.apache.hadoop.hive.ql.Driver>\n2015-11-05 16:02:53,612 INFO  [main]: ql.Driver (Driver.java:launchTask(1653)) - Starting task [Stage-2:DEPENDENCY_COLLECTION] in serial mode\n2015-11-05 16:02:53,612 INFO  [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=task.MOVE.Stage-0 from=org.apache.hadoop.hive.ql.Driver>\n2015-11-05 16:02:53,612 INFO  [main]: ql.Driver (Driver.java:launchTask(1653)) - Starting task [Stage-0:MOVE] in serial mode\n2015-11-05 16:02:53,612 INFO  [main]: exec.Task (SessionState.java:printInfo(951)) - Moving data to: /tmp/testindir from hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000\n2015-11-05 16:02:53,637 INFO  [main]: common.FileUtils (FileUtils.java:copy(551)) - Source is 491763261 bytes. (MAX: 40000)\n2015-11-05 16:02:53,638 INFO  [main]: common.FileUtils (FileUtils.java:copy(552)) - Launch distributed copy (distcp) job.\n2015-11-05 16:03:03,924 INFO  [main]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(296)) - Timeline service address: http://hdpsece02.sece.hwxsup.com:8188/ws/v1/timeline/\n2015-11-05 16:03:04,081 INFO  [main]: impl.TimelineClientImpl (TimelineClientImpl.java:serviceInit(296)) - Timeline service address: http://hdpsece02.sece.hwxsup.com:8188/ws/v1/timeline/\n2015-11-05 16:03:20,210 INFO  [main]: hdfs.DFSClient (DFSClient.java:getDelegationToken(1047)) - Created HDFS_DELEGATION_TOKEN token 1069 for haha on ha-hdfs:hdpsecehdfs\n2015-11-05 16:03:20,249 INFO  [main]: security.TokenCache (TokenCache.java:obtainTokensForNamenodesInternal(125)) - Got dt for hdfs://hdpsecehdfs; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:hdpsecehdfs, Ident: (HDFS_DELEGATION_TOKEN token 1069 for haha)\n2015-11-05 16:03:20,250 WARN  [main]: token.Token (Token.java:getClassForIdentifier(121)) - Cannot find class for token kind kms-dt\n2015-11-05 16:03:20,250 INFO  [main]: security.TokenCache (TokenCache.java:obtainTokensForNamenodesInternal(125)) - Got dt for hdfs://hdpsecehdfs; Kind: kms-dt, Service: 172.25.17.102:9292, Ident: 00 04 68 61 68 61 02 72 6d 00 8a 01 50 da 1a ca 29 8a 01 50 fe 27 4e 29 03 02\n2015-11-05 16:03:22,561 INFO  [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n2015-11-05 16:03:22,562 INFO  [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1173)) - io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n2015-11-05 16:03:33,733 ERROR [main]: exec.Task (SessionState.java:printError(960)) - Failed with exception Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir\norg.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir\n        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)\n        at org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(MoveTask.java:105)\n        at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:222)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1156)\n        at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)\n        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2647)\n        ... 21 more\nCaused by: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.\n        at org.apache.hadoop.mapreduce.Job.ensureNotSet(Job.java:1194)\n        at org.apache.hadoop.mapreduce.Job.setUseNewAPI(Job.java:1229)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1283)\n        at org.apache.hadoop.tools.DistCp.createAndSubmitJob(DistCp.java:183)\n        at org.apache.hadoop.tools.DistCp.execute(DistCp.java:153)\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1153)\n        ... 23 more\n\n2015-11-05 16:03:33,734 INFO  [main]: hooks.ATSHook (ATSHook.java:<init>(84)) - Created ATS Hook\n\n{code}\n"
        }
    },
    {
        "filename": "HIVE-8766.json",
        "creation_time": "2014-11-06T22:08:43.000+0000",
        "bug_report": {
            "Title": "Hive RetryHMSHandler should be retrying the metastore operation in case of NucleusException",
            "Description": "When we have Metastore operations and the Metastore Database is heavily loaded or takes a long time to respond, we might run into NucleusExceptions as shown in the below stacktrace. In the below scenario, the MetastoreDB is SQL Server and the SQLServer is configured to timeout and terminate with a connection reset after 'x' seconds if it doesnt return a ResultSet. While this needs configuration change at the Metastore DB side, we need to make sure that in such cases the HMS Retrying mechanism should not provide a rigid rule to fail such hive queries. The proposed fix would be allow retries when we hit a Nucleus Exception as shown below: \n\n{noformat}\n2014-11-04 06:40:03,208 ERROR bonecp.ConnectionHandle (ConnectionHandle.java:markPossiblyBroken(388)) - Database access problem. Killing off this connection and all remaining connections in the connection pool. SQL State = 08S01\n2014-11-04 06:40:03,213 ERROR DataNucleus.Transaction (Log4JLogger.java:error(115)) - Operation rollback failed on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16, error code UNKNOWN and transaction: [DataNucleus Transaction, ID=Xid=   \ufffd, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16]]\n2014-11-04 06:40:03,217 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(139)) - MetaException(message:org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5183)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1738)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1699)\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:101)\n\tat com.sun.proxy.$Proxy11.get_table(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:112)\n\tat sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)\n\tat com.sun.proxy.$Proxy12.getTable(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1060)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1015)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerde(DDLSemanticAnalyzer.java:1356)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:299)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:396)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0\n\tat org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:666)\n\tat org.datanucleus.store.rdbms.scostore.ElementContainerStore.size(ElementContainerStore.java:429)\n\tat org.datanucleus.store.types.backed.List.size(List.java:581)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToSkewedValues(ObjectStore.java:1190)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1168)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1178)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToTable(ObjectStore.java:1035)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:893)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)\n\tat com.sun.proxy.$Proxy10.getTable(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1727)\n\t... 41 more\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: SSL peer shut down incorrectly\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1352)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1339)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:1694)\n\tat com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:3734)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:5062)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:388)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:340)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1400)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:179)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:154)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:283)\n\tat com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)\n\tat org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)\n\tat org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)\n\tat org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:638)\n{noformat}"
        }
    },
    {
        "filename": "HIVE-5857.json",
        "creation_time": "2013-11-20T01:12:54.000+0000",
        "bug_report": {
            "Title": "Reduce tasks do not work in uber mode in YARN",
            "Description": "A Hive query fails when it tries to run a reduce task in uber mode in YARN.\n\nThe NullPointerException is thrown in the ExecReducer.configure method, because the plan file (reduce.xml) for a reduce task is not found.\n\nThe Utilities.getBaseWork method is expected to return BaseWork object, but it returns NULL due to FileNotFoundException. \n{code}\n// org.apache.hadoop.hive.ql.exec.Utilities\npublic static BaseWork getBaseWork(Configuration conf, String name) {\n  ...\n    try {\n    ...\n      if (gWork == null) {\n        Path localPath;\n        if (ShimLoader.getHadoopShims().isLocalMode(conf)) {\n          localPath = path;\n        } else {\n          localPath = new Path(name);\n        }\n        InputStream in = new FileInputStream(localPath.toUri().getPath());\n        BaseWork ret = deserializePlan(in);\n        ....\n      }\n      return gWork;\n    } catch (FileNotFoundException fnf) {\n      // happens. e.g.: no reduce work.\n      LOG.debug(\"No plan file found: \"+path);\n      return null;\n    } ...\n}\n{code}\n\nIt happens because, the ShimLoader.getHadoopShims().isLocalMode(conf)) method returns true, because immediately before running a reduce task, org.apache.hadoop.mapred.LocalContainerLauncher changes its configuration to local mode (\"mapreduce.framework.name\" is changed from\" \"yarn\" to \"local\"). On the other hand map tasks run successfully, because its configuration is not changed and still remains \"yarn\".\n\n{code}\n// org.apache.hadoop.mapred.LocalContainerLauncher\nprivate void runSubtask(..) {\n  ...\n  conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);\n  conf.set(MRConfig.MASTER_ADDRESS, \"local\");  // bypass shuffle\n\n  ReduceTask reduce = (ReduceTask)task;\n  reduce.setConf(conf);          \n\n  reduce.run(conf, umbilical);\n}\n{code}\n\nA super quick fix could just an additional if-branch, where we check if we run a reduce task in uber mode, and then look for a plan file in a different location.\n\n*Java stacktrace*\n\n{code}\n2013-11-20 00:50:56,862 INFO [uber-SubtaskRunner] org.apache.hadoop.hive.ql.exec.Utilities: No plan file found: hdfs://namenode.c.lon.spotify.net:54310/var/tmp/kawaa/hive_2013-11-20_00-50-43_888_3938384086824086680-2/-mr-10003/e3caacf6-15d6-4987-b186-d2906791b5b0/reduce.xml\n2013-11-20 00:50:56,862 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: Error in configuring object\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n\t... 7 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)\n\t... 12 more\n\n2013-11-20 00:50:56,862 INFO [uber-SubtaskRunner] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1384392632998_34791_r_000000_0\n2013-11-20 00:50:56,862 INFO [uber-SubtaskRunner] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1384392632998_34791_r_000000_0 is : 0.0\n2013-11-20 00:50:56,862 INFO [uber-SubtaskRunner] org.apache.hadoop.mapred.Task: Runnning cleanup for the task\n2013-11-20 00:50:56,863 INFO [uber-SubtaskRunner] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1384392632998_34791_r_000000_0: java.lang.RuntimeException: Error in configuring object\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n\t... 7 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)\n\t... 12 more\n\n2013-11-20 00:50:56,863 INFO [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1384392632998_34791_01_000001 taskAttempt attempt_1384392632998_34791_m_000000_0\n2013-11-20 00:50:56,863 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1384392632998_34791_r_000000_0: java.lang.RuntimeException: Error in configuring object\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n\t... 7 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)\n\t... 12 more\n{code}\n"
        }
    },
    {
        "filename": "HIVE-1547.json",
        "creation_time": "2010-08-17T02:09:54.000+0000",
        "bug_report": {
            "Title": "Unarchiving operation throws NPE",
            "Description": "Unarchiving a partition throws a null pointer exception similar to the following:\n\n2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n\nThis error seems to be DFS specific, as local file system in the unit tests don't catch this."
        }
    },
    {
        "filename": "HIVE-6113.json",
        "creation_time": "2013-12-27T07:07:00.000+0000",
        "bug_report": {
            "Title": "Upgrade DataNucleus [was: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient]",
            "Description": "CLEAR LIBRARY CACHE\n\nWhen I exccute SQL \"use fdm; desc formatted fdm.tableName;\"  in python, throw Error as followed.\nbut when I tryit again , It will success.\n\n2013-12-25 03:01:32,290 ERROR exec.DDLTask (DDLTask.java:execute(435)) - org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1143)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:260)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:217)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:507)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:875)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:769)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:708)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:197)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1217)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2372)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2383)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1139)\n\t... 20 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1210)\n\t... 25 more\nCaused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore\nNestedThrowables:\njava.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)\n\tat org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:165)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:358)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:404)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)\n\tat $Proxy9.createDatabase(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:422)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:441)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:326)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:286)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4060)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:121)\n\t... 30 more\nCaused by: java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'\n\tat com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2028)\n\tat com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1451)\n\tat com.jolbox.bonecp.StatementHandle.executeBatch(StatementHandle.java:469)\n\tat org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:372)\n\tat org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:628)\n\tat org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:596)\n\tat org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:683)\n\tat org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:86)\n\tat org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:454)\n\tat org.datanucleus.TransactionImpl.flush(TransactionImpl.java:199)\n\tat org.datanucleus.TransactionImpl.commit(TransactionImpl.java:263)\n\tat org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:98)\n\t... 46 more\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\n\tat com.mysql.jdbc.Util.getInstance(Util.java:386)\n\tat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3609)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3541)\n\tat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2002)\n\tat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2163)\n\tat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)\n\tat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)\n\tat com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2427)\n\tat com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1980)\n\t... 57 more\n\n"
        }
    },
    {
        "filename": "HIVE-9570.json",
        "creation_time": "2015-02-03T23:30:09.000+0000",
        "bug_report": {
            "Title": "Investigate test failure on union_view.q [Spark Branch]",
            "Description": "union_view.q failed with exception:\n\n{noformat}\n2015-02-03 15:27:05,723 ERROR [main]: ql.Driver (SessionState.java:printError(861)) - FAILED: NullPointerException null\njava.lang.NullPointerException\n  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:274)\n  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:253)\n  at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:222)\n  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10231)\n  at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)\n  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n  at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)\n  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)\n  at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)\n  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1160)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1039)\n  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)\n  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)\n  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)\n  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)\n  at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1019)\n  at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:993)\n  at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:136)\n  at org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_view(TestSparkCliDriver.java:120)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:606)\n  at junit.framework.TestCase.runTest(TestCase.java:176)\n  at junit.framework.TestCase.runBare(TestCase.java:141)\n  at junit.framework.TestResult$1.protect(TestResult.java:122)\n  at junit.framework.TestResult.runProtected(TestResult.java:142)\n  at junit.framework.TestResult.run(TestResult.java:125)\n  at junit.framework.TestCase.run(TestCase.java:129)\n  at junit.framework.TestSuite.runTest(TestSuite.java:255)\n  at junit.framework.TestSuite.run(TestSuite.java:250)\n  at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n{noformat}\n\nWe need to investigate this."
        }
    },
    {
        "filename": "HIVE-1678.json",
        "creation_time": "2010-10-01T05:41:21.000+0000",
        "bug_report": {
            "Title": "NPE in MapJoin ",
            "Description": "The query with two map joins and a group by fails with following NPE:\n\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)\n        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)\n        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)\n"
        }
    },
    {
        "filename": "HIVE-11820.json",
        "creation_time": "2015-09-14T22:20:28.000+0000",
        "bug_report": {
            "Title": "export tables with size of >32MB throws \"java.lang.IllegalArgumentException: Skip CRC is valid only with update options\"",
            "Description": "Tested a patch of HIVE-11607 and seeing the following exception:\n\n{noformat}\n2015-09-14 21:44:16,817 ERROR [main]: exec.Task (SessionState.java:printError(960)) - Failed with exception Skip CRC is valid only with update options\njava.lang.IllegalArgumentException: Skip CRC is valid only with update options\n        at org.apache.hadoop.tools.DistCpOptions.validate(DistCpOptions.java:556)\n        at org.apache.hadoop.tools.DistCpOptions.setSkipCRC(DistCpOptions.java:311)\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1147)\n        at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)\n        at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n{noformat}\n\nA possible resolution is to reverse the order of the following two lines from a patch of HIVE-11607:\n{noformat}\n+    options.setSkipCRC(true);\n+    options.setSyncFolder(true);\n{noformat}"
        }
    },
    {
        "filename": "HIVE-17274.json",
        "creation_time": "2017-08-08T22:20:33.000+0000",
        "bug_report": {
            "Title": "RowContainer spills for timestamp column throws exception",
            "Description": "Path names cannot contain \":\" (HADOOP-3257)\nJoin key toString() is used as part of filename.\nhttps://github.com/apache/hive/blob/16bfb9c9405b68a24c7e6c1b13bec00e38bbe213/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/RowContainer.java#L523\n\nIf join key is timestamp column then this will throw following exception.\n{code}\n2017-08-05 23:51:33,631 ERROR [main] org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc\njava.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc\n        at org.apache.hadoop.fs.Path.initialize(Path.java:205)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:171)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:93)\n        at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:94)\n        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:404)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926)\n        at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1137)\n        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)\n        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:530)\n        at org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(Utilities.java:1643)\n        at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.getHiveRecordWriter(HiveSequenceFileOutputFormat.java:64)\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:243)\n        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter(RowContainer.java:538)\n        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:299)\n        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.copyToDFSDirecory(RowContainer.java:407)\n        at org.apache.hadoop.hive.ql.exec.SkewJoinHandler.endGroup(SkewJoinHandler.java:185)\n        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:249)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:195)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc\n        at java.net.URI.checkPath(URI.java:1823)\n        at java.net.URI.<init>(URI.java:745)\n        at org.apache.hadoop.fs.Path.initialize(Path.java:202)\n        ... 26 more\n{code}"
        }
    },
    {
        "filename": "HIVE-12522.json",
        "creation_time": "2015-11-25T19:48:44.000+0000",
        "bug_report": {
            "Title": "Wrong FS error during Tez merge files when warehouse and scratchdir are on different FS",
            "Description": "When hive.merge.tezfiles=true, and the warehouse dir/scratchdir are on different filesystems.\n\n{noformat}\n2015-11-13 10:22:10,617 ERROR exec.Task (TezTask.java:execute(184)) - Failed to execute tez graph.\njava.lang.IllegalArgumentException: Wrong FS: wasb://chaoyiteztest@chaoyiteztest.blob.core.windows.net/hive/scratch/chaoyitest/c888f405-3c98-46b1-bf39-e57f067dfe4c/hive_2015-11-13_10-16-10_216_8161037519951665173-1/_tmp.-ext-10000, expected: hdfs://headnodehost:9000\nat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)\nat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)\nat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105)\nat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1136)\nat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1132)\nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\nat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1132)\nat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1423)\nat org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:579)\nat org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1083)\nat org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:329)\nat org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:156)\nat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\nat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\nat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1606)\nat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1367)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1179)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345)\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:733)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n2015-11-13 10:22:10,620 INFO hooks.ATSHook (ATSHook.java:<init>(84)) - Created ATS Hook\n{noformat}\n\nWhen the scratchdir is set to the same FS as the warehouse the problem goes away."
        }
    },
    {
        "filename": "HIVE-16845.json",
        "creation_time": "2017-06-07T17:07:55.000+0000",
        "bug_report": {
            "Title": "INSERT OVERWRITE a table with dynamic partitions on S3 fails with NPE",
            "Description": "*How to reproduce*\n- Create a partitioned table on S3:\n{noformat}\nCREATE EXTERNAL TABLE s3table(user_id string COMMENT '', event_name string COMMENT '') PARTITIONED BY (reported_date string, product_id int) LOCATION 's3a://<bucket name>'; \n{noformat}\n- Create a temp table:\n{noformat}\ncreate table tmp_table (id string, name string, date string, pid int) row format delimited fields terminated by '\\t' lines terminated by '\\n' stored as textfile;\n{noformat}\n- Load the following rows to the tmp table:\n{noformat}\nu1\tvalue1\t2017-04-10\t10000\nu2\tvalue2\t2017-04-10\t10000\nu3\tvalue3\t2017-04-10\t10001\n{noformat}\n- Set the following parameters:\n-- hive.exec.dynamic.partition.mode=nonstrict\n-- mapreduce.input.fileinputformat.split.maxsize=10\n-- hive.blobstore.optimizations.enabled=true\n-- hive.blobstore.use.blobstore.as.scratchdir=false\n-- hive.merge.mapfiles=true\n- Insert the rows from the temp table into the s3 table:\n{noformat}\nINSERT OVERWRITE TABLE s3table\nPARTITION (reported_date, product_id)\nSELECT\n  t.id as user_id,\n  t.name as event_name,\n  t.date as reported_date,\n  t.pid as product_id\nFROM tmp_table t;\n{noformat}\n\nA NPE will occur with the following stacktrace:\n{noformat}\n2017-05-08 21:32:50,607 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-184028]: Error running hive query: \norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null\nat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)\nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)\nat org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)\nat org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\nat org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\nat org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)\nat org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)\nat org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)\nat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\nat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\nat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)\nat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1201)\nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)\n... 11 more \n{noformat}"
        }
    },
    {
        "filename": "HIVE-9655.json",
        "creation_time": "2015-02-11T20:58:13.000+0000",
        "bug_report": {
            "Title": "Dynamic partition table insertion error",
            "Description": "We have these two tables:\n\n{code}\ncreate table t1 (c1 bigint, c2 string);\n\nCREATE TABLE t2 (c1 int, c2 string)\nPARTITIONED BY (p1 string);\n\nload data local inpath 'data' into table t1;\nload data local inpath 'data' into table t1;\nload data local inpath 'data' into table t1;\nload data local inpath 'data' into table t1;\nload data local inpath 'data' into table t1;\n{code}\n\nBut, when try to insert into table t2 from t1:\n{code}\nSET hive.exec.dynamic.partition.mode=nonstrict;\ninsert overwrite table t2 partition(p1) select *,c1 as p1 from t1 distribute by p1;\n{code}\n\nThe query failed with the following exception:\n\n{noformat}\n2015-02-11 12:50:52,756 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(178)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"c1\":1,\"c2\":\"one\"}\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)\n  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)\n  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]\n  at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n  at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)\n  ... 10 more\nCaused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]\n  at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)\n  at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)\n  at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)\n  at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)\n  at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)\n  ... 16 more\n{noformat}"
        }
    },
    {
        "filename": "HIVE-11441.json",
        "creation_time": "2015-08-03T17:42:24.000+0000",
        "bug_report": {
            "Title": "No DDL allowed on table if user accidentally set table location wrong",
            "Description": "If user makes a mistake, hive should either correct it in the first place, or allow user a chance to correct it. \n\nSTEPS TO REPRODUCE:\n\ncreate table testwrongloc(id int);\n\nalter table testwrongloc set location \"hdfs://a-valid-hostname/tmp/testwrongloc\";\n\n--at this time, hive should throw error, as hdfs://a-valid-hostname is not a valid path, it either needs to be hdfs://namenode-hostname:8020/ or hdfs://hdfs-nameservice for HA\n\nalter table testwrongloc set location \"hdfs://correct-host:8020/tmp/testwrongloc\"\nor \ndrop table testwrongloc;\n\nupon this hive throws error, that host 'a-valid-hostname' is not reachable\n\n\n{code}\n2015-07-30 12:19:43,573 DEBUG [main]: transport.TSaslTransport (TSaslTransport.java:readFrame(429)) - CLIENT: reading data length: 293\n2015-07-30 12:19:43,720 ERROR [main]: ql.Driver (SessionState.java:printError(833)) - FAILED: SemanticException Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\norg.apache.hadoop.hive.ql.parse.SemanticException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1323)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)\n        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)\n        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(DDLSemanticAnalyzer.java:1452)\n        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:295)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1069)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1131)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1072)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)\n        ... 23 more\n{code}\n\nNote this only happens when StorageBasedAuthorizationProvider is enabled."
        }
    },
    {
        "filename": "HIVE-10801.json",
        "creation_time": "2015-05-22T19:43:23.000+0000",
        "bug_report": {
            "Title": "'drop view' fails throwing java.lang.NullPointerException",
            "Description": "When trying to drop a view, hive log shows:\n{code}\n2015-05-21 11:53:06,126 ERROR [HiveServer2-Background-Pool: Thread-197]: hdfs.KeyProviderCache (KeyProviderCache.java:createKeyProviderURI(87)) - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!\n2015-05-21 11:53:06,134 ERROR [HiveServer2-Background-Pool: Thread-197]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(155)) - MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n\tat com.sun.proxy.$Proxy8.dropTable(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)\n\t... 40 more\n\n2015-05-21 11:53:06,135 ERROR [HiveServer2-Background-Pool: Thread-197]: exec.DDLTask (DDLTask.java:failed(520)) - org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1041)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n\tat com.sun.proxy.$Proxy8.dropTable(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)\n\t... 23 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)\n\t... 40 more\n\n2015-05-21 11:53:06,135 INFO  [HiveServer2-Background-Pool: Thread-197]: hooks.ATSHook (ATSHook.java:<init>(84)) - Created ATS Hook\n2015-05-21 11:53:06,136 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook from=org.apache.hadoop.hive.ql.Driver>\n2015-05-21 11:53:06,136 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=FailureHook.org.apache.hadoop.hive.ql.hooks.ATSHook start=1432209186136 end=1432209186136 duration=0 from=org.apache.hadoop.hive.ql.Driver>\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.NullPointerException)\n2015-05-21 11:53:06,136 ERROR [HiveServer2-Background-Pool: Thread-197]: ql.Driver (SessionState.java:printError(957)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.NullPointerException)\n2015-05-21 11:53:06,137 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=Driver.execute start=1432209185565 end=1432209186137 duration=572 from=org.apache.hadoop.hive.ql.Driver>\n2015-05-21 11:53:06,137 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogBegin(121)) - <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\n2015-05-21 11:53:06,137 INFO  [HiveServer2-Background-Pool: Thread-197]: log.PerfLogger (PerfLogger.java:PerfLogEnd(148)) - </PERFLOG method=releaseLocks start=1432209186137 end=1432209186137 duration=0 from=org.apache.hadoop.hive.ql.Driver>\n2015-05-21 11:53:06,139 ERROR [HiveServer2-Background-Pool: Thread-197]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query: \norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1041)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n\t... 11 more\nCaused by: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n\tat com.sun.proxy.$Proxy8.dropTable(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)\n\t... 23 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)\n\t... 40 more\n{code}\n\nThe following code in HiveMetaStore seems to have caused this issue :\n{code}\n        if(!ifPurge) {\n.....\n            if (shim.isPathEncrypted(tblPath)) {\n              throw new MetaException(\"Unable to drop table because it is in an encryption zone\" +\n                \" and trash is enabled.  Use PURGE option to skip trash.\");\n            }\n          }\n        }\n{code}\nIt seems that the tblPath is still null when shims.isPathEncrypted is called.\n\nThanks to [~asreekumar] for uncovering this issue !\n"
        }
    },
    {
        "filename": "HIVE-9141.json",
        "creation_time": "2014-12-17T07:23:05.000+0000",
        "bug_report": {
            "Title": "HiveOnTez: mix of union all, distinct, group by generates error",
            "Description": "Here is the way to produce it:\nin Hive q test setting (with src table)\nset hive.execution.engine=tez;\n\nSELECT key, value FROM\n  (\n  \tSELECT key, value FROM src\n\n    UNION ALL\n\n  \tSELECT key, key as value FROM \n  \t\n  \t\t(  \n  \t\t    SELECT distinct key FROM (\n\n      \t\tSELECT key, value FROM\n      \t\t(SELECT key, value FROM src\n        \t\tUNION ALL\n      \t\tSELECT key, value FROM src\n      \t\t)t1 \n      \t\tgroup by  key, value\n      \t\t)t2\n        )t3 \n      \n   )t4\n   group by  key, value;\n\nwill generate\n\n2014-12-16 23:19:13,593 ERROR ql.Driver (SessionState.java:printError(834)) - FAILED: ClassCastException org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork\njava.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork\n        at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:361)\n        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez2(TestMiniTezCliDriver.java:120)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
        }
    },
    {
        "filename": "HIVE-10010.json",
        "creation_time": "2015-03-18T17:48:46.000+0000",
        "bug_report": {
            "Title": "Alter table results in NPE [hbase-metastore branch]",
            "Description": "Doing an alter table results in:\n\n{code}\n2015-03-18 10:45:54,189 ERROR [main]: exec.DDLTask (DDLTask.java:failed(512)) - java.lang.NullPointerException\n    at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)\n    at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)\n    at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)\n    at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)\n    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)\n    at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)\n{code}"
        }
    },
    {
        "filename": "HIVE-7763.json",
        "creation_time": "2014-08-18T09:35:09.000+0000",
        "bug_report": {
            "Title": "Failed to query TABLESAMPLE on empty bucket table [Spark Branch]",
            "Description": "Get the following exception:\n{noformat}\n2014-08-18 16:23:15,213 ERROR [Executor task launch worker-0]: executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)\njava.lang.RuntimeException: Map operator initialization failed\n        at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:127)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:52)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:30)\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)\n        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)\n        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n        at org.apache.spark.scheduler.Task.run(Task.scala:54)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent\n        at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:404)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:93)\n        ... 16 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent\n        at org.apache.hadoop.hive.ql.exec\n{noformat}"
        }
    },
    {
        "filename": "HIVE-12083.json",
        "creation_time": "2015-10-09T22:45:47.000+0000",
        "bug_report": {
            "Title": "HIVE-10965 introduces thrift error if partNames or colNames are empty",
            "Description": "In the fix for HIVE-10965, there is a short-circuit path that causes an empty AggrStats object to be returned if partNames is empty or colNames is empty:\n\n{code}\ndiff --git metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java\nindex 0a56bac..ed810d2 100644\n--- metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java\n+++ metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java\n@@ -1100,6 +1100,7 @@ public ColumnStatistics getTableStats(\n   public AggrStats aggrColStatsForPartitions(String dbName, String tableName,\n       List<String> partNames, List<String> colNames, boolean useDensityFunctionForNDVEstimation)\n       throws MetaException {\n+    if (colNames.isEmpty() || partNames.isEmpty()) return new AggrStats(); // Nothing to aggregate.\n     long partsFound = partsFoundForPartitions(dbName, tableName, partNames, colNames);\n     List<ColumnStatisticsObj> colStatsList;\n     // Try to read from the cache first\n{code}\n\nThis runs afoul of thrift requirements that AggrStats have required fields:\n\n{code}\nstruct AggrStats {\n1: required list<ColumnStatisticsObj> colStats,\n2: required i64 partsFound // number of partitions for which stats were found\n}\n{code}\n\nThus, we get errors as follows:\n\n{noformat}\n2015-10-08 00:00:25,413 ERROR server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.\norg.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)\n        at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nNormally, this would not occur since HIVE-10965 does also include a guard on the client-side for colNames.isEmpty() to not call the metastore call at all, but there is no guard for partNames being empty, and would still cause an error on the metastore side if the thrift call were called directly, as would happen if the client is from an older version before this was patched."
        }
    },
    {
        "filename": "HIVE-14784.json",
        "creation_time": "2016-09-17T02:00:28.000+0000",
        "bug_report": {
            "Title": "Operation logs are disabled automatically if the parent directory does not exist.",
            "Description": "Operation logging is disabled automatically for the query if for some reason the parent directory (named after the hive session id) that gets created when the session is established gets deleted (for any reason). For ex: if the operation logdir is /tmp which automatically can get purged at a configured interval by the OS.\n\nRunning a query from that session leads to\n{code}\n2016-09-15 15:09:16,723 WARN org.apache.hive.service.cli.operation.Operation: Unable to create operation log file: /tmp/hive/operation_logs/b8809985-6b38-47ec-a49b-6158a67cd9fc/d35414f7-2418-426c-8489-c6f643ca4599\njava.io.IOException: No such file or directory\n\tat java.io.UnixFileSystem.createFileExclusively(Native Method)\n\tat java.io.File.createNewFile(File.java:1012)\n\tat org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)\n\tat org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:385)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n{code}\n\nThis later leads to errors like (more prominent when using HUE as HUE does not close hive sessions and attempts to retrieve the operations logs days after they were created).\n{code}\nWARN org.apache.hive.service.cli.thrift.ThriftCLIService: Error fetching results: \norg.apache.hive.service.cli.HiveSQLException: Couldn't find log associated with operation handle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d35414f7-2418-426c-8489-c6f643ca4599]\n\tat org.apache.hive.service.cli.operation.OperationManager.getOperationLogRowSet(OperationManager.java:259)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:701)\n\tat org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:451)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:676)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745) \n{code}\n"
        }
    }
]