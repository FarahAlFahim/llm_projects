[
    {
        "filename": "HADOOP-6989.json",
        "creation_time": "2010-10-04T23:55:16.000+0000",
        "bug_report": {
            "Title": "TestSetFile is failing on trunk",
            "Description": "Testsuite: org.apache.hadoop.io.TestSetFile\nTests run: 1, Failures: 0, Errors: 1, Time elapsed: 1.015 sec\n------------- Standard Output ---------------\n2010-10-04 16:32:01,030 INFO  io.TestSetFile (TestSetFile.java:generate(56)) - generating 10000 records in memory\n2010-10-04 16:32:01,249 INFO  io.TestSetFile (TestSetFile.java:generate(63)) - sorting 10000 records\n2010-10-04 16:32:01,350 INFO  io.TestSetFile (TestSetFile.java:writeTest(72)) - creating with 10000 records\n------------- ---------------- ---------------\n\nTestcase: testSetFile took 0.964 sec\n\tCaused an ERROR\nkey class or comparator option must be set\njava.lang.IllegalArgumentException: key class or comparator option must be set\n\tat org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)\n\tat org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)\n\tat org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)\n\tat org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)"
        }
    },
    {
        "filename": "HADOOP-10823.json",
        "creation_time": "2014-07-15T07:20:00.000+0000",
        "bug_report": {
            "Title": "TestReloadingX509TrustManager is flaky",
            "Description": "Pasting the log\n{quote}\nError Message\n\nexpected:<2> but was:<1>\nStacktrace\n\njunit.framework.AssertionFailedError: expected:<2> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:50)\n\tat junit.framework.Assert.failNotEquals(Assert.java:287)\n\tat junit.framework.Assert.assertEquals(Assert.java:67)\n\tat junit.framework.Assert.assertEquals(Assert.java:199)\n\tat junit.framework.Assert.assertEquals(Assert.java:205)\n\tat org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)\nStandard Output\n\n2014-07-06 06:12:21,170 WARN  ssl.ReloadingX509TrustManager (ReloadingX509TrustManager.java:run(197)) - Could not load truststore (keep using existing one) : java.io.EOFException\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:375)\n\tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)\n\tat sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)\n\tat java.security.KeyStore.load(KeyStore.java:1185)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)\n\tat java.lang.Thread.run(Thread.java:662)\n{quote}"
        }
    },
    {
        "filename": "HADOOP-9125.json",
        "creation_time": "2012-12-10T02:07:52.000+0000",
        "bug_report": {
            "Title": "LdapGroupsMapping threw CommunicationException after some idle time",
            "Description": "LdapGroupsMapping threw exception as below after some idle time. During the idle time no call to the group mapping provider should be made to repeat it.\n\n2012-12-07 02:20:59,738 WARN org.apache.hadoop.security.LdapGroupsMapping: Exception trying to get groups for user aduser2\njavax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)\n        at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)\n        at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)\n        at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)\n        at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)\n        at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)\n        at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)\n        at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)\n        at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)\n        at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)\n        at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)\nCaused by: java.io.IOException: connection closed\n        at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)\n        at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)\n        ... 28 more\n2012-12-07 02:20:59,739 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user aduser2\n"
        }
    },
    {
        "filename": "HADOOP-10252.json",
        "creation_time": "2014-01-22T16:43:27.000+0000",
        "bug_report": {
            "Title": "HttpServer can't start if hostname is not specified",
            "Description": "HADOOP-8362 added a checking to make sure configuration values are not null. By default, we don't specify the hostname for the HttpServer. So we could not start info server due to\n\n{noformat}\n2014-01-22 08:43:05,969 FATAL [M:0;localhost:48573] master.HMaster(2187): Unhandled exception. Starting shutdown.\njava.lang.IllegalArgumentException: Property value must not be null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:958)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:940)\n\tat org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)\n\tat org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)\n\tat org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)\n\tat java.lang.Thread.run(Thread.java:722){noformat}"
        }
    },
    {
        "filename": "HADOOP-12239.json",
        "creation_time": "2015-07-15T18:06:14.000+0000",
        "bug_report": {
            "Title": "StorageException complaining \" no lease ID\" when updating FolderLastModifiedTime in WASB",
            "Description": " This is a similar issue as HADOOP-11523 and HADOOP-12089, which I found in a customer's HBase cluster logs, but the piece of code is in a different place.\n{code}\n2015-07-09 13:38:57,388 INFO org.apache.hadoop.hbase.master.SplitLogManager: dead splitlog workers [workernode3.xxx.b6.internal.cloudapp.net,60020,1436448555180]\n2015-07-09 13:38:57,466 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN\njava.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436 448566374, will retry\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unable to write RenamePending file for folder rename from hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374 to hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374-splitting\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1998)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:325)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:412)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:390)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:288)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:204)\n\t... 4 more\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2598)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2609)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1366)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:255)\n\t... 11 more\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2892)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2593)\n\t... 19 more\n{code}"
        }
    },
    {
        "filename": "HADOOP-11878.json",
        "creation_time": "2015-04-27T08:06:22.000+0000",
        "bug_report": {
            "Title": "FileContext.java # fixRelativePart should check for not null for a more informative exception",
            "Description": "Following will come when job failed and deletion service trying to delete the log fiels\n\n2015-04-27 14:56:17,113 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : null\n2015-04-27 14:56:17,113 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService\njava.lang.NullPointerException\n        at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)\n        at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)\n        at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "HADOOP-14949.json",
        "creation_time": "2017-10-13T23:44:09.000+0000",
        "bug_report": {
            "Title": "TestKMS#testACLs fails intermittently",
            "Description": "We have seen some intermittent failures of this test:\r\n\r\nError Message\r\n{noformat}\r\njava.lang.AssertionError\r\n{noformat}\r\nStack Trace\r\n\r\n{noformat}java.lang.AssertionError: Should not have been able to reencryptEncryptedKey\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)\r\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)\r\n{noformat}\r\nStandard Output\r\n{noformat}\r\n2017-10-07 09:44:11,112 INFO  log - jetty-6.1.26.cloudera.4\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp - -------------------------------------------------------------\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp -   Java runtime version : 1.7.0_121-b00\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp -   User: slave\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp -   KMS Hadoop Version: 2.6.0-cdh5.14.0-SNAPSHOT\r\n2017-10-07 09:44:11,131 INFO  KMSWebApp - -------------------------------------------------------------\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'CREATE' ACL 'CREATE,SET_KEY_MATERIAL'\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'DELETE' ACL 'DELETE'\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'ROLLOVER' ACL 'ROLLOVER,SET_KEY_MATERIAL'\r\n2017-10-07 09:44:11,134 INFO  KMSACLs - 'GET' ACL 'GET'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'GET_KEYS' ACL 'GET_KEYS'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'GET_METADATA' ACL 'GET_METADATA'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'SET_KEY_MATERIAL' ACL 'SET_KEY_MATERIAL'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'GENERATE_EEK' ACL 'GENERATE_EEK'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - 'DECRYPT_EEK' ACL 'DECRYPT_EEK'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - KEY_NAME 'k0' KEY_OP 'ALL' ACL '*'\r\n2017-10-07 09:44:11,135 INFO  KMSACLs - KEY_NAME 'k1' KEY_OP 'ALL' ACL '*'\r\n2017-10-07 09:44:11,136 INFO  KMSAudit - No audit logger configured, using default.\r\n2017-10-07 09:44:11,137 INFO  KMSAudit - Initializing audit logger class org.apache.hadoop.crypto.key.kms.server.SimpleKMSAuditLogger\r\n2017-10-07 09:44:11,137 INFO  KMSWebApp - Initialized KeyProvider CachingKeyProvider: jceks://file@/tmp/run_tha_testUYG3Cl/hadoop-common-project/hadoop-kms/target/ddbffdf2-e7d8-4e75-982a-debebb227075/kms.keystore\r\n2017-10-07 09:44:11,138 INFO  KMSWebApp - Initialized KeyProviderCryptoExtension EagerKeyGeneratorKeyProviderCryptoExtension: KeyProviderCryptoExtension: CachingKeyProvider: jceks://file@/tmp/run_tha_testUYG3Cl/hadoop-common-project/hadoop-kms/target/ddbffdf2-e7d8-4e75-982a-debebb227075/kms.keystore\r\n2017-10-07 09:44:11,138 INFO  KMSWebApp - Default key bitlength is 128\r\n2017-10-07 09:44:11,138 INFO  KMSWebApp - KMS Started\r\n2017-10-07 09:44:11,141 INFO  PackagesResourceConfig - Scanning for root resource and provider classes in the packages:\r\n  org.apache.hadoop.crypto.key.kms.server\r\n2017-10-07 09:44:11,146 INFO  ScanningResourceConfig - Root resource classes found:\r\n  class org.apache.hadoop.crypto.key.kms.server.KMS\r\n2017-10-07 09:44:11,146 INFO  ScanningResourceConfig - Provider classes found:\r\n  class org.apache.hadoop.crypto.key.kms.server.KMSJSONWriter\r\n  class org.apache.hadoop.crypto.key.kms.server.KMSExceptionsProvider\r\n  class org.apache.hadoop.crypto.key.kms.server.KMSJSONReader\r\n2017-10-07 09:44:11,147 INFO  WebApplicationImpl - Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\r\n2017-10-07 09:44:11,224 INFO  log - Started SocketConnector@localhost:46764\r\nTest KMS running at: http://localhost:46764/kms\r\n2017-10-07 09:44:11,254 INFO  kms-audit - UNAUTHORIZED[op=CREATE_KEY, key=k, user=client] \r\n2017-10-07 09:44:11,255 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/keys caused exception.\r\n2017-10-07 09:44:11,256 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'CREATE_KEY' on 'k']!!\r\n2017-10-07 09:44:11,256 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,270 INFO  kms-audit - UNAUTHORIZED[op=CREATE_KEY, key=k, user=client] \r\n2017-10-07 09:44:11,270 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/keys caused exception.\r\n2017-10-07 09:44:11,271 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'CREATE_KEY' on 'k']!!\r\n2017-10-07 09:44:11,271 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,284 INFO  kms-audit - UNAUTHORIZED[op=ROLL_NEW_VERSION, key=k, user=client] \r\n2017-10-07 09:44:11,284 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/key/k caused exception.\r\n2017-10-07 09:44:11,285 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'ROLL_NEW_VERSION' on 'k']!!\r\n2017-10-07 09:44:11,285 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,300 INFO  kms-audit - UNAUTHORIZED[op=ROLL_NEW_VERSION, key=k, user=client] \r\n2017-10-07 09:44:11,300 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/key/k caused exception.\r\n2017-10-07 09:44:11,301 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'ROLL_NEW_VERSION' on 'k']!!\r\n2017-10-07 09:44:11,301 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,315 INFO  kms-audit - UNAUTHORIZED[op=GET_KEYS, user=client] \r\n2017-10-07 09:44:11,315 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/keys/names caused exception.\r\n2017-10-07 09:44:11,316 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEYS']!!\r\n2017-10-07 09:44:11,316 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,330 INFO  kms-audit - UNAUTHORIZED[op=GET_KEYS_METADATA, user=client] \r\n2017-10-07 09:44:11,330 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/keys/metadata?key=k caused exception.\r\n2017-10-07 09:44:11,331 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEYS_METADATA']!!\r\n2017-10-07 09:44:11,331 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,344 INFO  kms-audit - UNAUTHORIZED[op=GET_KEY_VERSION, user=client] \r\n2017-10-07 09:44:11,344 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/keyversion/k%400 caused exception.\r\n2017-10-07 09:44:11,345 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEY_VERSION']!!\r\n2017-10-07 09:44:11,345 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,358 INFO  kms-audit - UNAUTHORIZED[op=GET_CURRENT_KEY, key=k, user=client] \r\n2017-10-07 09:44:11,358 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k/_currentversion caused exception.\r\n2017-10-07 09:44:11,359 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_CURRENT_KEY' on 'k']!!\r\n2017-10-07 09:44:11,359 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,372 INFO  kms-audit - UNAUTHORIZED[op=GET_METADATA, key=k, user=client] \r\n2017-10-07 09:44:11,372 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k/_metadata caused exception.\r\n2017-10-07 09:44:11,373 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_METADATA' on 'k']!!\r\n2017-10-07 09:44:11,373 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,386 INFO  kms-audit - UNAUTHORIZED[op=GET_KEY_VERSIONS, key=k, user=client] \r\n2017-10-07 09:44:11,386 WARN  KMS - User client@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k/_versions caused exception.\r\n2017-10-07 09:44:11,387 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User:client not allowed to do 'GET_KEY_VERSIONS' on 'k']!!\r\n2017-10-07 09:44:11,387 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:11,407 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k0, user=CREATE] \r\n2017-10-07 09:44:11,409 INFO  kms-audit - OK[op=CREATE_KEY, key=k0, user=CREATE] UserProvidedMaterial:false Description:null\r\n2017-10-07 09:44:11,435 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k0, user=DELETE] \r\n2017-10-07 09:44:11,437 INFO  kms-audit - OK[op=DELETE_KEY, key=k0, user=DELETE] \r\n2017-10-07 09:44:11,457 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,459 INFO  kms-audit - OK[op=CREATE_KEY, key=k1, user=SET_KEY_MATERIAL] UserProvidedMaterial:true Description:null\r\n2017-10-07 09:44:11,482 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,487 INFO  kms-audit - OK[op=ROLL_NEW_VERSION, key=k1, user=ROLLOVER] UserProvidedMaterial:false NewVersion:k1@1\r\n2017-10-07 09:44:11,489 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,489 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,491 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,491 WARN  KMS - User ROLLOVER@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,506 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,506 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,507 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=ROLLOVER] \r\n2017-10-07 09:44:11,507 WARN  KMS - User ROLLOVER@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,530 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,532 INFO  kms-audit - OK[op=ROLL_NEW_VERSION, key=k1, user=SET_KEY_MATERIAL] UserProvidedMaterial:true NewVersion:k1@2\r\n2017-10-07 09:44:11,534 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,534 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,535 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,535 WARN  KMS - User SET_KEY_MATERIAL@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,550 INFO  kms-audit - UNAUTHORIZED[op=MANAGEMENT, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,550 INFO  kms-audit - OK[op=INVALIDATE_CACHE, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,551 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=SET_KEY_MATERIAL] \r\n2017-10-07 09:44:11,551 WARN  KMS - User SET_KEY_MATERIAL@EXAMPLE.COM (auth:KERBEROS) request GET http://localhost:46764/kms/v1/key/k1/_eek?num_keys=150&eek_op=generate caused exception.\r\n2017-10-07 09:44:11,571 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET] \r\n2017-10-07 09:44:11,571 INFO  kms-audit - OK[op=GET_KEY_VERSION, key=k1, user=GET, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,574 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET] \r\n2017-10-07 09:44:11,575 INFO  kms-audit - OK[op=GET_CURRENT_KEY, key=k1, user=GET, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,598 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,599 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,604 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,605 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,606 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,607 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,608 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,613 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,613 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,614 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,615 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,616 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,616 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,616 INFO  kms-audit - OK[op=GENERATE_EEK, key=k1, user=GENERATE_EEK, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,645 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,645 INFO  kms-audit - OK[op=REENCRYPT_EEK, key=k1, user=GENERATE_EEK, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,648 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:11,648 INFO  kms-audit - OK[op=REENCRYPT_EEK_BATCH, key=k1, user=GENERATE_EEK] reencrypted 2 keys\r\n2017-10-07 09:44:11,648 INFO  KMS - reencryptEncryptedKeys 2 keys for key k1 took 347.5 ?s\r\n2017-10-07 09:44:11,674 INFO  kms-audit - UNAUTHORIZED[op=DECRYPT_EEK, key=k1, user=DECRYPT_EEK] \r\n2017-10-07 09:44:11,674 INFO  kms-audit - OK[op=DECRYPT_EEK, key=k1, user=DECRYPT_EEK, accessCount=1, interval=0ms] \r\n2017-10-07 09:44:11,698 INFO  kms-audit - OK[op=GET_KEYS, user=GET_KEYS] \r\n2017-10-07 09:44:11,721 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET_METADATA] \r\n2017-10-07 09:44:11,722 INFO  kms-audit - OK[op=GET_METADATA, key=k1, user=GET_METADATA] \r\n2017-10-07 09:44:11,725 INFO  kms-audit - UNAUTHORIZED[op=READ, key=k1, user=GET_METADATA] \r\n2017-10-07 09:44:11,725 INFO  kms-audit - OK[op=GET_KEYS_METADATA, user=GET_METADATA] \r\n2017-10-07 09:44:12,000 INFO  SessionTrackerImpl - SessionTrackerImpl exited loop!\r\n2017-10-07 09:44:12,771 WARN  KMS - User CREATE@EXAMPLE.COM (auth:KERBEROS) request POST http://localhost:46764/kms/v1/keys caused exception.\r\n2017-10-07 09:44:12,772 WARN  LoadBalancingKMSClientProvider - KMS provider at [http://localhost:46764/kms/v1/] threw an IOException [User [CREATE] is not authorized to create key !!]!!\r\n2017-10-07 09:44:12,772 WARN  LoadBalancingKMSClientProvider - Aborting since the Request has failed with all KMS providers in the group. !!\r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,795 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,796 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,797 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,802 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,803 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,804 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,805 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,806 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,837 INFO  kms-audit - UNAUTHORIZED[op=GENERATE_EEK, key=k1, user=GENERATE_EEK] \r\n2017-10-07 09:44:12,838 INFO  log - Stopped SocketConnector@localhost:46764\r\n2017-10-07 09:44:12,840 INFO  KMSWebApp - KMS Stopped\r\n\r\n{noformat}"
        }
    },
    {
        "filename": "HADOOP-10540.json",
        "creation_time": "2014-04-11T05:14:07.000+0000",
        "bug_report": {
            "Title": "Datanode upgrade in Windows fails with hardlink error.",
            "Description": "I try to upgrade Hadoop from 1.x and 2.4, but DataNode failed to start due to hard link exception.\nRepro steps:\n*Installed Hadoop 1.x\n*hadoop dfsadmin -safemode enter\n*hadoop dfsadmin -saveNamespace\n*hadoop namenode -finalize\n*Stop all services\n*Uninstall Hadoop 1.x \n*Install Hadoop 2.4 \n*Start namenode with -upgrade option\n*Try to start datanode, begin to see Hardlink exception in datanode service log.\n\n{code}\n\n2014-04-10 22:47:11,655 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 8010: starting\n2014-04-10 22:47:11,656 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting\n2014-04-10 22:47:11,999 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -56\n2014-04-10 22:47:12,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on d:\\hadoop\\data\\hdfs\\dn\\in_use.lock acquired by nodename 7268@myhost\n2014-04-10 22:47:12,011 INFO org.apache.hadoop.hdfs.server.common.Storage: Recovering storage directory D:\\hadoop\\data\\hdfs\\dn from previous upgrade\n2014-04-10 22:47:12,017 INFO org.apache.hadoop.hdfs.server.common.Storage: Upgrading storage directory d:\\hadoop\\data\\hdfs\\dn.\n   old LV = -44; old CTime = 0.\n   new LV = -55; new CTime = 1397168400373\n2014-04-10 22:47:12,021 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-39008719-10.0.0.1-1397168400092 directory d:\\hadoop\\data\\hdfs\\dn\\current\\BP-39008719-10.0.0.1-1397168400092\\current\n2014-04-10 22:47:12,254 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (Datanode Uuid unassigned) service to myhost/10.0.0.1:8020\njava.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:274)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:815)\n\tat java.lang.Thread.run(Thread.java:722)\n2014-04-10 22:47:12,258 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to myhost/10.0.0.1:8020\n2014-04-10 22:47:12,359 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:91)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:859)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n2014-04-10 22:47:12,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool <registering> (Datanode Uuid unassigned)\n2014-04-10 22:47:12,360 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool ID needed, but service not yet registered with NN\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:861)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n2014-04-10 22:47:14,360 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode\n2014-04-10 22:47:14,361 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0\n2014-04-10 22:47:14,362 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down DataNode at myhost/10.0.0.1\n************************************************************/\n{code}"
        }
    },
    {
        "filename": "HADOOP-7629.json",
        "creation_time": "2011-09-09T17:45:14.000+0000",
        "bug_report": {
            "Title": "regression with MAPREDUCE-2289 - setPermission passed immutable FsPermission (rpc failure)",
            "Description": "MAPREDUCE-2289 introduced the following change:\n\n{noformat}\n+        fs.setPermission(stagingArea, JOB_DIR_PERMISSION);\n{noformat}\n\nJOB_DIR_PERMISSION is an immutable FsPermission which cannot be used in RPC calls, it results in the following exception:\n\n{noformat}\n2011-09-08 16:31:45,187 WARN org.apache.hadoop.ipc.Server: Unable to read call parameters for client 127.0.0.1\njava.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)\n        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)\n        at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)\n        at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)\n        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)\n        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)\nCaused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at java.lang.Class.getConstructor0(Class.java:2706)\n        at java.lang.Class.getDeclaredConstructor(Class.java:1985)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)\n        ... 8 more\n{noformat}\n"
        }
    },
    {
        "filename": "HADOOP-15060.json",
        "creation_time": "2017-11-22T00:18:55.000+0000",
        "bug_report": {
            "Title": "TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime flaky",
            "Description": "{code}\r\n[ERROR] testFiniteGroupResolutionTime(org.apache.hadoop.security.TestShellBasedUnixGroupsMapping)  Time elapsed: 61.975 s  <<< FAILURE!\r\njava.lang.AssertionError: \r\nExpected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser\r\nPartialGroupNameException The user name 'foobarnonexistinguser' is not found. \r\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)\r\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)\r\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\r\n\tat org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)\r\n{code}"
        }
    },
    {
        "filename": "HADOOP-10937.json",
        "creation_time": "2014-08-04T21:22:55.000+0000",
        "bug_report": {
            "Title": "Need to set version name correctly before decrypting EEK",
            "Description": "Touchz-ing a file results in a Null Pointer Exception\n\n{noformat}\n[hdfs@mynode hadoop-common]$ hdfs dfs -touchz /enc3/touchFIle\n2014-08-01 08:45:10,148 INFO  [main] hdfs.DFSClient (DFSClient.java:<init>(605)) - Found KeyProvider: KeyProviderCryptoExtension: KMSClientProvider[http://mynode.myhost.com:16000/kms/v1/]\n-touchz: Fatal internal error\njava.lang.NullPointerException\n\tat org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)\n\tat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)\n\tat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n{noformat}"
        }
    },
    {
        "filename": "HADOOP-9103.json",
        "creation_time": "2012-04-20T01:07:25.000+0000",
        "bug_report": {
            "Title": "UTF8 class does not properly decode Unicode characters outside the basic multilingual plane",
            "Description": "this the log information  of the  exception  from the SecondaryNameNode: \n2012-03-28 00:48:42,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for\n non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????\n??????????tor.qzone.qq.com/keypart-00174\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)\n        at java.lang.Thread.run(Thread.java:619)\n\nthis is the log information  about the file from namenode:\n2012-03-28 00:32:26,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss\tip=/10.131.16.34\tcmd=create\tsrc=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174\tdst=null\tperm=boss:boss:rw-r--r--\n2012-03-28 00:37:42,387 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174. blk_2751836614265659170_184668759\n2012-03-28 00:37:42,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174 is closed by DFSClient_attempt_201203271849_0016_r_000174_0\n2012-03-28 00:37:50,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss\tip=/10.131.16.34\tcmd=rename\tsrc=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174\tdst=/user/boss/pgv/fission/task16/split/  @?            tor.qzone.qq.com/keypart-00174\tperm=boss:boss:rw-r--r--\n\n\nafter check the code that save FSImage,I found there are a problem that maybe a bug of HDFS Code,I past below:\n-------------this is the saveFSImage method  in  FSImage.java, I make some mark at the problem code------------\n\n/**\n   * Save the contents of the FS image to the file.\n   */\n  void saveFSImage(File newFile) throws IOException {\n    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();\n    FSDirectory fsDir = fsNamesys.dir;\n    long startTime = FSNamesystem.now();\n    //\n    // Write out data\n    //\n    DataOutputStream out = new DataOutputStream(\n                                                new BufferedOutputStream(\n                                                                         new FileOutputStream(newFile)));\n    try {\n      .........\n    \n      // save the rest of the nodes\n      saveImage(strbuf, 0, fsDir.rootDir, out);------------------problem\n      fsNamesys.saveFilesUnderConstruction(out);------------------problem  detail is below\n      strbuf = null;\n    } finally {\n      out.close();\n    }\n\n    LOG.info(\"Image file of size \" + newFile.length() + \" saved in \" \n        + (FSNamesystem.now() - startTime)/1000 + \" seconds.\");\n  }\n\n /**\n   * Save file tree image starting from the given root.\n   * This is a recursive procedure, which first saves all children of\n   * a current directory and then moves inside the sub-directories.\n   */\n  private static void saveImage(ByteBuffer parentPrefix,\n                                int prefixLength,\n                                INodeDirectory current,\n                                DataOutputStream out) throws IOException {\n    int newPrefixLength = prefixLength;\n    if (current.getChildrenRaw() == null)\n      return;\n    for(INode child : current.getChildren()) {\n      // print all children first\n      parentPrefix.position(prefixLength);\n      parentPrefix.put(PATH_SEPARATOR).put(child.getLocalNameBytes());------------------problem\n      saveINode2Image(parentPrefix, child, out);\n    }\n   ..........\n  }\n\n\n // Helper function that writes an INodeUnderConstruction\n  // into the input stream\n  //\n  static void writeINodeUnderConstruction(DataOutputStream out,\n                                           INodeFileUnderConstruction cons,\n                                           String path) \n                                           throws IOException {\n    writeString(path, out);------------------problem\n    ..........\n  }\n  \n  static private final UTF8 U_STR = new UTF8();\n  static void writeString(String str, DataOutputStream out) throws IOException {\n    U_STR.set(str);\n    U_STR.write(out);------------------problem \n  }\n\n  /**\n   * Converts a string to a byte array using UTF8 encoding.\n   */\n  static byte[] string2Bytes(String str) {\n    try {\n      return str.getBytes(\"UTF8\");------------------problem \n    } catch(UnsupportedEncodingException e) {\n      assert false : \"UTF8 encoding is not supported \";\n    }\n    return null;\n  }\n------------------------------------------below is the explain------------------------\nin  saveImage method:  child.getLocalNameBytes(),the  bytes use the method of str.getBytes(\"UTF8\");\n\nbut in writeINodeUnderConstruction, the bytes user the method of Class  UTF8 to get the bytes.\n\nI make a test use our messy code file name , found the the two bytes arrsy are not equal. so I both use the class UTF8,then the problem desappare.\n\nI think this is a bug of HDFS or UTF8."
        }
    },
    {
        "filename": "HADOOP-11151.json",
        "creation_time": "2014-09-29T08:18:04.000+0000",
        "bug_report": {
            "Title": "Automatically refresh auth token and retry on auth failure",
            "Description": "Enable CFS and KMS service in the cluster, initially it worked to put/copy file into encryption zone. But after a while (might be one day), it fails to put/copy file into the encryption zone with the error\njava.util.concurrent.ExecutionException: java.io.IOException: HTTP status [403], message [Forbidden]\n\nThe kms.log shows below\nAbstractDelegationTokenSecretManager - Updating the current master key for generating delegation tokens\n2014-09-29 13:18:46,599 WARN  AuthenticationFilter - AuthenticationToken ignored: org.apache.hadoop.security.authentication.util.SignerException: Invalid signature\n2014-09-29 13:18:46,599 WARN  AuthenticationFilter - Authentication exception: Anonymous requests are disallowed\norg.apache.hadoop.security.authentication.client.AuthenticationException: Anonymous requests are disallowed\n        at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)\n        at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "HADOOP-8031.json",
        "creation_time": "2012-02-07T20:22:24.000+0000",
        "bug_report": {
            "Title": "Configuration class fails to find embedded .jar resources; should use URL.openStream()",
            "Description": "While running a hadoop client within RHQ (monitoring software) using its classloader, I see this:\n\n2012-02-07 09:15:25,313 INFO  [ResourceContainer.invoker.daemon-2] (org.apache.hadoop.conf.Configuration)- parsing jar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml\n2012-02-07 09:15:25,318 ERROR [InventoryManager.discovery-1] (rhq.core.pc.inventory.InventoryManager)- Failed to start component for Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com] from synchronized merge.\norg.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].\nCaused by: java.lang.RuntimeException: core-site.xml not found\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:438)\n\nThis is because the URL\n\njar:file:/usr/local/rhq-agent/data/tmp/rhq-hadoop-plugin-4.3.0-SNAPSHOT.jar6856622641102893436.classloader/hadoop-core-0.20.2+737+1.jar7204287718482036191.tmp!/core-default.xml\n\ncannot be found by DocumentBuilder (doesn't understand it). (Note: the logs are for an old version of Configuration class, but the new version has the same code.)\n\nThe solution is to obtain the resource stream directly from the URL object itself.\n\nThat is to say:\n\n{code}\n         URL url = getResource((String)name);\n-        if (url != null) {\n-          if (!quiet) {\n-            LOG.info(\"parsing \" + url);\n-          }\n-          doc = builder.parse(url.toString());\n-        }\n+        doc = builder.parse(url.openStream());\n{code}\n\nNote: I have a full patch pending approval at Apple for this change, including some cleanup."
        }
    },
    {
        "filename": "HADOOP-15411.json",
        "creation_time": "2018-04-24T23:18:39.000+0000",
        "bug_report": {
            "Title": "AuthenticationFilter should use Configuration.getPropsWithPrefix instead of iterator",
            "Description": "Node manager\u00a0start up fails\u00a0with the following stack trace\r\n\r\n{code}\r\n2018-04-19 13:08:30,638 ERROR nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(921)) - Error starting NodeManager\r\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.\r\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)\r\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\r\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)\r\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)\r\nCaused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)\r\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)\r\n ... 5 more\r\nCaused by: java.io.IOException: java.util.ConcurrentModificationException\r\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)\r\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)\r\n at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)\r\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)\r\n ... 8 more\r\nCaused by: java.util.ConcurrentModificationException\r\n at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)\r\n at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)\r\n at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)\r\n at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)\r\n at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)\r\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)\r\n ... 11 more\r\n2018-04-19 13:08:30,639 INFO timeline.HadoopTimelineMetricsSink (AbstractTimelineMetricsSink.java:getCurrentCollectorHost(291)) - No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times.\r\n{code}"
        }
    },
    {
        "filename": "HADOOP-15850.json",
        "creation_time": "2018-10-13T14:24:29.000+0000",
        "bug_report": {
            "Title": "CopyCommitter#concatFileChunks should check that the blocks per chunk is not 0",
            "Description": "I was investigating test failure of TestIncrementalBackupWithBulkLoad from hbase against hadoop 3.1.1\r\n\r\nhbase MapReduceBackupCopyJob$BackupDistCp would create listing file:\r\n{code}\r\n        LOG.debug(\"creating input listing \" + listing + \" , totalRecords=\" + totalRecords);\r\n        cfg.set(DistCpConstants.CONF_LABEL_LISTING_FILE_PATH, listing);\r\n        cfg.setLong(DistCpConstants.CONF_LABEL_TOTAL_NUMBER_OF_RECORDS, totalRecords);\r\n{code}\r\nFor the test case, two bulk loaded hfiles are in the listing:\r\n{code}\r\n2018-10-13 14:09:24,123 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(195): BackupDistCp : hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_\r\n2018-10-13 14:09:24,125 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(195): BackupDistCp : hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_\r\n2018-10-13 14:09:24,125 DEBUG [Time-limited test] mapreduce.MapReduceBackupCopyJob$BackupDistCp(197): BackupDistCp execute for 2 files of 10242\r\n{code}\r\nLater on, CopyCommitter#concatFileChunks would throw the following exception:\r\n{code}\r\n2018-10-13 14:09:25,351 WARN  [Thread-936] mapred.LocalJobRunner$Job(590): job_local1795473782_0004\r\njava.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/       160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-   2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}\r\n  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)\r\n  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)\r\n  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)\r\n{code}\r\nThe above warning shouldn't happen - the two bulk loaded hfiles are independent.\r\n\r\nFrom the contents of the two CopyListingFileStatus instances, we can see that their isSplit() return false. Otherwise the following from toString should be logged:\r\n{code}\r\n    if (isSplit()) {\r\n      sb.append(\", chunkOffset = \").append(this.getChunkOffset());\r\n      sb.append(\", chunkLength = \").append(this.getChunkLength());\r\n    }\r\n{code}\r\nFrom hbase side, we can specify one bulk loaded hfile per job but that defeats the purpose of using DistCp.\r\n\r\n"
        }
    },
    {
        "filename": "HADOOP-11693.json",
        "creation_time": "2015-03-05T23:19:13.000+0000",
        "bug_report": {
            "Title": "Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.",
            "Description": "One of our customers' production HBase clusters was periodically throttled by Azure storage, when HBase was archiving old WALs. HMaster aborted the region server and tried to restart it.\n\nHowever, since the cluster was still being throttled by Azure storage, the upcoming distributed log splitting also failed. Sometimes hbase:meta table was on this region server and finally showed offline, which cause the whole cluster in bad state.\n\n{code}\n2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error:\nABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller\nCause:\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)\n\tat org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)\n\tat org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 8 more\n\n2015-03-01 18:43:29,072 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN\njava.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)\n\t... 4 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 11 more\n\nSun Mar 01 18:59:51 GMT 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@aa93ac7, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}\n\nWhen archiving old WALs, WASB will do rename operation by copying src blob to destination blob and deleting the src blob. Copy blob is very costly in Azure storage and during Azure storage gc, it will be highly likely throttled. The throttling by Azure storage usually ends within 15mins. Current WASB retry policy is exponential retry, but only last at most for 2min. Short term fix will be adding a more intensive exponential retry when copy blob is throttled."
        }
    },
    {
        "filename": "HADOOP-12441.json",
        "creation_time": "2015-09-25T23:26:43.000+0000",
        "bug_report": {
            "Title": "Fix kill command behavior under some Linux distributions.",
            "Description": "After HADOOP-12317, kill command's execution will be failure under Ubuntu12. After NM restarts, it cannot get if a process is alive or not via pid of containers, and it cannot kill process correctly when RM/AM tells NM to kill a container.\n\nLogs from NM (customized logs):\n{code}\n2015-09-25 21:58:59,348 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:containerIsAlive(431)) -  ================== check alive cmd:[[Ljava.lang.String;@496e442d]\n2015-09-25 21:58:59,349 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=hrt_qa       IP=10.0.1.14    OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1443218269460_0001    CONTAINERID=container_1443218269460_0001_01_000001\n2015-09-25 21:58:59,363 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:containerIsAlive(438)) -  ===========================\nExitCodeException exitCode=1: ERROR: garbage process ID \"--\".\nUsage:\n  kill pid ...              Send SIGTERM to every process listed.\n  kill signal pid ...       Send a signal to every process listed.\n  kill -s signal pid ...    Send a signal to every process listed.\n  kill -l                   List all signal names.\n  kill -L                   List all signal names in a nice table.\n  kill -l signal            Convert between signal numbers and names.\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)\n        at org.apache.hadoop.util.Shell.run(Shell.java:461)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)\n        at java.lang.Thread.run(Thread.java:745)\n{code}"
        }
    },
    {
        "filename": "HADOOP-11685.json",
        "creation_time": "2015-03-06T22:00:14.000+0000",
        "bug_report": {
            "Title": "StorageException complaining \" no lease ID\" during HBase distributed log splitting",
            "Description": "This is similar to HADOOP-11523, but in a different place. During HBase distributed log splitting, multiple threads will access the same folder called \"recovered.edits\". However, lots of places in our WASB code did not acquire lease and simply passed null to Azure storage, which caused this issue.\n\n{code}\n2015-02-26 03:21:28,871 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: log splitting of WALs/workernode4.xxx.g6.internal.cloudapp.net,60020,1422071058425-splitting/workernode4.xxx.g6.internal.cloudapp.net%2C60020%2C1422071058425.1424914216773 failed, returning error\njava.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1019)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:223)\n\tat org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:142)\n\tat org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:79)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1477)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1862)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1812)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getRegionSplitEditsPath(HLogSplitter.java:502)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.createWAP(HLogSplitter.java:1211)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.getWriterAndPath(HLogSplitter.java:1200)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.append(HLogSplitter.java:1243)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:851)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:843)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:813)\nCaused by: java.io.IOException\n\tat com.microsoft.windowsazure.storage.core.Utility.initIOException(Utility.java:493)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:282)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1472)\n\t... 10 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:248)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:319)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:279)\n\t... 11 more\n{code}"
        }
    },
    {
        "filename": "HADOOP-8589.json",
        "creation_time": "2012-07-11T23:27:00.000+0000",
        "bug_report": {
            "Title": "ViewFs tests fail when tests and home dirs are nested",
            "Description": "TestFSMainOperationsLocalFileSystem fails in case when the test root directory is under the user's home directory, and the user's home dir is deeper than 2 levels from /. This happens with the default 1-node installation of Jenkins. \n\nThis is the failure log:\n\n{code}\norg.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here\n\tat org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)\n\tat org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2128)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)\n\tat org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)\n\n...\n\nStandard Output\n2012-07-11 22:07:20,239 INFO  mortbay.log (Slf4jLog.java:info(67)) - Home dir base /var/lib\n{code}\n\nThe reason for the failure is that the code tries to mount links for both \"/var\" and \"/var/lib\", and it fails for the 2nd one as the \"/var\" is mounted already.\n\nThe fix was provided in HADOOP-8036 but later it was reverted in HADOOP-8129."
        }
    },
    {
        "filename": "HADOOP-11754.json",
        "creation_time": "2015-03-26T05:22:54.000+0000",
        "bug_report": {
            "Title": "RM fails to start in non-secure mode due to authentication filter failure",
            "Description": "RM fails to start in the non-secure mode with the following exception:\n\n{noformat}\n2015-03-25 22:02:42,526 WARN org.mortbay.log: failed RMAuthenticationFilter: javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n2015-03-25 22:02:42,526 WARN org.mortbay.log: Failed startup of context org.mortbay.jetty.webapp.WebAppContext@6de50b08{/,jar:file:/Users/sjlee/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/cluster}\njavax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)\n\tat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)\n\tat org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)\n\tat org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)\n\tat org.mortbay.jetty.servlet.Context.startContext(Context.java:140)\n\tat org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n\tat org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n\tat org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)\n\tat org.mortbay.jetty.Server.doStart(Server.java:224)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)\n\t... 23 more\n...\n2015-03-25 22:02:42,538 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager\norg.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.io.IOException: Problem in starting http server. Server handlers failed\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)\n\t... 4 more\n{noformat}\n\nThis is likely a regression introduced by HADOOP-10670."
        }
    },
    {
        "filename": "HADOOP-8225.json",
        "creation_time": "2012-03-13T20:33:53.000+0000",
        "bug_report": {
            "Title": "DistCp fails when invoked by Oozie",
            "Description": "When DistCp is invoked through a proxy-user (e.g. through Oozie), the delegation-token-store isn't picked up by DistCp correctly. One sees failures such as:\n\nERROR [main] org.apache.hadoop.tools.DistCp: Couldn't complete DistCp\noperation: \njava.lang.SecurityException: Intercepted System.exit(-999)\n    at\norg.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)\n    at java.lang.Runtime.exit(Runtime.java:88)\n    at java.lang.System.exit(System.java:904)\n    at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)\n    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n\nLooking over the DistCp code, one sees that HADOOP_TOKEN_FILE_LOCATION isn't being copied to mapreduce.job.credentials.binary, in the job-conf. I'll post a patch for this shortly."
        }
    },
    {
        "filename": "HADOOP-10866.json",
        "creation_time": "2014-07-18T22:00:15.000+0000",
        "bug_report": {
            "Title": "RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses non-curly quotes",
            "Description": "Symlink tests failure happened from time to time,\n\nhttps://builds.apache.org/job/PreCommit-HDFS-Build/7383//testReport/\nhttps://builds.apache.org/job/PreCommit-HDFS-Build/7376/testReport/\n\n{code}\nFailed\n\norg.apache.hadoop.fs.TestSymlinkLocalFSFileContext.testDanglingLink\n\nFailing for the past 1 build (Since Failed#7376 )\nTook 83 ms.\nError Message\n\nPath file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link\nStacktrace\n\njava.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link\n\tat org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)\n\tat org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\nStandard Output\n\n2014-07-17 23:31:37,770 WARN  fs.FileUtil (FileUtil.java:symLink(829)) - Command 'ln -s /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/file /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test2/linkToFile' failed 1 with: ln: failed to create symbolic link '/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test2/linkToFile': No such file or directory\n\n2014-07-17 23:31:38,109 WARN  fs.FileUtil (FileUtil.java:symLink(829)) - Command 'ln -s /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/file /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile' failed 1 with: ln: failed to create symbolic link '/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile': File exists\n{code}\n"
        }
    },
    {
        "filename": "HADOOP-12089.json",
        "creation_time": "2015-06-15T17:34:46.000+0000",
        "bug_report": {
            "Title": "StorageException complaining \" no lease ID\" when updating FolderLastModifiedTime in WASB",
            "Description": "This is a similar issue to HADOOP-11523. HADOOP-11523 happens when HBase is doing distributed log splitting. This JIRA happens when HBase is deleting old WALs and trying to update /hbase/oldWALs folder.\n\nThe fix is the same as HADOOP-11523.\n\n{code}\n2015-06-10 08:11:40,636 WARN org.apache.hadoop.hbase.master.cleaner.CleanerChore: Error while deleting: wasb://basecus1-1@basestoragecus1.blob.core.windows.net/hbase/oldWALs/workernode10.dthbasecus1.g1.internal.cloudapp.net%2C60020%2C1433908062461.1433921692855\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)\n\tat org.apache.hadoop.hbase.Chore.run(Chore.java:80)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2991)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2597)\n\t... 8 more\n{code}"
        }
    },
    {
        "filename": "HADOOP-11934.json",
        "creation_time": "2015-05-07T00:38:53.000+0000",
        "bug_report": {
            "Title": "Use of JavaKeyStoreProvider in LdapGroupsMapping causes infinite loop",
            "Description": "I was attempting to use the LdapGroupsMapping code and the JavaKeyStoreProvider at the same time, and hit a really interesting, yet fatal, issue.  The code goes into what ought to have been an infinite loop, were it not for it overflowing the stack and Java ending the loop.  Here is a snippet of the stack; my annotations are at the bottom.\n\n{noformat}\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296){noformat}\n\nHere's my annotation, going from bottom to top.\n* Somehow we enter Path.getFileSystem()\n* This goes to FileSystem cache stuff, and then it wants the current user\n* So we get to UserGroupInformation.getCurrentUser(), which as you can imagine gets to\n* getUserToGroupsMappingService and thence to LdapGroupsMapping.setConf().\n* That code gets the needed passwords, and we're using the CredentialProvider, so unsurprisingly we get to\n* getPasswordFromCredentialProviders() - which chooses the JavaKeyStoreProvider like I told it to.\n* The JavaKeyStoreProvider, in its constructor, does \"fs = path.getFileSystem(conf);\"\n* And guess what, we're back in Path.getFileSystem, where we started at the beginning.\n\nPlease let me know if I've somehow configured something incorrectly, but if I have I can't figure out what it is..."
        }
    },
    {
        "filename": "HADOOP-11722.json",
        "creation_time": "2015-03-16T21:39:07.000+0000",
        "bug_report": {
            "Title": "Some Instances of Services using ZKDelegationTokenSecretManager go down when old token cannot be deleted",
            "Description": "The delete node code in {{ZKDelegationTokenSecretManager}} is as follows :\n{noformat}\n       while(zkClient.checkExists().forPath(nodeRemovePath) != null){\n          zkClient.delete().guaranteed().forPath(nodeRemovePath);\n       }\n{noformat}\n\nWhen instances of a Service using {{ZKDelegationTokenSecretManager}} try deleting a node simutaneously, It is possible that all of them enter into the while loop in which case, all peers will try to delete the node.. Only 1 will succeed and the rest will throw an exception.. which will bring down the node.\n\nThe Exception is as follows :\n{noformat}\n2015-03-15 10:24:54,000 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception\njava.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)\n\t... 4 more\n{noformat}  "
        }
    },
    {
        "filename": "HADOOP-15331.json",
        "creation_time": "2018-03-21T01:15:56.000+0000",
        "bug_report": {
            "Title": "Fix a race condition causing parsing error of java.io.BufferedInputStream in class org.apache.hadoop.conf.Configuration",
            "Description": "There is a race condition in the way Hadoop handles the Configuration class. The scenario is the following. Let's assume that there are two threads sharing the same Configuration class. One adds some resources to the configuration, while the other one clones it. Resources are loaded lazily in a deferred call to {{loadResources()}}. If the cloning happens after adding the resources but before parsing them, some temporary resources like input stream pointers are cloned. Eventually both copies will load the input stream resources pointing to the same input streams. One parses the input stream XML and closes it updating it's own copy of the resource. The other one has another pointer to the same input stream. When it tries to load it, it will crash with a stream closed exception.\r\n\r\nHere is an example unit test:\r\n{code:java}\r\n@Test\r\npublic void testResourceRace() {\r\n  InputStream is =\r\n      new BufferedInputStream(new ByteArrayInputStream(\r\n          \"<configuration></configuration>\".getBytes()));\r\n  Configuration conf = new Configuration();\r\n  // Thread 1\r\n  conf.addResource(is);\r\n  // Thread 2\r\n  Configuration confClone = new Configuration(conf);\r\n  // Thread 2\r\n  confClone.get(\"firstParse\");\r\n  // Thread 1\r\n  conf.get(\"secondParse\");\r\n}{code}\r\nExample real world stack traces:\r\n{code:java}\r\n2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\r\ncom.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\r\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\r\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\r\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\r\n\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)\r\n\tat org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)\r\n\tat org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:165)\r\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)\r\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)\r\n{code}\r\nAnother example:\r\n{code:java}\r\n2018-02-28 08:23:20,702 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\r\ncom.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\r\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\r\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\r\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\r\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\r\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\r\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Stream closed\r\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\r\n\t... 50 more\r\n2018-02-28 08:23:20,705 WARN org.eclipse.jetty.servlet.ServletHandler: /jmx\r\njava.lang.RuntimeException: com.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3048)\r\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\r\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\r\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\r\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\r\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\r\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\r\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\r\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\r\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.execute(ExecuteProduceConsume.java:100)\r\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.ctc.wstx.exc.WstxIOException: Stream closed\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\r\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\r\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\r\n\t... 46 more\r\nCaused by: java.io.IOException: Stream closed\r\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\r\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\r\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\r\n\t... 49 more\r\n2018-02-28 08:23:20,715 INFO org.{code}"
        }
    },
    {
        "filename": "HADOOP-14062.json",
        "creation_time": "2016-12-20T00:35:48.000+0000",
        "bug_report": {
            "Title": "ApplicationMasterProtocolPBClientImpl.allocate fails with EOFException when RPC privacy is enabled",
            "Description": "When privacy is enabled for RPC (hadoop.rpc.protection = privacy), {{ApplicationMasterProtocolPBClientImpl.allocate}} sometimes (but not always) fails with an EOFException. I've reproduced this with Spark 2.0.2 built against latest branch-2.8 and with a simple distcp job on latest branch-2.8.\n\nSteps to reproduce using distcp:\n\n1. Set hadoop.rpc.protection equal to privacy\n2. Write data to HDFS. I did this with Spark as follows: \n\n{code}\nsc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789\")).mkString(\"|\")).toDF().repartition(100).write.parquet(\"hdfs:///tmp/testData\")\n{code}\n\n3. Attempt to distcp that data to another location in HDFS. For example:\n\n{code}\nhadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy\n{code}\n\nI observed this error in the ApplicationMaster's syslog:\n\n{code}\n2016-12-19 19:13:50,097 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Event Writer setup for JobId: job_1482189777425_0004, File: hdfs://<namenode_host>:8020/tmp/hadoop-yarn/staging/<hdfs_user>/.staging/job_1482189777425_0004/job_1482189777425_0004_1.jhist\n2016-12-19 19:13:51,004 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:0 ScheduledMaps:4 ScheduledReds:0 AssignedMaps:0 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:0 ContRel:0 HostLocal:0 RackLocal:0\n2016-12-19 19:13:51,031 INFO [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1482189777425_0004: ask=1 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:22528, vCores:23> knownNMs=3\n2016-12-19 19:13:52,043 INFO [RMCommunicator Allocator] org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking ApplicationMasterProtocolPBClientImpl.allocate over null. Retrying after sleeping for 30000ms.\njava.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1428)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1338)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy80.allocate(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n\tat com.sun.proxy.$Proxy81.allocate(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)\n{code}\n\nMarking as \"critical\" since this blocks YARN users from encrypting RPC in their Hadoop clusters."
        }
    },
    {
        "filename": "HADOOP-11149.json",
        "creation_time": "2014-09-27T16:37:06.000+0000",
        "bug_report": {
            "Title": "Increase the timeout of TestZKFailoverController",
            "Description": "{code}\nRunning org.apache.hadoop.ha.TestZKFailoverController\nTests run: 19, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 56.875 sec <<< FAILURE! - in org.apache.hadoop.ha.TestZKFailoverController\ntestGracefulFailover(org.apache.hadoop.ha.TestZKFailoverController)  Time elapsed: 25.045 sec  <<< ERROR!\njava.lang.Exception: test timed out after 25000 milliseconds\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)\n\tat org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)\n\tat org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)\n\tat org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)\n\tat org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)\n\tat org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)\n\n\nResults :\n\nTests in error:\n  TestZKFailoverController.testGracefulFailover:448->Object.wait:-2 \u00bb  test time...\n{code}\n\nRunning on centos6.5"
        }
    },
    {
        "filename": "HADOOP-15059.json",
        "creation_time": "2017-11-21T20:54:52.000+0000",
        "bug_report": {
            "Title": "3.0 deployment cannot work with old version MR tar ball which breaks rolling upgrade",
            "Description": "I tried to deploy 3.0 cluster with 2.9 MR tar ball. The MR job is failed because following error:\r\n{noformat}\r\n2017-11-21 12:42:50,911 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1511295641738_0003_000001\r\n2017-11-21 12:42:51,070 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2017-11-21 12:42:51,118 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster\r\njava.lang.RuntimeException: Unable to determine current user\r\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)\r\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)\r\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)\r\n\tat org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)\r\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)\r\nCaused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens\r\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)\r\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)\r\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)\r\n\t... 4 more\r\nCaused by: java.io.IOException: Unknown version 1 in token storage.\r\n\tat org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)\r\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)\r\n\t... 8 more\r\n2017-11-21 12:42:51,122 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: Unable to determine current user\r\n{noformat}\r\nI think it is due to token incompatiblity change between 2.9 and 3.0. As we claim \"rolling upgrade\" is supported in Hadoop 3, we should fix this before we ship 3.0 otherwise all MR running applications will get stuck during/after upgrade."
        }
    },
    {
        "filename": "HADOOP-15307.json",
        "creation_time": "2018-03-12T14:56:59.000+0000",
        "bug_report": {
            "Title": "NFS: flavor AUTH_SYS should use VerifierNone",
            "Description": "When NFS gateway starts and if the portmapper request is denied by rpcbind for any reason (in our case, /etc/hosts.allow did not have the localhost), NFS gateway fails with the following obscure exception:\r\n{noformat}\r\n\r\n2018-03-05 12:49:31,976 INFO org.apache.hadoop.oncrpc.SimpleUdpServer: Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1\r\n2018-03-05 12:49:31,988 INFO org.apache.hadoop.oncrpc.SimpleTcpServer: Started listening to TCP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1\r\n2018-03-05 12:49:31,993 TRACE org.apache.hadoop.oncrpc.RpcCall: Xid:692394656, messageType:RPC_CALL, rpcVersion:2, program:100000, version:2, procedure:1, credential:(AuthFlavor:AUTH_NONE), verifier:(AuthFlavor:AUTH_NONE)\r\n2018-03-05 12:49:31,998 FATAL org.apache.hadoop.mount.MountdBase: Failed to start the server. Cause:\r\njava.lang.UnsupportedOperationException: Unsupported verifier flavorAUTH_SYS\r\nat org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)\r\nat org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)\r\nat org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)\r\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)\r\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)\r\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)\r\nat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)\r\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\r\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\r\nat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\r\n2018-03-05 12:49:32,007 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1{noformat}\r\n\u00a0Reading the code comment for class Verifier, I think this bug existed since its inception\r\n{code:java}\r\n/**\r\n * Base class for verifier. Currently our authentication only supports 3 types\r\n * of auth flavors: {@link RpcAuthInfo.AuthFlavor#AUTH_NONE}, {@link RpcAuthInfo.AuthFlavor#AUTH_SYS},\r\n * and {@link RpcAuthInfo.AuthFlavor#RPCSEC_GSS}. Thus for verifier we only need to handle\r\n * AUTH_NONE and RPCSEC_GSS\r\n */\r\npublic abstract class Verifier extends RpcAuthInfo {{code}\r\nThe verifier should also handle AUTH_SYS too."
        }
    },
    {
        "filename": "HADOOP-11446.json",
        "creation_time": "2014-12-23T22:15:23.000+0000",
        "bug_report": {
            "Title": "S3AOutputStream should use shared thread pool to avoid OutOfMemoryError",
            "Description": "When working with Terry Padgett who used s3a for hbase snapshot, the following issue was uncovered.\nHere is part of the output including the OOME when hbase snapshot is exported to s3a (nofile ulimit was increased to 102400):\n{code}\n2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: OutputStream for key 'FastQueryPOC/2014-12-11/EVENT1-IDX-snapshot/.hbase-snapshot/.tmp/EVENT1_IDX_snapshot_2012_12_11/    650a5678810fbdaa91809668d11ccf09/.regioninfo' closed. Now beginning upload\n2014-12-19 13:15:03,895 INFO  [main] s3a.S3AFileSystem: Minimum upload part size: 16777216 threshold2147483647\nException in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:713)\n        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)\n        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)\n        at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)\n        at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)\n{code}\nIn S3AOutputStream#close():\n{code}\n      TransferManager transfers = new TransferManager(client);\n{code}\nThis results in each TransferManager creating its own thread pool, leading to the OOME.\nOne solution is to pass shared thread pool to TransferManager."
        }
    },
    {
        "filename": "HADOOP-12689.json",
        "creation_time": "2016-01-04T23:30:49.000+0000",
        "bug_report": {
            "Title": "S3 filesystem operations stopped working correctly",
            "Description": "HADOOP-10542 was resolved by replacing \"return null;\" with throwing  IOException.   This causes several S3 filesystem operations to fail (possibly more code is expecting that null return value; these are just the calls I noticed):\n\nS3FileSystem.getFileStatus() (which no longer raises FileNotFoundException but instead IOException)\nFileSystem.exists() (which no longer returns false but instead raises IOException)\nS3FileSystem.create() (which no longer succeeds but instead raises IOException)\n\nRun command:\n\nhadoop distcp hdfs://localhost:9000/test s3://xxx:yyy@com.bar.foo/\n\nResulting stack trace:\n\n2015-12-11 10:04:34,030 FATAL [IPC Server handler 6 on 44861] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1449826461866_0005_m_000006_0 - exited : java.io.IOException: /test doesn't exist\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\nat com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)\nat org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)\nat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\nat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\nchanging the \"raise IOE...\" to \"return null\" fixes all of the above code sites and allows distcp to succeed.\n"
        }
    },
    {
        "filename": "HADOOP-13132.json",
        "creation_time": "2016-05-11T11:42:30.000+0000",
        "bug_report": {
            "Title": "Handle ClassCastException on AuthenticationException in LoadBalancingKMSClientProvider",
            "Description": "An Oozie job with a single shell action fails (may not be important, but if you needs the exact details I can provide them) with an error message coming from NodeManager:\n{code}\n2016-05-10 11:10:14,290 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[LogAggregationService #652,5,main] threw an Exception.\njava.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException\nat org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)\nat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)\nat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)\nat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:59)\nat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:683)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:679)\nat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\nat org.apache.hadoop.fs.FileContext.create(FileContext.java:679)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:382)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:377)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:456)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:421)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:384)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n{code}\n\nThe unsafe cast is here:\nhttps://github.com/apache/hadoop/blob/2e1d0ff4e901b8313c8d71869735b94ed8bc40a0/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java#L189\n\nBecause of this ClassCastException:\n- an uncaught exception is raised\n- we do not see the exact \"caused by\" exception/message\n- the oozie job fails\n- YARN logs are not reported/saved"
        }
    },
    {
        "filename": "HADOOP-15121.json",
        "creation_time": "2017-12-15T07:41:39.000+0000",
        "bug_report": {
            "Title": "Encounter NullPointerException when using DecayRpcScheduler",
            "Description": "I set ipc.8020.scheduler.impl to org.apache.hadoop.ipc.DecayRpcScheduler, but got excetion in namenode:\r\n{code}\r\n2017-12-15 15:26:34,662 ERROR impl.MetricsSourceAdapter (MetricsSourceAdapter.java:getMetrics(202)) - Error getting metrics from source DecayRpcSchedulerMetrics2.ipc.8020\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)\r\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)\r\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)\r\n        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)\r\n        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)\r\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)\r\n        at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)\r\n        at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)\r\n        at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)\r\n        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)\r\n        at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)\r\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)\r\n{code}\r\nIt seems that {{metricsProxy}} in DecayRpcScheduler should initiate its {{delegate}} field in its Initialization method\r\n"
        }
    },
    {
        "filename": "HADOOP-8110.json",
        "creation_time": "2012-02-24T18:49:27.000+0000",
        "bug_report": {
            "Title": "TestViewFsTrash occasionally fails",
            "Description": "{noformat}\njunit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.failNotEquals(Assert.java:283)\n\tat junit.framework.Assert.assertEquals(Assert.java:64)\n\tat junit.framework.Assert.assertEquals(Assert.java:195)\n\tat org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)\n\tat org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)\n\t...\n{noformat}\nThere are quite a few TestViewFsTrash failures recently.  E.g. [build #624 for trunk|https://builds.apache.org/job/PreCommit-HADOOP-Build/624//testReport/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/] and [build #2 for 0.23-PB|https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Common-0.23-PB-Build/2/testReport/junit/org.apache.hadoop.fs.viewfs/TestViewFsTrash/testTrash/].\n"
        }
    },
    {
        "filename": "HADOOP-11400.json",
        "creation_time": "2014-12-12T10:05:52.000+0000",
        "bug_report": {
            "Title": "GraphiteSink does not reconnect to Graphite after 'broken pipe'",
            "Description": "I see that after network error GraphiteSink does not reconnects to Graphite server and in effect metrics are not sent. \n\nHere is stacktrace I see (this is from nodemanager):\n\n2014-12-11 16:39:21,655 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception, retry in 4806ms\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)\n        ... 5 more\n2014-12-11 16:39:26,463 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception and over retry limit, suppressing further error messages\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)\n        ... 5 more\n\n\nGraphiteSinkFixed.java is simply GraphiteSink.java from Hadoop 2.6.0 (with fixed https://issues.apache.org/jira/browse/HADOOP-11182) because I cannot simply upgrade Hadoop (I am using CDH5).\n\nI see that GraphiteSink is using OutputStreamWriter which is created only in init method (which is probably called only once per application runtime) and there is no reconnection logic."
        }
    },
    {
        "filename": "HADOOP-9865.json",
        "creation_time": "2013-08-12T23:22:58.000+0000",
        "bug_report": {
            "Title": "FileContext.globStatus() has a regression with respect to relative path",
            "Description": "I discovered the problem when running unit test TestMRJobClient on Windows. The cause is indirect in this case. In the unit test, we try to launch a job and list its status. The job failed, and caused the list command get a result of 0, which triggered the unit test assert. From the log and debug, the job failed because we failed to create the Jar with classpath (see code around {{FileUtil.createJarWithClassPath}}) in {{ContainerLaunch}}. This is a Windows specific step right now; so the test still passes on Linux. This step failed because we passed in a relative path to {{FileContext.globStatus()}} in {{FileUtil.createJarWithClassPath}}. The relevant log looks like the following.\n\n{noformat}\n2013-08-12 16:12:05,937 WARN  [ContainersLauncher #0] launcher.ContainerLaunch (ContainerLaunch.java:call(270)) - Failed to launch container.\norg.apache.hadoop.HadoopIllegalArgumentException: Path is relative\n\tat org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)\n\tat org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)\n\tat org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:128)\n\tat org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)\n\tat org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n{noformat}\n\nI think this is a regression from HADOOP-9817. I modified some code and the unit test passed. (See the attached patch.) However, I think the impact is larger. I will add some unit tests to verify the behavior, and work on a more complete fix."
        }
    },
    {
        "filename": "HADOOP-9977.json",
        "creation_time": "2013-09-17T22:11:54.000+0000",
        "bug_report": {
            "Title": "Hadoop services won't start with different keypass and keystorepass when https is enabled",
            "Description": "Enable ssl in the configuration. While creating keystore, give different keypass and keystore password. (here, keypass = hadoop and storepass=hadoopKey)\n\nkeytool -genkey -alias host1 -keyalg RSA -keysize 1024 -dname \"CN=host1,OU=cm,O=cm,L=san jose,ST=ca,C=us\" -keypass hadoop -keystore keystore.jks -storepass hadoopKey\n\nIn , ssl-server.xml set below two properties.\n<property><name>ssl.server.keystore.keypassword</name><value>hadoop</value></property>\n<property><name>ssl.server.keystore.password</name><value>hadoopKey</value></property>\n\nNamenode, ResourceManager, Datanode, Nodemanager, SecondaryNamenode fails to start with below error.\n\n2013-09-17 21:39:00,794 FATAL namenode.NameNode (NameNode.java:main(1325)) - Exception in namenode join\njava.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)\nCaused by: java.security.UnrecoverableKeyException: Cannot recover key\n        at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)\n        at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)\n        at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)\n        at java.security.KeyStore.getKey(KeyStore.java:792)\n        at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)\n        at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)\n        at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:259)\n        at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)\n        at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)\n        ... 9 more"
        }
    },
    {
        "filename": "HADOOP-12611.json",
        "creation_time": "2015-12-01T17:14:38.000+0000",
        "bug_report": {
            "Title": "TestZKSignerSecretProvider#testMultipleInit occasionally fail",
            "Description": "https://builds.apache.org/job/Hadoop-Common-trunk/2053/testReport/junit/org.apache.hadoop.security.authentication.util/TestZKSignerSecretProvider/testMultipleInit/\r\n\r\nError Message\r\n\r\nexpected null, but was:<[B@142bad79>\r\n\r\nStacktrace\r\n\r\njava.lang.AssertionError: expected null, but was:<[B@142bad79>\r\n\tat org.junit.Assert.fail(Assert.java:88)\r\n\tat org.junit.Assert.failNotNull(Assert.java:664)\r\n\tat org.junit.Assert.assertNull(Assert.java:646)\r\n\tat org.junit.Assert.assertNull(Assert.java:656)\r\n\tat org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)\r\n\r\n\r\nI think the failure was introduced after HADOOP-12181\r\n\r\nThis is likely where the root cause is:\r\n\r\n2015-11-29 00:24:33,325 ERROR ZKSignerSecretProvider - An unexpected exception occurred while pulling data fromZooKeeper\r\njava.lang.IllegalStateException: instance must be started before calling this method\r\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\r\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.CGLIB$rollSecret$2(<generated>)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8$$FastClassByMockitoWithCGLIB$$6f94a716.invoke(<generated>)\r\n\tat org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216)\r\n\tat org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10)\r\n\tat org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22)\r\n\tat org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27)\r\n\tat org.mockito.internal.invocation.Invocation.callRealMethod(Invocation.java:211)\r\n\tat org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36)\r\n\tat org.mockito.internal.MockHandler.handle(MockHandler.java:99)\r\n\tat org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)\r\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.rollSecret(<generated>)\r\n\tat org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider$1.run(RolloverSignerSecretProvider.java:97)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "HADOOP-10142.json",
        "creation_time": "2013-12-03T06:33:32.000+0000",
        "bug_report": {
            "Title": "Avoid groups lookup for unprivileged users such as \"dr.who\"",
            "Description": "Reduce the logs generated by ShellBasedUnixGroupsMapping.\nFor ex: Using WebHdfs from windows generates following log for each request\n\n{noformat}2013-12-03 11:34:56,589 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user dr.who\norg.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)\n        at org.apache.hadoop.util.Shell.run(Shell.java:417)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)\n        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)\n        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)\n        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n2013-12-03 11:34:56,590 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user dr.who{noformat}"
        }
    },
    {
        "filename": "HADOOP-14727.json",
        "creation_time": "2017-08-02T21:56:38.000+0000",
        "bug_report": {
            "Title": "Socket not closed properly when reading Configurations with BlockReaderRemote",
            "Description": "This is caught by Cloudera's internal testing over the alpha4 release.\n\nWe got reports that some hosts ran out of FDs. Triaging that, found out both oozie server and Yarn JobHistoryServer have tons of sockets on {{CLOSE_WAIT}} state.\n\n[~haibochen] helped narrow down to a consistent reproduction by simply visiting the JHS web UI, and clicking through a job and its logs.\n\nI then look at the {{BlockReaderRemote}} and related code, and didn't spot any leaks in the implementation. After adding a debug log whenever a {{Peer}} is created/closed/in/out {{PeerCache}}, it looks like all the {{CLOSE_WAIT}} sockets are created from this call stack:\n{noformat}\n2017-08-02 13:58:59,901 INFO org.apache.hadoop.hdfs.client.impl.BlockReaderFactory: ____ associated peer NioInetPeer(Socket[addr=/10.17.196.28,port=20002,localport=42512]) with blockreader org.apache.hadoop.hdfs.client.impl.BlockReaderRemote@717ce109\njava.lang.Exception: test\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)\n        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)\n        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)\n        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)\n        at java.io.DataInputStream.read(DataInputStream.java:149)\n        at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n        at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:647)\n        at com.ctc.wstx.stax.WstxInputFactory.createXMLStreamReader(WstxInputFactory.java:366)\n        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)\n        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1126)\n        at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1344)\n        at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)\n        at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.<init>(CompletedJob.java:105)\n        at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:100)\n        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)\n        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)\n        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)\n        at com.google.common.cache.LocalCache.get(LocalCache.java:3965)\n        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)\n        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)\n        at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)\n        at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)\n        at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1552)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n        at org.eclipse.jetty.server.Server.handle(Server.java:534)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:748)\n{noformat}\n\nI was able to further confirm this theory by backing out the 4 recent commits to {{Configuration}} on alpha3 and no longer seeing {{CLOSE_WAIT}} sockets.\n- HADOOP-14501. \n- HADOOP-14399. (only reverted to make other reverts easier)\n- HADOOP-14216. Addendum \n- HADOOP-14216. \n\nIt's not clear to me who's responsible to close the InputStream though."
        }
    }
]