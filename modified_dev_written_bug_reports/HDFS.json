[
    {
        "filename": "HDFS-4558.json",
        "creation_time": "2013-03-06T11:10:08.000+0000",
        "bug_report": {
            "Title": "start balancer failed with NPE",
            "Description": "start balancer failed with NPE\n File this issue to track for QE and dev take a look\n\nbalancer.log:\n 2013-03-06 00:19:55,174 ERROR org.apache.hadoop.hdfs.server.balancer.Balancer: java.lang.NullPointerException\n at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)\n\nBalancer.java\n private void checkReplicationPolicyCompatibility(Configuration conf)\n throws UnsupportedActionException {\n if (!(BlockPlacementPolicy.getInstance(conf, null, null) <== here\n instanceof BlockPlacementPolicyDefault)) \n{ throw new UnsupportedActionException( \"Balancer without BlockPlacementPolicyDefault\"); }\n }\n"
        }
    },
    {
        "filename": "HDFS-13039.json",
        "creation_time": "2018-01-19T18:52:12.000+0000",
        "bug_report": {
            "Title": "StripedBlockReader#createBlockReader leaks socket on IOException",
            "Description": "When running EC on one cluster, DataNode has millions of {{CLOSE_WAIT}} connections\r\n{code:java}\r\n$ grep CLOSE_WAIT lsof.out | wc -l\r\n10358700\r\n\r\n// All CLOSW_WAITs belong to the same DataNode process (pid=88527)\r\n$ grep CLOSE_WAIT lsof.out | awk '{print $2}' | sort | uniq\r\n88527\r\n{code}\r\n\r\nAnd DN can not open any file / socket, as shown in the log:\r\n\r\n{noformat}\r\n2018-01-19 06:47:09,424 WARN io.netty.channel.DefaultChannelPipeline: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\r\njava.io.IOException: Too many open files\r\n        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\r\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\r\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\r\n        at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)\r\n        at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)\r\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)\r\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)\r\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)\r\n        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n{noformat}\r\n"
        }
    },
    {
        "filename": "HDFS-13023.json",
        "creation_time": "2018-01-16T20:27:39.000+0000",
        "bug_report": {
            "Title": "Journal Sync does not work on a secure cluster",
            "Description": "Fails with the following exception.\r\n\r\n{code}\r\n\r\n2018-01-10 01:15:40,517 INFO server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(235)) - Syncing Journal /0.0.0.0:8485 with xxx, journal id: mycluster\r\n 2018-01-10 01:15:40,583 ERROR server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(259)) - Could not sync with Journal at xxx/xxx:8485\r\n com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)\r\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)\r\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)\r\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)\r\n at java.lang.Thread.run(Thread.java:748)\r\n Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx\r\n at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)\r\n at org.apache.hadoop.ipc.Client.call(Client.java:1437)\r\n at org.apache.hadoop.ipc.Client.call(Client.java:1347)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n ... 6 more\r\n\r\n{code}"
        }
    },
    {
        "filename": "HDFS-3157.json",
        "creation_time": "2012-03-28T14:35:53.000+0000",
        "bug_report": {
            "Title": "Error in deleting block is keep on coming from DN even after the block report and directory scanning has happened",
            "Description": "Cluster setup:\n\n1NN,Three DN(DN1,DN2,DN3),replication factor-2,\"dfs.blockreport.intervalMsec\" 300,\"dfs.datanode.directoryscan.interval\" 1\n\nstep 1: write one file \"a.txt\" with sync(not closed)\nstep 2: Delete the blocks in one of the datanode say DN1(from rbw) to which replication happened.\nstep 3: close the file.\n\nSince the replication factor is 2 the blocks are replicated to the other datanode.\n\nThen at the NN side the following cmd is issued to DN from which the block is deleted\n-------------------------------------------------------------------------------------\n{noformat}\n2012-03-19 13:41:36,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK NameSystem.addToCorruptReplicasMap: duplicate requested for blk_2903555284838653156 to add as corrupt on XX.XX.XX.XX by /XX.XX.XX.XX because reported RBW replica with genstamp 1002 does not match COMPLETE block's genstamp in block map 1003\n2012-03-19 13:41:39,588 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* Removing block blk_2903555284838653156_1003 from neededReplications as it has enough replicas.\n{noformat}\n\nFrom the datanode side in which the block is deleted the following exception occured\n\n\n{noformat}\n2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block blk_2903555284838653156_1003. BlockInfo not found in volumeMap.\n2012-02-29 13:54:13,126 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command\njava.io.IOException: Error in deleting blocks.\n\tat org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)\n\tat java.lang.Thread.run(Thread.java:619)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4850.json",
        "creation_time": "2013-05-25T00:18:15.000+0000",
        "bug_report": {
            "Title": "fix OfflineImageViewer to work on fsimages with empty files or snapshots",
            "Description": "I deployed hadoop-trunk HDFS and created _/user/schu/_. I then forced a checkpoint, fetched the fsimage, and ran the default OfflineImageViewer successfully on the fsimage.\n\n{code}\nschu-mbp:~ schu$ hdfs oiv -i fsimage_0000000000000000004 -o oiv_out_1\nschu-mbp:~ schu$ cat oiv_out_1\ndrwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /\ndrwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /user\ndrwxr-xr-x  -     schu supergroup          0 2013-05-24 16:59 /user/schu\nschu-mbp:~ schu$ \n{code}\n\nI then touched an empty file _/user/schu/testFile1_\n{code}\nschu-mbp:~ schu$ hadoop fs -lsr /\nlsr: DEPRECATED: Please use 'ls -R' instead.\ndrwxr-xr-x   - schu supergroup          0 2013-05-24 16:59 /user\ndrwxr-xr-x   - schu supergroup          0 2013-05-24 17:00 /user/schu\n-rw-r--r--   1 schu supergroup          0 2013-05-24 17:00 /user/schu/testFile1\n{code}\n\nand forced another checkpoint, fetched the fsimage, and reran the OfflineImageViewer. I encountered a NegativeArraySizeException:\n\n\n{code}\nschu-mbp:~ schu$ hdfs oiv -i fsimage_0000000000000000008 -o oiv_out_2\nInput ended unexpectedly.\n2013-05-24 17:01:13,622 ERROR [main] offlineImageViewer.OfflineImageViewer (OfflineImageViewer.java:go(140)) - image loading failed at offset 402\nException in thread \"main\" java.lang.NegativeArraySizeException\n\tat org.apache.hadoop.io.Text.readString(Text.java:458)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)\n{code}\n\nThis is reproducible. I've reproduced this scenario after formatting HDFS and restarting and touching an empty file _/testFile1_.\n\nAttached are the data dirs, the fsimage before creating the empty file (fsimage_0000000000000000004) and the fsimage afterwards (fsimage_0000000000000000004) and their outputs, oiv_out_1 and oiv_out_2 respectively.\n\n\nThe oiv_out_2 does not include the empty _/user/schu/testFile1_.\n\nI don't run into this problem using hadoop-2.0.4-alpha."
        }
    },
    {
        "filename": "HDFS-3415.json",
        "creation_time": "2012-05-13T13:44:43.000+0000",
        "bug_report": {
            "Title": "During NameNode starting up, it may pick wrong storage directory inspector when the layout versions of the storage directories are different",
            "Description": "Scenario:\n=========\nstart Namenode and datanode by configuring three storage dir's for namenode\nwrite 10 files\nedit version file of one of the storage dir and give layout version as 123 which different with default(-40).\nStop namenode\nstart Namenode.\n\n\nThen I am getting follwong exception...\n\n\n{noformat}\n2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)\n2012-05-13 19:01:41,485 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: \n\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-2245.json",
        "creation_time": "2011-08-10T22:55:33.000+0000",
        "bug_report": {
            "Title": "BlockManager.chooseTarget(..) throws NPE",
            "Description": "{noformat}\n2011-08-10 20:20:51,350 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call: addBlock(/user/had\noopqa/passwd.1108102020.<NN hostname>.txt, DFSClient_NONMAPREDUCE_1875954430_1, null, null), rpc\n version=1, client version=68, methodsFingerPrint=-1239577025 from <gateway>:38874, error:\njava.io.IOException: java.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)\n        ...\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-10320.json",
        "creation_time": "2016-04-20T23:02:09.000+0000",
        "bug_report": {
            "Title": "Rack failures may result in NN terminate",
            "Description": "If there're rack failures which end up leaving only 1 rack available, {{BlockPlacementPolicyDefault#chooseRandom}} may get {{InvalidTopologyException}} when calling {{NetworkTopology#chooseRandom}}, which then throws all the way out to {{BlockManager}}'s {{ReplicationMonitor}} thread and terminate the NN.\n\nLog:\n{noformat}\n2016-02-24 09:22:01,514  WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy\n\n2016-02-24 09:22:01,958  ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception. \norg.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4201.json",
        "creation_time": "2012-11-16T21:28:57.000+0000",
        "bug_report": {
            "Title": "NPE in BPServiceActor#sendHeartBeat",
            "Description": "Saw the following NPE in a log.\n\nThink this is likely due to {{dn}} or {{dn.getFSDataset()}} being null, (not {{bpRegistration}}) due to a configuration or local directory failure.\n\n{code}\n2012-09-25 04:33:20,782 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode svsrs00127/11.164.162.226:8020 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec Initial delay: 0msec; heartBeatInterval=3000\n2012-09-25 04:33:20,782 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in BPOfferService for Block pool BP-1678908700-11.164.162.226-1342785481826 (storage id DS-1031100678-11.164.162.251-5010-1341933415989) service to svsrs00127/11.164.162.226:8020\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)\n        at java.lang.Thread.run(Thread.java:722)\n{code}"
        }
    },
    {
        "filename": "HDFS-6904.json",
        "creation_time": "2014-08-21T10:17:21.000+0000",
        "bug_report": {
            "Title": "YARN unable to renew delegation token fetched via webhdfs due to incorrect service port",
            "Description": "YARN is unable to renew delegation tokens obtained via the WebHDFS REST API. The scenario is as follows -\n\n1. User creates a delegation token using the WebHDFS REST API\n2. User passes this token to YARN as part of app submission(via the YARN REST API)\n3. When YARN tries to renew this delegation token, it fails because the token service is pointing to the RPC port but the token kind is WebHDFS.\n\nThe exception is\n\n{noformat}\n2014-08-19 03:12:54,733 WARN  security.DelegationTokenRenewer (DelegationTokenRenewer.java:handleDTRenewerAppSubmitEvent(661)) - Unable to add the application to the delegation token renewer.\njava.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$5(DelegationTokenRenewer.java:357)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:657)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:638)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:331)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:598)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:448)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:477)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:473)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.renewDelegationToken(WebHdfsFileSystem.java:1318)\n        at org.apache.hadoop.hdfs.web.TokenAspect$TokenManager.renew(TokenAspect.java:73)\n        at org.apache.hadoop.security.token.Token.renew(Token.java:377)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:477)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:1)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:392)\n        ... 6 more\nCaused by: java.io.IOException: The error stream is null.\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:304)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:329)\n        ... 24 more\n2014-08-19 03:12:54,735 DEBUG event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent.EventType: APP_REJECTED\n{noformat}\n\nI suspect the issue is that the Namenode generates a delegation token of kind WebHDFS but doesn't change the service port. When YARN tries to renew the delegation token, it ends up trying to contact WebHDFS on the RPC port.\n\nFrom NamenodeWebHdfsMethods.java\n{noformat}\n    case GETDELEGATIONTOKEN:\n    {\n      if (delegation.getValue() != null) {\n        throw new IllegalArgumentException(delegation.getName()\n            + \" parameter is not null.\");\n      }\n      final Token<? extends TokenIdentifier> token = generateDelegationToken(\n          namenode, ugi, renewer.getValue());\n      final String js = JsonUtil.toJsonString(token);\n      return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n    }\n{noformat}\nwhich in turn calls\n{noformat}\n  private Token<? extends TokenIdentifier> generateDelegationToken(\n      final NameNode namenode, final UserGroupInformation ugi,\n      final String renewer) throws IOException {\n    final Credentials c = DelegationTokenSecretManager.createCredentials(\n        namenode, ugi, renewer != null? renewer: ugi.getShortUserName());\n    final Token<? extends TokenIdentifier> t = c.getAllTokens().iterator().next();\n    Text kind = request.getScheme().equals(\"http\") ? WebHdfsFileSystem.TOKEN_KIND\n        : SWebHdfsFileSystem.TOKEN_KIND;\n    t.setKind(kind);\n    return t;\n  }\n{noformat}\n\nThe command we used to get the delegation token is -\n{noformat}\ncurl -i -k -s --negotiate -u : 'http://NameNodeHost:50070/webhdfs/v1?op=GETDELEGATIONTOKEN&renewer=yarn'\n{noformat}"
        }
    },
    {
        "filename": "HDFS-13721.json",
        "creation_time": "2018-07-05T20:11:57.000+0000",
        "bug_report": {
            "Title": "NPE in DataNode due to uninitialized DiskBalancer",
            "Description": "{noformat}\r\n2018-06-28 05:11:47,650 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception\r\njavax.management.RuntimeMBeanException: java.lang.NullPointerException\r\n ***** TRACEBACK 4 *****\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)\r\n at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)\r\n at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)\r\n at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)\r\n at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)\r\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\r\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\r\n at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)\r\n at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\r\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\r\n at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\r\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\r\n at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\r\n at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\r\n at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\r\n at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\r\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\r\n at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\r\n at org.eclipse.jetty.server.Server.handle(Server.java:534)\r\n at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\r\n at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\r\n at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\r\n at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\r\n at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\r\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\r\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\r\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\r\n at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\r\n at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\r\n at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.NullPointerException\r\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)\r\n at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\r\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n at java.lang.reflect.Method.invoke(Method.java:498)\r\n at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)\r\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)\r\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)\r\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)\r\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)\r\n at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)\r\n at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)\r\n at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)\r\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)\r\n2018-06-28 05:12:08,400 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception\r\njavax.management.RuntimeMBeanException: java.lang.NullPointerException\r\n{noformat}\r\n\r\nWe have seen the above exception at datanode startup time. Should improve the NPE. Changing it to an IOE will also allow jmx to return '' correctly for \\{{getDiskBalancerStatus}}\r\n\r\n.\r\n\r\n\u00a0"
        }
    },
    {
        "filename": "HDFS-7180.json",
        "creation_time": "2014-10-02T03:07:04.000+0000",
        "bug_report": {
            "Title": "NFSv3 gateway frequently gets stuck due to GC",
            "Description": "We are using Hadoop 2.5.0 (HDFS only) and start and mount the NFSv3 gateway on one node in the cluster to let users upload data with rsync.\n\nHowever, we find the NFSv3 daemon seems frequently get stuck while the HDFS seems working well. (hdfds dfs -ls and etc. works just well). The last stuck we found is after around 1 day running and several hundreds GBs of data uploaded.\n\nThe NFSv3 daemon is started on one node and on the same node the NFS is mounted.\n\nFrom the node where the NFS is mounted:\n\ndmsg shows like this:\n\n[1859245.368108] nfs: server localhost not responding, still trying\n[1859245.368111] nfs: server localhost not responding, still trying\n[1859245.368115] nfs: server localhost not responding, still trying\n[1859245.368119] nfs: server localhost not responding, still trying\n[1859245.368123] nfs: server localhost not responding, still trying\n[1859245.368127] nfs: server localhost not responding, still trying\n[1859245.368131] nfs: server localhost not responding, still trying\n[1859245.368135] nfs: server localhost not responding, still trying\n[1859245.368138] nfs: server localhost not responding, still trying\n[1859245.368142] nfs: server localhost not responding, still trying\n[1859245.368146] nfs: server localhost not responding, still trying\n[1859245.368150] nfs: server localhost not responding, still trying\n[1859245.368153] nfs: server localhost not responding, still trying\n\nThe mounted directory can not be `ls` and `df -hT` gets stuck too.\n\nThe latest lines from the nfs3 log in the hadoop logs directory:\n\n2014-10-02 05:43:20,452 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated user map size: 35\n2014-10-02 05:43:20,461 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated group map size: 54\n2014-10-02 05:44:40,374 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:44:40,732 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:46:06,535 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:46:26,075 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:47:56,420 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:48:56,477 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:51:46,750 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:53:23,809 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:53:24,508 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:55:57,334 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:57:07,428 INFO org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx: Have to change stable write to unstable write:FILE_SYNC\n2014-10-02 05:58:32,609 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Update cache now\n2014-10-02 05:58:32,610 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Not doing static UID/GID mapping because '/etc/nfs.map' does not exist.\n2014-10-02 05:58:32,620 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated user map size: 35\n2014-10-02 05:58:32,628 INFO org.apache.hadoop.nfs.nfs3.IdUserGroup: Updated group map size: 54\n2014-10-02 06:01:32,098 WARN org.apache.hadoop.hdfs.DFSClient: Slow ReadProcessor read fields took 60062ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.0.3.172:50010, 10.0.3.176:50010]\n2014-10-02 06:01:32,099 WARN org.apache.hadoop.hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643\njava.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:828)\n2014-10-02 06:07:00,368 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 in pipeline 10.0.3.172:50010, 10.0.3.176:50010: bad datanode 10.0.3.176:50010\n\nThe logs seems suggest 10.0.3.176 is bad. However, from the `hdfs dfsadmin -report`, all nodes in the cluster seems working.\n\nAny help will be appreciated. Thanks in advance."
        }
    },
    {
        "filename": "HDFS-6102.json",
        "creation_time": "2014-03-13T18:27:36.000+0000",
        "bug_report": {
            "Title": "Lower the default maximum items per directory to fix PB fsimage loading",
            "Description": "Found by [~schu] during testing. We were creating a bunch of directories in a single directory to blow up the fsimage size, and it ends up we hit this error when trying to load a very large fsimage:\n\n{noformat}\n2014-03-13 13:57:03,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 24523605 INodes.\n2014-03-13 13:57:59,038 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Failed to load image from FSImageFile(file=/dfs/nn/current/fsimage_0000000000024532742, cpktTxId=0000000000024532742)\ncom.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.\n        at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)\n        at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)\n        at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)\n        at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)\n        at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)\n        at 52)\n...\n{noformat}\n\nSome further research reveals there's a 64MB max size per PB message, which seems to be what we're hitting here."
        }
    },
    {
        "filename": "HDFS-6250.json",
        "creation_time": "2014-04-16T16:14:32.000+0000",
        "bug_report": {
            "Title": "TestBalancerWithNodeGroup.testBalancerWithRackLocality fails",
            "Description": "It was seen in https://builds.apache.org/job/PreCommit-HDFS-Build/6669/\n\n{panel}\njava.lang.AssertionError: expected:<1800> but was:<1810>\n\tat org.junit.Assert.fail(Assert.java:93)\n\tat org.junit.Assert.failNotEquals(Assert.java:647)\n\tat org.junit.Assert.assertEquals(Assert.java:128)\n\tat org.junit.Assert.assertEquals(Assert.java:147)\n\tat org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup\n .testBalancerWithRackLocality(TestBalancerWithNodeGroup.java:253)\n{panel}"
        }
    },
    {
        "filename": "HDFS-11377.json",
        "creation_time": "2017-01-26T23:40:53.000+0000",
        "bug_report": {
            "Title": "Balancer hung due to no available mover threads",
            "Description": "When running balancer on large cluster which have more than 3000 Datanodes, it might be hung due to \"No mover threads available\".\nThe stack trace shows it waiting forever like below.\n{code}\n\"main\" #1 prio=5 os_prio=0 tid=0x00007ff6cc014800 nid=0x6b2c waiting on condition [0x00007ff6d1bad000]\n   java.lang.Thread.State: TIMED_WAITING (sleeping)\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)\n{code}\n\nIn the log, there are lots of WARN about \"No mover threads available\".\n{quote}\n2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_13700554102_1112815018180 with size=268435456 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.137:50010\n2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_4009558842_1103118359883 with size=268435456 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.137:50010\n2017-01-26 15:36:40,085 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: No mover threads available: skip moving blk_13881956058_1112996460026 with size=133509566 from 10.115.67.137:50010:DISK to 10.140.21.55:50010:DISK through 10.115.67.36:50010\n{quote}\n\nWhat happened here is, when there are no mover threads available, DDatanode.isPendingQEmpty() will return false, so Balancer hung."
        }
    },
    {
        "filename": "HDFS-6753.json",
        "creation_time": "2014-07-25T11:36:43.000+0000",
        "bug_report": {
            "Title": "Initialize checkDisk when DirectoryScanner not able to get files list for scanning",
            "Description": "Env Details :\n=============\nCluster has 3 Datanode\nCluster installed with \"Rex\" user\ndfs.datanode.failed.volumes.tolerated  = 3\ndfs.blockreport.intervalMsec                  = 18000\ndfs.datanode.directoryscan.interval     = 120\nDN_XX1.XX1.XX1.XX1 data dir                         = /mnt/tmp_Datanode,/home/REX/data/dfs1/data,/home/REX/data/dfs2/data,/opt/REX/dfs/data\n \n \n/home/REX/data/dfs1/data,/home/REX/data/dfs2/data,/opt/REX/dfs/data - permission is denied ( hence DN considered the volume as failed )\n \nExpected behavior is observed when disk is not full:\n========================================\n \nStep 1: Change the permissions of /mnt/tmp_Datanode to root\n \nStep 2: Perform write operations ( DN detects that all Volume configured is failed and gets shutdown )\n \nScenario 1: \n===========\n \nStep 1 : Make /mnt/tmp_Datanode disk full and change the permissions to root\nStep 2 : Perform client write operations ( disk full exception is thrown , but Datanode is not getting shutdown ,  eventhough all the volume configured has failed)\n \n{noformat}\n \n2014-07-21 14:10:52,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: XX1.XX1.XX1.XX1:50010:DataXceiver error processing WRITE_BLOCK operation  src: /XX2.XX2.XX2.XX2:10106 dst: /XX1.XX1.XX1.XX1:50010\n \norg.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).\n \nat org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)\n \n{noformat}\n \nObservations :\n==============\n1. Write operations does not shutdown Datanode , eventhough all the volume configured is failed ( When one of the disk is full and for all the disk permission is denied)\n \n2. Directory scannning fails , still DN is not getting shutdown\n \n \n \n{noformat}\n \n2014-07-21 14:13:00,180 WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception occured while compiling report: \n \njava.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized\n \nat org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)\n \nat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)\n \n{noformat}"
        }
    },
    {
        "filename": "HDFS-3443.json",
        "creation_time": "2012-05-18T11:30:50.000+0000",
        "bug_report": {
            "Title": "Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer",
            "Description": "Start NN\nLet NN standby services be started.\nBefore the editLogTailer is initialised start ZKFC and allow the activeservices start to proceed further.\n\n\nHere editLogTailer.catchupDuringFailover() will throw NPE.\n\nvoid startActiveServices() throws IOException {\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = dir.fsImage.getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs.\");\n        editLogTailer.catchupDuringFailover();\n\n\n{noformat}\n2012-05-18 16:51:27,585 WARN org.apache.hadoop.ipc.Server: IPC Server Responder, call org.apache.hadoop.ha.HAServiceProtocol.getServiceStatus from XX.XX.XX.55:58003: output error\n2012-05-18 16:51:27,586 WARN org.apache.hadoop.ipc.Server: IPC Server handler 8 on 8020, call org.apache.hadoop.ha.HAServiceProtocol.transitionToActive from XX.XX.XX.55:58004: error: java.lang.NullPointerException\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)\n\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n\tat org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)\n2012-05-18 16:51:27,586 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 8020 caught an exception\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:2092)\n\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:107)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:930)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1738)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-11479.json",
        "creation_time": "2017-03-01T11:08:07.000+0000",
        "bug_report": {
            "Title": "Socket re-use address option should be used in SimpleUdpServer",
            "Description": "Nfs gateway restart can fail because of bind error in SimpleUdpServer.\n\nre-use address option should be used in SimpleUdpServer to so that socket bind can happen when it is in TIME_WAIT state\n\n{noformat}\n2017-02-28 04:19:53,495 FATAL mount.MountdBase (MountdBase.java:startUDPServer(66)) - Failed to start the UDP server.\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)\n        at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)\n        at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)\n        at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n        at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n        at sun.nio.ch.Net.bind0(Native Method)\n        at sun.nio.ch.Net.bind(Net.java:433)\n        at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)\n        at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)\n        at org.jboss.netty.channel.Channels.bind(Channels.java:561)\n        at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)\n        ... 11 more\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-8055.json",
        "creation_time": "2015-04-02T23:21:55.000+0000",
        "bug_report": {
            "Title": "NullPointerException when topology script is missing.",
            "Description": "We've received reports that the NameNode can get a NullPointerException when the topology script is missing. This issue tracks investigating whether or not we can improve the validation logic and give a more informative error message.\n\nHere is a sample stack trace :\nGetting NPE from HDFS:\n \n 2015-02-06 23:02:12,250 ERROR [pool-4-thread-1] util.HFileV1Detector: Got exception while reading trailer for file:hdfs://hqhd02nm01.pclc0.merkle.local:8020/hbase/.META./1028785192/info/1490a396aea448b693da563f76a28486^M\n org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException^M\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)^M\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)^M\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)^M\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)^M\n         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)^M\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)^M\n         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)^M\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)^M\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)^M\n         at java.security.AccessController.doPrivileged(Native Method)^M\n         at javax.security.auth.Subject.doAs(Subject.java:415)^M\n         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)^M\n         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)^M\n ^M\n         at org.apache.hadoop.ipc.Client.call(Client.java:1468)^M\n         at org.apache.hadoop.ipc.Client.call(Client.java:1399)^M\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)^M\n         at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)^M\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)^M\n         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)^M\n         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)^M\n         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)^M\n         at java.lang.reflect.Method.invoke(Method.java:606)^M\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)^M\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)^M\n         at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)^M\n         at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)^M\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)^M\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)^M\n         at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)^M\n         at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)^M\n         at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)^M\n         at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)^M\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)^M\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)^M\n         at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)^M\n         at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)^M\n         at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)^M\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:320)^M\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:300)^M\n         at java.util.concurrent.FutureTask.run(FutureTask.java:262)^M\n         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)^M\n         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)^M\n         at java.lang.Thread.run(Thread.java:745)^M\n 2015-02-06 23:02:12,263 ERROR [pool-4-thread-1] util.HFileV1Detector: Got exception while reading trailer for file:hdfs://hqhd02nm01.pclc0.merkle.local:8020/hbase/.META./1028785192/info/a06f2483f6864d818884d0a451cb91d5^M\n org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException^M\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)^M\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)^M\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)^M\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)^M\n"
        }
    },
    {
        "filename": "HDFS-6533.json",
        "creation_time": "2014-06-14T16:15:02.000+0000",
        "bug_report": {
            "Title": "TestBPOfferService#testBasicFunctionalitytest fails intermittently",
            "Description": "Per https://builds.apache.org/job/Hadoop-Hdfs-trunk/1774/testReport, the following test failed. However, local rerun is successful.\n\n{code}\norg.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality\n\nError Message\n\nWanted but not invoked:\ndatanodeProtocolClientSideTranslatorPB.registerDatanode(\n    <any>\n);\n-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nActually, there were zero interactions with this mock.\nStacktrace\n\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\ndatanodeProtocolClientSideTranslatorPB.registerDatanode(\n    <any>\n);\n-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nStandard Output\n\n2014-06-14 12:42:08,723 INFO  datanode.DataNode (SimulatedFSDataset.java:registerMBean(968)) - Registered FSDatasetState MBean\n2014-06-14 12:42:08,730 INFO  datanode.DataNode (BPServiceActor.java:run(805)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:0 starting to offer service\n2014-06-14 12:42:08,730 DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(170)) - Block pool <registering> (Datanode Uuid unassigned) service to 0.0.0.0/0.0.0.0:0 received versionRequest response: lv=-57;cid=fake cluster;nsid=1;c=0;bpid=fake bpid\n2014-06-14 12:42:08,731 INFO  datanode.DataNode (BPServiceActor.java:register(765)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 beginning handshake with NN\n2014-06-14 12:42:08,731 INFO  datanode.DataNode (BPServiceActor.java:register(778)) - Block pool Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0 successfully registered with NN\n2014-06-14 12:42:08,732 INFO  datanode.DataNode (BPServiceActor.java:offerService(637)) - For namenode 0.0.0.0/0.0.0.0:0 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\n2014-06-14 12:42:08,732 DEBUG datanode.DataNode (BPServiceActor.java:sendHeartBeat(562)) - Sending heartbeat with 1 storage reports from service actor: Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:0\n2014-06-14 12:42:08,734 INFO  datanode.DataNode (BPServiceActor.java:blockReport(498)) - Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 0 msecs for RPC and NN processing.  Got back commands none\n2014-06-14 12:42:08,738 INFO  datanode.DataNode (BPServiceActor.java:run(805)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 starting to offer service\n2014-06-14 12:42:08,739 DEBUG datanode.DataNode (BPServiceActor.java:retrieveNamespaceInfo(170)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 received versionRequest response: lv=-57;cid=fake cluster;nsid=1;c=0;bpid=fake bpid\n2014-06-14 12:42:08,739 INFO  datanode.DataNode (BPServiceActor.java:register(765)) - Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 beginning handshake with NN\n2014-06-14 12:42:08,740 INFO  datanode.DataNode (BPServiceActor.java:register(778)) - Block pool Block pool fake bpid (Datanode Uuid null) service to 0.0.0.0/0.0.0.0:1 successfully registered with NN\n{code}\n\n"
        }
    },
    {
        "filename": "HDFS-10609.json",
        "creation_time": "2016-07-11T17:14:39.000+0000",
        "bug_report": {
            "Title": "Uncaught InvalidEncryptionKeyException during pipeline recovery may abort downstream applications",
            "Description": "In normal operations, if SASL negotiation fails due to {{InvalidEncryptionKeyException}}, it is typically a benign exception, which is caught and retried :\n\n{code:title=SaslDataTransferServer#doSaslHandshake}\n  if (ioe instanceof SaslException &&\n      ioe.getCause() != null &&\n      ioe.getCause() instanceof InvalidEncryptionKeyException) {\n    // This could just be because the client is long-lived and hasn't gotten\n    // a new encryption key from the NN in a while. Upon receiving this\n    // error, the client will get a new encryption key from the NN and retry\n    // connecting to this DN.\n    sendInvalidKeySaslErrorMessage(out, ioe.getCause().getMessage());\n  } \n{code}\n\n{code:title=DFSOutputStream.DataStreamer#createBlockOutputStream}\nif (ie instanceof InvalidEncryptionKeyException && refetchEncryptionKey > 0) {\n            DFSClient.LOG.info(\"Will fetch a new encryption key and retry, \" \n                + \"encryption key was invalid when connecting to \"\n                + nodes[0] + \" : \" + ie);\n{code}\n\nHowever, if the exception is thrown during pipeline recovery, the corresponding code does not handle it properly, and the exception is spilled out to downstream applications, such as SOLR, aborting its operation:\n\n{quote}\n2016-07-06 12:12:51,992 ERROR org.apache.solr.update.HdfsTransactionLog: Exception closing tlog.\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n2016-07-06 12:12:51,997 ERROR org.apache.solr.update.CommitTracker: auto commit error...:org.apache.solr.common.SolrException: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.solr.update.HdfsTransactionLog.close(HdfsTransactionLog.java:316)\n        at org.apache.solr.update.TransactionLog.decref(TransactionLog.java:505)\n        at org.apache.solr.update.UpdateLog.addOldLog(UpdateLog.java:380)\n        at org.apache.solr.update.UpdateLog.postCommit(UpdateLog.java:676)\n        at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:623)\n        at org.apache.solr.update.CommitTracker.run(CommitTracker.java:216)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n{quote}\n\nThis exception should be contained within HDFS, caught and retried just like in {{createBlockOutputStream()}}"
        }
    },
    {
        "filename": "HDFS-2310.json",
        "creation_time": "2011-09-05T15:31:00.000+0000",
        "bug_report": {
            "Title": "TestBackupNode fails since HADOOP-7524 went in.",
            "Description": "Logs give the following error. This happens because the JournalProtocol is never registered with the server.\n\n2011-09-05 10:44:36,811 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 60758, call: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3), rpc version=2, client version=1, methodsFingerPrint=-852377201 from 127.0.0.1:60760, error: \njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n2011-09-05 10:44:36,813 ERROR namenode.FSEditLog (FSEditLog.java:mapJournalsAndReportErrors(1048)) - Error starting log segment 3 (journal JournalAndStream(mgr=org.apache.hadoop.hdfs.server.namenode.BackupJournalManager@2876b359, stream=null))\njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n"
        }
    },
    {
        "filename": "HDFS-3385.json",
        "creation_time": "2012-05-08T12:45:31.000+0000",
        "bug_report": {
            "Title": "ClassCastException when trying to append a file",
            "Description": "When I try to append a file I got \n\n{noformat}\n2012-05-08 18:13:40,506 WARN  util.KerberosName (KerberosName.java:<clinit>(87)) - Kerberos krb5 configuration not found, setting default realm to empty\nException in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)\n        ...\n\tat org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)\n\tat org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4006.json",
        "creation_time": "2012-10-04T22:05:14.000+0000",
        "bug_report": {
            "Title": "TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit",
            "Description": "TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to unexpected exit, due to an NPE while checkpointing. It looks like the background checkpoint fails, conflicts with the explicit checkpoints done by the tests (note the backtrace is not for the doCheckpoint calls in the tests.\n\n{noformat}\n2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\nstack trace\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)\nat org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)\nat java.lang.Thread.run(Thread.java:662)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-6715.json",
        "creation_time": "2014-07-21T21:26:25.000+0000",
        "bug_report": {
            "Title": "webhdfs wont fail over when it gets java.io.IOException: Namenode is in startup mode",
            "Description": "Noticed in our HA testing when we run MR job with webhdfs file system we some times run into \n\n{code}\n2014-04-17 05:08:06,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1397710493213_0001_r_000008_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n\n2014-04-17 05:08:10,205 ERROR [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job\njava.io.IOException: Namenode is in startup mode\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n{code}"
        }
    },
    {
        "filename": "HDFS-2392.json",
        "creation_time": "2011-09-30T20:29:09.000+0000",
        "bug_report": {
            "Title": "Dist with hftp is failing again",
            "Description": "$ hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3\n11/09/30 18:57:59 INFO tools.DistCp: srcPaths=[hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000]\n11/09/30 18:57:59 INFO tools.DistCp: destPath=/user/hadoopqa/out3\n11/09/30 18:58:00 INFO security.TokenCache: Got dt for\nhftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000;uri=<NN IP>:50470;t.service=<NN IP>:50470\n11/09/30 18:58:00 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 24 for hadoopqa on <NN IP>:8020\n11/09/30 18:58:00 INFO security.TokenCache: Got dt for\n/user/hadoopqa/out3;uri=<NN IP>:8020;t.service=<NN IP>:8020\n11/09/30 18:58:00 INFO tools.DistCp: /user/hadoopqa/out3 does not exist.\n11/09/30 18:58:00 INFO tools.DistCp: sourcePathsCount=1\n11/09/30 18:58:00 INFO tools.DistCp: filesToCopyCount=1\n11/09/30 18:58:00 INFO tools.DistCp: bytesToCopyCount=1.0g\n11/09/30 18:58:01 INFO mapred.JobClient: Running job: job_201109300819_0007\n11/09/30 18:58:02 INFO mapred.JobClient:  map 0% reduce 0%\n11/09/30 18:58:25 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_0, Status : FAILED\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\n11/09/30 18:58:41 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_1, Status : FAILED\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\n11/09/30 18:58:56 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_2, Status : FAILED\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\n11/09/30 18:59:14 INFO mapred.JobClient: Job complete: job_201109300819_0007\n11/09/30 18:59:14 INFO mapred.JobClient: Counters: 6\n11/09/30 18:59:14 INFO mapred.JobClient:   Job Counters \n11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=62380\n11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n11/09/30 18:59:14 INFO mapred.JobClient:     Launched map tasks=4\n11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0\n11/09/30 18:59:14 INFO mapred.JobClient:     Failed map tasks=1\n11/09/30 18:59:14 INFO mapred.JobClient: Job Failed: # of failed Map Tasks exceeded allowed limit. FailedCount: 1.\nLastFailedTask: task_201109300819_0007_m_000000\nWith failures, global counters are inaccurate; consider running with -i\nCopy failed: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1257)\n        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)\n        at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)\n\n\n\n\n\n"
        }
    },
    {
        "filename": "HDFS-11472.json",
        "creation_time": "2017-02-28T18:39:13.000+0000",
        "bug_report": {
            "Title": "Fix inconsistent replica size after a data pipeline failure",
            "Description": "We observed a case where a replica's on disk length is less than acknowledged length, breaking the assumption in recovery code.\n\n{noformat}\n2017-01-08 01:41:03,532 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to obtain replica info for block (=BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394519586) from datanode (=DatanodeInfoWithStorage[10.204.138.17:1004,null,null])\njava.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW\n  getNumBytes()     = 27530\n  getBytesOnDisk()  = 27006\n  getVisibleLength()= 27268\n  getVolume()       = /data/6/hdfs/datanode/current\n  getBlockFile()    = /data/6/hdfs/datanode/current/BP-947993742-10.204.0.136-1362248978912/current/rbw/blk_2526438952\n  bytesAcked=27268\n  bytesOnDisk=27006\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nIt turns out that if an exception is thrown within {{BlockReceiver#receivePacket}}, the in-memory replica on disk length may not be updated, but the data is written to disk anyway.\n\nFor example, here's one exception we observed\n{noformat}\n2017-01-08 01:40:59,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394499067\njava.nio.channels.ClosedByInterruptException\n        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n        at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:269)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition(FsDatasetImpl.java:1484)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition(BlockReceiver.java:994)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:670)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:857)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:797)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nThere are potentially other places and causes where an exception is thrown within {{BlockReceiver#receivePacket}}, so it may not make much sense to alleviate it for this particular exception. Instead, we should improve replica recovery code to handle the case where ondisk size is less than acknowledged size, and update in-memory checksum accordingly."
        }
    },
    {
        "filename": "HDFS-10760.json",
        "creation_time": "2016-08-12T08:29:24.000+0000",
        "bug_report": {
            "Title": "DataXceiver#run() should not log InvalidToken exception as an error",
            "Description": "DataXceiver#run() just log InvalidToken exception as an error.\nWhen client has an expired token and just refetch a new token, the DN log will has an error like below:\n{noformat}\n2016-08-11 02:41:09,817 ERROR datanode.DataNode (DataXceiver.java:run(269)) - XXXXXXX:50010:DataXceiver error processing READ_BLOCK operation  src: /10.17.1.5:38844 dst: /10.17.1.5:50010\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)\n        at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\nThis is not a server error and the DataXceiver#checkAccess() has already loged the InvalidToken as a warning.\nA simple fix by catching the InvalidToken exception in DataXceiver#run(), only keeping the warning logged by DataXceiver#checkAccess() in the DN log."
        }
    },
    {
        "filename": "HDFS-13635.json",
        "creation_time": "2018-05-29T19:54:50.000+0000",
        "bug_report": {
            "Title": "Incorrect message when block is not found",
            "Description": "When client opens a file, it asks DataNode to\u00a0check the blocks' visible length. If somehow the block is not on the DN, it throws \"Cannot append to a non-existent replica\" message, which is incorrect, because\u00a0getReplicaVisibleLength() is called for different use, just not for appending to a block. It should just state \"block is not found\"\r\n\r\nThe following stacktrace comes from a CDH5.13, but it looks like the same warning exists in Apache Hadoop trunk.\r\n{noformat}\r\n2018-05-29 09:23:41,966 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getReplicaVisibleLength from 10.0.0.14:53217 Call#38334117 Retry#0\r\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346\r\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)\r\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)\r\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)\r\n at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)\r\n at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)\r\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\r\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211){noformat}"
        }
    },
    {
        "filename": "HDFS-11608.json",
        "creation_time": "2017-03-31T17:37:20.000+0000",
        "bug_report": {
            "Title": "HDFS write crashed with block size greater than 2 GB",
            "Description": "We've seen HDFS write crashes in the case of huge block size. For example, writing a 3 GB file using block size > 2 GB (e.g., 3 GB), HDFS client throws out of memory exception. DataNode gives out IOException. After changing heap size limit,  DFSOutputStream ResponseProcessor exception is seen followed by Broken pipe and pipeline recovery.\n\nGive below:\nDN exception,\n{noformat}\n2017-03-30 16:34:33,828 ERROR datanode.DataNode (DataXceiver.java:run(278)) - c6401.ambari.apache.org:50010:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.64.101:47167 dst: /192.168.64.101:50010\njava.io.IOException: Incorrect value for packet payload size: 2147483128\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:898)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-12638.json",
        "creation_time": "2017-10-11T11:29:04.000+0000",
        "bug_report": {
            "Title": "Delete copy-on-truncate block along with the original block, when deleting a file being truncated",
            "Description": "Active NamNode exit due to NPE, I can confirm that the BlockCollection passed in when creating ReplicationWork is null, but I do not know why BlockCollection is null, By view history I found [HDFS-9754|https://issues.apache.org/jira/browse/HDFS-9754] remove judging  whether  BlockCollection is null.\r\n\r\nNN logs are as following:\r\n{code:java}\r\n2017-10-11 16:29:06,161 ERROR [ReplicationMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.\r\njava.lang.NullPointerException\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)\r\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)\r\n        at java.lang.Thread.run(Thread.java:834)\r\n{code}"
        }
    },
    {
        "filename": "HDFS-12383.json",
        "creation_time": "2017-08-31T20:43:50.000+0000",
        "bug_report": {
            "Title": "Re-encryption updater should handle canceled tasks better",
            "Description": "Seen an instance where the re-encryption updater exited due to an exception, and later tasks no longer executes. Logs below:\n{noformat}\n2017-08-31 09:54:08,104 INFO org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager: Zone /tmp/encryption-zone-3(16819) is submitted for re-encryption.\n2017-08-31 09:54:08,104 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Executing re-encrypt commands on zone 16819. Current zones:[zone:16787 state:Completed lastProcessed:null filesReencrypted:1 fileReencryptionFailures:0][zone:16813 state:Completed lastProcessed:null filesReencrypted:1 fileReencryptionFailures:0][zone:16819 state:Submitted lastProcessed:null filesReencrypted:0 fileReencryptionFailures:0]\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.protocol.ReencryptionStatus: Zone 16819 starts re-encryption processing\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Re-encrypting zone /tmp/encryption-zone-3(id=16819)\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Submitted batch (start:/tmp/encryption-zone-3/data1, size:1) of zone 16819 to re-encrypt.\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Submission completed of zone 16819 for re-encryption.\n2017-08-31 09:54:08,105 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Processing batched re-encryption for zone 16819, batch size 1, start:/tmp/encryption-zone-3/data1\n2017-08-31 09:54:08,979 INFO BlockStateChange: BLOCK* BlockManager: ask 172.26.1.71:20002 to delete [blk_1073742291_1467]\n2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater: Cancelling 1 re-encryption tasks\n2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager: Cancelled zone /tmp/encryption-zone-3(16819) for re-encryption.\n2017-08-31 09:54:18,295 INFO org.apache.hadoop.hdfs.protocol.ReencryptionStatus: Zone 16819 completed re-encryption.\n2017-08-31 09:54:18,296 INFO org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler: Completed re-encrypting one batch of 1 edeks from KMS, time consumed: 10.19 s, start: /tmp/encryption-zone-3/data1.\n2017-08-31 09:54:18,296 ERROR org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater: Re-encryption updater thread exiting.\njava.util.concurrent.CancellationException\n        at java.util.concurrent.FutureTask.report(FutureTask.java:121)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nUpdater should be fixed to handle canceled tasks better."
        }
    },
    {
        "filename": "HDFS-5322.json",
        "creation_time": "2013-10-07T22:52:54.000+0000",
        "bug_report": {
            "Title": "HDFS delegation token not found in cache errors seen on secure HA clusters",
            "Description": "While running HA tests we have seen issues were we see HDFS delegation token not found in cache errors causing jobs running to fail.\n\n{code}\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)\n|2013-10-06 20:14:51,193 INFO  [main] mapreduce.Job: Task Id : attempt_1381090351344_0001_m_000007_0, Status : FAILED\nError: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache\nat org.apache.hadoop.ipc.Client.call(Client.java:1347)\nat org.apache.hadoop.ipc.Client.call(Client.java:1300)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\nat com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)\n{code}"
        }
    },
    {
        "filename": "HDFS-11741.json",
        "creation_time": "2017-05-02T17:57:25.000+0000",
        "bug_report": {
            "Title": "Long running balancer may fail due to expired DataEncryptionKey",
            "Description": "We found a long running balancer may fail despite using keytab, because KeyManager returns expired DataEncryptionKey, and it throws the following exception:\n\n{noformat}\n2017-04-30 05:03:58,661 WARN  [pool-1464-thread-10] balancer.Dispatcher (Dispatcher.java:dispatch(325)) - Failed to move blk_1067352712_3913241 with size=546650 from 10.0.0.134:50010:DISK to 10.0.0.98:50010:DISK through 10.0.0.134:50010\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nThis bug is similar in nature to HDFS-10609. While balancer KeyManager actively synchronizes itself with NameNode w.r.t block keys, it does not update DataEncryptionKey accordingly.\n\nIn a specific cluster, with Kerberos ticket life time 10 hours, and default block token expiration/life time 10 hours, a long running balancer failed after 20~30 hours."
        }
    },
    {
        "filename": "HDFS-3936.json",
        "creation_time": "2012-09-14T05:35:12.000+0000",
        "bug_report": {
            "Title": "MiniDFSCluster shutdown races with BlocksMap usage",
            "Description": "Looks like HDFS-3664 didn't fix the whole issue because the added join times out because the thread closing the BM (FSN#stopCommonServices) holds the FSN lock while closing the BM and the BM is block uninterruptedly trying to aquire the FSN lock.\n\n{noformat}\n2012-09-13 18:54:12,526 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\nstack trace\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)\n\tat java.lang.Thread.run(Thread.java:662)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-6348.json",
        "creation_time": "2014-05-07T12:34:54.000+0000",
        "bug_report": {
            "Title": "SecondaryNameNode not terminating properly on runtime exceptions",
            "Description": "Secondary Namenode is not exiting when there is RuntimeException occurred during startup.\n\nSay I configured wrong configuration, due to that validation failed and thrown RuntimeException as shown below. But when I check the environment SecondaryNamenode process is alive. When analysed, RMI Thread is still alive, since it is not a daemon thread JVM is nit exiting. \n\nI'm attaching threaddump to this JIRA for more details about the thread.\n{code}\njava.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)\n\t... 7 more\n2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state\n2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state\n2014-05-07 14:31:04,926 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: \n{code}\n\n"
        }
    },
    {
        "filename": "HDFS-7884.json",
        "creation_time": "2015-03-04T15:47:50.000+0000",
        "bug_report": {
            "Title": "NullPointerException in BlockSender",
            "Description": "{noformat}\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}\nBlockSender.java:264 is shown below\n{code}\n      this.volumeRef = datanode.data.getVolume(block).obtainReference();\n{code}"
        }
    },
    {
        "filename": "HDFS-7996.json",
        "creation_time": "2015-03-26T21:19:55.000+0000",
        "bug_report": {
            "Title": "After swapping a volume, BlockReceiver reports ReplicaNotFoundException",
            "Description": "When removing a disk from an actively writing DataNode, the BlockReceiver working on the disk throws {{ReplicaNotFoundException}} because the replicas are removed from the memory:\n\n{code}\n2015-03-26 08:02:43,154 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removed volume: /data/2/dfs/dn/current\n2015-03-26 08:02:43,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Removing block level storage: /data/2/dfs/dn/current/BP-51301509-10.20.202.114-1427296597742\n2015-03-26 08:02:43,163 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in BlockReceiver.run():\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n\n{{FsVolumeList#removeVolume}} waits all threads release {{FsVolumeReference}} on the volume to be removed, however, in {{PacketResponder#finalizeBlock()}}, it calls\n\n{code}\nprivate void finalizeBlock(long startTime) throws IOException {\n      BlockReceiver.this.close();\n      final long endTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime()\n          : 0;\n      block.setNumBytes(replicaInfo.getNumBytes());\n      datanode.data.finalizeBlock(block);\n{code}\n\nThe {{FsVolumeReference}} was released in {{BlockReceiver.this.close()}} before calling {{datanode.data.finalizeBlock(block)}}."
        }
    },
    {
        "filename": "HDFS-4302.json",
        "creation_time": "2012-12-11T19:44:59.000+0000",
        "bug_report": {
            "Title": "Precondition in EditLogFileInputStream's length() method is checked too early in NameNode startup, causing fatal exception",
            "Description": "When bringing up a namenode in standby mode, where DEBUG is enabled for namenode, the namenode will hit the following code in {{hadoop/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java}}:\n\n{code}\n if (LOG.isDebugEnabled()) {\n      LOG.debug(\"edit log length: \" + in.length() + \", start txid: \"\n          + expectedStartingTxId + \", last txid: \" + lastTxId);\n    }\n{code}.\n\nHowever, if {{in}} has an {{EditLogFileInputStream}} as its {{streams[0]}}, this code is hit before the {{EditLogFileInputStream}}'s {{advertizedSize}} is initialized (before the HTTP client connects to the remote edit log server (i.e. the journal node)). This causes the following precondition to fail in {{EditLogFileInputStream:length()}}:\n\n{code}\n      Preconditions.checkState(advertisedSize != -1,\n          \"must get input stream before length is available\");\n{code}\n\nwhich shuts down the namenode with the following log messages and stack trace:\n\n{code}\n2012-12-11 10:45:33,319 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(217)) - Call: getEditLogManifest took 88ms\n2012-12-11 10:45:33,336 DEBUG client.QuorumJournalManager (QuorumJournalManager.java:selectInputStreams(459)) - selectInputStream manifests:\n172.16.175.1:8485: [[1,3]]\n2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(605)) - Planning to load image :\nFSImageFile(file=/tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\n2012-12-11 10:45:33,351 DEBUG namenode.FSImage (FSImage.java:loadFSImage(607)) - Planning to load edit log stream: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9\n2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(168)) - Loading image file /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000 using no compression\n2012-12-11 10:45:33,355 INFO  namenode.FSImage (FSImageFormat.java:load(171)) - Number of files = 1\n2012-12-11 10:45:33,356 INFO  namenode.FSImage (FSImageFormat.java:loadFilesUnderConstruction(383)) - Number of files under construction = 0\n2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImageFormat.java:load(193)) - Image file of size 119 loaded in 0 seconds.\n2012-12-11 10:45:33,357 INFO  namenode.FSImage (FSImage.java:loadFSImage(753)) - Loaded image for txid 0 from /tmp/hadoop-data/dfs/name/current/fsimage_0000000000000000000\n2012-12-11 10:45:33,357 DEBUG namenode.FSImage (FSImage.java:loadEdits(686)) - About to load edits:\n  org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9\n2012-12-11 10:45:33,359 INFO  namenode.FSImage (FSImage.java:loadEdits(694)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@465098f9 expecting start txid #1\n2012-12-11 10:45:33,361 DEBUG ipc.Client (Client.java:stop(1060)) - Stopping client\n2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:close(1016)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: closed\n2012-12-11 10:45:33,363 DEBUG ipc.Client (Client.java:run(848)) - IPC Client (1462017562) connection to Eugenes-MacBook-Pro.local/172.16.175.1:8485 from hdfs/eugenes-macbook-pro.local@EXAMPLE.COM: stopped, remaining connections 0\n2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join\njava.lang.IllegalStateException: must get input stream before length is available\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)\n2012-12-11 10:45:33,470 INFO  util.ExitUtil (ExitUtil.java:terminate(84)) - Exiting with status 1\n2012-12-11 10:45:33,471 INFO  namenode.NameNode (StringUtils.java:run(620)) - SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at Eugenes-MacBook-Pro.local/172.16.175.1\n************************************************************/\n{code}\n\n\n"
        }
    },
    {
        "filename": "HDFS-11849.json",
        "creation_time": "2017-05-18T11:31:48.000+0000",
        "bug_report": {
            "Title": "JournalNode startup failure exception should be logged in log file",
            "Description": "JournalNode failed to start because of kerberos login. \n{noformat}\nException in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1\n        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)\n        at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)\n{noformat}\n\nbut this exception is not written in log file.\n\n{noformat}\nSTARTUP_MSG:   java = 1.x.x\n************************************************************/\n2017-05-18 16:08:14,961 INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: registered UNIX signal handlers for [TERM, HUP, INT]\n2017-05-18 16:08:15,511 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n2017-05-18 16:08:15,660 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n2017-05-18 16:08:15,660 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: JournalNode metrics system started\n2017-05-18 16:08:16,429 INFO org.apache.hadoop.hdfs.qjournal.server.JournalNode: SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down JournalNode at w-x-y-z\n************************************************************/\n{noformat}"
        }
    },
    {
        "filename": "HDFS-4841.json",
        "creation_time": "2013-05-22T17:01:36.000+0000",
        "bug_report": {
            "Title": "FsShell commands using secure webhfds fail ClientFinalizer shutdown hook",
            "Description": "Hadoop version:\n{code}\nbash-4.1$ $HADOOP_HOME/bin/hadoop version\nHadoop 3.0.0-SNAPSHOT\nSubversion git://github.com/apache/hadoop-common.git -r d5373b9c550a355d4e91330ba7cc8f4c7c3aac51\nCompiled by root on 2013-05-22T08:06Z\nFrom source with checksum 8c4cc9b1e8d6e8361431e00f64483f\nThis command was run using /var/lib/hadoop-hdfs/hadoop-3.0.0-SNAPSHOT/share/hadoop/common/hadoop-common-3.0.0-SNAPSHOT.jar\n{code}\n\nI'm seeing a problem when issuing FsShell commands using the webhdfs:// URI when security is enabled. The command completes but leaves a warning that ShutdownHook 'ClientFinalizer' failed.\n\n{code}\nbash-4.1$ hadoop-3.0.0-SNAPSHOT/bin/hadoop fs -ls webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/\n2013-05-22 09:46:55,710 INFO  [main] util.Shell (Shell.java:isSetsidSupported(311)) - setsid exited with exit code 0\nFound 3 items\ndrwxr-xr-x   - hbase supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/hbase\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/user\n2013-05-22 09:46:58,660 WARN  [Thread-3] util.ShutdownHookManager (ShutdownHookManager.java:run(56)) - ShutdownHook 'ClientFinalizer' failed, java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook\njava.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)\n\tat org.apache.hadoop.security.token.Token.cancel(Token.java:382)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)\n\tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)\n\tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n{code}\n\nI've checked that FsShell + hdfs:// commands and WebHDFS operations through curl work successfully:\n\n{code}\nbash-4.1$ hadoop-3.0.0-SNAPSHOT/bin/hadoop fs -ls /\n2013-05-22 09:46:43,663 INFO  [main] util.Shell (Shell.java:isSetsidSupported(311)) - setsid exited with exit code 0\nFound 3 items\ndrwxr-xr-x   - hbase supergroup          0 2013-05-22 09:46 /hbase\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 /tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2013-05-22 09:46 /user\nbash-4.1$ curl -i --negotiate -u : \"http://hdfs-upgrade-pseudo.ent.cloudera.com:50070/webhdfs/v1/?op=GETHOMEDIRECTORY\"\nHTTP/1.1 401 \nCache-Control: must-revalidate,no-cache,no-store\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nContent-Type: text/html; charset=iso-8859-1\nWWW-Authenticate: Negotiate\nSet-Cookie: hadoop.auth=;Path=/;Expires=Thu, 01-Jan-1970 00:00:00 GMT\nContent-Length: 1358\nServer: Jetty(6.1.26)\n\nHTTP/1.1 200 OK\nCache-Control: no-cache\nExpires: Thu, 01-Jan-1970 00:00:00 GMT\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nDate: Wed, 22 May 2013 16:47:14 GMT\nPragma: no-cache\nContent-Type: application/json\nSet-Cookie: hadoop.auth=\"u=hdfs&p=hdfs/hdfs-upgrade-pseudo.ent.cloudera.com@ENT.CLOUDERA.COM&t=kerberos&e=1369277234852&s=m3vJ7/pV831tBLkpOBb0Naa5N+g=\";Path=/\nTransfer-Encoding: chunked\nServer: Jetty(6.1.26)\n\n{\"Path\":\"/user/hdfs\"}bash-4.1$ \n{code}\n\nWhen I disable security, the warning goes away.\n\nI'll attach my core-site.xml, hdfs-site.xml, NN and DN output logs.\n"
        }
    },
    {
        "filename": "HDFS-3384.json",
        "creation_time": "2012-05-08T09:01:10.000+0000",
        "bug_report": {
            "Title": "DataStreamer thread should be closed immediatly when failed to setup a PipelineForAppendOrRecovery",
            "Description": "Scenraio:\n=========\nwrite a file\ncorrupt block manually\ncall append..\n\n{noformat}\n\n2012-04-19 09:33:10,776 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1059)) - Exception in createBlockOutputStream\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)\n2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1511)) - Error while syncing\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-5657.json",
        "creation_time": "2013-12-11T21:59:46.000+0000",
        "bug_report": {
            "Title": "race condition causes writeback state error in NFS gateway",
            "Description": "A race condition between NFS gateway writeback executor thread and new write handler thread can cause writeback state check failure, e.g.,\n{noformat}\n2013-11-26 10:34:07,859 DEBUG nfs3.RpcProgramNfs3 (Nfs3Utils.java:writeChannel(113)) - WRITE_RPC_CALL_END______957880843\n2013-11-26 10:34:07,863 DEBUG nfs3.OpenFileCtx (OpenFileCtx.java:offerNextToWrite(832)) - The asyn write task has no pending writes, fileId: 30938\n2013-11-26 10:34:07,871 ERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)\n        at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n\n2013-11-26 10:34:07,901 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:write(707)) - requesed offset=917504 and current filesize=917504\n2013-11-26 10:34:07,902 DEBUG nfs3.WriteManager (WriteManager.java:handleWrite(131)) - handleWrite fileId: 30938 offset: 917504 length:65536 stableHow:0\n{noformat}"
        }
    },
    {
        "filename": "HDFS-11827.json",
        "creation_time": "2017-05-16T06:28:49.000+0000",
        "bug_report": {
            "Title": "NPE is thrown when log level changed in BlockPlacementPolicyDefault#chooseRandom() method ",
            "Description": "This issue was found by my colleague when changing log-level of BlockPlacementPolicy using \"hadoop daemonlog\" command. \n\nThe exception stack trace is below:\n{noformat}\n2017-05-16 13:36:51,053 ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\n\nAfter checking _BlockPlacementPolicyDefault_ code of trunk, I found that _BlockPlacementPolicyDefault_ class missed some NPE check code in _chooseRandom()_ method. "
        }
    },
    {
        "filename": "HDFS-6804.json",
        "creation_time": "2014-08-01T05:37:23.000+0000",
        "bug_report": {
            "Title": "Add test for race condition between transferring block and appending block causes \"Unexpected checksum mismatch exception\" ",
            "Description": "We found some error log in the datanode. like this\n{noformat}\n2014-07-22 01:49:51,338 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Ex\nception for BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248 from /192.168.2.101:39495\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n        at java.lang.Thread.run(Thread.java:744)\n{noformat}\nWhile on the source datanode, the log says the block is transmitted.\n{noformat}\n2014-07-22 01:49:50,805 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Da\ntaTransfer: Transmitted BP-2072804351-192.168.2.104-1406008383435:blk_1073741997\n_9248 (numBytes=16188152) to /192.168.2.103:50010\n{noformat}\n\nWhen the destination datanode gets the checksum mismatch, it reports bad block to NameNode and NameNode marks the replica on the source datanode as corrupt. But actually, the replica on the source datanode is valid. Because the replica can pass the checksum verification.\n\nIn all, the replica on the source data is wrongly marked as corrupted."
        }
    },
    {
        "filename": "HDFS-5843.json",
        "creation_time": "2014-01-28T06:05:16.000+0000",
        "bug_report": {
            "Title": "DFSClient.getFileChecksum() throws IOException if checksum is disabled",
            "Description": "If a file is created with checksum disabled (using {{ChecksumOpt.disabled()}} for example), calling {{FileSystem.getFileChecksum()}} throws the following IOException:\n\n{noformat}\njava.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)\n[...]\n{noformat}\n\nFrom the logs, the datanode is doing some wrong arithmetics because of the crcPerBlock:\n{noformat}\n2014-01-27 21:58:46,329 ERROR datanode.DataNode (DataXceiver.java:run(225)) - 127.0.0.1:52398:DataXceiver error processing BLOCK_CHECKSUM operation  src: /127.0.0.1:52407 dest: /127.0.0.1:52398\njava.lang.ArithmeticException: / by zero\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n\tat java.lang.Thread.run(Thread.java:695)\n{noformat}"
        }
    },
    {
        "filename": "HDFS-8070.json",
        "creation_time": "2015-04-06T19:57:58.000+0000",
        "bug_report": {
            "Title": "Pre-HDFS-7915 DFSClient cannot use short circuit on post-HDFS-7915 DataNode",
            "Description": "HDFS ShortCircuitShm layer keeps the task locked up during multi-threaded split-generation.\n\nI hit this immediately after I upgraded the data, so I wonder if the ShortCircuitShim wire protocol has trouble when 2.8.0 DN talks to a 2.7.0 Client?\n\n{code}\n2015-04-06 00:04:30,780 INFO [ORC_GET_SPLITS #3] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (IS_NULL ss_sold_date_sk)\nexpr = (not leaf-0)\n2015-04-06 00:04:30,781 ERROR [ShortCircuitCache_SlotReleaser] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x29e82045): failed to release short-circuit shared memory slot Slot(slotIdx=2, shm=DfsClientShm(a86ee34576d93c4964005d90b0d97c38)) by sending ReleaseShortCircuitAccessRequestProto to /grid/0/cluster/hdfs/dn_socket.  Closing shared memory segment.\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-04-06 00:04:30,781 INFO [ORC_GET_SPLITS #5] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (IS_NULL ss_sold_date_sk)\nexpr = (not leaf-0)\n2015-04-06 00:04:30,781 WARN [ShortCircuitCache_SlotReleaser] shortcircuit.DfsClientShmManager: EndpointShmManager(172.19.128.60:50010, parent=ShortCircuitShmManager(5e763476)): error shutting down shm: got IOException calling shutdown(SHUT_RDWR)\njava.nio.channels.ClosedChannelException\n\tat org.apache.hadoop.util.CloseableReferenceCount.reference(CloseableReferenceCount.java:57)\n\tat org.apache.hadoop.net.unix.DomainSocket.shutdown(DomainSocket.java:387)\n\tat org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.shutdown(DfsClientShmManager.java:378)\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:223)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-04-06 00:04:30,783 INFO [ORC_GET_SPLITS #7] orc.OrcInputFormat: ORC pushdown predicate: leaf-0 = (IS_NULL cs_sold_date_sk)\nexpr = (not leaf-0)\n2015-04-06 00:04:30,785 ERROR [ShortCircuitCache_SlotReleaser] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x29e82045): failed to release short-circuit shared memory slot Slot(slotIdx=4, shm=DfsClientShm(a86ee34576d93c4964005d90b0d97c38)) by sending ReleaseShortCircuitAccessRequestProto to /grid/0/cluster/hdfs/dn_socket.  Closing shared memory segment.\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}\n\nLooks like a double free-fd condition?\n\n{code}\n2015-04-02 18:58:47,653 [DataXceiver for client unix:/grid/0/cluster/hdfs/dn_socket [Passing file descriptors for block BP-942051088-172.18.0.41-1370508013893:blk_1076973408_1099515627985]] INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Unregistering SlotId(3bd7fd9aed791e95acfb5034e6617d83:0) because the requestShortCircuitFdsForRead operation failed.\n2015-04-02 18:58:47,653 [DataXceiver for client unix:/grid/0/cluster/hdfs/dn_socket [Passing file descriptors for block BP-942051088-<ip>-1370508013893:blk_1076973408_1099515627985]] INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1076973408, srvID: ba7b6f19-47e0-4b86-af50-23981649318c, success: false\n2015-04-02 18:58:47,654 [DataXceiver for client unix:/grid/0/cluster/hdfs/dn_socket [Passing file descriptors for block BP-942051088-172.18.0.41-1370508013893:blk_1076973408_1099515627985]] ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: cn060-10.l42scl.hortonworks.com:50010:DataXceiver error processing REQUEST_SHORT_CIRCUIT_FDS operation  src: unix:/grid/0/cluster/hdfs/dn_socket dst: <local>\njava.io.EOFException\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitFds(DataXceiver.java:352)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds(Receiver.java:187)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:89)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n{code}\n\nInvestigating more, since the exact exception from the DataNode call is not logged."
        }
    },
    {
        "filename": "HDFS-1085.json",
        "creation_time": "2010-04-06T22:49:40.000+0000",
        "bug_report": {
            "Title": "hftp read  failing silently",
            "Description": "When performing a massive distcp through hftp, we saw many tasks fail with \n\n{quote}\n2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)\nbut expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n        at org.apache.hadoop.mapred.Child.main(Child.java:159)\n{quote}\n\nThis means that read itself didn't fail but the resulted file was somehow smaller.\n"
        }
    },
    {
        "filename": "HDFS-12339.json",
        "creation_time": "2017-08-23T03:16:49.000+0000",
        "bug_report": {
            "Title": "NFS Gateway on Shutdown Gives Unregistration Failure. Does Not Unregister with rpcbind Portmapper",
            "Description": "\n\nWhen stopping NFS Gateway the following error is thrown in the NFS gateway role logs.\n\n2017-08-17 18:09:16,529 ERROR org.apache.hadoop.oncrpc.RpcProgram: Unregistration failure with localhost:2049, portmap entry: (PortmapMapping-100003:3:6:2049)\n\n2017-08-17 18:09:16,531 WARN org.apache.hadoop.util.ShutdownHookManager: ShutdownHook 'NfsShutdownHook' failed, java.lang.RuntimeException: Unregistration failure\njava.lang.RuntimeException: Unregistration failure\n..\nCaused by: java.net.SocketException: Socket is closed\nat java.net.DatagramSocket.send(DatagramSocket.java:641)\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)\n\nChecking rpcinfo -p : the following entry is still there:\n\" 100003 3 tcp 2049 nfs\"\n"
        }
    },
    {
        "filename": "HDFS-6520.json",
        "creation_time": "2014-06-12T02:32:29.000+0000",
        "bug_report": {
            "Title": "hdfs fsck -move passes invalid length value when creating BlockReader",
            "Description": "I met some error when I run fsck -move.\nMy steps are as the following:\n1. Set up a pseudo cluster\n2. Copy a file to hdfs\n3. Corrupt a block of the file\n4. Run fsck to check:\n{code}\nConnecting to namenode via http://localhost:50070\nFSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path /user/hadoop at Wed Jun 11 15:58:38 CST 2014\n.\n/user/hadoop/fsck-test: CORRUPT blockpool BP-654596295-10.37.7.84-1402466764642 block blk_1073741825\n\n/user/hadoop/fsck-test: MISSING 1 blocks of total size 1048576 B.Status: CORRUPT\n Total size:    4104304 B\n Total dirs:    1\n Total files:   1\n Total symlinks:                0\n Total blocks (validated):      4 (avg. block size 1026076 B)\n  ********************************\n  CORRUPT FILES:        1\n  MISSING BLOCKS:       1\n  MISSING SIZE:         1048576 B\n  CORRUPT BLOCKS:       1\n  ********************************\n Minimally replicated blocks:   3 (75.0 %)\n Over-replicated blocks:        0 (0.0 %)\n Under-replicated blocks:       0 (0.0 %)\n Mis-replicated blocks:         0 (0.0 %)\n Default replication factor:    1\n Average block replication:     0.75\n Corrupt blocks:                1\n Missing replicas:              0 (0.0 %)\n Number of data-nodes:          1\n Number of racks:               1\nFSCK ended at Wed Jun 11 15:58:38 CST 2014 in 1 milliseconds\n\n\nThe filesystem under path '/user/hadoop' is CORRUPT\n{code}\n5. Run fsck -move to move the corrupted file to /lost+found and the error message in the namenode log:\n{code}\n2014-06-11 15:48:16,686 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: FSCK started by hadoop (auth:SIMPLE) from /127.0.0.1 for path /user/hadoop at Wed Jun 11 15:48:16 CST 2014\n2014-06-11 15:48:16,894 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 35 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 0 Number of syncs: 25 SyncTimes(ms): 73\n2014-06-11 15:48:16,991 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Error reading block\njava.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536\nseqno: 1\nlastPacketInBlock: false\ndataLen: 65536\n\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n2014-06-11 15:48:16,992 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Fsck: could not copy block BP-654596295-10.37.7.84-1402466764642:blk_1073741825_1001 to /lost+found/user/hadoop/fsck-test\njava.lang.Exception: Could not copy block data for BP-654596295-10.37.7.84-1402466764642:blk_1073741825_1001\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:664)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n2014-06-11 15:48:16,994 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /lost+found/user/hadoop/fsck-test/0 is closed by DFSClient_NONMAPREDUCE_-774755866_14\n{code}"
        }
    },
    {
        "filename": "HDFS-10715.json",
        "creation_time": "2016-08-02T08:10:23.000+0000",
        "bug_report": {
            "Title": "NPE when applying AvailableSpaceBlockPlacementPolicy",
            "Description": "As HDFS-8131 introduced an AvailableSpaceBlockPlacementPolicy, but In some cases, it caused NPE. \n\nHere are my namenode daemon logs : \n\n2016-08-02 13:05:03,271 WARN org.apache.hadoop.ipc.Server: IPC Server handler 13 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.132.89.79:14001 Call#56 Retry#0\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n\nI reviewed the source code, and found the bug in method chooseDataNode. clusterMap.chooseRandom may return null, which cannot compare using equals a.equals(b) method.  \n\nThough this exception can be caught, and then retry another call. I think this bug should be fixed."
        }
    },
    {
        "filename": "HDFS-3332.json",
        "creation_time": "2012-04-27T04:42:38.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DN when directoryscanner is trying to report bad blocks",
            "Description": "There is 1 NN and 1 DN (NN is started with HA conf)\nI corrupted 1 block and found \n{code}\n2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(401)) - BlockReport of 2 blocks took 0 msec to generate and 5 msecs for RPC and NN processing\n2012-04-27 09:59:01,214 INFO  datanode.DataNode (BPServiceActor.java:blockReport(420)) - sent block report, processed command:org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b756db3\n2012-04-27 09:59:01,726 INFO  datanode.DirectoryScanner (DirectoryScanner.java:scan(390)) - BlockPool BP-2087868617-10.18.40.95-1335500488012 Total blocks: 2, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:1\n2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1366)) - Updating size of block -4466699320171028643 from 1024 to 1034\n2012-04-27 09:59:01,727 WARN  impl.FsDatasetImpl (FsDatasetImpl.java:checkAndUpdate(1374)) - Reporting the block blk_-4466699320171028643_1004 as corrupt due to length mismatch\n2012-04-27 09:59:01,728 DEBUG ipc.Client (Client.java:sendParam(807)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root sending #257\n2012-04-27 09:59:01,730 DEBUG ipc.Client (Client.java:receiveResponse(848)) - IPC Client (1957050620) connection to /10.18.40.95:8020 from root got value #257\n2012-04-27 09:59:01,730 DEBUG ipc.ProtobufRpcEngine (ProtobufRpcEngine.java:invoke(193)) - Call: reportBadBlocks 2\n2012-04-27 09:59:01,731 ERROR datanode.DirectoryScanner (DirectoryScanner.java:run(288)) - Exception during DirectoryScanner execution - will continue next cycle\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)\n\tat org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n{code}\n\nHere when Directory scanner is trying to report badblock we got a NPE."
        }
    },
    {
        "filename": "HDFS-6130.json",
        "creation_time": "2014-03-20T07:15:47.000+0000",
        "bug_report": {
            "Title": "NPE when upgrading namenode from fsimages older than -32",
            "Description": "I want upgrade an old cluster(0.20.2-cdh3u1) to trunk instance, \n\nI can upgrade successfully if I don't configurage HA, but if HA enabled,\nthere is NPE when I run ' hdfs namenode -initializeSharedEdits'\n\n{code}\n14/03/20 15:06:41 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n14/03/20 15:06:41 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n14/03/20 15:06:41 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n14/03/20 15:06:41 INFO util.GSet: VM type       = 64-bit\n14/03/20 15:06:41 INFO util.GSet: 0.029999999329447746% max memory 896 MB = 275.3 KB\n14/03/20 15:06:41 INFO util.GSet: capacity      = 2^15 = 32768 entries\n14/03/20 15:06:41 INFO namenode.AclConfigFlag: ACLs enabled? false\n14/03/20 15:06:41 INFO common.Storage: Lock on /data/hadoop/data1/dfs/name/in_use.lock acquired by nodename 7326@10-150-170-176\n14/03/20 15:06:42 INFO common.Storage: Lock on /data/hadoop/data2/dfs/name/in_use.lock acquired by nodename 7326@10-150-170-176\n14/03/20 15:06:42 INFO namenode.FSImage: No edit log streams selected.\n14/03/20 15:06:42 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\n14/03/20 15:06:42 FATAL namenode.NameNode: Exception in namenode join\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat.java:120)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)\n14/03/20 15:06:42 INFO util.ExitUtil: Exiting with status 1\n14/03/20 15:06:42 INFO namenode.NameNode: SHUTDOWN_MSG: \n/************************************************************\nSHUTDOWN_MSG: Shutting down NameNode at 10-150-170-176/10.150.170.176\n************************************************************/\n{code}\n"
        }
    },
    {
        "filename": "HDFS-2827.json",
        "creation_time": "2012-01-23T09:34:51.000+0000",
        "bug_report": {
            "Title": "Cannot save namespace after renaming a directory above a file with an open lease",
            "Description": "When i execute the following operations and wait for checkpoint to complete.\n\nfs.mkdirs(new Path(\"/test1\"));\nFSDataOutputStream create = fs.create(new Path(\"/test/abc.txt\")); //dont close\nfs.rename(new Path(\"/test/\"), new Path(\"/test1/\"));\n\nCheck-pointing is failing with the following exception.\n\n2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\\HDFS-1623\\hadoop-hdfs-project\\hadoop-hdfs\\build\\test\\data\\dfs\\name3\njava.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)\n\tat java.lang.Thread.run(Unknown Source)"
        }
    },
    {
        "filename": "HDFS-11056.json",
        "creation_time": "2016-10-25T23:00:33.000+0000",
        "bug_report": {
            "Title": "Concurrent append and read operations lead to checksum error",
            "Description": "If there are two clients, one of them open-append-close a file continuously, while the other open-read-close the same file continuously, the reader eventually gets a checksum error in the data read.\n\nOn my local Mac, it takes a few minutes to produce the error. This happens to httpfs clients, but there's no reason not believe this happens to any append clients.\n\nI have a unit test that demonstrates the checksum error. Will attach later.\n\nRelevant log:\n{quote}\n2016-10-25 15:34:45,153 INFO  audit - allowed=true\tugi=weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=open\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:45,155 INFO  DataNode - Receiving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 src: /127.0.0.1:51130 dest: /127.0.0.1:50131\n2016-10-25 15:34:45,155 INFO  FsDatasetImpl - Appending to FinalizedReplica, blk_1073741825_1182, FINALIZED\n  getNumBytes()     = 182\n  getBytesOnDisk()  = 182\n  getVisibleLength()= 182\n  getVolume()       = /Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1\n  getBlockURI()     = file:/Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1/current/BP-837130339-172.16.1.88-1477434851452/current/finalized/subdir0/subdir0/blk_1073741825\n2016-10-25 15:34:45,167 INFO  DataNode - opReadBlock BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 received exception java.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n2016-10-25 15:34:45,167 WARN  DataNode - DatanodeRegistration(127.0.0.1:50131, datanodeUuid=41c96335-5e4b-4950-ac22-3d21b353abb8, infoPort=50133, infoSecurePort=0, ipcPort=50134, storageInfo=lv=-57;cid=testClusterID;nsid=1472068852;c=1477434851452):Got exception while serving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 to /127.0.0.1:51121\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-10-25 15:34:45,168 INFO  FSNamesystem - updatePipeline(blk_1073741825_1182, newGS=1183, newLength=182, newNodes=[127.0.0.1:50131], client=DFSClient_NONMAPREDUCE_-1743096965_197)\n2016-10-25 15:34:45,168 ERROR DataNode - 127.0.0.1:50131:DataXceiver error processing READ_BLOCK operation  src: /127.0.0.1:51121 dst: /127.0.0.1:50131\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-10-25 15:34:45,168 INFO  FSNamesystem - updatePipeline(blk_1073741825_1182 => blk_1073741825_1183) success\n2016-10-25 15:34:45,170 WARN  DFSClient - Found Checksum error for BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 from DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK] at 0\n2016-10-25 15:34:45,170 WARN  DFSClient - No live nodes contain block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK]], ignoredNodes = null\n2016-10-25 15:34:45,170 INFO  DFSClient - Could not obtain BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:50131,DS-a1878418-4f7f-4fc9-b3f7-d7ed780b5373,DISK]. Will get new block locations from namenode and retry...\n2016-10-25 15:34:45,170 WARN  DFSClient - DFS chooseDataNode: got # 1 IOException, will wait for 981.8085941094539 msec.\n2016-10-25 15:34:45,171 INFO  clienttrace - src: /127.0.0.1:51130, dest: /127.0.0.1:50131, bytes: 183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1743096965_197, offset: 0, srvID: 41c96335-5e4b-4950-ac22-3d21b353abb8, blockid: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1183, duration: 2175363\n2016-10-25 15:34:45,171 INFO  DataNode - PacketResponder: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1183, type=LAST_IN_PIPELINE terminating\n2016-10-25 15:34:45,172 INFO  FSNamesystem - BLOCK* blk_1073741825_1183 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /tmp/bar.txt\n2016-10-25 15:34:45,576 INFO  StateChange - DIR* completeFile: /tmp/bar.txt is closed by DFSClient_NONMAPREDUCE_-1743096965_197\n2016-10-25 15:34:45,577 INFO  httpfsaudit - [/tmp/bar.txt]\n2016-10-25 15:34:45,579 INFO  AppendTestUtil - seed=-3144873070946578911, size=1\n2016-10-25 15:34:45,590 INFO  audit - allowed=true\tugi=weichiu (auth:PROXY) via weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=append\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:45,593 INFO  DataNode - Receiving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1183 src: /127.0.0.1:51132 dest: /127.0.0.1:50131\n2016-10-25 15:34:45,593 INFO  FsDatasetImpl - Appending to FinalizedReplica, blk_1073741825_1183, FINALIZED\n  getNumBytes()     = 183\n  getBytesOnDisk()  = 183\n  getVisibleLength()= 183\n  getVolume()       = /Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1\n  getBlockURI()     = file:/Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1/current/BP-837130339-172.16.1.88-1477434851452/current/finalized/subdir0/subdir0/blk_1073741825\n2016-10-25 15:34:45,603 INFO  FSNamesystem - updatePipeline(blk_1073741825_1183, newGS=1184, newLength=183, newNodes=[127.0.0.1:50131], client=DFSClient_NONMAPREDUCE_-1743096965_197)\n2016-10-25 15:34:45,603 INFO  FSNamesystem - updatePipeline(blk_1073741825_1183 => blk_1073741825_1184) success\n2016-10-25 15:34:45,605 INFO  clienttrace - src: /127.0.0.1:51132, dest: /127.0.0.1:50131, bytes: 184, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1743096965_197, offset: 0, srvID: 41c96335-5e4b-4950-ac22-3d21b353abb8, blockid: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1184, duration: 1377229\n2016-10-25 15:34:45,605 INFO  DataNode - PacketResponder: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1184, type=LAST_IN_PIPELINE terminating\n2016-10-25 15:34:45,606 INFO  FSNamesystem - BLOCK* blk_1073741825_1184 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /tmp/bar.txt\n2016-10-25 15:34:46,009 INFO  StateChange - DIR* completeFile: /tmp/bar.txt is closed by DFSClient_NONMAPREDUCE_-1743096965_197\n2016-10-25 15:34:46,010 INFO  httpfsaudit - [/tmp/bar.txt]\n2016-10-25 15:34:46,012 INFO  AppendTestUtil - seed=-263001291976323720, size=1\n2016-10-25 15:34:46,022 INFO  audit - allowed=true\tugi=weichiu (auth:PROXY) via weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=append\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:46,024 INFO  DataNode - Receiving BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1184 src: /127.0.0.1:51133 dest: /127.0.0.1:50131\n2016-10-25 15:34:46,024 INFO  FsDatasetImpl - Appending to FinalizedReplica, blk_1073741825_1184, FINALIZED\n  getNumBytes()     = 184\n  getBytesOnDisk()  = 184\n  getVisibleLength()= 184\n  getVolume()       = /Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1\n  getBlockURI()     = file:/Users/weichiu/sandbox/hadoop/hadoop-hdfs-project/hadoop-hdfs-httpfs/target/test-dir/dfs/data/data1/current/BP-837130339-172.16.1.88-1477434851452/current/finalized/subdir0/subdir0/blk_1073741825\n2016-10-25 15:34:46,032 INFO  FSNamesystem - updatePipeline(blk_1073741825_1184, newGS=1185, newLength=184, newNodes=[127.0.0.1:50131], client=DFSClient_NONMAPREDUCE_-1743096965_197)\n2016-10-25 15:34:46,032 INFO  FSNamesystem - updatePipeline(blk_1073741825_1184 => blk_1073741825_1185) success\n2016-10-25 15:34:46,033 INFO  clienttrace - src: /127.0.0.1:51133, dest: /127.0.0.1:50131, bytes: 185, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1743096965_197, offset: 0, srvID: 41c96335-5e4b-4950-ac22-3d21b353abb8, blockid: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1185, duration: 1112564\n2016-10-25 15:34:46,033 INFO  DataNode - PacketResponder: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1185, type=LAST_IN_PIPELINE terminating\n2016-10-25 15:34:46,033 INFO  FSNamesystem - BLOCK* blk_1073741825_1185 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /tmp/bar.txt\n2016-10-25 15:34:46,156 INFO  audit - allowed=true\tugi=weichiu (auth:SIMPLE)\tip=/127.0.0.1\tcmd=open\tsrc=/tmp/bar.txt\tdst=null\tperm=null\tproto=rpc\n2016-10-25 15:34:46,158 INFO  StateChange - *DIR* reportBadBlocks for block: BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182 on datanode: 127.0.0.1:50131\nException in thread \"Thread-144\" java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:309)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)\n\tat org.apache.hadoop.hdfs.ByteArrayStrategy.readFromBlock(ReaderStrategy.java:119)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:302)\n\t... 1 more\n2016-10-25 15:34:46,437 INFO  StateChange - DIR* completeFile: /tmp/bar.txt is closed by DFSClient_NONMAPREDUCE_-1743096965_197\n2016-10-25 15:34:46,437 INFO  httpfsaudit - [/tmp/bar.txt]\n2016-10-25 15:34:46,440 INFO  AppendTestUtil - seed=8756761565208093670, size=1\n2016-10-25 15:34:46,450 WARN  StateChange - DIR* NameSystem.append: append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.\n2016-10-25 15:34:46,450 INFO  Server - IPC Server handler 7 on 50130, call Call#25082 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.append from 127.0.0.1:50147\njava.io.IOException: append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:136)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2423)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:773)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:444)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1795)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2535)\n\nException in thread \"Thread-143\" java.lang.RuntimeException: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:283)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:159)\n\tat org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream.close(HttpFSFileSystem.java:470)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:279)\n\t... 1 more\n\norg.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\n\tat org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)\n\tat org.apache.hadoop.hdfs.ByteArrayStrategy.readFromBlock(ReaderStrategy.java:119)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:302)\n\tat java.lang.Thread.run(Thread.java:745)\n{quote}\n\n"
        }
    },
    {
        "filename": "HDFS-6825.json",
        "creation_time": "2014-08-06T01:31:34.000+0000",
        "bug_report": {
            "Title": "Edit log corruption due to delayed block removal",
            "Description": "Observed the following stack:\n{code}\n2014-08-04 23:49:44,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-.., newgenerationstamp=..., newlength=..., newtargets=..., closeFile=true, deleteBlock=false)\n2014-08-04 23:49:44,133 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Unexpected exception while updating disk space. \njava.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)\n        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n{code}\n\nFound this is what happened:\n\n- client created file /solr/hierarchy/core_node1/data/tlog/tlog.xyz\n- client tried to append to this file, but the lease expired, so lease recovery is started, thus the append failed\n- the file get deleted, however, there are still pending blocks of this file not deleted\n- then commitBlockSynchronization() method is called (see stack above), an InodeFile is created out of the pending block, not aware of that the file was deleted already\n- FileNotExistException was thrown by FSDirectory.updateSpaceConsumed, but swallowed by commitOrCompleteLastBlock\n- closeFileCommitBlocks continue to call finalizeINodeFileUnderConstruction and wrote CloseOp to the edit log\n"
        }
    },
    {
        "filename": "HDFS-5710.json",
        "creation_time": "2014-01-01T04:06:03.000+0000",
        "bug_report": {
            "Title": "FSDirectory#getFullPathName should check inodes against null",
            "Description": "From https://builds.apache.org/job/hbase-0.96-hadoop2/166/testReport/junit/org.apache.hadoop.hbase.mapreduce/TestTableInputFormatScan1/org_apache_hadoop_hbase_mapreduce_TestTableInputFormatScan1/ :\n{code}\n2014-01-01 00:10:15,571 INFO  [IPC Server handler 2 on 50198] blockmanagement.BlockManager(1009): BLOCK* addToInvalidates: blk_1073741967_1143 127.0.0.1:40188 127.0.0.1:46149 127.0.0.1:41496 \n2014-01-01 00:10:16,559 WARN  [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] namenode.FSDirectory(1854): Could not get full path. Corresponding file might have deleted already.\n2014-01-01 00:10:16,560 FATAL [org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor@93935b] blockmanagement.BlockManager$ReplicationMonitor(3127): ReplicationMonitor thread received Runtime exception. \njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)\n\tat org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)\n\tat java.lang.Thread.run(Thread.java:724)\n{code}\nLooks like getRelativePathINodes() returned null but getFullPathName() didn't check inodes against null, leading to NPE."
        }
    },
    {
        "filename": "HDFS-3555.json",
        "creation_time": "2012-06-20T17:36:48.000+0000",
        "bug_report": {
            "Title": "idle client socket triggers DN ERROR log (should be INFO or DEBUG)",
            "Description": "Datanode service is logging java.net.SocketTimeoutException at ERROR level.\nThis message indicates that the datanode is not able to send data to the client because the client has stopped reading. This message is not really a cause for alarm and should be INFO level.\n\n2012-06-18 17:47:13 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode DatanodeRegistration(x.x.x.x:50010, storageID=DS-196671195-10.10.120.67-50010-1334328338972, infoPort=50075, ipcPort=50020):DataXceiver\njava.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]\nat org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)\nat org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)\nat org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)"
        }
    },
    {
        "filename": "HDFS-10962.json",
        "creation_time": "2016-10-05T04:31:45.000+0000",
        "bug_report": {
            "Title": "TestRequestHedgingProxyProvider is flaky",
            "Description": "This test fails occasionally with an error like this:\n\n{noformat}\norg.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails\n\n\nError Message\n\nWanted but not invoked:\nnamenodeProtocols.getStats();\n-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nActually, there were zero interactions with this mock.\nStacktrace\n\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\nnamenodeProtocols.getStats();\n-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nStandard Output\n\n2016-09-26 15:26:02,234 WARN  hdfs.DFSUtil (DFSUtil.java:getAddressesForNameserviceId(689)) - Namenode for mycluster-26780990 remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.\n2016-09-26 15:26:02,340 WARN  hdfs.DFSUtil (DFSUtil.java:getAddressesForNameserviceId(689)) - Namenode for mycluster-26780990 remains unresolved for ID nn2.  Check your hdfs-site.xml file to ensure namenodes are configured properly.\n{noformat}"
        }
    },
    {
        "filename": "HDFS-12363.json",
        "creation_time": "2017-08-28T05:34:21.000+0000",
        "bug_report": {
            "Title": "Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages",
            "Description": "Saw NN going down with NPE below:\n\n{noformat}\nERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)\nat java.lang.Thread.run(Thread.java:745)\n2017-08-21 22:14:05,303 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1\n2017-08-21 22:14:05,313 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: \n{noformat}\n\nIn that version, {{BlockManager}} code is:\n{code}\n3896  try {\n3897           DatanodeStorageInfo storage = datanodeManager.\n3898                 getDatanode(datanodesAndStorages.get(i)).\n3899                getStorageInfo(datanodesAndStorages.get(i + 1));\n3900            if (storage != null) {\n{code}"
        }
    },
    {
        "filename": "HDFS-7916.json",
        "creation_time": "2015-03-11T10:40:00.000+0000",
        "bug_report": {
            "Title": "'reportBadBlocks' from datanodes to standby Node BPServiceActor goes for infinite loop",
            "Description": "if any badblock found, then BPSA for StandbyNode will go for infinite times to report it.\n\n{noformat}2015-03-11 19:43:41,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode: stobdtserver3/10.224.54.70:18010\norg.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:\n        at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n"
        }
    },
    {
        "filename": "HDFS-9549.json",
        "creation_time": "2015-12-11T16:25:08.000+0000",
        "bug_report": {
            "Title": "TestCacheDirectives#testExceedsCapacity is flaky",
            "Description": "I have observed that this test (TestCacheDirectives.testExceedsCapacity) fails quite frequently in Jenkins (trunk, trunk-Java8)  \n\nError Message\n\nPending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]\n\nStacktrace\n\njava.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)\n\n\n\n"
        }
    },
    {
        "filename": "HDFS-11164.json",
        "creation_time": "2016-11-22T05:58:03.000+0000",
        "bug_report": {
            "Title": "Mover should avoid unnecessary retries if the block is pinned",
            "Description": "When mover is trying to move a pinned block to another datanode, it will internally hits the following IOException and mark the block movement as {{failure}}. Since the Mover has {{dfs.mover.retry.max.attempts}} configs, it will continue moving this block until it reaches {{retryMaxAttempts}}. If the block movement failure(s) are only due to block pinning, then retry is unnecessary. The idea of this jira is to avoid retry attempts of pinned blocks as they won't be able to move to a different node. \n\n{code}\n2016-11-22 10:56:10,537 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: Failed to move blk_1073741825_1001 with size=52 from 127.0.0.1:19501:DISK to 127.0.0.1:19758:ARCHIVE through 127.0.0.1:19501\njava.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}"
        }
    },
    {
        "filename": "HDFS-5291.json",
        "creation_time": "2013-10-02T23:52:02.000+0000",
        "bug_report": {
            "Title": "Clients need to retry when Active NN is in SafeMode",
            "Description": "In our test, we saw NN immediately went into safemode after transitioning to active state. This can cause HBase region server to timeout and kill itself. We should allow clients to retry when HA is enabled and ANN is in SafeMode.\n\n============================================\nSome log snippets:\n\nstandby state to active transition\n{code}\n2013-10-02 00:13:49,482 INFO  ipc.Server (Server.java:run(2068)) - IPC Server handler 69 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.renewLease from IP:33911 Call#1483 Retry#1: error: org.apache.hadoop.ipc.StandbyException: Operation category WRITE is not supported in state standby\n2013-10-02 00:13:49,689 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for nn/hostname@EXAMPLE.COM (auth:SIMPLE)\n2013-10-02 00:13:49,696 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for nn/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.ha.HAServiceProtocol\n2013-10-02 00:13:49,700 INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1013)) - Stopping services started for standby state\n2013-10-02 00:13:49,701 WARN  ha.EditLogTailer (EditLogTailer.java:doWork(336)) - Edit log tailer interrupted\njava.lang.InterruptedException: sleep interrupted\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:356)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTail\n2013-10-02 00:13:49,704 INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(885)) - Starting services required for active state\n2013-10-02 00:13:49,719 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(419)) - Starting recovery process for unclosed journal segments...\n2013-10-02 00:13:49,755 INFO  ipc.Server (Server.java:saslProcess(1342)) - Auth successful for hbase/hostname@EXAMPLE.COM (auth:SIMPLE)\n2013-10-02 00:13:49,761 INFO  authorize.ServiceAuthorizationManager (ServiceAuthorizationManager.java:authorize(111)) - Authorization successful for hbase/hostname@EXAMPLE.COM (auth:KERBEROS) for protocol=interface org.apache.hadoop.hdfs.protocol.ClientProtocol\n2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnfinalizedSegments(421)) - Successfully started new epoch 85\n2013-10-02 00:13:49,839 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(249)) - Beginning recovery of unclosed segment starting at txid 887112\n2013-10-02 00:13:49,874 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recoverUnclosedSegment(258)) - Recovery prepare phase complete. Responses:\nIP:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530\n172.18.145.97:8485: segmentState { startTxId: 887112 endTxId: 887531 isInProgress: true } lastWriterEpoch: 84 lastCommittedTxId: 887530\n2013-10-02 00:13:49,875 INFO  client.QuorumJournalManager (QuorumJournalManager.java:recover\n{code}\n\n\nAnd then we get into safemode\n\n{code}\nConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,277 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP157{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[IP:1019|RBW], ReplicaUnderConstruction[172.18.145.96:1019|RBW], ReplicaUnde\nrConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,279 INFO  hdfs.StateChange (FSNamesystem.java:reportStatus(4703)) - STATE* Safe mode ON.\nThe reported blocks 1071 needs additional 5 blocks to reach the threshold 1.0000 of total blocks 1075.\nSafe mode will be turned off automatically\n2013-10-02 00:13:50,279 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: IP:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,280 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.99:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0\n2013-10-02 00:13:50,281 INFO  BlockStateChange (BlockManager.java:logAddStoredBlock(2237)) - BLOCK* addStoredBlock: blockMap updated: 172.18.145.97:1019 is added to blk_IP158{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[172.18.145.99:1019|RBW], ReplicaUnderConstruction[172.18.145.97:1019|RBW], ReplicaUnderConstruction[IP:1019|RBW]]} size 0\n{code}"
        }
    },
    {
        "filename": "HDFS-12836.json",
        "creation_time": "2017-11-17T20:26:32.000+0000",
        "bug_report": {
            "Title": "startTxId could be greater than endTxId when tailing in-progress edit log",
            "Description": "When {{dfs.ha.tail-edits.in-progress}} is true, edit log tailer will also tail those in progress edit log segments. However, in the following code:\r\n\r\n{code}\r\n        if (onlyDurableTxns && inProgressOk) {\r\n          endTxId = Math.min(endTxId, committedTxnId);\r\n        }\r\n\r\n        EditLogInputStream elis = EditLogFileInputStream.fromUrl(\r\n            connectionFactory, url, remoteLog.getStartTxId(),\r\n            endTxId, remoteLog.isInProgress());\r\n{code}\r\n\r\nit is possible that {{remoteLog.getStartTxId()}} could be greater than {{endTxId}}, and therefore will cause the following error:\r\n\r\n{code}\r\n2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576.  Expected transaction ID was 87\r\nRecent opcode offsets: 1048576\r\norg.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\r\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\r\n2017-11-17 19:55:41,165 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Error while reading edits from disk. Will try again.\r\norg.apache.hadoop.hdfs.server.namenode.EditLogInputException: Error replaying edit log at offset 1048576.  Expected transaction ID was 87\r\nRecent opcode offsets: 1048576\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:218)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\r\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\r\nCaused by: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)\r\n        ... 9 more\r\n{code}"
        }
    },
    {
        "filename": "HDFS-8113.json",
        "creation_time": "2015-04-09T11:29:31.000+0000",
        "bug_report": {
            "Title": "Add check for null BlockCollection pointers in BlockInfoContiguous structures",
            "Description": "The following copy constructor can throw NullPointerException if {{bc}} is null.\n{code}\n  protected BlockInfoContiguous(BlockInfoContiguous from) {\n    this(from, from.bc.getBlockReplication());\n    this.bc = from.bc;\n  }\n{code}\n\nWe have observed that some DataNodes keeps failing doing block reports with NameNode. The stacktrace is as follows. Though we are not using the latest version, the problem still exists.\n{quote}\n2015-03-08 19:28:13,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)\nat org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)\nat org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26382)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n{quote}"
        }
    },
    {
        "filename": "HDFS-10512.json",
        "creation_time": "2016-06-09T14:34:04.000+0000",
        "bug_report": {
            "Title": "VolumeScanner may terminate due to NPE in DataNode.reportBadBlocks",
            "Description": "VolumeScanner may terminate due to unexpected NullPointerException thrown in {{DataNode.reportBadBlocks()}}. This is different from HDFS-8850/HDFS-9190\n\nI observed this bug in a production CDH 5.5.1 cluster and the same bug still persist in upstream trunk.\n\n{noformat}\n2016-04-07 20:30:53,830 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-1800173197-10.204.68.5-1444425156296:blk_1170134484_96468685 on /dfs/dn\n2016-04-07 20:30:53,831 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting because of exception\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)\n2016-04-07 20:30:53,832 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting.\n{noformat}\n\nI think the NPE comes from the volume variable in the following code snippet. Somehow the volume scanner know the volume, but the datanode can not lookup the volume using the block.\n{code}\npublic void reportBadBlocks(ExtendedBlock block) throws IOException{\n    BPOfferService bpos = getBPOSForBlock(block);\n    FsVolumeSpi volume = getFSDataset().getVolume(block);\n    bpos.reportBadBlocks(\n        block, volume.getStorageID(), volume.getStorageType());\n  }\n{code}"
        }
    },
    {
        "filename": "HDFS-10729.json",
        "creation_time": "2016-08-08T02:36:05.000+0000",
        "bug_report": {
            "Title": "Improve log message for edit loading failures caused by FS limit checks.",
            "Description": "We encountered a bug where Standby NameNode crashes due to an NPE when loading edits.\n\n{noformat}\n2016-08-05 15:06:00,983 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader: Encountered exception on operation AddOp [length=0, inodeId=789272719, path=[path], replication=3, mtime=1470379597935, atime=1470379597935, blockSize=134217728, blocks=[], permissions=<user>:supergroup:rw-r--r--, aclEntries=null, clientName=DFSClient_NONMAPREDUCE_1495395702_1, clientMachine=10.210.119.136, overwrite=true, RpcClientId=a1512eeb-65e4-43dc-8aa8-d7a1af37ed30, RpcCallId=417, storagePolicyId=0, opCode=OP_ADD, txid=4212503758]\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:284)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:360)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1651)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:410)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)\n{noformat}\n\nThe NameNode crashes and can not be restarted. After some research, we turned on debug log of org.apache.hadoop.hdfs.StateChange, restart the NN, and we saw the following exception which induced NPE:\n\n{noformat}\n16/08/07 18:51:15 DEBUG hdfs.StateChange: DIR* FSDirectory.unprotectedAddFile: exception when add [path] to the file system\norg.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException: The directory item limit of [path] is exceeded: limit=1048576 items=1049332\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:2060)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:2112)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:2081)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1900)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedAddFile(FSDirectory.java:368)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:365)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:182)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:445)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:426)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:182)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1205)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1762)\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1635)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1351)\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n{noformat}\n\nThe exception is thrown, caught and logged at debug level in {{FSDirectory#unprotectedAddFile}}. Afterwards, a null is returned, which is used and subsequently caused NPE. HDFS-7567 reported a similar bug, but it was deemed the NPE could only be thrown if edit is corrupt. However, here we see an example where NPE could be thrown without corrupt edits.\n\nLooks like the maximum number of items per directory is exceeded. This is similar to HDFS-6102 and HDFS-7482, but this happens at loading edits, rather than loading fsimage. \n\nA possible workaround is to increas the value of {{dfs.namenode.fs-limits.max-directory-items}} to 6400000. But I am not sure if it would cause any side effects."
        }
    },
    {
        "filename": "HDFS-13040.json",
        "creation_time": "2018-01-19T19:47:02.000+0000",
        "bug_report": {
            "Title": "Kerberized inotify client fails despite kinit properly",
            "Description": "This issue is similar to HDFS-10799.\r\n\r\nHDFS-10799 turned out to be a client side issue where client is responsible for renewing kerberos ticket actively.\r\n\r\nHowever we found in a slightly setup even if client has valid Kerberos credentials, inotify still fails.\r\n\r\nSuppose client uses principal hdfs@EXAMPLE.COM, \r\n namenode 1 uses server principal hdfs/nn1.example.com@EXAMPLE.COM\r\n namenode 2 uses server principal hdfs/nn2.example.com@EXAMPLE.COM\r\n\r\n*After Namenodes starts for longer than kerberos ticket lifetime*, the client fails with the following error:\r\n{noformat}\r\n18/01/19 11:23:02 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs@GCE.CLOUDERA.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684.  If you continue, metadata will be lost forever!\r\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)\r\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)\r\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:415)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\r\n{noformat}\r\nTypically if NameNode has an expired Kerberos ticket, the error handling for the typical edit log tailing would let NameNode to relogin with its own Kerberos principal. However, when inotify uses the same code path to retrieve edits, since the current user is the inotify client's principal, unless client uses the same principal as the NameNode, NameNode can't do it on behalf of the client.\r\n\r\nTherefore, a more appropriate approach is to use proxy user so that NameNode can retrieving edits on behalf of the client.\r\n\r\nI will attach a patch to fix it. This patch has been verified to work for a CDH5.10.2 cluster, however it seems impossible to craft a unit test for this fix because the way Hadoop UGI handles Kerberos credentials (I can't have a single process that logins as two Kerberos principals simultaneously and let them establish connection)\r\n\r\nA possible workaround is for the inotify client to use the active NameNode's server principal. However, that's not going to work when there's a namenode failover, because then the client's principal will not be consistent with the active NN's one, and then fails to authenticate.\r\n\r\nCredit: this bug was confirmed and reproduced by [~pifta] and [~r1pp3rj4ck]"
        }
    },
    {
        "filename": "HDFS-3374.json",
        "creation_time": "2012-05-04T22:08:19.000+0000",
        "bug_report": {
            "Title": "hdfs' TestDelegationToken fails intermittently with a race condition",
            "Description": "The testcase is failing because the MiniDFSCluster is shutdown before the secret manager can change the key, which calls system.exit with no edit streams available.\n\n{code}\n\n    [junit] 2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1\n    [junit] 2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible\n    [junit] java.lang.Exception: No edit streams are accessible\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)\n    [junit]     at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)\n    [junit]     at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)\n    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)\n    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)\n    [junit]     at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)\n    [junit]     at java.lang.Thread.run(Thread.java:662)\n    [junit] Running org.apache.hadoop.hdfs.security.TestDelegationToken\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec\n    [junit] Test org.apache.hadoop.hdfs.security.TestDelegationToken FAILED (crashed)\n{code}"
        }
    },
    {
        "filename": "HDFS-2359.json",
        "creation_time": "2011-09-23T00:20:53.000+0000",
        "bug_report": {
            "Title": "NPE found in Datanode log while Disk failed during different HDFS operation",
            "Description": "Scenario:\nI have a cluster of 4 DN ,each of them have 12disks.\n\nIn hdfs-site.xml I have \"dfs.datanode.failed.volumes.tolerated=3\" \n\nDuring the execution of distcp (hdfs->hdfs), I am failing 3 disks in one Datanode, by making Data Directory permission 000, The distcp job is successful but , I am getting some NullPointerException in Datanode log\n\nIn one thread\n$hadoop distcp  /user/$HADOOPQA_USER/data1 /user/$HADOOPQA_USER/data3\n\nIn another thread in a datanode\n$ chmod 000 /xyz/{0,1,2}/hadoop/var/hdfs/data\n\nwhere [ dfs.data.dir is set as /xyz/{0..11}/hadoop/var/hdfs/data ]\n\nLog Snippet from the Datanode\n=============\n\n2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7065198814142552283_62557. BlockInfo not found in volumeMap.\n2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7066946313092770579_39189. BlockInfo not found in volumeMap.\n2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7070305189404753930_49359. BlockInfo not found in volumeMap.\n2011-09-19 12:43:40,327 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command\njava.io.IOException: Error in deleting blocks.\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:1820)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1074)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1036)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:891)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1419)\n        at java.lang.Thread.run(Thread.java:619)\n2011-09-19 12:43:41,304 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode:\nDatanodeRegistration(xx.xxx.xxx.xxx:xxxx, storageID=xx-xxxxxxxxxxxx-xx.xxx.xxx.xxx-xxxx-xxxxxxxxxxx, infoPort=1006,\nipcPort=8020):DataXceiver\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7071818644980664768_40827. BlockInfo not found in volumeMap.\n2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block\nblk_7073840977856837621_62108. BlockInfo not found in volumeMap."
        }
    },
    {
        "filename": "HDFS-10986.json",
        "creation_time": "2016-10-08T00:50:36.000+0000",
        "bug_report": {
            "Title": "DFSAdmin should log detailed error message if any",
            "Description": "There are some subcommands in {{DFSAdmin}} that swallow IOException and give very limited error message, if any, to the stderr.\n\n{code}\n$ hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9866\nDatanode unreachable.\n$ hdfs dfsadmin -getDatanodeInfo localhost:9866\nDatanode unreachable.\n$ hdfs dfsadmin -evictWriters 127.0.0.1:9866\n$ echo $?\n-1\n{code}\n\nUser is not able to get the exception stack even the LOG level is DEBUG. This is not very user friendly. Fortunately, if the port number is not accessible (say 9999), users can infer the detailed error message by IPC logs:\n{code}\n$ hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9999\n2016-10-07 18:01:35,115 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-10-07 18:01:36,335 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9999. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n.....\n2016-10-07 18:01:45,361 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9999. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n2016-10-07 18:01:45,362 WARN ipc.Client: Failed to connect to server: localhost/127.0.0.1:9999: retries get failed due to exceeded maximum allowed retries number: 10\njava.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n        ...\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)\nDatanode unreachable.\n{code}\n\nWe should fix this by providing detailed error message. Actually, the {{DFSAdmin#run}} already handles exception carefully, including:\n# set the exit ret value to -1\n# print the error message\n# log the exception stack trace (in DEBUG level)\n\nAll we need to do is to not swallow exceptions without good reason."
        }
    },
    {
        "filename": "HDFS-6455.json",
        "creation_time": "2014-05-28T18:01:15.000+0000",
        "bug_report": {
            "Title": "NFS: Exception should be added in NFS log for invalid separator in nfs.exports.allowed.hosts",
            "Description": "The error for invalid separator in dfs.nfs.exports.allowed.hosts property should be added in nfs log file instead nfs.out file.\n\nSteps to reproduce:\n1. Pass invalid separator in dfs.nfs.exports.allowed.hosts\n{noformat}\n<property><name>dfs.nfs.exports.allowed.hosts</name><value>host1  ro:host2 rw</value></property>\n{noformat}\n\n2. restart NFS server. NFS server fails to start and print exception console.\n{noformat}\n[hrt_qa@host1 hwqe]$ ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null host1 \"sudo su - -c \\\"/usr/lib/hadoop/sbin/hadoop-daemon.sh start nfs3\\\" hdfs\"\nstarting nfs3, logging to /tmp/log/hadoop/hdfs/hadoop-hdfs-nfs3-horst1.out\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n\tat org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n\tat org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n\tat org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)\n{noformat}\n\nNFS log does not print any error message. It directly shuts down. \n{noformat}\nSTARTUP_MSG:   java = 1.6.0_31\n************************************************************/\n2014-05-27 18:47:13,972 INFO  nfs3.Nfs3Base (SignalLogger.java:register(91)) - registered UNIX signal handlers for [TERM, HUP, INT]\n2014-05-27 18:47:14,169 INFO  nfs3.IdUserGroup (IdUserGroup.java:updateMapInternal(159)) - Updated user map size:259\n2014-05-27 18:47:14,179 INFO  nfs3.IdUserGroup (IdUserGroup.java:updateMapInternal(159)) - Updated group map size:73\n2014-05-27 18:47:14,192 INFO  nfs3.Nfs3Base (StringUtils.java:run(640)) - SHUTDOWN_MSG:\n/************************************************************\nSHUTDOWN_MSG: Shutting down Nfs3 at \n{noformat}\n\nNFS.out file has exception.\n{noformat}\nEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n        at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n        at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n        at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)\nulimit -a for user hdfs\ncore file size          (blocks, -c) 409600\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 188893\nmax locked memory       (kbytes, -l) unlimited\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 32768\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 10240\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 65536\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n{noformat}"
        }
    },
    {
        "filename": "HDFS-2882.json",
        "creation_time": "2012-02-02T18:11:20.000+0000",
        "bug_report": {
            "Title": "DN continues to start up, even if block pool fails to initialize",
            "Description": "I started a DN on a machine that was completely out of space on one of its drives. I saw the following:\n\n2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-12978\n42002148) service to styx01.sf.cloudera.com/172.29.5.192:8021\njava.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)\n\nbut the DN continued to run, spewing NPEs when it tried to do block reports, etc. This was on the HDFS-1623 branch but may affect trunk as well."
        }
    },
    {
        "filename": "HDFS-5185.json",
        "creation_time": "2013-09-11T14:38:23.000+0000",
        "bug_report": {
            "Title": "DN fails to startup if one of the data dir is full",
            "Description": "DataNode fails to startup if one of the data dirs configured is out of space. \n\n\nfails with following exception\n{noformat}2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110\njava.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)\n        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)\n        at java.lang.Thread.run(Thread.java:662)\n{noformat}\n\n\nIt should continue to start-up with other data dirs available."
        }
    },
    {
        "filename": "HDFS-13164.json",
        "creation_time": "2018-02-17T00:40:21.000+0000",
        "bug_report": {
            "Title": "File not closed if streamer fail with DSQuotaExceededException",
            "Description": "\u00a0This is found during yarn log aggregation but theoretically could happen to any client.\r\n\r\nIf the dir's space quota is exceeded, the following would happen when a file is created:\r\n - client {{startFile}} rpc to NN, gets a {{DFSOutputStream}}.\r\n - writing to the stream would trigger the streamer to {{getAdditionalBlock}} rpc to NN, which would get the DSQuotaExceededException\r\n - client closes the stream\r\n \u00a0\r\n The fact that this would leave a 0-sized (or whatever size left in the quota) file in HDFS is beyond the scope of this jira. However, the file would be left in openforwrite status (shown in\u00a0{{fsck -openforwrite)}} at least, and could potentially leak leaseRenewer too.\r\n\r\nThis is because in the close implementation,\r\n # {{isClosed}} is first checked, and the close call will be a no-op if {{isClosed == true}}.\r\n # {{flushInternal}} checks {{isClosed}}, and throws the exception right away if\u00a0true\r\n\r\n{{isClosed}} does this: {{return closed || getStreamer().streamerClosed;}}\r\n\r\nWhen the disk quota is reached, {{getAdditionalBlock}} will throw when the streamer calls. Because the streamer runs in a separate thread, at the time the client calls close on the stream, the streamer may or may not have reached the Quota exception. If it has, then due to #1, the close call on the stream will be no-op. If it hasn't, then due to #2 the {{completeFile}} logic will be skipped.\r\n{code:java}\r\nprotected synchronized void closeImpl() throws IOException {\r\n    if (isClosed()) {\r\n      IOException e = lastException.getAndSet(null);\r\n      if (e == null)\r\n        return;\r\n      else\r\n        throw e;\r\n    }\r\n  try {\r\n    flushBuffer(); // flush from all upper layers\r\n    ...\r\n    flushInternal(); // flush all data to Datanodes\r\n\r\n    // get last block before destroying the streamer\r\n    ExtendedBlock lastBlock = getStreamer().getBlock();\r\n\r\n    try (TraceScope ignored =\r\n       dfsClient.getTracer().newScope(\"completeFile\")) {\r\n       completeFile(lastBlock);\r\n    }\r\n   } catch (ClosedChannelException ignored) {\r\n   } finally {\r\n     closeThreads(true);\r\n   }\r\n }\r\n\r\n\u00a0{code}\r\nLog snippets:\r\n{noformat}\r\n2018-02-16 15:59:32,916 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer Quota Exception\r\norg.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\r\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\r\n\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1833)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1626)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:788)\r\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /tmp/logs/systest/logs is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\r\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\r\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\r\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\r\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\r\n\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1504)\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1441)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\r\n        at com.sun.proxy.$Proxy82.addBlock(Unknown Source)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:423)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\r\n        at com.sun.proxy.$Proxy83.addBlock(Unknown Source)\r\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1830)\r\n        ... 2 more\r\n{noformat}"
        }
    },
    {
        "filename": "HDFS-11508.json",
        "creation_time": "2017-03-07T04:55:50.000+0000",
        "bug_report": {
            "Title": "Fix bind failure in SimpleTCPServer & Portmap where bind fails because socket is in TIME_WAIT state",
            "Description": "Bind can fail in SimpleTCPServer & Portmap because socket is in TIME_WAIT state.\n\nSocket options should be changed here to use the setReuseAddress option.\n\n{noformat}\n2017-03-05 08:37:13,283 INFO  oncrpc.SimpleUdpServer (SimpleUdpServer.java:run(73)) - Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1\n2017-03-05 08:37:13,298 FATAL mount.MountdBase (MountdBase.java:startTCPServer(85)) - Failed to start the TCP server.\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n\tat org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)\n\tat org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)\n\tat org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)\n\tat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:433)\n\tat sun.nio.ch.Net.bind(Net.java:425)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n2017-03-05 08:37:13,304 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1\n2017-03-05 08:37:13,308 INFO  nfs3.Nfs3Base (LogAdapter.java:info(45)) - SHUTDOWN_MSG: \n/************************************************************\n{noformat}"
        }
    },
    {
        "filename": "HDFS-2991.json",
        "creation_time": "2012-02-23T02:06:51.000+0000",
        "bug_report": {
            "Title": "failure to load edits: ClassCastException",
            "Description": "In doing scale testing of trunk at r1291606, I hit the following:\n\njava.io.IOException: Error replaying edit log at offset 1354251\nRecent opcode offsets: 1350014 1350176 1350312 1354251\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)\n...\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)\n        ... 13 more\n"
        }
    },
    {
        "filename": "HDFS-4404.json",
        "creation_time": "2013-01-15T01:43:17.000+0000",
        "bug_report": {
            "Title": "Create file failure when the machine of first attempted NameNode is down",
            "Description": "test Environment: NN1,NN2,DN1,DN2,DN3\nmachine1:NN1,DN1\nmachine2:NN2,DN2\nmachine3:DN3\n\nmathine1 is down.\n\n2013-01-12 09:51:21,248 DEBUG ipc.Client (Client.java:setupIOstreams(562)) - Connecting to /160.161.0.155:8020\n2013-01-12 09:51:38,442 DEBUG ipc.Client (Client.java:close(932)) - closing ipc connection to vm2/160.161.0.155:8020: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\njava.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\n2013-01-12 09:51:38,443 DEBUG ipc.Client (Client.java:close(940)) - IPC Client (31594013) connection to /160.161.0.155:8020 from hdfs/hadoop@HADOOP.COM: closed\n2013-01-12 09:52:47,834 WARN  retry.RetryInvocationHandler (RetryInvocationHandler.java:invoke(95)) - Exception while invoking class org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create. Not retrying because the invoked method is not idempotent, and unable to determine whether it was invoked\njava.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)\n at org.apache.hadoop.ipc.Client.call(Client.java:1180)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\nCaused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n ... 20 more\njava.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)\n at org.apache.hadoop.ipc.Client.call(Client.java:1180)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\nCaused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n ... 20 more\n2013-01-12 09:54:52,269 DEBUG ipc.Client (Client.java:stop(1021)) - Stopping client\n"
        }
    },
    {
        "filename": "HDFS-8276.json",
        "creation_time": "2015-04-28T10:11:35.000+0000",
        "bug_report": {
            "Title": "LazyPersistFileScrubber should be disabled if scrubber interval configured zero",
            "Description": "bq. but I think it is simple enough to change the meaning of the value so that zero means 'never scrub'. Let me post an updated patch.\n\nAs discussed in [HDFS-6929|https://issues.apache.org/jira/browse/HDFS-6929], scrubber should be disable if *dfs.namenode.lazypersist.file.scrub.interval.sec* is zero.\n\nCurrently namenode startup is failing if interval configured zero\n\n{code}\n2015-04-27 23:47:31,744 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.\njava.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)\n{code}"
        }
    },
    {
        "filename": "HDFS-12369.json",
        "creation_time": "2017-08-28T22:56:02.000+0000",
        "bug_report": {
            "Title": "Edit log corruption due to hard lease recovery of not-closed file which has snapshots",
            "Description": "HDFS-6257 and HDFS-7707 worked hard to prevent corruption from combinations of client operations.\n\nRecently, we have observed NN not able to start with the following exception:\n{noformat}\n2017-08-17 14:32:18,418 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.\njava.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)\n{noformat}\n\nQuoting a nicely analysed edits:\n{quote}\nIn the edits logged about 1 hour later, we see this failing OP_CLOSE. The sequence in the edits shows the file going through:\n\n  OPEN\n  ADD_BLOCK\n  CLOSE\n  ADD_BLOCK # perhaps this was an append\n  DELETE\n  (about 1 hour later) CLOSE\n\nIt is interesting that there was no CLOSE logged before the delete.\n{quote}\n\nGrepping that file name, it turns out the close was triggered by {{LeaseManager}}, when the lease reaches hard limit.\n{noformat}\n2017-08-16 15:05:45,927 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: \n  Recovering [Lease.  Holder: DFSClient_NONMAPREDUCE_-1997177597_28, pending creates: 75], \n  src=/home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M\n2017-08-16 15:05:45,927 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* \n  internalReleaseLease: All existing blocks are COMPLETE, lease removed, file \n  /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M closed.\n{noformat}"
        }
    },
    {
        "filename": "HDFS-6462.json",
        "creation_time": "2014-05-29T00:19:03.000+0000",
        "bug_report": {
            "Title": "NFS: fsstat request fails with the secure hdfs",
            "Description": "Fsstat fails in secure environment with below error.\n\nSteps to reproduce:\n1) Create user named UserB and UserA\n2) Create group named GroupB\n3) Add root and UserB users to GroupB\n    Make sure UserA is not in GroupB\n4) Set below properties\n{noformat}\n===================================\nhdfs-site.xml\n===================================\n <property>\n    <name>dfs.nfs.keytab.file</name>\n    <value>/tmp/keytab/UserA.keytab</value>\n  </property>\n  <property>\n    <name>dfs.nfs.kerberos.principal</name>\n    <value>UserA@EXAMPLE.COM</value>\n  </property>\n==================================\ncore-site.xml\n==================================\n<property>\n    <name>hadoop.proxyuser.UserA.groups</name>\n   <value>GroupB</value>\n </property>\n<property>\n   <name>hadoop.proxyuser.UserA.hosts</name>\n   <value>*</value>\n </property>\n{noformat}\n4) start nfs server as UserA\n5) mount nfs as root user\n6) run below command \n{noformat}\n[root@host1 ~]# df /tmp/tmp_mnt/\ndf: `/tmp/tmp_mnt/': Input/output error\ndf: no file systems processed\n{noformat}\n\nNFS Logs complains as below\n{noformat}\n2014-05-29 00:09:13,698 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1654)) - NFS FSSTAT fileId: 16385\n2014-05-29 00:09:13,706 WARN  ipc.Client (Client.java:run(672)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n2014-05-29 00:09:13,710 WARN  nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1681)) - Exception\njava.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"host1/0.0.0.0\"; destination host is: \"host1\":8020;\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1414)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)\n        at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal(RpcProgramNfs3.java:1961)\n        at org.apache.hadoop.oncrpc.RpcProgram.messageReceived(RpcProgram.java:162)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:281)\n        at org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage.messageReceived(RpcUtil.java:132)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\n        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\n        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1381)\n        ... 42 more\n{noformat}"
        }
    },
    {
        "filename": "HDFS-5425.json",
        "creation_time": "2013-10-25T06:53:28.000+0000",
        "bug_report": {
            "Title": "Renaming underconstruction file with snapshots can make NN failure on restart",
            "Description": "I faced this When i am doing some snapshot operations like createSnapshot,renameSnapshot,i restarted my NN,it is shutting down with exception,\n2013-10-24 21:07:03,040 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join\njava.lang.IllegalStateException\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)\n2013-10-24 21:07:03,050 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1\n2013-10-24 21:07:03,052 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: \n"
        }
    },
    {
        "filename": "HDFS-13145.json",
        "creation_time": "2018-02-13T19:22:56.000+0000",
        "bug_report": {
            "Title": "SBN crash when transition to ANN with in-progress edit tailing enabled",
            "Description": "With edit log in-progress edit log tailing enabled, {{QuorumOutputStream}} will send two batches to JNs, one normal edit batch followed by a dummy batch to update the commit ID on JNs.\r\n\r\n{code}\r\n      QuorumCall<AsyncLogger, Void> qcall = loggers.sendEdits(\r\n          segmentTxId, firstTxToFlush,\r\n          numReadyTxns, data);\r\n      loggers.waitForWriteQuorum(qcall, writeTimeoutMs, \"sendEdits\");\r\n      \r\n      // Since we successfully wrote this batch, let the loggers know. Any future\r\n      // RPCs will thus let the loggers know of the most recent transaction, even\r\n      // if a logger has fallen behind.\r\n      loggers.setCommittedTxId(firstTxToFlush + numReadyTxns - 1);\r\n\r\n      // If we don't have this dummy send, committed TxId might be one-batch\r\n      // stale on the Journal Nodes\r\n      if (updateCommittedTxId) {\r\n        QuorumCall<AsyncLogger, Void> fakeCall = loggers.sendEdits(\r\n            segmentTxId, firstTxToFlush,\r\n            0, new byte[0]);\r\n        loggers.waitForWriteQuorum(fakeCall, writeTimeoutMs, \"sendEdits\");\r\n      }\r\n{code}\r\n\r\nBetween each batch, it will wait for the JNs to reach a quorum. However, if the ANN crashes in between, then SBN will crash while transiting to ANN:\r\n\r\n{code}\r\njava.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......\r\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\r\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)\r\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\r\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2490)\r\n2018-02-13 00:43:20,728 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1\r\n{code}\r\n\r\nThis is because without the dummy batch, the {{commitTxnId}} will lag behind the {{endTxId}}, which caused the check in {{openForWrite}} to fail:\r\n{code}\r\n    List<EditLogInputStream> streams = new ArrayList<EditLogInputStream>();\r\n    journalSet.selectInputStreams(streams, segmentTxId, true, false);\r\n    if (!streams.isEmpty()) {\r\n      String error = String.format(\"Cannot start writing at txid %s \" +\r\n        \"when there is a stream available for read: %s\",\r\n        segmentTxId, streams.get(0));\r\n      IOUtils.cleanupWithLogger(LOG,\r\n          streams.toArray(new EditLogInputStream[0]));\r\n      throw new IllegalStateException(error);\r\n    }\r\n{code}\r\n\r\nIn our environment, this can be reproduced pretty consistently, which will leave the cluster with no running namenodes. Even though we are using a 2.8.2 backport, I believe the same issue also exist in 3.0.x. "
        }
    },
    {
        "filename": "HDFS-8807.json",
        "creation_time": "2015-07-22T20:45:55.000+0000",
        "bug_report": {
            "Title": "dfs.datanode.data.dir does not handle spaces between storageType and URI correctly",
            "Description": "if you add a space between the storage type and file URI then datanodes fail during startup.\nHere is an example of \"mis-configration\" that leads to datanode failure.\n{code}\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>\n      [DISK] file://tmp/hadoop-aengineer/disk1/dfs/data\n    </value>\n  </property>\n{code}\nHere is the \"fixed\" version. Please *note* the lack of space between \\[DISK\\] and file URI\n{code}\n<property>\n    <name>dfs.datanode.data.dir</name>\n    <value>\n      [DISK]file://tmp/hadoop-aengineer/disk1/dfs/data\n    </value>\n  </property>\n{code}\nwe fail with a parsing error, here is the info from the datanode logs.\n{code}\n2015-07-22 13:34:36,812 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain\njava.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at org.apache.hadoop.fs.Path.initialize(Path.java:204)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:170)\n        at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)\nCaused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at java.net.URI$Parser.fail(URI.java:2829)\n        at java.net.URI$Parser.checkChars(URI.java:3002)\n        at java.net.URI$Parser.checkChar(URI.java:3012)\n        at java.net.URI$Parser.parse(URI.java:3028)\n        at java.net.URI.<init>(URI.java:753)\n        at org.apache.hadoop.fs.Path.initialize(Path.java:201)\n        ... 7 more\n{code}"
        }
    },
    {
        "filename": "HDFS-3436.json",
        "creation_time": "2012-05-17T10:36:31.000+0000",
        "bug_report": {
            "Title": "adding new datanode to existing  pipeline fails in case of Append/Recovery",
            "Description": "Scenario:\n=========\n\n1. Cluster with 4 DataNodes.\n2. Written file to 3 DNs, DN1->DN2->DN3\n3. Stopped DN3,\nNow Append to file is failing due to addDatanode2ExistingPipeline is failed.\n\n *CLinet Trace* \n{noformat}\n2012-04-24 22:06:09,947 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1063)) - Exception in createBlockOutputStream\njava.io.IOException: Bad connect ack with firstBadLink as *******:50010\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n2012-04-24 22:06:09,947 WARN  hdfs.DFSClient (DFSOutputStream.java:setupPipelineForAppendOrRecovery(916)) - Error Recovery for block BP-1023239-10.18.40.233-1335275282109:blk_296651611851855249_1253 in pipeline *****:50010, ******:50010, *****:50010: bad datanode ******:50010\n2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1515)) - Error while syncing\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n{noformat}\n\n *DataNode Trace*  \n\n{noformat}\n\n2012-05-17 15:39:12,261 ERROR datanode.DataNode (DataXceiver.java:run(193)) - host0.foo.com:49744:DataXceiver error processing TRANSFER_BLOCK operation  src: /127.0.0.1:49811 dest: /127.0.0.1:49744\njava.io.IOException: BP-2001850558-xx.xx.xx.xx-1337249347060:blk_-8165642083860293107_1002 is neither a RBW nor a Finalized, r=ReplicaBeingWritten, blk_-8165642083860293107_1003, RBW\n  getNumBytes()     = 1024\n  getBytesOnDisk()  = 1024\n  getVisibleLength()= 1024\n  getVolume()       = E:\\MyWorkSpace\\branch-2\\Test\\build\\test\\data\\dfs\\data\\data1\\current\n  getBlockFile()    = E:\\MyWorkSpace\\branch-2\\Test\\build\\test\\data\\dfs\\data\\data1\\current\\BP-2001850558-xx.xx.xx.xx-1337249347060\\current\\rbw\\blk_-8165642083860293107\n  bytesAcked=1024\n  bytesOnDisk=102\nat org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2038)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:525)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:114)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:78)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)\n\tat java.lang.Thread.run(Unknown Source)\n{noformat}"
        }
    }
]