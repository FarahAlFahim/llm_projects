[
    {
        "filename": "HDFS-4558.json",
        "creation_time": "2013-03-06T11:10:08.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)\n at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)\n```"
    },
    {
        "filename": "HDFS-13039.json",
        "creation_time": "2018-01-19T18:52:12.000+0000",
        "stack_trace": "```\njava.io.IOException: Too many open files\n        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)\n        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)\n        at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)\n        at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)\n        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)\n        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)\n        at java.lang.Thread.run(Thread.java:748)\n```"
    },
    {
        "filename": "HDFS-13023.json",
        "creation_time": "2018-01-16T20:27:39.000+0000",
        "stack_trace": "```\ncom.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)\n at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)\n at java.lang.Thread.run(Thread.java:748)\n Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx\n at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)\n at org.apache.hadoop.ipc.Client.call(Client.java:1437)\n at org.apache.hadoop.ipc.Client.call(Client.java:1347)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)```"
    },
    {
        "filename": "HDFS-3157.json",
        "creation_time": "2012-03-28T14:35:53.000+0000",
        "stack_trace": "```\njava.io.IOException: Error in deleting blocks.\n\tat org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)\n\tat java.lang.Thread.run(Thread.java:619)\n```"
    },
    {
        "filename": "HDFS-4850.json",
        "creation_time": "2013-05-25T00:18:15.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.NegativeArraySizeException\n\tat org.apache.hadoop.io.Text.readString(Text.java:458)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)\n\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)\n```"
    },
    {
        "filename": "HDFS-3415.json",
        "creation_time": "2012-05-13T13:44:43.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)\n```"
    },
    {
        "filename": "HDFS-2245.json",
        "creation_time": "2011-08-10T22:55:33.000+0000",
        "stack_trace": "```\njava.io.IOException: java.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)\n```"
    },
    {
        "filename": "HDFS-10320.json",
        "creation_time": "2016-04-20T23:02:09.000+0000",
        "stack_trace": "```\norg.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)\n\tat org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-4201.json",
        "creation_time": "2012-11-16T21:28:57.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)\n        at java.lang.Thread.run(Thread.java:722)\n```"
    },
    {
        "filename": "HDFS-6904.json",
        "creation_time": "2014-08-21T10:17:21.000+0000",
        "stack_trace": "```\njava.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$5(DelegationTokenRenewer.java:357)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:657)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:638)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:331)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:598)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:448)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:477)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:473)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.renewDelegationToken(WebHdfsFileSystem.java:1318)\n        at org.apache.hadoop.hdfs.web.TokenAspect$TokenManager.renew(TokenAspect.java:73)\n        at org.apache.hadoop.security.token.Token.renew(Token.java:377)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:477)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:1)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:392)\n        ... 6 more\nCaused by: java.io.IOException: The error stream is null.\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:304)\n        at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:329)```"
    },
    {
        "filename": "HDFS-13721.json",
        "creation_time": "2018-07-05T20:11:57.000+0000",
        "stack_trace": "```\njavax.management.RuntimeMBeanException: java.lang.NullPointerException\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)\n at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)\n at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)\n at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)\n at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)\n at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n at org.eclipse.jetty.server.Server.handle(Server.java:534)\n at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)\n at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n at java.lang.reflect.Method.invoke(Method.java:498)\n at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)\n at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)\n at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)\n at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)\n at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)\n at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)\n at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)\n```"
    },
    {
        "filename": "HDFS-7180.json",
        "creation_time": "2014-10-02T03:07:04.000+0000",
        "stack_trace": "```\njava.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:828)\n```"
    },
    {
        "filename": "HDFS-6102.json",
        "creation_time": "2014-03-13T18:27:36.000+0000",
        "stack_trace": "```\ncom.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.\n        at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)\n        at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)\n        at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)\n        at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)\n        at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)\n        at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)\n```"
    },
    {
        "filename": "HDFS-6250.json",
        "creation_time": "2014-04-16T16:14:32.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: expected:<1800> but was:<1810>\n\tat org.junit.Assert.fail(Assert.java:93)\n\tat org.junit.Assert.failNotEquals(Assert.java:647)\n\tat org.junit.Assert.assertEquals(Assert.java:128)\n\tat org.junit.Assert.assertEquals(Assert.java:147)\n\tat org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup.testBalancerWithRackLocality(TestBalancerWithNodeGroup.java:253)\n```"
    },
    {
        "filename": "HDFS-11377.json",
        "creation_time": "2017-01-26T23:40:53.000+0000",
        "stack_trace": "```\njava.lang.Thread.State: TIMED_WAITING (sleeping)\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n        at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)\n```"
    },
    {
        "filename": "HDFS-6753.json",
        "creation_time": "2014-07-25T11:36:43.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).\n \nat org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)\n\njava.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized\n \nat org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)\n \nat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)\n```"
    },
    {
        "filename": "HDFS-3443.json",
        "creation_time": "2012-05-18T11:30:50.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)\n\tat org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)\n\tat org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n\tat org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)\n\njava.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:2092)\n\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:107)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:930)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:994)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1738)\n```"
    },
    {
        "filename": "HDFS-11479.json",
        "creation_time": "2017-03-01T11:08:07.000+0000",
        "stack_trace": "```\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)\n        at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)\n        at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)\n        at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n        at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n        at sun.nio.ch.Net.bind0(Native Method)\n        at sun.nio.ch.Net.bind(Net.java:433)\n        at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)\n        at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)\n        at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)\n        at org.jboss.netty.channel.Channels.bind(Channels.java:561)\n        at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)\n        at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)```"
    },
    {
        "filename": "HDFS-8055.json",
        "creation_time": "2015-04-02T23:21:55.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n         at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)\n         at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)\n         at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)\n         at java.security.AccessController.doPrivileged(Native Method)\n         at javax.security.auth.Subject.doAs(Subject.java:415)\n         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n         at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)\n         at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n         at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n         at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n         at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:254)\n         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n         at java.lang.reflect.Method.invoke(Method.java:606)\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n         at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n         at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)\n         at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)\n         at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)\n         at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)\n         at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)\n         at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)\n         at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)\n         at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n         at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)\n         at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:320)\n         at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:300)\n         at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n         at java.lang.Thread.run(Thread.java:745)\n\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\n         at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)\n         at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)\n         at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)\n         at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)\n```"
    },
    {
        "filename": "HDFS-6533.json",
        "creation_time": "2014-06-14T16:15:02.000+0000",
        "stack_trace": "```\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\ndatanodeProtocolClientSideTranslatorPB.registerDatanode(\n    <any>\n);\n-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)\n```"
    },
    {
        "filename": "HDFS-10609.json",
        "creation_time": "2016-07-11T17:14:39.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n\nCaused by: org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)\n```"
    },
    {
        "filename": "HDFS-2310.json",
        "creation_time": "2011-09-05T15:31:00.000+0000",
        "stack_trace": "```\njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\njava.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol\n\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n```"
    },
    {
        "filename": "HDFS-3385.json",
        "creation_time": "2012-05-08T12:45:31.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)\n        ...\n\tat org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)\n\tat org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)\n\tat org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)\n```"
    },
    {
        "filename": "HDFS-4006.json",
        "creation_time": "2012-10-04T22:05:14.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)\nat org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)\nat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)\nat java.lang.Thread.run(Thread.java:662)\n```"
    },
    {
        "filename": "HDFS-6715.json",
        "creation_time": "2014-07-21T21:26:25.000+0000",
        "stack_trace": "```\njava.io.IOException: Namenode is in startup mode\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n```"
    },
    {
        "filename": "HDFS-2392.json",
        "creation_time": "2011-09-30T20:29:09.000+0000",
        "stack_trace": "```\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\njava.io.IOException: Copied: 0 Skipped: 0 Failed: 1\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\nCopy failed: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1257)\n        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)\n        at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)\n```"
    },
    {
        "filename": "HDFS-11472.json",
        "creation_time": "2017-02-28T18:39:13.000+0000",
        "stack_trace": "```\njava.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)\n        at java.lang.Thread.run(Thread.java:745)\n\njava.nio.channels.ClosedByInterruptException\n        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)\n        at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:269)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition(FsDatasetImpl.java:1484)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition(BlockReceiver.java:994)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:670)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:857)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:797)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-10760.json",
        "creation_time": "2016-08-12T08:29:24.000+0000",
        "stack_trace": "```\norg.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)\n        at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)\n        at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-13635.json",
        "creation_time": "2018-05-29T19:54:50.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)\n at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)\n at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)\n at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)\n at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\n at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)\n```"
    },
    {
        "filename": "HDFS-11608.json",
        "creation_time": "2017-03-31T17:37:20.000+0000",
        "stack_trace": "```\njava.io.IOException: Incorrect value for packet payload size: 2147483128\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:898)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-12638.json",
        "creation_time": "2017-10-11T11:29:04.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)\n        at java.lang.Thread.run(Thread.java:834)\n```"
    },
    {
        "filename": "HDFS-12383.json",
        "creation_time": "2017-08-31T20:43:50.000+0000",
        "stack_trace": "```\njava.util.concurrent.CancellationException\n        at java.util.concurrent.FutureTask.report(FutureTask.java:121)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)\n        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-5322.json",
        "creation_time": "2013-10-07T22:52:54.000+0000",
        "stack_trace": "```\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)\nError: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache\nat org.apache.hadoop.ipc.Client.call(Client.java:1347)\nat org.apache.hadoop.ipc.Client.call(Client.java:1300)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\nat com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)\n```"
    },
    {
        "filename": "HDFS-11741.json",
        "creation_time": "2017-05-02T17:57:25.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)\n        at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-3936.json",
        "creation_time": "2012-09-14T05:35:12.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null\nstack trace\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)\n\tat java.lang.Thread.run(Thread.java:662)\n```"
    },
    {
        "filename": "HDFS-6348.json",
        "creation_time": "2014-05-07T12:34:54.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)\n\t... 6 more\nCaused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)```"
    },
    {
        "filename": "HDFS-7884.json",
        "creation_time": "2015-03-04T15:47:50.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-7996.json",
        "creation_time": "2015-03-26T21:19:55.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-4302.json",
        "creation_time": "2012-12-11T19:44:59.000+0000",
        "stack_trace": "```\nFATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join\njava.lang.IllegalStateException: must get input stream before length is available\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:259)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)\n```"
    },
    {
        "filename": "HDFS-11849.json",
        "creation_time": "2017-05-18T11:31:48.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1\n        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)\n        at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n        at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)\n```"
    },
    {
        "filename": "HDFS-4841.json",
        "creation_time": "2013-05-22T17:01:36.000+0000",
        "stack_trace": "```\njava.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)\n\tat org.apache.hadoop.security.token.Token.cancel(Token.java:382)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)\n\tat org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)\n\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)\n\tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)\n\tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n```"
    },
    {
        "filename": "HDFS-3384.json",
        "creation_time": "2012-05-08T09:01:10.000+0000",
        "stack_trace": "```\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)\n\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n```"
    },
    {
        "filename": "HDFS-5657.json",
        "creation_time": "2013-12-11T21:59:46.000+0000",
        "stack_trace": "```\nERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n        at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)\n        at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n```"
    },
    {
        "filename": "HDFS-11827.json",
        "creation_time": "2017-05-16T06:28:49.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-6804.json",
        "creation_time": "2014-08-01T05:37:23.000+0000",
        "stack_trace": "```\njava.io.IOException: Terminating due to a checksum error.java.io.IOException: Unexpected checksum mismatch while writing BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248 from /192.168.2.101:39495\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)\n        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n        at java.lang.Thread.run(Thread.java:744)\n```"
    },
    {
        "filename": "HDFS-5843.json",
        "creation_time": "2014-01-28T06:05:16.000+0000",
        "stack_trace": "```\njava.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)\n\njava.lang.ArithmeticException: / by zero\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)\n\tat java.lang.Thread.run(Thread.java:695)\n```"
    },
    {
        "filename": "HDFS-8070.json",
        "creation_time": "2015-04-06T19:57:58.000+0000",
        "stack_trace": "```\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.nio.channels.ClosedChannelException\n\tat org.apache.hadoop.util.CloseableReferenceCount.reference(CloseableReferenceCount.java:57)\n\tat org.apache.hadoop.net.unix.DomainSocket.shutdown(DomainSocket.java:387)\n\tat org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager$EndpointShmManager.shutdown(DfsClientShmManager.java:378)\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:223)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38\n\tat org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.io.EOFException\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.requestShortCircuitFds(DataXceiver.java:352)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds(Receiver.java:187)\n        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:89)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-1085.json",
        "creation_time": "2010-04-06T22:49:40.000+0000",
        "stack_trace": "```\njava.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)\nbut expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)\n        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n        at org.apache.hadoop.mapred.Child.main(Child.java:159)\n```"
    },
    {
        "filename": "HDFS-12339.json",
        "creation_time": "2017-08-23T03:16:49.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Unregistration failure\nCaused by: java.net.SocketException: Socket is closed\nat java.net.DatagramSocket.send(DatagramSocket.java:641)\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)\n```"
    },
    {
        "filename": "HDFS-6520.json",
        "creation_time": "2014-06-12T02:32:29.000+0000",
        "stack_trace": "```\njava.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536\nseqno: 1\nlastPacketInBlock: false\ndataLen: 65536\n\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)\n        at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n\njava.lang.Exception: Could not copy block data for BP-654596295-10.37.7.84-1402466764642:blk_1073741825_1001\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:664)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)\n        at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n```"
    },
    {
        "filename": "HDFS-10715.json",
        "creation_time": "2016-08-02T08:10:23.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)\n        at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)\n        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n```"
    },
    {
        "filename": "HDFS-3332.json",
        "creation_time": "2012-04-27T04:42:38.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)\n\tat org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)\n\tat org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n```"
    },
    {
        "filename": "HDFS-6130.json",
        "creation_time": "2014-03-20T07:15:47.000+0000",
        "stack_trace": "```\nFATAL namenode.NameNode: Exception in namenode join\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat$LoaderDelegator.java:120)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)\n```"
    },
    {
        "filename": "HDFS-2827.json",
        "creation_time": "2012-01-23T09:34:51.000+0000",
        "stack_trace": "```\njava.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)\n\tat java.lang.Thread.run(Unknown Source)\n```"
    },
    {
        "filename": "HDFS-11056.json",
        "creation_time": "2016-10-25T23:00:33.000+0000",
        "stack_trace": "```\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.io.IOException: No data exists for block BP-837130339-172.16.1.88-1477434851452:blk_1073741825_1182\n\tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getBlockInputStream(FsDatasetImpl.java:773)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:400)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:581)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:150)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:102)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:289)\n\tat java.lang.Thread.run(Thread.java:745)\n\nException in thread \"Thread-144\" java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:309)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C\n\tat org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)\n\tat org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)\n\tat org.apache.hadoop.hdfs.ByteArrayStrategy.readFromBlock(ReaderStrategy.java:119)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:302)\n\t... 1 more\n\njava.io.IOException: append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:136)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2423)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:773)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:444)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1795)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2535)\n\nException in thread \"Thread-143\" java.lang.RuntimeException: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:283)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: HTTP status [500], exception [org.apache.hadoop.ipc.RemoteException], message [append: lastBlock=blk_1073741825_1185 of src=/tmp/bar.txt is not sufficiently replicated yet.] \n\tat org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:159)\n\tat org.apache.hadoop.fs.http.client.HttpFSFileSystem$HttpFSDataOutputStream.close(HttpFSFileSystem.java:470)\n\tat org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1.run(BaseTestHttpFSWith.java:279)```"
    },
    {
        "filename": "HDFS-6825.json",
        "creation_time": "2014-08-06T01:31:34.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)\n        at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)\n        at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)\n```"
    },
    {
        "filename": "HDFS-5710.json",
        "creation_time": "2014-01-01T04:06:03.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)\n\tat org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)\n\tat java.lang.Thread.run(Thread.java:724)\n```"
    },
    {
        "filename": "HDFS-3555.json",
        "creation_time": "2012-06-20T17:36:48.000+0000",
        "stack_trace": "```\njava.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]\nat org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)\nat org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)\nat org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)\nat org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)\nat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)\n```"
    },
    {
        "filename": "HDFS-10962.json",
        "creation_time": "2016-10-05T04:31:45.000+0000",
        "stack_trace": "```\norg.mockito.exceptions.verification.WantedButNotInvoked: \nWanted but not invoked:\nnamenodeProtocols.getStats();\n-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\nActually, there were zero interactions with this mock.\n\n\tat org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)\n```"
    },
    {
        "filename": "HDFS-12363.json",
        "creation_time": "2017-08-28T05:34:21.000+0000",
        "stack_trace": "```\nERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.\njava.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)\nat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-7916.json",
        "creation_time": "2015-03-11T10:40:00.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:\n        at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-9549.json",
        "creation_time": "2015-12-11T16:25:08.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)\n\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)\n```"
    },
    {
        "filename": "HDFS-11164.json",
        "creation_time": "2016-11-22T05:58:03.000+0000",
        "stack_trace": "```\njava.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)\n\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HDFS-5291.json",
        "creation_time": "2013-10-02T23:52:02.000+0000",
        "stack_trace": "```\njava.lang.InterruptedException: sleep interrupted\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:356)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTail\n```"
    },
    {
        "filename": "HDFS-12836.json",
        "creation_time": "2017-11-17T20:26:32.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\n\norg.apache.hadoop.hdfs.server.namenode.EditLogInputException: Error replaying edit log at offset 1048576.  Expected transaction ID was 87\nRecent opcode offsets: 1048576\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:218)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)\n```"
    },
    {
        "filename": "HDFS-8113.json",
        "creation_time": "2015-04-09T11:29:31.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)\nat org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)\nat org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26382)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)\nat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n```"
    },
    {
        "filename": "HDFS-10512.json",
        "creation_time": "2016-06-09T14:34:04.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)\n        at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)\n```"
    },
    {
        "filename": "HDFS-10729.json",
        "creation_time": "2016-08-08T02:36:05.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:284)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:360)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1651)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:410)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)\n\norg.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException: The directory item limit of [path] is exceeded: limit=1048576 items=1049332\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:2060)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addChild(FSDirectory.java:2112)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:2081)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1900)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedAddFile(FSDirectory.java:368)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:365)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:188)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$1.run(EditLogTailer.java:182)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:445)\n        at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:426)\n        at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.catchupDuringFailover(EditLogTailer.java:182)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1205)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1762)\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1635)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1351)\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)\n```"
    },
    {
        "filename": "HDFS-13040.json",
        "creation_time": "2018-01-19T19:47:02.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3.  During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one!  The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684.  If you continue, metadata will be lost forever!\n        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)\n        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\n```"
    },
    {
        "filename": "HDFS-3374.json",
        "creation_time": "2012-05-04T22:08:19.000+0000",
        "stack_trace": "```\njava.lang.Exception: No edit streams are accessible\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)\n    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)\n    at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)\n    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)\n    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)\n    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)\n    at java.lang.Thread.run(Thread.java:662)\n```"
    },
    {
        "filename": "HDFS-2359.json",
        "creation_time": "2011-09-23T00:20:53.000+0000",
        "stack_trace": "```\njava.io.IOException: Error in deleting blocks.\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:1820)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1074)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1036)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:891)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1419)\n        at java.lang.Thread.run(Thread.java:619)\n\njava.lang.NullPointerException\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)\n        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)\n        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)\n        at java.lang.Thread.run(Thread.java:619)\n```"
    },
    {
        "filename": "HDFS-10986.json",
        "creation_time": "2016-10-08T00:50:36.000+0000",
        "stack_trace": "```\njava.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)\n```"
    },
    {
        "filename": "HDFS-6455.json",
        "creation_time": "2014-05-28T18:01:15.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n\tat org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n\tat org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n\tat org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)\n\nException in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'\n        at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)\n        at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)\n        at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)\n        at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)```"
    },
    {
        "filename": "HDFS-2882.json",
        "creation_time": "2012-02-02T18:11:20.000+0000",
        "stack_trace": "```\njava.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp\n        at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)```"
    },
    {
        "filename": "HDFS-5185.json",
        "creation_time": "2013-09-11T14:38:23.000+0000",
        "stack_trace": "```\njava.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)\n        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)\n        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)\n        at java.lang.Thread.run(Thread.java:662)```"
    },
    {
        "filename": "HDFS-13164.json",
        "creation_time": "2018-02-17T00:40:21.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\n\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /tmp/logs/systest/logs is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)\n        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)\n        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)\n        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine$Server$ProtoBufRpcInvoker.java:617)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1504)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1441)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n        at com.sun.proxy.$Proxy82.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:423)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(sun.reflect.NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:104)\n        at com.sun.proxy.$Proxy83.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1830)```"
    },
    {
        "filename": "HDFS-11508.json",
        "creation_time": "2017-03-07T04:55:50.000+0000",
        "stack_trace": "```\norg.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242\n\tat org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)\n\tat org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)\n\tat org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)\n\tat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\n\tat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)\nCaused by: java.net.BindException: Address already in use\n\tat sun.nio.ch.Net.bind0(Native Method)\n\tat sun.nio.ch.Net.bind(Net.java:433)\n\tat sun.nio.ch.Net.bind(Net.java:425)\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)\n\tat org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)```"
    },
    {
        "filename": "HDFS-2991.json",
        "creation_time": "2012-02-23T02:06:51.000+0000",
        "stack_trace": "```\njava.io.IOException: Error replaying edit log at offset 1354251\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)```"
    },
    {
        "filename": "HDFS-4404.json",
        "creation_time": "2013-01-15T01:43:17.000+0000",
        "stack_trace": "```\njava.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\n\njava.net.SocketTimeoutException: Call From szxy1x001833091/172.0.0.13 to vm2:8020 failed on socket timeout exception: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\n at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:743)\n at org.apache.hadoop.ipc.Client.call(Client.java:1180)\n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)\n at $Proxy9.create(Unknown Source)\n at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)\n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)\n at $Proxy10.create(Unknown Source)\n at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)\n at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)\n at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)\n at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)\n at test.TestLease.main(TestLease.java:45)\n\nCaused by: java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]\n at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)\n at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)\n at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)\n at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)\n at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)\n at org.apache.hadoop.ipc.Client.call(Client.java:1156)```"
    },
    {
        "filename": "HDFS-8276.json",
        "creation_time": "2015-04-28T10:11:35.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.\nat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)```"
    },
    {
        "filename": "HDFS-12369.json",
        "creation_time": "2017-08-28T22:56:02.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)```"
    },
    {
        "filename": "HDFS-6462.json",
        "creation_time": "2014-05-29T00:19:03.000+0000",
        "stack_trace": "```\njavax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1414)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)\n        at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)\n        at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal(RpcProgramNfs3.java:1961)\n        at org.apache.hadoop.oncrpc.RpcProgram.messageReceived(RpcProgram.java:162)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:281)\n        at org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage.messageReceived(RpcUtil.java:132)\n        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\n        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\n        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\n        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\n        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\n        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\n        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\n        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1381)```"
    },
    {
        "filename": "HDFS-5425.json",
        "creation_time": "2013-10-25T06:53:28.000+0000",
        "stack_trace": "```\njava.lang.IllegalStateException\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)\n\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)\n\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)\n```"
    },
    {
        "filename": "HDFS-13145.json",
        "creation_time": "2018-02-13T19:22:56.000+0000",
        "stack_trace": "```\njava.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......\n        at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)\n        at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)\n        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)\n        at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)\n        at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2490)\n```"
    },
    {
        "filename": "HDFS-8807.json",
        "creation_time": "2015-07-22T20:45:55.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at org.apache.hadoop.fs.Path.initialize(Path.java:204)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:170)\n        at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)\nCaused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data\n        at java.net.URI$Parser.fail(URI.java:2829)\n        at java.net.URI$Parser.checkChars(URI.java:3002)\n        at java.net.URI$Parser.checkChar(URI.java:3012)\n        at java.net.URI$Parser.parse(URI.java:3028)\n        at java.net.URI.<init>(URI.java:753)\n        at org.apache.hadoop.fs.Path.initialize(Path.java:201)```"
    },
    {
        "filename": "HDFS-3436.json",
        "creation_time": "2012-05-17T10:36:31.000+0000",
        "stack_trace": "```\njava.io.IOException: Bad connect ack with firstBadLink as *******:50010\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.EOFException: Premature EOF: no length prefix available\n\tat org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)\n\njava.io.IOException: BP-2001850558-xx.xx.xx.xx-1337249347060:blk_-8165642083860293107_1002 is neither a RBW nor a Finalized, r=ReplicaBeingWritten, blk_-8165642083860293107_1003, RBW\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2038)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:525)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:114)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:78)\n\tat org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)\n\tat java.lang.Thread.run(Unknown Source)\n```"
    }
]