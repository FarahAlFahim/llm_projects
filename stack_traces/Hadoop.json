[
    {
        "filename": "HADOOP-6989.json",
        "creation_time": "2010-10-04T23:55:16.000+0000",
        "stack_trace": "```\nCaused by: \njava.lang.IllegalArgumentException: key class or comparator option must be set\n\tat org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)\n\tat org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)\n\tat org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)\n\tat org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)\n```"
    },
    {
        "filename": "HADOOP-10823.json",
        "creation_time": "2014-07-15T07:20:00.000+0000",
        "stack_trace": "```\njunit.framework.AssertionFailedError: expected:<2> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:50)\n\tat junit.framework.Assert.failNotEquals(Assert.java:287)\n\tat junit.framework.Assert.assertEquals(Assert.java:67)\n\tat junit.framework.Assert.assertEquals(Assert.java:199)\n\tat junit.framework.Assert.assertEquals(Assert.java:205)\n\tat org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)\n\njava.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:375)\n\tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)\n\tat sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)\n\tat java.security.KeyStore.load(KeyStore.java:1185)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)\n\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)\n\tat java.lang.Thread.run(Thread.java:662)\n```"
    },
    {
        "filename": "HADOOP-9125.json",
        "creation_time": "2012-12-10T02:07:52.000+0000",
        "stack_trace": "```\njavax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)\n        at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)\n        at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)\n        at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)\n        at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)\n        at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)\n        at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)\n        at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)\n        at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)\n        at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)\n        at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)\n        at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)\n        at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)\n        at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)\n        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)\nCaused by: java.io.IOException: connection closed\n        at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)\n        at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)\n        at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)\n        ... 28 more\n```"
    },
    {
        "filename": "HADOOP-10252.json",
        "creation_time": "2014-01-22T16:43:27.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Property value must not be null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:958)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:940)\n\tat org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)\n\tat org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)\n\tat org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)\n\tat org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)\n\tat java.lang.Thread.run(Thread.java:722)\n```"
    },
    {
        "filename": "HADOOP-12239.json",
        "creation_time": "2015-07-15T18:06:14.000+0000",
        "stack_trace": "```\njava.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436448556374, will retry\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: Unable to write RenamePending file for folder rename from hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374 to hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374-splitting\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1998)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:325)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:412)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:390)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:288)\n\tat org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:204)\n\t... 4 more\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2598)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2609)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1366)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:255)\n\t... 11 more\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2892)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2593)```"
    },
    {
        "filename": "HADOOP-11878.json",
        "creation_time": "2015-04-27T08:06:22.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)\n        at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)\n        at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HADOOP-14949.json",
        "creation_time": "2017-10-13T23:44:09.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: Should not have been able to reencryptEncryptedKey\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)\n\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)\n```"
    },
    {
        "filename": "HADOOP-10540.json",
        "creation_time": "2014-04-11T05:14:07.000+0000",
        "stack_trace": "```\njava.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)\n\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)\n\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:274)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:815)\n\tat java.lang.Thread.run(Thread.java:722)\n\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.BlockPoolManager.remove(BlockPoolManager.java:91)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:859)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n\njava.lang.Exception: trace\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.getBlockPoolId(BPOfferService.java:143)\n\tat org.apache.hadoop.hdfs.server.datanode.DataNode.shutdownBlockPool(DataNode.java:861)\n\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.shutdownActor(BPOfferService.java:350)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.cleanUp(BPServiceActor.java:619)\n\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:837)\n\tat java.lang.Thread.run(Thread.java:722)\n```"
    },
    {
        "filename": "HADOOP-7629.json",
        "creation_time": "2011-09-09T17:45:14.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)\n        at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)\n        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)\n        at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)\n        at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)\n        at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)\n        at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)\n        at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)\nCaused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()\n        at java.lang.Class.getConstructor0(Class.java:2706)\n        at java.lang.Class.getDeclaredConstructor(Class.java:1985)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)```"
    },
    {
        "filename": "HADOOP-15060.json",
        "creation_time": "2017-11-22T00:18:55.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: \nExpected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser\nPartialGroupNameException The user name 'foobarnonexistinguser' is not found. \n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)\n\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)\n\tat org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)\n```"
    },
    {
        "filename": "HADOOP-10937.json",
        "creation_time": "2014-08-04T21:22:55.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)\n\tat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)\n\tat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)\n\tat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)```"
    },
    {
        "filename": "HADOOP-9103.json",
        "creation_time": "2012-04-20T01:07:25.000+0000",
        "stack_trace": "```\njava.io.IOException: Found lease for\n non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)\n        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)\n        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)\n        at java.lang.Thread.run(Thread.java:619)\n```"
    },
    {
        "filename": "HADOOP-11151.json",
        "creation_time": "2014-09-29T08:18:04.000+0000",
        "stack_trace": "```\norg.apache.hadoop.security.authentication.client.AuthenticationException: Anonymous requests are disallowed\n        at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)\n        at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HADOOP-8031.json",
        "creation_time": "2012-02-07T20:22:24.000+0000",
        "stack_trace": "```\norg.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].\nCaused by: java.lang.RuntimeException: core-site.xml not found\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:438)```"
    },
    {
        "filename": "HADOOP-15411.json",
        "creation_time": "2018-04-24T23:18:39.000+0000",
        "stack_trace": "```\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)\n at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)\n at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)\nCaused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)\n at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)\n ... 5 more\nCaused by: java.io.IOException: java.util.ConcurrentModificationException\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)\n at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)\n at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)\n ... 8 more\nCaused by: java.util.ConcurrentModificationException\n at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)\n at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)\n at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)\n at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)\n at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)\n at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)```"
    },
    {
        "filename": "HADOOP-15850.json",
        "creation_time": "2018-10-13T14:24:29.000+0000",
        "stack_trace": "```\njava.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/       160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-   2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}\n  at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)\n  at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)\n```"
    },
    {
        "filename": "HADOOP-11693.json",
        "creation_time": "2015-03-05T23:19:13.000+0000",
        "stack_trace": "```\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)\n\tat org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)\n\tat org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)\n\tat org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 8 more\n\njava.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)\n\tat org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)\n\tat org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)\n\t... 4 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)\n\tat org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)\n\t... 11 more\n\norg.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)\n\tat org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)\n\tat org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)\n\tat org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)\n\tat org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HADOOP-12441.json",
        "creation_time": "2015-09-25T23:26:43.000+0000",
        "stack_trace": "```\nExitCodeException exitCode=1: ERROR: garbage process ID \"--\".\nUsage:\n  kill pid ...              Send SIGTERM to every process listed.\n  kill signal pid ...       Send a signal to every process listed.\n  kill -s signal pid ...    Send a signal to every process listed.\n  kill -l                   List all signal names.\n  kill -L                   List all signal names in a nice table.\n  kill -l signal            Convert between signal numbers and names.\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)\n        at org.apache.hadoop.util.Shell.run(Shell.java:461)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HADOOP-11685.json",
        "creation_time": "2015-03-06T22:00:14.000+0000",
        "stack_trace": "```\njava.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1019)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:223)\n\tat org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:142)\n\tat org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:79)\n\tat org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.fs.azure.AzureException: java.io.IOException\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1477)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1862)\n\tat org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1812)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getRegionSplitEditsPath(HLogSplitter.java:502)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.createWAP(HLogSplitter.java:1211)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.getWriterAndPath(HLogSplitter.java:1200)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.append(HLogSplitter.java:1243)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:851)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:843)\n\tat org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:813)\nCaused by: java.io.IOException\n\tat com.microsoft.windowsazure.storage.core.Utility.initIOException(Utility.java:493)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:282)\n\tat org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1472)\n\t... 10 more\nCaused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)\n\tat com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)\n\tat com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)\n\tat com.microsoft.windowsazure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:248)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:319)\n\tat com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:279)```"
    },
    {
        "filename": "HADOOP-8589.json",
        "creation_time": "2012-07-11T23:27:00.000+0000",
        "stack_trace": "```\norg.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here\n\tat org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)\n\tat org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem$Cache.java:2128)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)\n\tat org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)\n\tat org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)\n```"
    },
    {
        "filename": "HADOOP-11754.json",
        "creation_time": "2015-03-26T05:22:54.000+0000",
        "stack_trace": "```\njavax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)\n\tat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)\n\tat org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)\n\tat org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)\n\tat org.mortbay.jetty.servlet.Context.startContext(Context.java:140)\n\tat org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)\n\tat org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)\n\tat org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)\n\tat org.mortbay.jetty.Server.doStart(Server.java:224)\n\tat org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret\n\tat org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)\n\t... 23 more\n\norg.apache.hadoop.yarn.webapp.WebAppException: Error starting http server\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\nCaused by: java.io.IOException: Problem in starting http server. Server handlers failed\n\tat org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)\n\tat org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)```"
    },
    {
        "filename": "HADOOP-8225.json",
        "creation_time": "2012-03-13T20:33:53.000+0000",
        "stack_trace": "```\njava.lang.SecurityException: Intercepted System.exit(-999)\n    at org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)\n    at java.lang.Runtime.exit(Runtime.java:88)\n    at java.lang.System.exit(System.java:904)\n    at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)\n    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)\n    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)\n```"
    },
    {
        "filename": "HADOOP-10866.json",
        "creation_time": "2014-07-18T22:00:15.000+0000",
        "stack_trace": "```\njava.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link\n\tat org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)\n\tat org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)\n```"
    },
    {
        "filename": "HADOOP-12089.json",
        "creation_time": "2015-06-15T17:34:46.000+0000",
        "stack_trace": "```\norg.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)\n\tat org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)\n\tat org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)\n\tat org.apache.hadoop.hbase.Chore.run(Chore.java:80)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.\n\tat com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)\n\tat com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)\n\tat com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)\n\tat com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2991)\n\tat org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)\n\tat org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2597)```"
    },
    {
        "filename": "HADOOP-11934.json",
        "creation_time": "2015-05-07T00:38:53.000+0000",
        "stack_trace": "```\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)\n\tat org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)\n\tat org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)\n\tat org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)\n\tat org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)\n\tat org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)\n\tat org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:70)\n\tat org.apache.hadoop.security.Groups.<init>(Groups.java:66)\n\tat org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)\n\tat org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)\n\tat org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)\n\tat org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n```"
    },
    {
        "filename": "HADOOP-11722.json",
        "creation_time": "2015-03-16T21:39:07.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)\n\tat org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:111)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)\n\tat org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)\n\t... 4 more\n```"
    },
    {
        "filename": "HADOOP-15331.json",
        "creation_time": "2018-03-21T01:15:56.000+0000",
        "stack_trace": "```\n2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\ncom.ctc.wstx.exc.WstxIOException: Stream closed\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\n\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)\n\tat org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)\n\tat org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:165)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n\tat org.apache.hadoop.service.CompositeService.serviceStart(AbstractService.java:121)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)\n\n2018-02-28 08:23:20,702 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346\ncom.ctc.wstx.exc.WstxIOException: Stream closed\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint$2.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:100)\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Stream closed\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\n\t... 50 more\n\n2018-02-28 08:23:20,705 WARN org.eclipse.jetty.servlet.ServletHandler: /jmx\njava.lang.RuntimeException: com.ctc.wstx.exc.WstxIOException: Stream closed\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3048)\n\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)\n\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1326)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:1298)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.buildRedirectPath(RMWebApp.java:103)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.getRedirectPath(RMWebApp.java:91)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:125)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n\tat com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n\tat com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java.ServletHandler.java:1759)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1560)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint$2.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:100)\n\tat org.eclipse.jetty.io.ManagedSelector.run(ManagedSelector.java:147)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.ctc.wstx.exc.WstxIOException: Stream closed\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)\n\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)\n\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)\n\t... 46 more\nCaused by: java.io.IOException: Stream closed\n\tat java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:336)\n\tat com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n\tat com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n\tat com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)```"
    },
    {
        "filename": "HADOOP-14062.json",
        "creation_time": "2016-12-20T00:35:48.000+0000",
        "stack_trace": "```\njava.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1428)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1338)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy80.allocate(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n\tat com.sun.proxy.$Proxy81.allocate(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)\n```"
    },
    {
        "filename": "HADOOP-11149.json",
        "creation_time": "2014-09-27T16:37:06.000+0000",
        "stack_trace": "```\njava.lang.Exception: test timed out after 25000 milliseconds\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)\n\tat org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)\n\tat org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)\n\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)\n\tat org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)\n\tat org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)\n\tat org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)```"
    },
    {
        "filename": "HADOOP-15059.json",
        "creation_time": "2017-11-21T20:54:52.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Unable to determine current user\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)\n\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)\n\tat org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)\nCaused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)\n\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)\n\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)\n\t... 4 more\nCaused by: java.io.IOException: Unknown version 1 in token storage.\n\tat org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)\n\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)```"
    },
    {
        "filename": "HADOOP-15307.json",
        "creation_time": "2018-03-12T14:56:59.000+0000",
        "stack_trace": "```\njava.lang.UnsupportedOperationException: Unsupported verifier flavorAUTH_SYS\nat org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)\nat org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)\nat org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)\nat org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)\nat org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)\nat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)\nat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)\nat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)```"
    },
    {
        "filename": "HADOOP-11446.json",
        "creation_time": "2014-12-23T22:15:23.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:713)\n        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)\n        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)\n        at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)\n        at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)\n        at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)\n        at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)\n        at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)\n```"
    },
    {
        "filename": "HADOOP-12689.json",
        "creation_time": "2016-01-04T23:30:49.000+0000",
        "stack_trace": "```\njava.io.IOException: /test doesn't exist\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)\nat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\nat com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)\nat org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)\nat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)\nat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\nat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)```"
    },
    {
        "filename": "HADOOP-13132.json",
        "creation_time": "2016-05-11T11:42:30.000+0000",
        "stack_trace": "```\njava.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException\nat org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)\nat org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)\nat org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)\nat org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)\nat org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:59)\nat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:683)\nat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:679)\nat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\nat org.apache.hadoop.fs.FileContext.create(FileContext.java:679)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:382)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:377)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:422)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\nat org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:456)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:421)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:384)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HADOOP-15121.json",
        "creation_time": "2017-12-15T07:41:39.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)\n        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)\n        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)\n        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)\n        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)\n        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)\n        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)\n        at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)\n        at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)\n        at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)\n        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)\n        at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)\nat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)\n```"
    },
    {
        "filename": "HADOOP-8110.json",
        "creation_time": "2012-02-24T18:49:27.000+0000",
        "stack_trace": "```\njunit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:47)\n\tat junit.framework.Assert.failNotEquals(Assert.java:283)\n\tat junit.framework.Assert.assertEquals(Assert.java:64)\n\tat junit.framework.Assert.assertEquals(Assert.java:195)\n\tat org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)\n\tat org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)\n```"
    },
    {
        "filename": "HADOOP-11400.json",
        "creation_time": "2014-12-12T10:05:52.000+0000",
        "stack_trace": "```\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)\n        ... 5 more\n\norg.apache.hadoop.metrics2.MetricsException: Error flushing metrics\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)\n        at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)\n        at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)\nCaused by: java.net.SocketException: Broken pipe\n        at java.net.SocketOutputStream.socketWrite0(Native Method)\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:159)\n        at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)\n        at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)\n        at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)\n        at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n        at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n        at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)```"
    },
    {
        "filename": "HADOOP-9865.json",
        "creation_time": "2013-08-12T23:22:58.000+0000",
        "stack_trace": "```\norg.apache.hadoop.HadoopIllegalArgumentException: Path is relative\n\tat org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)\n\tat org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)\n\tat org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:128)\n\tat org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)\n\tat org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\n```"
    },
    {
        "filename": "HADOOP-9977.json",
        "creation_time": "2013-09-17T22:11:54.000+0000",
        "stack_trace": "```\njava.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)\nCaused by: java.security.UnrecoverableKeyException: Cannot recover key\n        at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)\n        at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)\n        at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)\n        at java.security.KeyStore.getKey(KeyStore.java:792)\n        at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)\n        at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)\n        at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:259)\n        at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)\n        at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)\n        at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)```"
    },
    {
        "filename": "HADOOP-12611.json",
        "creation_time": "2015-12-01T17:14:38.000+0000",
        "stack_trace": "```\njava.lang.AssertionError: expected null, but was:<[B@142bad79>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotNull(Assert.java:664)\n\tat org.junit.Assert.assertNull(Assert.java:646)\n\tat org.junit.Assert.assertNull(Assert.java:656)\n\tat org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)\n\njava.lang.IllegalStateException: instance must be started before calling this method\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:145)\n\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.CGLIB$rollSecret$2(<generated>)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8$$FastClassByMockitoWithCGLIB$$6f94a716.invoke(<generated>)\n\tat org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216)\n\tat org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10)\n\tat org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22)\n\tat org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27)\n\tat org.mockito.internal.invocation.Invocation.callRealMethod(Invocation.java:211)\n\tat org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36)\n\tat org.mockito.internal.MockHandler.handle(MockHandler.java:99)\n\tat org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)\n\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.rollSecret(<generated>)\n\tat org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider$1.run(RolloverSignerSecretProvider.java:97)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HADOOP-10142.json",
        "creation_time": "2013-12-03T06:33:32.000+0000",
        "stack_trace": "```\norg.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user\n\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)\n        at org.apache.hadoop.util.Shell.run(Shell.java:417)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)\n        at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)\n        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)\n        at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)\n        at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)\n        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)\n        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)\n        at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)\n        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n```"
    },
    {
        "filename": "HADOOP-14727.json",
        "creation_time": "2017-08-02T21:56:38.000+0000",
        "stack_trace": "```\njava.lang.Exception: test\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)\n        at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n        at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)\n        at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)\n        at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)\n        at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)\n        at java.io.DataInputStream.read(DataInputStream.java:149)\n        at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)\n        at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)\n        at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)\n        at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)\n        at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:647)\n        at com.ctc.wstx.stax.WstxInputFactory.createXMLStreamReader(WstxInputFactory.java:366)\n        at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)\n        at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1126)\n        at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1344)\n        at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)\n        at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)\n        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.<init>(CompletedJob.java:105)\n        at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:100)\n        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)\n        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)\n        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)\n        at com.google.common.cache.LocalCache.get(LocalCache.java:3965)\n        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)\n        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)\n        at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)\n        at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)\n        at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)\n        at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n        at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$1.call(GuiceFilter.java:203)\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1552)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(ServletHandler.java:185)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ServletHandler.java:1112)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n        at org.eclipse.jetty.server.Server.handle(Server.java:534)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:748)\n```"
    }
]