[
    {
        "filename": "HIVE-10992.json",
        "creation_time": "2015-06-12T19:52:35.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Exception occurred while finding child jobs \n at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:204) \n at org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(WebHCatJTShim23.java:158) \n at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(LaunchMapper.java:156) \n at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(LaunchMapper.java:124) \n at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(LaunchMapper.java:261) \n at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784) \n at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) \n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) \n at java.security.AccessController.doPrivileged(Native Method) \n at javax.security.auth.Subject.doAs(Subject.java:415) \n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) \n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) \nCaused by: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache \n at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) \n at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) \n at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) \n at java.lang.reflect.Constructor.newInstance(Constructor.java:526) \n at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53) \n at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104) \n at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:250) \n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \n at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method) \n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \n at java.lang.reflect.Method.invoke(Method.java:606) \n at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) \n at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) \n at com.sun.proxy.$Proxy26.getApplications(Unknown Source) \n at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:198) ... 11 more\n \nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache \n at org.apache.hadoop.ipc.Client.call(Client.java:1469) \n at org.apache.hadoop.ipc.Client.call(Client.java:1400) \n at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) \n at com.sun.proxy.$Proxy25.getApplications(Unknown Source) \n at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:247)```"
    },
    {
        "filename": "HIVE-16450.json",
        "creation_time": "2017-04-14T13:59:12.000+0000",
        "stack_trace": "```\njavax.jdo.JDOException: Exception thrown when executing query\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)\n        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(ObjectStore.java:6546)\n        at org.apache.hadoop.hive.metastore.ObjectStore.access$1200(ObjectStore.java:171)\n        at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6606)\n        at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6595)\n        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2633)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(ObjectStore.java:6594)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatistics(ObjectStore.java:6588)\n        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:103)\n        at com.sun.proxy.$Proxy0.getTableColumnStatistics(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:787)\n        at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:247)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3809)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3779)\n        at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)\n        at com.sun.proxy.$Proxy3.alter_table_with_environment_context(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9617)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9601)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-6389.json",
        "creation_time": "2014-02-07T01:33:32.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":null,\"mymap\":null,\"isnull\":null}\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)\n  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)\n  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text\n  at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)\n  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)\n  at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)\n  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)\n  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)```"
    },
    {
        "filename": "HIVE-2372.json",
        "creation_time": "2011-08-12T09:07:34.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":129390185139228,\"reducesinkkey1\":\"00008AF10000000063CA6F\"},\"value\":{\"_col0\":\"00008AF10000000063CA6F\",\"_col1\":\"2011-07-27 22:48:52\",\"_col2\":129390185139228,\"_col3\":2006,\"_col4\":4100,\"_col5\":\"10017388=6\",\"_col6\":1063,\"_col7\":\"NULL\",\"_col8\":\"address.com\",\"_col9\":\"NULL\",\"_col10\":\"NULL\"},\"alias\":0}\n\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator\n\tat org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)\n\tat org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)\n\t... 7 more\nCaused by: java.io.IOException: Cannot run program \"/usr/bin/perl\": java.io.IOException: error=7, Argument list too long\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:460)\n\tat org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)\n\t... 15 more\nCaused by: java.io.IOException: java.io.IOException: error=7, Argument list too long\n\tat java.lang.UNIXProcess.<init>(UNIXProcess.java:148)\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:65)\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:453)```"
    },
    {
        "filename": "HIVE-2958.json",
        "creation_time": "2012-04-17T15:02:38.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:264)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)\n\t... 9 more\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger\n\tat org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(LazyIntObjectInspector.java:43)\n\tat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:239)\n\tat org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory$ListKeyWrapper.java:150)\n\tat org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory$ListKeyWrapper.java:142)\n\tat org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.copyKey(KeyWrapperFactory$ListKeyWrapper.java:119)\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:750)\n\tat org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:722)```"
    },
    {
        "filename": "HIVE-13392.json",
        "creation_time": "2016-03-30T22:32:50.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2451)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)\n```"
    },
    {
        "filename": "HIVE-11301.json",
        "creation_time": "2015-07-18T00:41:40.000+0000",
        "stack_trace": "```\norg.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)\n        at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\njava.net.SocketException: Socket closed\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:153)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n        at java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n        at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)\n        at org.apache.thrift.transport.TSocket.close(TSocket.java:196)\n        at org.apache.hadoop.hive.thrift.TFilterTransport.close(TFilterTransport.java:52)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:304)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\norg.apache.thrift.transport.TTransportException\n        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)\n        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)\n        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)\n        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)\n        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)\n        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_aggr_stats_for(ThriftHiveMetastore.java:3029)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_aggr_stats_for(ThriftHiveMetastore.java:3016)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2067)\n        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n        at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1991)\n        at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3124)\n        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:252)\n        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:137)\n        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:125)\n        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:111)\n```"
    },
    {
        "filename": "HIVE-11028.json",
        "creation_time": "2015-06-16T23:03:38.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)\n        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635)\n        at java.util.ArrayList.get(ArrayList.java:411)\n        at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118)\n        at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.<init>(StandardStructObjectInspector.java:109)\n        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:290)\n        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:275)\n        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:175)\n        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:313)\n        at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:71)\n        at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:99)\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:362)\n        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)```"
    },
    {
        "filename": "HIVE-14380.json",
        "creation_time": "2016-07-29T00:14:58.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Unable to determine if hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table, expected: hdfs://bar.ygrid.yahoo.com:8020\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2204)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStrongestEncryptedTablePath(SemanticAnalyzer.java:2274)\n```"
    },
    {
        "filename": "HIVE-7799.json",
        "creation_time": "2014-08-20T09:45:21.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.next(HiveKVResultCache.java:113)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:124)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:82)\n        at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)\n        at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n        at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:65)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n        at org.apache.spark.scheduler.Task.run(Task.scala:54)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n```"
    },
    {
        "filename": "HIVE-6537.json",
        "creation_time": "2014-03-03T18:57:44.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)\n        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)\n        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)\n        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)\n        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)\n        at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)\n        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: java.lang.NullPointerException\n        at java.util.Arrays.fill(Arrays.java:2685)\n        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)\n        at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:81)```"
    },
    {
        "filename": "HIVE-13691.json",
        "creation_time": "2016-05-04T23:40:03.000+0000",
        "stack_trace": "```\nException: MetaException(message:Timeout when executing method: getTable)\n        at org.apache.hadoop.hive.metastore.Deadline.newMetaException(Deadline.java:187)\n        at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:177)\n        at org.apache.hadoop.hive.metastore.Deadline.checkTimeout(Deadline.java:160)\n        at org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1839)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2255)\n        at org.apache.hadoop.hive.metastore.ObjectStore.access$300(ObjectStore.java:165)\n        at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2051)\n        at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2043)\n        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2400)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal(ObjectStore.java:2043)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNames(ObjectStore.java:2037)\n        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)\n        at com.sun.proxy.$Proxy0.getPartitionsByNames(Unknown Source)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolvePartition(CompactorThread.java:111)\n        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:129)\nCaused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable\n        at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)\n        ... 16 more\n\nException: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE\n        at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)\n        at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)\n```"
    },
    {
        "filename": "HIVE-17758.json",
        "creation_time": "2017-10-10T12:33:52.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: timeout value is negative\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:7407)\n        at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:7361)\n        at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7424)\n        at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n```"
    },
    {
        "filename": "HIVE-14898.json",
        "creation_time": "2016-10-06T00:02:36.000+0000",
        "stack_trace": "```\norg.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:170)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:83)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:349)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:952)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:167)\n\t... 23 more\nCaused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(ThriftHttpServlet.java:311)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.access$100(ThriftHttpServlet.java:59)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:212)\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:175)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)```"
    },
    {
        "filename": "HIVE-5546.json",
        "creation_time": "2013-10-15T15:06:59.000+0000",
        "stack_trace": "```\njava.lang.OutOfMemoryError: Java heap space\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n```"
    },
    {
        "filename": "HIVE-7557.json",
        "creation_time": "2014-07-30T19:25:12.000+0000",
        "stack_trace": "```\nError: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector\n\tat org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\n ]\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector\n\tat org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\n ]\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:382)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\t... 6 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector\n\tat org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)\n\tat org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:394)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)\n ]\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:486)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)\n\t... 8 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.serialize(VectorizedOrcSerde.java:75)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcSerde.serializeVector(OrcSerde.java:148)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.processOp(VectorFileSinkOperator.java:79)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.processOp(VectorExtractOperator.java:99)\n\tat org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:470)```"
    },
    {
        "filename": "HIVE-1712.json",
        "creation_time": "2010-10-14T17:17:44.000+0000",
        "stack_trace": "```\nERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException\n        at java.util.Hashtable.put(Hashtable.java:394)\n        at java.util.Hashtable.putAll(Hashtable.java:466)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)\n        at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n```"
    },
    {
        "filename": "HIVE-12608.json",
        "creation_time": "2015-12-07T21:26:01.000+0000",
        "stack_trace": "```\n2015-12-07 11:47:28,503 ERROR [main]: CliDriver (SessionState.java:printError(921)) - Failed with exception java.io.IOException:java.lang.RuntimeException: cannot find field c2 in [c1]\njava.io.IOException: java.lang.RuntimeException: cannot find field c2 in [c1]\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)\n        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1029)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1003)\n        at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:139)\n        at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_type_promotion(TestCliDriver.java:123)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at junit.framework.TestCase.runTest(TestCase.java:176)\n        at junit.framework.TestCase.runBare(TestCase.java:141)\n        at junit.framework.TestResult$1.protect(TestResult.java:122)\n        at junit.framework.TestResult.runProtected(TestResult.java:142)\n        at junit.framework.TestResult.run(TestResult.java:125)\n        at junit.framework.TestCase.run(TestCase.java:129)\n        at junit.framework.TestSuite.runTest(TestSuite.java:255)\n        at junit.framework.TestSuite.run(TestSuite.java:250)\n        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: java.lang.RuntimeException: cannot find field c2 in [c1]\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getStructFieldTypeInfo(HiveStructConverter.java:130)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getFieldTypeIgnoreCase(HiveStructConverter.java:103)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.init(HiveStructConverter.java:90)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:67)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:59)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:63)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:75)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter$ElementConverter.<init>(HiveCollectionConverter.java:141)\n        at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter.<init>(HiveCollectionConverter.java:52)\n```"
    },
    {
        "filename": "HIVE-17774.json",
        "creation_time": "2017-10-11T20:02:01.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\n\tat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)\n\tat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-14564.json",
        "creation_time": "2016-08-18T00:11:34.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)\n\t... 9 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat java.lang.System.arraycopy(Native Method)\n\tat org.apache.hadoop.io.Text.set(Text.java:225)\n\tat org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(LazyBinaryString.java:48)\n\tat org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)\n\tat org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)\n\tat org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:94)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(ReduceSinkOperator.java:550)\n\tat org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:377)```"
    },
    {
        "filename": "HIVE-3651.json",
        "creation_time": "2012-11-01T23:31:20.000+0000",
        "stack_trace": "```\njava.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)\n        at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:679)\n```"
    },
    {
        "filename": "HIVE-5199.json",
        "creation_time": "2013-09-03T20:40:29.000+0000",
        "stack_trace": "```\njava.io.IOException: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488)\nat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)\nat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1412)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:601)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:144)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:307)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:138)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:406)\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)\n```"
    },
    {
        "filename": "HIVE-17368.json",
        "creation_time": "2017-08-22T01:27:32.000+0000",
        "stack_trace": "```\njavax.security.sasl.SaslException: GSS initiate failed\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_121]\n        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[libthrift-0.9.3.jar:0.9.3]\n        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]\n        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:488) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:255) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_121]\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [?:1.8.0_121]\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.8.0_121]\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [?:1.8.0_121]\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3595) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3647) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3627) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod) ~[?:1.8.0_121]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) [?:1.8.0_121]\n        at java.lang.reflect.Method.invoke(Method.java:498) [?:1.8.0_121]\n        at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnTokenStore(DBTokenStore.java:157) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.DBTokenStore.addToken(DBTokenStore.java:74) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:142) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:56) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.security.token.Token.<init>(Token.java:59) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getDelegationToken(DelegationTokenSecretManager.java:109) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:123) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]\n        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationToken(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationTokenWithService(HiveDelegationTokenManager.java:130) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.auth.HiveAuthFactory.getDelegationToken(HiveAuthFactory.java:261) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken(HiveSessionImplwithUGI.java:174) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod) ~[?:1.8.0_121]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) [?:1.8.0_121]\n        at java.lang.reflect.Method.invoke(Method.java:498) [?:1.8.0_121]\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]\n        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at com.sun.proxy.$Proxy36.getDelegationToken(Unknown Source) [?:?]\n        at org.apache.hive.service.cli.CLIService.getDelegationToken(CLIService.java:589) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetDelegationToken(ThriftCLIService.java:254) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1737) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1722) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.9.3.jar:0.9.3]\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:621) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [libthrift-0.9.3.jar:0.9.3]\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]\n        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]\nCaused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) ~[?:1.8.0_121]\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) ~[?:1.8.0_121]\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) ~[?:1.8.0_121]\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) ~[?:1.8.0_121]\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) ~[?:1.8.0_121]\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_121]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_121]\n        ... 65 more\n```\n\n```\njava.lang.RuntimeException: org.apache.thrift.transport.TTransportException: DIGEST-MD5: IO error acquiring password\n```"
    },
    {
        "filename": "HIVE-4233.json",
        "creation_time": "2013-03-26T13:02:20.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1084)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:51)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:61)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2140)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2151)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(Hive.java:2275)\n        at org.apache.hive.service.cli.CLIService.getDelegationTokenFromMetaStore(CLIService.java:358)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:127)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1073)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1058)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:565)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer$WorkerProcess.java:206)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.GeneratedConstructorAccessor52.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1082)\n        ... 16 more\nCaused by: java.lang.IllegalStateException: This ticket is no longer valid\n        at javax.security.auth.kerberos.KerberosTicket.toString(KerberosTicket.java:601)\n        at java.lang.String.valueOf(String.java:2826)\n        at java.lang.StringBuilder.append(StringBuilder.java:115)\n        at sun.security.jgss.krb5.SubjectComber.findAux(SubjectComber.java:120)\n        at sun.security.jgss.krb5.SubjectComber.find(SubjectComber.java:41)\n        at sun.security.jgss.krb5.Krb5Util.getTicket(Krb5Util.java:130)\n        at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:328)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at sun.security.jgss.krb5.Krb5InitCredential.getTgt(Krb5InitCredential.java:325)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:128)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)\n        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)\n        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)\n        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)\n        at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:277)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:163)```"
    },
    {
        "filename": "HIVE-14303.json",
        "creation_time": "2016-07-21T03:16:20.000+0000",
        "stack_trace": "```\nError: java.lang.RuntimeException: Hive Runtime Error while closing operators: null\n\n        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)\n\n        at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\n\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\n        at java.security.AccessController.doPrivileged(Native Method)\n\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\nCaused by: java.lang.NullPointerException\n\n        at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)\n\n        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)\n\n        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)```"
    },
    {
        "filename": "HIVE-19248.json",
        "creation_time": "2018-04-19T17:45:21.000+0000",
        "stack_trace": "```\njava.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0\n at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:299)\n at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:266)\n at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)\n at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\n at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)\nCaused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0\n at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)\n at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:296)\n ... 10 more\nCaused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)\n at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:212)\n at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)\n at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)\n at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)```"
    },
    {
        "filename": "HIVE-7167.json",
        "creation_time": "2014-06-02T18:13:36.000+0000",
        "stack_trace": "```\nException in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:347)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1413)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2444)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2456)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:341)\n        ... 7 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1411)\n        ... 12 more\nCaused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused: connect\n        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:336)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:214)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1411)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2444)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2456)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:341)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.net.ConnectException: Connection refused: connect\n        at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method)\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:157)\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)\n        at java.net.Socket.connect(Socket.java:579)\n        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)\n        ... 19 more\n)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:382)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:214)```"
    },
    {
        "filename": "HIVE-12360.json",
        "creation_time": "2015-11-06T18:04:00.000+0000",
        "stack_trace": "```\njava.io.IOException: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)\n\tat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)\n\tat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)\n\tat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.seek(InStream.java:139)\n\tat org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.read(InStream.java:87)\n\tat java.io.InputStream.read(InputStream.java:102)\n\tat com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)\n\tat com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)\n\tat com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7429)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7393)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7482)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7477)\n\tat com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200)\n\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)\n\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:223)\n\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.parseFrom(OrcProto.java:7593)\n\tat org.apache.hadoop.hive.ql.io.orc.MetadataReader.readRowIndex(MetadataReader.java:88)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1166)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1151)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:750)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)\n\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1235)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1117)\n\tat org.apache.hadoop.hive.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)\n\tat org.apache.hadoop.hive.exec.FetchOperator.getRecordReader(FetchOperator.java:324)\n\tat org.apache.hadoop.hive.exec.FetchOperator.getNextRow(FetchOperator.java:446)```"
    },
    {
        "filename": "HIVE-13160.json",
        "creation_time": "2016-02-26T00:02:11.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)\n\njava.lang.NullPointerException\n        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)\n        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)\n        at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:69)\n        at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:545)\n```"
    },
    {
        "filename": "HIVE-12008.json",
        "creation_time": "2015-10-01T19:26:46.000+0000",
        "stack_trace": "```\nException running child : java.lang.RuntimeException: Error in configuring object\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n        ... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)\n        ... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n        ... 17 more\nCaused by: java.lang.RuntimeException: Map operator initialization failed\n        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:147)\n        ... 22 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\n        at java.util.ArrayList.rangeCheck(ArrayList.java:635)\n        at java.util.ArrayList.get(ArrayList.java:411)\n```"
    },
    {
        "filename": "HIVE-6205.json",
        "creation_time": "2014-01-15T07:34:15.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:197)\n```"
    },
    {
        "filename": "HIVE-15309.json",
        "creation_time": "2016-11-29T21:56:28.000+0000",
        "stack_trace": "```\norg.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/r\\\nrslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1831)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1744)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolSe\\\nrverSideTranslatorPB.java:373)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientName\\\nnodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)\n\n        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1496)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1396)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n        at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB\\\n.java:270)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)\n        at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1211)\n        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)\n        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)\n        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)\n        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1536)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:330)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:326)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:782)\n        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)\n        at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:460)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1525)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:631)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:610)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n```"
    },
    {
        "filename": "HIVE-10808.json",
        "creation_time": "2015-05-23T02:24:16.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Error in configuring object\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\nat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\nat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\nCaused by: java.lang.reflect.InvocationTargetException\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\nat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\nat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\nat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)\n... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n... 17 more\nCaused by: java.lang.RuntimeException: Map operator initialization failed\nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:157)\n... 22 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector\nat org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:334)\nat org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:352)\nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)\n... 22 more\nCaused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspectorUtils.java:1111)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1149)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:219)\nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:183)\nat org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:316)\n```"
    },
    {
        "filename": "HIVE-18429.json",
        "creation_time": "2018-01-10T20:45:15.000+0000",
        "stack_trace": "```\njava.io.FileNotFoundException: File hdfs://OTCHaaS/apps/hive/warehouse/momi.db/sensor_data/babyid=5911806ebf69640100004257/_tmp_b4c5a3f3-44e5-4d45-86af-5b773bf0fc96 does not exist.\nat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:923)\nat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:114)\nat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:985)\nat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:981)\nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\nat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:992)\nat rg.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:785)\nat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\nat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)\nat org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-10776.json",
        "creation_time": "2015-05-21T00:56:28.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:7257)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6100)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6271)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8972)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8863)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9708)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9601)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10037)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:323)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n```"
    },
    {
        "filename": "HIVE-6301.json",
        "creation_time": "2014-01-24T01:42:18.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String)  on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2\nCaused by: java.lang.IllegalStateException: No match found\n\tat java.util.regex.Matcher.group(Matcher.java:468)\n\tat org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)\n\tat org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)```"
    },
    {
        "filename": "HIVE-8295.json",
        "creation_time": "2014-09-29T21:16:32.000+0000",
        "stack_trace": "```\njavax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\", \"SDS\".\"SD_ID\", \"SDS\".\"CD_ID\", \"SERDES\".\"SERDE_ID\", \"PARTITIONS\".\"CREATE_TIME\", \"PARTITIONS\".\"LAST_ACCESS_TIME\", \"SDS\".\"INPUT_FORMAT\", \"SDS\".\"IS_COMPRESSED\", \"SDS\".\"IS_STOREDASSUBDIRECTORIES\", \"SDS\".\"LOCATION\", \"SDS\".\"NUM_BUCKETS\", \"SDS\".\"OUTPUT_FORMAT\", \"SERDES\".\"NAME\", \"SERDES\".\"SLIB\" from \"PARTITIONS\"  left outer join \"SDS\" on \"PARTITIONS\".\"SD_ID\" = \"SDS\".\"SD_ID\"   left outer join \"SERDES\" on \"SDS\".\"SERDE_ID\" = \"SERDES\".\"SERDE_ID\" where \"PART_ID\" in (136,140,143,147,152,156,160,163,167,171,174,180,185,191,196,198,203,208,212,217...\n) order by \"PART_NAME\" asc\".\n    at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:422)\n    at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)\n    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:331)\n    at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)\n    at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1920)\n    at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1914)\n    at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2213)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1914)\n    at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1887)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)\n    at com.sun.proxy.$Proxy8.getPartitionsByExpr(Unknown Source)\n    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3800)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9366)\n    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9350)\n    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)\n    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)\n    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:206)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)\n    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nNestedThrowablesStackTrace:\njava.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000\n```"
    },
    {
        "filename": "HIVE-8915.json",
        "creation_time": "2014-11-19T19:40:17.000+0000",
        "stack_trace": "```\n2014-11-19 01:44:57,654 ERROR compactor.Cleaner\n(Cleaner.java:run(143)) - Caught an exception in the main loop of\ncompactor cleaner, MetaException(message:Unable to connect to\ntransaction database\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table\n'hive.COMPACTION_QUEUE' doesn't exist\nat sun.reflect.GeneratedConstructorAccessor20.newInstance(Unknown Source)\nat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\nat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\nat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\nat com.mysql.jdbc.Util.getInstance(Util.java:386)\nat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)\nat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)\nat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)\nat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)\nat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2569)\nat com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1524)\nat com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)\nat org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:266)\nat org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)\n)\nat org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:291)\nat org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)\n```"
    },
    {
        "filename": "HIVE-7249.json",
        "creation_time": "2014-06-18T00:09:42.000+0000",
        "stack_trace": "```\nERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - NoSuchLockException(message:No such lock: 1)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(TxnHandler.java:1407)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(TxnHandler.java:477)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.unlock(HiveMetaStore.java:4817)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)\n        at com.sun.proxy.$Proxy14.unlock(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.unlock(HiveMetaStoreClient.java:1598)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(DbLockManager.java:110)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close(DbLockManager.java:162)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct(DbTxnManager.java:300)\n        at org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager(HiveTxnManagerImpl.java:39)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.closeTxnManager(DbTxnManager.java:43)\n        at org.apache.hive.hcatalog.mapreduce.TransactionContext.cleanup(TransactionContext.java:327)\n        at org.apache.hive.hcatalog.mapreduce.TransactionContext.onCommitJob(TransactionContext.java:142)\n        at org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.commitJob(OutputCommitterContainer.java:61)\n        at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(FileOutputCommitterContainer.java:251)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:537)\n```"
    },
    {
        "filename": "HIVE-11540.json",
        "creation_time": "2015-08-12T23:12:18.000+0000",
        "stack_trace": "```\n2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)\n\tat org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)\n\n2015-08-12 15:36:18,444 ERROR [upladevhwd04v.researchnow.com-18]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(327))```"
    },
    {
        "filename": "HIVE-15755.json",
        "creation_time": "2017-01-30T20:48:25.000+0000",
        "stack_trace": "```\nError: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)\n\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)\n        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)\n        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)\n        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)\n        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)\n        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)\n        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:298)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:506)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-9390.json",
        "creation_time": "2015-01-15T18:50:32.000+0000",
        "stack_trace": "```\norg.apache.thrift.TException: MetaException(message:Unable to get jdbc connection from pool, Read timed out)\n\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5324)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)\n\n\tat com.sun.proxy.$Proxy11.get_open_txns(Unknown Source)\n\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidTxns(HiveMetaStoreClient.java:1696)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)\n\n\tat com.sun.proxy.$Proxy12.getValidTxns(Unknown Source)\n\n\tat org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidTxns(DbTxnManager.java:289)\n\n\tat org.apache.hadoop.hive.ql.Driver.recordValidTxns(Driver.java:882)\n\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:399)\n\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)\n\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)\n\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)\n\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)\n\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)\n\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)\n\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)\n\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)\n\n\tat sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)\n\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)\n\n\tat java.security.AccessController.doPrivileged(Native Method)\n\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)\n\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)\n\n\tat com.sun.proxy.$Proxy21.executeStatementAsync(Unknown Source)\n\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)\n\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:401)\n\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\n\tat org.apache.thrift.server.TServlet.doPost(TServlet.java:83)\n\n\tat org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:101)\n\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\n\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)\n\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)\n\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)\n\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)\n\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)\n\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)\n\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)\n\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)\n\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)\n\n\tat org.eclipse.jetty.server.Server.handle(Server.java:349)\n\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)\n\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)\n\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857)\n\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)\n\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)\n\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)\n\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\n\tat java.lang.Thread.run(Thread.java:745)\n\nCaused by: MetaException(message:Unable to get jdbc connection from pool, Read timed out)\n\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:850)\n\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:196)\n\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5322)```"
    },
    {
        "filename": "HIVE-7623.json",
        "creation_time": "2014-08-05T23:58:27.000+0000",
        "stack_trace": "```\nError:\n=====\n2014-08-05 21:46:14,522 ERROR [pool-3-thread-1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - InvalidOperationException(message:table new location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=123 is on a different file system than the old location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=456. This operation is not supported)\nat org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(HiveAlterHandler.java:361)\nat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2629)\nat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2602)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:622)\nat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)\nat com.sun.proxy.$Proxy5.rename_partition(Unknown Source)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9057)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9041)\nat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:416)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n```"
    },
    {
        "filename": "HIVE-15997.json",
        "creation_time": "2017-02-21T16:49:46.000+0000",
        "stack_trace": "```\n2017-02-02 06:23:25,410 WARN hive.ql.Context: [HiveServer2-Background-Pool: Thread-61]: Error Removing Scratch: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: \"ychencdh511t-1.vpc.cloudera.com/172.26.11.50\"; destination host is: \"ychencdh511t-1.vpc.cloudera.com\":8020; \nat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772) \nat org.apache.hadoop.ipc.Client.call(Client.java:1476) \nat org.apache.hadoop.ipc.Client.call(Client.java:1409) \nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) \nat com.sun.proxy.$Proxy25.delete(Unknown Source) \nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:535)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \nat java.lang.reflect.Method.invoke(Method.java:606) \nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256) \nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104) \nat com.sun.proxy.$Proxy26.delete(Unknown Source) \nat org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:2059) \nat org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:675) \nat org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:671) \nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) \nat org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:671) \nat org.apache.hadoop.hive.ql.Context.removeScratchDir(Context.java:405) \nat org.apache.hadoop.hive.ql.Context.clear(Context.java:541) \nat org.apache.hadoop.hive.ql.Driver.releaseContext(Driver.java:2109) \nat org.apache.hadoop.hive.ql.Driver.closeInProcess(Driver.java:2150) \nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1472) \nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212) \nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207) \nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237) \nat org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88) \nat org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:415) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796) \nat org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306) \nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) \nat java.util.concurrent.FutureTask.run(FutureTask.java:262) \nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) \nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) \nat java.lang.Thread.run(Thread.java:745) \nCaused by: java.nio.channels.ClosedByInterruptException \nat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202) \nat sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:681) \nat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192) \nat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530) \nat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494) \nat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615) \nat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:714) \nat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376) \nat org.apache.hadoop.ipc.Client.getConnection(Client.java:1525) \nat org.apache.hadoop.ipc.Client.call(Client.java:1448) \n... 35 more \n\n2017-02-02 06:21:05,054 ERROR ZooKeeperHiveLockManager: [HiveServer2-Background-Pool: Thread-61]: Failed to release ZooKeeper lock: \njava.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Object.wait(Object.java:503)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)\n\tat org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:871)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)\n\tat org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive(ZooKeeperHiveLockManager.java:488)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockWithRetry(ZooKeeperHiveLockManager.java:466)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlock(ZooKeeperHiveLockManager.java:454)\n\tat org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.releaseLocks(ZooKeeperHiveLockManager.java:236)\n\tat org.apache.hadoop.hive.ql.Driver.releaseLocksAndCommitOrRollback(Driver.java:1175)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1432)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237) \nat org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88) \nat org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:415) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796) \nat org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306) \nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) \nat java.util.concurrent.FutureTask.run(FutureTask.java:262) \nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) \nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) \nat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-7009.json",
        "creation_time": "2014-05-02T20:50:24.000+0000",
        "stack_trace": "```\njava.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri\n\tat org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)\n\tat org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)\n\tat org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:912)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n```"
    },
    {
        "filename": "HIVE-2031.json",
        "creation_time": "2011-03-08T11:38:53.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)\n\tat org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)\n\tat org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)\n\tat org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)\n\tat org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\n```"
    },
    {
        "filename": "HIVE-4018.json",
        "creation_time": "2013-02-13T09:02:20.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException\n\tat org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:189)\n\tat org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:203)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1421)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:614)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:266)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:416)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:260)\n```"
    },
    {
        "filename": "HIVE-11255.json",
        "creation_time": "2015-07-14T15:39:11.000+0000",
        "stack_trace": "```\njavax.jdo.JDOException: Exception thrown when executing query\nat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)\nat org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)\nat org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:945)\nat sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)\nat com.sun.proxy.$Proxy0.getTableObjectsByName(Unknown Source)\nat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1618)\nat sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:106)\nat com.sun.proxy.$Proxy5.get_table_objects_by_name(Unknown Source)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8172)\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8156)\nat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\nat org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)\nat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\nat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-10151.json",
        "creation_time": "2015-03-31T00:17:22.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: serious problem\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)\n        at org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(BucketizedHiveInputFormat.java:141)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:624)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:616)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:492)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n        at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:430)\n        at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.ql.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:225)\n        at org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn2(TestTxnCommands2.java:148)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n        at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\n        at org.junit.rules.RunRules.evaluate(RunRules.java:20)\n        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:254)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:149)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_\n        at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:188)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:998)\n        ... 56 more\nCaused by: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_\n        at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)\n        at org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(AcidUtils.java:172)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:655)\n        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:620)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-13546.json",
        "creation_time": "2016-04-19T07:43:42.000+0000",
        "stack_trace": "```\nError log is as follows\n2016-04-19 15:15:35,252 FATAL [main] ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":null},\"value\":{\"_col0\":null,\"_col1\":5588,\"_col2\":170300,\"_col3\":null,\"_col4\":756,\"_col5\":91384,\"_col6\":16,\"_col7\":null,\"_col8\":855582,\"_col9\":28,\"_col10\":null,\"_col11\":48.83,\"_col12\":null,\"_col13\":0.0,\"_col14\":null,\"_col15\":899.64,\"_col16\":null,\"_col17\":6.14,\"_col18\":0.0,\"_col19\":null,\"_col20\":null,\"_col21\":null,\"_col22\":null}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:180)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:174)\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n\tat java.util.ArrayList.rangeCheck(ArrayList.java:653)\n\tat java.util.ArrayList.get(ArrayList.java:429)\n\tat org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:151)\n\tat org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:131)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:1003)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:919)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:713)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)```"
    },
    {
        "filename": "HIVE-7049.json",
        "creation_time": "2014-05-12T21:46:48.000+0000",
        "stack_trace": "```\norg.apache.avro.AvroRuntimeException: Not a union: \"string\" \n        at org.apache.avro.Schema.getTypes(Schema.java:272)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)\n        at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)\n        at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)\n        at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)\n```"
    },
    {
        "filename": "HIVE-9755.json",
        "creation_time": "2015-02-23T20:50:43.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":[\"0\",\"0\",\"0\",\"0\"]},\"alias\":0} \nat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:258) \nat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:506) \nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:447) \nat org.apache.hadoop.mapred.Child$4.run(Child.java:268) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:396) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408) \nat org.apache.hadoop.mapred.Child.main(Child.java:262) \nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: GenericUDAFnGramEvaluator: mismatch in value for 'n', which usually is caused by a non-constant expression. Found '0' and '1'. \nat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams$GenericUDAFnGramEvaluator.merge(GenericUDAFnGrams.java:242) \nat org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:142) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:658) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:911) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:753) \nat org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:819) \nat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:474) \nat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:249) \n```"
    },
    {
        "filename": "HIVE-19130.json",
        "creation_time": "2018-04-09T10:18:33.000+0000",
        "stack_trace": "```\nERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4016)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3983)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:341)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:162)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1765)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1506)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1303)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1165)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)\n        at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n        at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:266)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)```"
    },
    {
        "filename": "HIVE-13090.json",
        "creation_time": "2016-02-18T21:58:48.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token\n\tat org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)\n\tat org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)\n\tat org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.NullPointerException\n\tat java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)\n\tat org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)\n\tat org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)```"
    },
    {
        "filename": "HIVE-5664.json",
        "creation_time": "2013-10-28T03:50:29.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Database does not exist: db2\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3473)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1441)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1219)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1047)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:915)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)\nCaused by: NoSuchObjectException(message:db2.tab1_indx table not found)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1376)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)\n        at com.sun.proxy.$Proxy7.get_table(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:890)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:660)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:652)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:546)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)\n        at com.sun.proxy.$Proxy8.dropDatabase(Unknown Source)\n        at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:284)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3470)```"
    },
    {
        "filename": "HIVE-15778.json",
        "creation_time": "2017-02-01T04:20:12.000+0000",
        "stack_trace": "```\nERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.NullPointerException\n\n2017-01-31 16:27:29,421 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-5-thread-3]: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5823)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rethrowException(HiveMetaStore.java:4892)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4403)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)\n\tat com.sun.proxy.$Proxy16.drop_index_by_name(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10803)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10787)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)\n\tat org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropIndexMessage(JSONMessageFactory.java:159)\n\tat org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name_core(HiveMetaStore.java:4469)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4396)```"
    },
    {
        "filename": "HIVE-8386.json",
        "creation_time": "2014-10-07T22:30:12.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: cannot find field givenName(lowercase form: givenname) in [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase]\n\tat org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(StructTypeInfo.java:109)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(HCatSchemaUtils.java:154)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(HCatSchemaUtils.java:165)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:127)\n\tat org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:115)\n\tat org.apache.hive.hcatalog.api.HCatTable.<init>(HCatTable.java:59)\n\tat org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(HCatClientHMSImpl.java:157)\n\tat org.apache.falcon.catalog.HiveCatalogService.tableExists(HiveCatalogService.java:143)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validateStorageExists(FeedEntityParser.java:367)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validateFeedStorage(FeedEntityParser.java:309)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:79)\n\tat org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:54)\n\tat org.apache.falcon.resource.AbstractEntityManager.validate(AbstractEntityManager.java:364)\n\tat org.apache.falcon.resource.AbstractEntityManager.submitInternal(AbstractEntityManager.java:331)\n\tat org.apache.falcon.resource.AbstractEntityManager.submit(AbstractEntityManager.java:153)\n\tat org.apache.falcon.resource.ConfigSyncService.submit(ConfigSyncService.java:44)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.falcon.resource.channel.IPCChannel.invoke(IPCChannel.java:48)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$1.doExecute(SchedulableEntityManagerProxy.java:118)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$EntityProxy.execute(SchedulableEntityManagerProxy.java:410)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody0(SchedulableEntityManagerProxy.java:120)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure1.run(SchedulableEntityManagerProxy.java:1)\n\tat org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n\tat org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit(SchedulableEntityManagerProxy.java:107)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody12(SchedulableEntityManagerProxy.java:341)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure13.run(SchedulableEntityManagerProxy.java:1)\n\tat org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n\tat org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule_aroundBody16(SchedulableEntityManagerProxy.java:341)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure17.run(SchedulableEntityManagerProxy.java:1)\n\tat org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)\n\tat org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)\n\tat org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule(SchedulableEntityManagerProxy.java:335)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n\tat org.apache.falcon.security.BasicAuthFilter$2.doFilter(BasicAuthFilter.java:183)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:392)\n\tat org.apache.falcon.security.BasicAuthFilter.doFilter(BasicAuthFilter.java:221)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n```"
    },
    {
        "filename": "HIVE-14714.json",
        "creation_time": "2016-09-07T15:46:07.000+0000",
        "stack_trace": "```\njava.io.IOException: Stream closed\n        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)\n        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)\n        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)\n        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)\n        at java.io.InputStreamReader.read(InputStreamReader.java:184)\n        at java.io.BufferedReader.fill(BufferedReader.java:154)\n        at java.io.BufferedReader.readLine(BufferedReader.java:317)\n        at java.io.BufferedReader.readLine(BufferedReader.java:382)\n        at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-5428.json",
        "creation_time": "2013-10-02T20:46:10.000+0000",
        "stack_trace": "```\njavax.jdo.JDODataStoreException: Error executing SQL query \"select \"DB_ID\" from \"DBS\"\".\n  at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)\n  at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:230)\n  at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:108)\n  at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:249)\n  at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)\n  at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)\n  at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n  at org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)\n  at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:418)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:405)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:444)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:329)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:289)\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4084)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Constructor)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1211)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n  at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2404)\n  at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2415)\n  at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:871)\n  at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:853)\n  at org.apache.hadoop.hive.ql.QTestUtil.cleanUp(QTestUtil.java:534)\n  at org.apache.hadoop.hive.cli.TestCliDriver.<clinit>(TestCliDriver.java:44)\n  at java.lang.Class.forName0(Native Method)\n  at java.lang.Class.forName(Class.java:190)\n  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:374)\n  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)\n  at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)\n\nNestedThrowablesStackTrace:\njava.sql.SQLSyntaxErrorException: Table/View 'DBS' does not exist.\n  at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n  at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedPreparedStatement40.<init>(Unknown Source)\n```"
    },
    {
        "filename": "HIVE-12567.json",
        "creation_time": "2015-12-02T16:38:52.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:132)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)\n        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-08176: consistent read failure; rollback data not available\n\n        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)\n        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)\n        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)\n        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)\n        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)\n        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:30)\n        at oracle.jdbc.driver.T4CStatement.executeForDescribe(T4CStatement.java:762)\n        at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)\n        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)\n        at oracle.jdbc.driver.OracleStatement.executeQuery(OracleStatement.java:1309)\n        at oracle.jdbc.driver.OracleStatementWrapper.executeQuery(OracleStatementWrapper.java:422)\n        at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.getLockInfoFromLockId(TxnHandler.java:1951)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(TxnHandler.java:1600)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:1576)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:480)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n        at com.sun.proxy.$Proxy8.lock(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:1869)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n        at com.sun.proxy.$Proxy9.lock(Unknown Source)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:93)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)\n        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)\n        at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:485)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5586)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n        at com.sun.proxy.$Proxy8.lock(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:1869)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n        at com.sun.proxy.$Proxy9.lock(Unknown Source)\n        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:93)```"
    },
    {
        "filename": "HIVE-6984.json",
        "creation_time": "2014-04-28T23:08:43.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)\n\tat org.apache.hado\n\n2014-04-28 15:39:25,073 FATAL org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)```"
    },
    {
        "filename": "HIVE-10736.json",
        "creation_time": "2015-05-18T03:25:45.000+0000",
        "stack_trace": "```\njava.util.ConcurrentModificationException\n        at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)\n        at java.util.LinkedList$ListItr.next(LinkedList.java:888)\n        at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)\n        at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)\n        at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)\n```"
    },
    {
        "filename": "HIVE-7710.json",
        "creation_time": "2014-08-13T10:46:32.000+0000",
        "stack_trace": "```\njava.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)\n\tat com.jolbox.bonecp.PreparedStatementHandle.executeUpdate(PreparedStatementHandle.java:205)\n\tat org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:399)\n\tat org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:439)\n\tat org.datanucleus.store.rdbms.request.UpdateRequest.execute(UpdateRequest.java:374)\n\tat org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateTable(RDBMSPersistenceHandler.java:417)\n\tat org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateObject(RDBMSPersistenceHandler.java:390)\n\tat org.datanucleus.state.JDOStateManager.flush(JDOStateManager.java:5027)\n\tat org.datanucleus.flush.FlushOrdered.execute(FlushOrdered.java:106)\n\tat org.datanucleus.ExecutionContextImpl.flushInternal(ExecutionContextImpl.java:4119)\n\tat org.datanucleus.ExecutionContextThreadedImpl.flushInternal(ExecutionContextThreadedImpl.java:450)\n\tat org.datanucleus.ExecutionContextImpl.markDirty(ExecutionContextImpl.java:3879)\n\tat org.datanucleus.ExecutionContextThreadedImpl.markDirty(ExecutionContextThreadedImpl.java:422)\n\tat org.datanucleus.state.JDOStateManager.postWriteField(JDOStateManager.java:4815)\n\tat org.datanucleus.state.JDOStateManager.replaceField(JDOStateManager.java:3356)\n\tat org.datanucleus.state.JDOStateManager.updateField(JDOStateManager.java:2018)\n\tat org.datanucleus.state.JDOStateManager.setStringField(JDOStateManager.java:1791)\n\tat org.apache.hadoop.hive.metastore.model.MStorageDescriptor.jdoSetlocation(MStorageDescriptor.java)\n\tat org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setLocation(MStorageDescriptor.java:88)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.copyMSD(ObjectStore.java:2699)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2572)\n\tat sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)\n\tat com.sun.proxy.$Proxy6.alterTable(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:205)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2771)\n\tat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)\n\tat com.sun.proxy.$Proxy8.alter_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:293)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(SessionHiveMetaStoreClient.java:201)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:288)\n\tat sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)\n\tat com.sun.proxy.$Proxy9.alter_table(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:404)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3542)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:318)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1538)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1305)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1118)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:833)\n\tat org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)\n\tat org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_table(TestCliDriver.java:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:168)\n\tat junit.framework.TestCase.runBare(TestCase.java:134)\n\tat junit.framework.TestResult$1.protect(TestResult.java:110)\n\tat junit.framework.TestResult.runProtected(TestResult.java:128)\n\tat junit.framework.TestResult.run(TestResult.java:113)\n\tat junit.framework.TestCase.run(TestCase.java:124)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:243)\n\tat junit.framework.TestSuite.run(TestSuite.java:238)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: java.sql.SQLException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 86 more\nCaused by: ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.IndexChanger.finish(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.IndexSetChanger.finish(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.RowChangerImpl.finish(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.UpdateResultSet.open(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)```"
    },
    {
        "filename": "HIVE-8735.json",
        "creation_time": "2014-11-04T22:20:02.000+0000",
        "stack_trace": "```\njava.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)\n\tat org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:147)\n\tat org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:144)\n\tat org.apache.hadoop.hive.ql.exec.Utilities.executeWithRetry(Utilities.java:2910)\n\tat org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.publishStat(JDBCStatsPublisher.java:160)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats(FileSinkOperator.java:1153)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:992)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)\n\tat org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:205)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:722)\nCaused by: java.sql.SQLException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 31 more\nCaused by: ERROR 22001: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.types.SQLChar.hasNonBlankChars(Unknown Source)\n\tat org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)\n\tat org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)\n\tat org.apache.derby.iapi.types.DataTypeDescriptor.normalize(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeColumn(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeRow(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(Unknown Source)\n\tat org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)\n\tat org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)```"
    },
    {
        "filename": "HIVE-13209.json",
        "creation_time": "2016-03-04T21:39:50.000+0000",
        "stack_trace": "```\nERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/<hostname@realm> from IP null)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-13065.json",
        "creation_time": "2016-02-16T21:11:31.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)\n\t... 7 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)\n\t... 7 more\nCaused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)\n\t... 14 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)\n\tat org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)\n\tat org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)```"
    },
    {
        "filename": "HIVE-11470.json",
        "creation_time": "2015-08-05T18:45:26.000+0000",
        "stack_trace": "```\njava.io.IOException: java.lang.NullPointerException\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)\nat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\nat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.NullPointerException\nat org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)\nat org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)\nat org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)\nat org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)\nat org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)\nat org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)\nat org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\nat org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)\nat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)\n```"
    },
    {
        "filename": "HIVE-12476.json",
        "creation_time": "2015-11-20T03:30:18.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)\n        at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)\n        at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)\n        at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)\n        at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)\n        at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)\n        at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)\n        at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)\n        at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)\n        at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-10559.json",
        "creation_time": "2015-04-30T21:12:40.000+0000",
        "stack_trace": "```\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n\tat java.util.ArrayList.rangeCheck(ArrayList.java:635)\n\tat java.util.ArrayList.get(ArrayList.java:411)\n\tat org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process(RemoveDynamicPruningBySize.java:61)\n\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)\n\tat org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:77)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)\n\tat org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:281)\n\tat org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:123)\n\tat org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10092)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9932)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n\tat org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1026)\n\tat org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1000)\n\tat org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:139)\n\tat org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_q85(TestMiniTezCliDriver.java:123)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat junit.framework.TestCase.runTest(TestCase.java:176)\n\tat junit.framework.TestCase.runBare(TestCase.java:141)\n\tat junit.framework.TestResult$1.protect(TestResult.java:122)\n\tat junit.framework.TestResult.runProtected(TestResult.java:142)\n\tat junit.framework.TestResult.run(TestResult.java:125)\n\tat junit.framework.TestCase.run(TestCase.java:129)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:255)\n\tat junit.framework.TestSuite.run(TestSuite.java:250)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n\tat org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n\tat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\n```"
    },
    {
        "filename": "HIVE-9721.json",
        "creation_time": "2015-02-19T06:56:17.000+0000",
        "stack_trace": "```\njava.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus\n\tat org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)\n\tat org.apache.hadoop.fs.FilterFileSystem.getAclStatus(FilterFileSystem.java:562)\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:645)\n\tat org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:524)\n\tat org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)\n\tat org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Hadoop23Shims.java:668)\n        at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:527)\n        at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)\n        at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)\n        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)\n        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)\n        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)\n        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)\n        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)\n        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-4216.json",
        "creation_time": "2013-03-21T20:53:24.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}\n        at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:268)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:448)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:399)\n        at org.apache.hadoop.yarn.applications.distributedshell.YarnChild$2.run(YarnChild.java:157)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:152)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}\n        at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)\n        ... 7 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:237)\n        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:477)\n        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:525)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)\n        at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)\n        at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)\n        ... 7 more\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.mapreduce.TaskID$CharTaskTypeMaps.getRepresentingCharacter(TaskID.java:265)\n        at org.apache.hadoop.mapreduce.TaskID.appendTo(TaskID.java:153)\n        at org.apache.hadoop.mapreduce.TaskAttemptID.appendTo(TaskAttemptID.java:119)\n        at org.apache.hadoop.mapreduce.TaskAttemptID.toString(TaskAttemptID.java:151)\n        at java.lang.String.valueOf(String.java:2826)\n        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209)\n        at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:69)\n        at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.getRecordWriter(HFileOutputFormat.java:90)\n        at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getFileWriter(HiveHFileOutputFormat.java:67)\n        at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getHiveRecordWriter(HiveHFileOutputFormat.java:104)\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:246)\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:234)```"
    },
    {
        "filename": "HIVE-13836.json",
        "creation_time": "2016-05-24T22:37:59.000+0000",
        "stack_trace": "```\norg.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started\n        at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)\n        at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)\n        at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)\n        at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)\n        at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:463)\n        at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7522)\n        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)\n        at com.sun.proxy.$Proxy10.addNotificationEvent(Unknown Source)\n        at org.apache.hive.hcatalog.listener.DbNotificationListener.enqueue(DbNotificationListener.java:261)\n        at org.apache.hive.hcatalog.listener.DbNotificationListener.onCreateTable(DbNotificationListener.java:123)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1483)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1502)\n        at sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)\n        at com.sun.proxy.$Proxy14.create_table_with_environment_context(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_table_with_environment_context.getResult(ThriftHiveMetastore.java:9267)\n```"
    },
    {
        "filename": "HIVE-9873.json",
        "creation_time": "2015-03-05T17:35:33.000+0000",
        "stack_trace": "```\nException running child : java.io.IOException: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:105)\n\tat org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)\n\tat org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:224)\n\t... 11 more\nCaused by: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:199)\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:52)\n\tat org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)```"
    },
    {
        "filename": "HIVE-13174.json",
        "creation_time": "2016-02-26T23:34:36.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)\n\tat org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)\n\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)\n\tat org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)\n\tat org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)\n\tat org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)\n\tat org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)\n\tat org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)\n\tat org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n\tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n\tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n```"
    },
    {
        "filename": "HIVE-5431.json",
        "creation_time": "2013-10-03T03:35:44.000+0000",
        "stack_trace": "```\njava.io.IOException: java.lang.IllegalArgumentException: Property value must not be null\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)\n        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.IllegalArgumentException: Property value must not be null\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)\n        at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)\n        at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)\n        at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1826)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:380)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)```"
    },
    {
        "filename": "HIVE-13115.json",
        "creation_time": "2016-02-22T21:43:32.000+0000",
        "stack_trace": "```\nMetaException(message:Unexpected null for one of the IDs, SD 6437, column null, serde 6437 for a non- view)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)\n        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)\n        at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)\n        at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)\n        at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:483)\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)\n        at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-4723.json",
        "creation_time": "2013-06-12T20:37:55.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2912)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2877)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(DDLSemanticAnalyzer.java:2730)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:316)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:277)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n```"
    },
    {
        "filename": "HIVE-15686.json",
        "creation_time": "2017-01-20T22:29:36.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Wrong FS: hdfs://remote-cluster-nn1.myth.net:8020/dbs/mythdb/myth_table/dt=20170120, expected: hdfs://local-cluster-n1.myth.net:8020\n        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1985)\n        at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)\n        at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1290)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.checkTrashPurgeCombination(HiveMetaStore.java:1746)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2974)\n        at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:483)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n        at com.sun.proxy.$Proxy5.drop_partitions_req(Unknown Source)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:10005)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:9989)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:767)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:763)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\n        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:763)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:285)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-4975.json",
        "creation_time": "2013-08-01T16:21:38.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)\n\tat org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n ]\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:162)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)\n\tat org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)\n\tat org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\n ]\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:671)\n\tat org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)\n\t... 8 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating d\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:80)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)\n\tat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)\n\tat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:654)\n\t... 9 more\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 4\n\tat org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)\n\tat org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)\n\tat org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.evaluate(ExprNodeColumnEvaluator.java:98)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:76)```"
    },
    {
        "filename": "HIVE-10538.json",
        "creation_time": "2015-04-29T20:06:38.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"113\",\"_col1\":\"val_113\"}}\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)\n\tat org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)\n\tat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)\n\tat org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)```"
    },
    {
        "filename": "HIVE-11902.json",
        "creation_time": "2015-09-21T16:12:37.000+0000",
        "stack_trace": "```\ncom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n        at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)\n        at com.mysql.jdbc.Util.getInstance(Util.java:360)\n        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)\n        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)\n        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)\n        at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)\n        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)\n        at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)\n        at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)\n        at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)\n        at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-18918.json",
        "creation_time": "2018-03-09T00:47:55.000+0000",
        "stack_trace": "```\njava.io.IOException: Major\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob(CompactorMR.java:314)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:269)\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:175)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:172)\n```"
    },
    {
        "filename": "HIVE-8107.json",
        "creation_time": "2014-09-15T19:49:09.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:97)\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:66)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:217)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:406)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:302)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1051)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1121)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:988)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:978)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)\n\tat org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:441)\n\tat org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:457)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found no_such_table\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:978)\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:128)```"
    },
    {
        "filename": "HIVE-1326.json",
        "creation_time": "2010-04-25T20:50:54.000+0000",
        "stack_trace": "```\norg.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:346)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1013)\n\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)\n\tat org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat$1.write(HiveSequenceFileOutputFormat.java:70)\n\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)\n\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)\n\tat org.apache.hadoop.hive.ql.exec.JoinOperator.processOp(JoinOperator.java:118)\n\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:456)\n\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:244)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:158)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:260)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)```"
    },
    {
        "filename": "HIVE-11369.json",
        "creation_time": "2015-07-24T16:28:47.000+0000",
        "stack_trace": "```\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:146)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:173)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:379)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegating Method)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n\tat org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\n\tat com.sun.proxy.$Proxy23.executeStatement(Unknown Source)\n\tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:258)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-9055.json",
        "creation_time": "2014-12-09T19:51:18.000+0000",
        "stack_trace": "```\njava.lang.IndexOutOfBoundsException: Index: -1, Size: 1\n        at java.util.LinkedList.checkElementIndex(LinkedList.java:553)\n        at java.util.LinkedList.get(LinkedList.java:474)\n        at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:354)\n        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:420)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1108)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1035)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:199)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:151)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:362)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:297)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez(TestMiniTezCliDriver.java:120)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n```"
    },
    {
        "filename": "HIVE-13856.json",
        "creation_time": "2016-05-25T21:50:49.000+0000",
        "stack_trace": "```\njava.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended\n\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)\n\tat oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)\n\tat oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)\n\tat oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)\n\tat oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)\n\tat oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)\n\tat com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy15.open_txns(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended\n\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)\n\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)\n\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)\n\tat oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)\n\tat oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)\n\tat oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)\n\tat oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)\n\tat oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)\n\tat oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)\n\tat oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)\n\tat oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)\n\tat oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)\n\tat com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)\n\tat org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy15.open_txns(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)\n\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n\tat org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-7374.json",
        "creation_time": "2014-07-09T16:02:15.000+0000",
        "stack_trace": "```\norg.apache.thrift.protocol.TProtocolException: Required field 'compacts' is unset! Struct:ShowCompactResponse(compacts:null)\n        at org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate(ShowCompactResponse.java:310)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.validate(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.write(ThriftHiveMetastore.java)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n```"
    },
    {
        "filename": "HIVE-12206.json",
        "creation_time": "2015-10-17T00:30:22.000+0000",
        "stack_trace": "```\norg.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator\nSerialization trace:\ngenericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)\ncolExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)\nparentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)\nchildOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)\nchildOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)\n        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138)\n        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:99)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)\n        at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)\n        at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n        at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1081)\n        at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:970)\n        at org.apache.hadoop.hive.ql.exec.Utilities.cloneOperatorTree(Utilities.java:928)\n        at org.apache.hadoop.hive.ql.parse.GenTezUtils.removeUnionOperators(GenTezUtils.java:228)\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:373)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:205)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10193)\n        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\nCaused by: java.lang.ClassNotFoundException: com.aginity.amp.hive.udf.UniqueNumberGenerator\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:270)\n        at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136)```"
    },
    {
        "filename": "HIVE-10098.json",
        "creation_time": "2015-03-26T17:12:20.000+0000",
        "stack_trace": "```\njava.lang.reflect.UndeclaredThrowableException \nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:634) \nat org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:363) \nat org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:337) \nat org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:303)\nat org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:735) \nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) \nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) \nat java.lang.reflect.Method.invoke(Method.java:606) \nat org.apache.hadoop.util.RunJar.main(RunJar.java:212) \nCaused by: java.io.IOException: java.lang.reflect.UndeclaredThrowableException \nat org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:826) \nat org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension.addDelegationTokens(KeyProviderDelegationTokenExtension.java:86) \nat org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2017) \nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121) \nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100) \nat org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80) \nat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205) \nat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313) \nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:413) \nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:559) \n... 9 more \nCaused by: java.lang.reflect.UndeclaredThrowableException \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1655) \nat org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:808) \n... 18 more \nCaused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) \nat org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:306) \nat org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:196) \nat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:127)\n```"
    },
    {
        "filename": "HIVE-7745.json",
        "creation_time": "2014-08-16T01:20:06.000+0000",
        "stack_trace": "Here is the extracted stack trace:\n\n```\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(GenMapRedUtils.java:1738)\n        at org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.processFileSink(GenSparkUtils.java:281)\n        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:187)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:199)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9508)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)\n        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:414)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1005)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1070)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:197)\n```"
    },
    {
        "filename": "HIVE-11762.json",
        "creation_time": "2015-09-08T20:10:54.000+0000",
        "stack_trace": "```\njava.lang.NoSuchMethodError: org.apache.hadoop.hdfs.DFSClient.setKeyProvider(Lorg/apache/hadoop/crypto/key/KeyProviderCryptoExtension;)V\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Hadoop23Shims.java:534)\n\tat org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.initEncryptionShim(TestHCatLoaderEncryption.java:252)\n\tat org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.setup(TestHCatLoaderEncryption.java:200)\n```"
    },
    {
        "filename": "HIVE-6990.json",
        "creation_time": "2014-04-30T04:24:25.000+0000",
        "stack_trace": "```\njavax.jdo.JDODataStoreException: Error executing SQL query \"select PARTITIONS.PART_ID from PARTITIONS  inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID   inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))\".\n        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)\n        at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)\n        at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:98)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)\n        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1806)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n        at java.lang.reflect.Method.invoke(Method.java:619)\n        at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)\n        at com.sun.proxy.$Proxy11.getPartitionsByFilter(Unknown Source)\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:3310)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n        at java.lang.reflect.Method.invoke(Method.java:619)\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)\n        at com.sun.proxy.$Proxy12.get_partitions_by_filter(Unknown Source)\n```"
    },
    {
        "filename": "HIVE-7114.json",
        "creation_time": "2014-05-22T14:58:09.000+0000",
        "stack_trace": "```\njava.lang.Exception: Opening session\n        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)\n        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)\n        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)\n        at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)\n        at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)\n        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)\n        at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)\n        at org.apache.hive.service.CompositeService.init(CompositeService.java:59)\n        at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)\n        at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)\n        at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n```"
    },
    {
        "filename": "HIVE-11991.json",
        "creation_time": "2015-09-29T21:46:06.000+0000",
        "stack_trace": "```\njava.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)\n        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1621)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:221)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:153)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:364)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:299)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:832)\n        at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11(TestCliDriver.java:120)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at junit.framework.TestCase.runTest(TestCase.java:176)\n        at junit.framework.TestCase.runBare(TestCase.java:141)\n        at junit.framework.TestResult$1.protect(TestResult.java:122)\n        at junit.framework.TestResult.runProtected(TestResult.java:142)\n        at junit.framework.TestResult.run(TestResult.java:125)\n        at junit.framework.TestCase.run(TestCase.java:129)\n        at junit.framework.TestSuite.runTest(TestSuite.java:255)\n        at junit.framework.TestSuite.run(TestSuite.java:250)\n        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)\n        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)\n        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text\n        at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:85)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:572)\n        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:564)\n        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)\n        ... 27 more\nCaused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text\n        at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)\n        at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:225)\n        at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:492)\n        at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:445)\n        at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)\n        at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:429)\n        at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:50)\n        at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:71)\n        at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:40)\n        at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)```"
    },
    {
        "filename": "HIVE-17900.json",
        "creation_time": "2017-10-25T17:12:42.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement\n        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)\n        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1158)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1148)\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:294)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)\n\njava.io.IOException: Could not update stats for table mobiusad.zces_img_data_small_pt/month=201608/dates=9 due to: (40000,FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement,42000line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement)\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:296)\n        at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)\n        at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)\n```"
    },
    {
        "filename": "HIVE-10816.json",
        "creation_time": "2015-05-25T08:08:37.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n```"
    },
    {
        "filename": "HIVE-13017.json",
        "creation_time": "2016-02-05T23:40:09.000+0000",
        "stack_trace": "```\nERROR : Execution failed with exit status: 2\nERROR : Obtaining error information\nERROR : \nTask failed!\nTask ID:\n  Stage-5\n\nLogs:\n\nERROR : /var/log/hive/hiveServer2.log\nError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)\nAborting command set because \"force\" is false and command failed: \"select registration \nfrom s10k s join v10k v \non (s.name = v.name) join studentparttab30k p \non (p.name = v.name) \nwhere s.age < 25 and v.age < 25 and p.age < 25;\"\nClosing: 0: jdbc:hive2://zk2-hs21-h.hdinsight.net:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;principal=hive/_HOST@HDINSIGHT.NET;transportMode=http;httpPath=cliservice\nhiveServer2.log shows:\n2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask\n        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)\n        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)\n        at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-11303.json",
        "creation_time": "2015-07-18T01:31:02.000+0000",
        "stack_trace": "```\norg.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200\n        at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)\n        at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)\n        at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)\n        at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)\n        at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)\n        at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)\n        at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)\n        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)\n        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)\n        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n```"
    },
    {
        "filename": "HIVE-5899.json",
        "creation_time": "2013-11-27T02:48:39.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\nat org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:367)\nat org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats(ColumnStatisticsData.java:444)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:414)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStatsForColumn(StatsUtils.java:369)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:465)\nat org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:109)\nat org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:102)\nat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)\nat org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)\nat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)\nat org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:76)\nat org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:136)\nat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8913)\nat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)\nat org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)\nat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)\nat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:341)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:994)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:905)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n```"
    },
    {
        "filename": "HIVE-11102.json",
        "creation_time": "2015-06-24T22:54:55.000+0000",
        "stack_trace": "```\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0\n\tat java.util.Collections$EmptyList.get(Collections.java:3212)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)\n\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:938)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:847)\n\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:713)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)\n```"
    },
    {
        "filename": "HIVE-9195.json",
        "creation_time": "2014-12-23T01:08:45.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.\n        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:146)\n        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:1160)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:3794)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:4467)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5536)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8884)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9745)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9638)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10086)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:877)\n        at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23(TestCliDriver.java:120)\n```"
    },
    {
        "filename": "HIVE-11285.json",
        "creation_time": "2015-07-16T20:46:22.000+0000",
        "stack_trace": "```\nERROR:\n2015-07-15 13:39:04,333 WARN main org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row\n{\"key\":1,\"value\":\"One\"}\n\nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185) \nat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) \nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450) \nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) \nat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) \nat java.security.AccessController.doPrivileged(Native Method) \nat javax.security.auth.Subject.doAs(Subject.java:415) \nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) \nat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) \nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row\n{\"key\":1,\"value\":\"One\"}\n\nat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503) \nat org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176) \n... 8 more \nCaused by: java.lang.RuntimeException: Map local work failed \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:569) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchNextGroup(SMBMapJoinOperator.java:429) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:260) \nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) \nat org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120) \nat org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) \nat org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) \nat org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) \nat org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493) \n... 9 more \nCaused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Integer \nat org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(JavaIntObjectInspector.java:35) \nat org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:305) \nat org.apache.hadoop.hive.ql.exec.JoinUtil.computeValues(JoinUtil.java:193) \nat org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getFilteredValue(CommonJoinOperator.java:408) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:270) \nat org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:558)```"
    },
    {
        "filename": "HIVE-10288.json",
        "creation_time": "2015-04-09T23:24:16.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:232)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1048)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1265)\n\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:79)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)\n\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:205)\n\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:149)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10383)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10338)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3815)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3594)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8864)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8819)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9663)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9556)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:9992)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:306)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10003)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:195)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\n```"
    },
    {
        "filename": "HIVE-8771.json",
        "creation_time": "2014-11-07T00:30:25.000+0000",
        "stack_trace": "```\njava.lang.Exception: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator\n        at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:100)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:679)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator\n        at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:233)\n        at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.closeOp(OrcFileMergeOperator.java:220)\n        at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:98)\n        ... 10 more\nCaused by: java.io.FileNotFoundException: Destination exists and is not a directory: /home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0\n        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:423)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:267)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:257)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n        at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:339)\n        at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:507)\n        at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)\n        at org.apache.hadoop.fs.ProxyFileSystem.rename(ProxyFileSystem.java:177)\n        at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)\n        at org.apache.hadoop.hive.ql.exec.Utilities.renameOrMoveFiles(Utilities.java:1589)\n        at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:218)```"
    },
    {
        "filename": "HIVE-8008.json",
        "creation_time": "2014-09-05T23:00:11.000+0000",
        "stack_trace": "```\njava.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)\n  at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)\n  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)\n  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)\n  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)\n  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)\n  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)\n  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:606)\n  at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\n  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)\n  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)\n  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)\n  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)\n  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)\n  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)\n  ... 12 more\nCaused by: java.lang.NullPointerException\n  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)\n  at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)\n  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)\n  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)\n  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)\n  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)```"
    },
    {
        "filename": "HIVE-6915.json",
        "creation_time": "2014-04-15T20:20:15.000+0000",
        "stack_trace": "```\njavax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n\tat com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)\n\tat org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:918)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:915)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:915)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1065)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1032)\n\tat org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1474)\n\tat org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1684)\n\tat org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClientProtos.java:1737)\n\tat org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:29288)\n\tat org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1562)\n\tat org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:87)\n\tat org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:84)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:121)\n\tat org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:97)\n\tat org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)\n\tat org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:67)\n\tat org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:174)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:172)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)\n\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:334)\n\tat org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:201)\n\tat org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:415)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)\n\tat org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)\n\tat org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)\n\tat org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:722)\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n\tat sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n\tat sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n\tat sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n\tat sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n\tat sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n\tat sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n\tat com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)```"
    },
    {
        "filename": "HIVE-12364.json",
        "creation_time": "2015-11-07T02:04:25.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir\n        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)\n        at org.apache.hadoop.hive.exec.MoveTask.moveFile(MoveTask.java:105)\n        at org.apache.hadoop.hive.exec.MoveTask.execute(MoveTask.java:222)\n        at org.apache.hadoop.hive.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1156)\n        at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)\n        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2647)\n        ... 21 more\nCaused by: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatability mode.\n        at org.apache.hadoop.mapreduce.Job.ensureNotSet(Job.java:1194)\n        at org.apache.hadoop.mapreduce.Job.setUseNewAPI(Job.java:1229)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1283)\n        at org.apache.hadoop.tools.DistCp.createAndSubmitJob(DistCp.java:183)\n        at org.apache.hadoop.tools.DistCp.execute(DistCp.java:153)\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1153)```"
    },
    {
        "filename": "HIVE-8766.json",
        "creation_time": "2014-11-06T22:08:43.000+0000",
        "stack_trace": "```\norg.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5183)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1738)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1699)\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:101)\n\tat com.sun.proxy.$Proxy11.get_table(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:112)\n\tat sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)\n\tat com.sun.proxy.$Proxy12.getTable(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1060)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1015)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerde(DDLSemanticAnalyzer.java:1356)\n\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:299)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:396)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0\n\tat org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:666)\n\tat org.datanucleus.store.rdbms.scostore.ElementContainerStore.size(ElementContainerStore.java:429)\n\tat org.datanucleus.store.types.backed.List.size(List.java:581)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToSkewedValues(ObjectStore.java:1190)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1168)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1178)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.convertToTable(ObjectStore.java:1035)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:893)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)\n\tat com.sun.proxy.$Proxy10.getTable(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1727)\n\t... 41 more\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: SSL peer shut down incorrectly\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1352)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1339)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:1694)\n\tat com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:3734)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:5062)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:388)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:340)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1400)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:179)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:154)\n\tat com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:283)\n\tat com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)\n\tat org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)\n\tat org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)\n\tat org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:638)\n```"
    },
    {
        "filename": "HIVE-5857.json",
        "creation_time": "2013-11-20T01:12:54.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Error in configuring object\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n\t... 7 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)\n\t... 12 more\n\njava.lang.RuntimeException: Error in configuring object\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\n\t... 7 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)```"
    },
    {
        "filename": "HIVE-1547.json",
        "creation_time": "2010-08-17T02:09:54.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\n```"
    },
    {
        "filename": "HIVE-6113.json",
        "creation_time": "2013-12-27T07:07:00.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1143)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:260)\n\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:217)\n\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:507)\n\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:875)\n\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:769)\n\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:708)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:197)\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1217)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2372)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2383)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1139)\n\t... 20 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1210)\n\t... 25 more\nCaused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore\nNestedThrowables:\njava.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'\n\tat org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)\n\tat org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:165)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:358)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:404)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)\n\tat $Proxy9.createDatabase(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:422)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:441)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:326)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:286)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4060)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:121)\n\t... 30 more\nCaused by: java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'\n\tat com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2028)\n\tat com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1451)\n\tat com.jolbox.bonecp.StatementHandle.executeBatch(StatementHandle.java:469)\n\tat org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:372)\n\tat org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:628)\n\tat org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:596)\n\tat org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:683)\n\tat org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:86)\n\tat org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:454)\n\tat org.datanucleus.TransactionImpl.flush(TransactionImpl.java:199)\n\tat org.datanucleus.TransactionImpl.commit(TransactionImpl.java:263)\n\tat org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:98)\n\t... 46 more\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\n\tat com.mysql.jdbc.Util.getInstance(Util.java:386)\n\tat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3609)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3541)\n\tat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2002)\n\tat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2163)\n\tat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)\n\tat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)\n\tat com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2427)\n\tat com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1980)```"
    },
    {
        "filename": "HIVE-9570.json",
        "creation_time": "2015-02-03T23:30:09.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:274)\n  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:253)\n  at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:222)\n  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10231)\n  at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)\n  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n  at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)\n  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)\n  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)\n  at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)\n  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1160)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1039)\n  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)\n  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)\n  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)\n  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)\n  at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1019)\n  at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:993)\n  at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:136)\n  at org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_view(TestSparkCliDriver.java:120)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:606)\n  at junit.framework.TestCase.runTest(TestCase.java:176)\n  at junit.framework.TestCase.runBare(TestCase.java:141)\n  at junit.framework.TestResult$1.protect(TestResult.java:122)\n  at junit.framework.TestResult.runProtected(TestResult.java:142)\n  at junit.framework.TestResult.run(TestResult.java:125)\n  at junit.framework.TestCase.run(TestCase.java:129)\n  at junit.framework.TestSuite.runTest(TestSuite.java:255)\n  at junit.framework.TestSuite.run(TestSuite.java:250)\n  at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n```"
    },
    {
        "filename": "HIVE-1678.json",
        "creation_time": "2010-10-01T05:41:21.000+0000",
        "stack_trace": "```\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)\n        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n        at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)\n        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)\n        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)\n```"
    },
    {
        "filename": "HIVE-11820.json",
        "creation_time": "2015-09-14T22:20:28.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Skip CRC is valid only with update options\n        at org.apache.hadoop.tools.DistCpOptions.validate(DistCpOptions.java:556)\n        at org.apache.hadoop.tools.DistCpOptions.setSkipCRC(DistCpOptions.java:311)\n        at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1147)\n        at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)\n        at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n```"
    },
    {
        "filename": "HIVE-17274.json",
        "creation_time": "2017-08-08T22:20:33.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc\n        at org.apache.hadoop.fs.Path.initialize(Path.java:205)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:171)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:93)\n        at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:94)\n        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:404)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926)\n        at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1137)\n        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)\n        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:530)\n        at org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(Utilities.java:1643)\n        at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.getHiveRecordWriter(HiveSequenceFileOutputFormat.java:64)\n        at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:243)\n        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter(RowContainer.java:538)\n        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:299)\n        at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.copyToDFSDirecory(RowContainer.java:407)\n        at org.apache.hadoop.hive.ql.exec.SkewJoinHandler.endGroup(SkewJoinHandler.java:185)\n        at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:249)\n        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:195)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc\n        at java.net.URI.checkPath(URI.java:1823)\n        at java.net.URI.<init>(URI.java:745)\n        at org.apache.hadoop.fs.Path.initialize(Path.java:202)```"
    },
    {
        "filename": "HIVE-12522.json",
        "creation_time": "2015-11-25T19:48:44.000+0000",
        "stack_trace": "```\njava.lang.IllegalArgumentException: Wrong FS: wasb://chaoyiteztest@chaoyiteztest.blob.core.windows.net/hive/scratch/chaoyitest/c888f405-3c98-46b1-bf39-e57f067dfe4c/hive_2015-11-13_10-16-10_216_8161037519951665173-1/_tmp.-ext-10000, expected: hdfs://headnodehost:9000\nat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)\nat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)\nat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105)\nat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1136)\nat org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1132)\nat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\nat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1132)\nat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1423)\nat org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:579)\nat org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1083)\nat org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:329)\nat org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:156)\nat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\nat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)\nat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1606)\nat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1367)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1179)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345)\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:733)\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n```"
    },
    {
        "filename": "HIVE-16845.json",
        "creation_time": "2017-06-07T17:07:55.000+0000",
        "stack_trace": "```\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null\nat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)\nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)\nat org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)\nat org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\nat org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\nat org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)\nat org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)\nat org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)\nat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\nat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\nat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)\nat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1201)\nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)```"
    },
    {
        "filename": "HIVE-9655.json",
        "creation_time": "2015-02-11T20:58:13.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"c1\":1,\"c2\":\"one\"}\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)\n  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)\n  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n  at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n  at java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]\n  at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)\n  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)\n  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)\n  at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)\n  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)\n  ... 10 more\nCaused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]\n  at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)\n  at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)\n  at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)\n  at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)\n  at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)```"
    },
    {
        "filename": "HIVE-11441.json",
        "creation_time": "2015-08-03T17:42:24.000+0000",
        "stack_trace": "```\norg.apache.hadoop.hive.ql.parse.SemanticException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1323)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)\n        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)\n        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(DDLSemanticAnalyzer.java:1452)\n        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:295)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1069)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1131)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1072)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)```"
    },
    {
        "filename": "HIVE-10801.json",
        "creation_time": "2015-05-22T19:43:23.000+0000",
        "stack_trace": "```\n2015-05-21 11:53:06,134 ERROR [HiveServer2-Background-Pool: Thread-197]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(155)) - MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n\tat com.sun.proxy.$Proxy8.dropTable(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)\n\t... 40 more\n\n2015-05-21 11:53:06,135 ERROR [HiveServer2-Background-Pool: Thread-197]: exec.DDLTask (DDLTask.java:failed(520)) - org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1041)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n\tat org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1041)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)\n\t... 11 more\nCaused by: MetaException(message:java.lang.NullPointerException)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n\tat com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\n\tat com.sun.proxy.$Proxy8.dropTable(Unknown Source)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)\n\t... 23 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)```"
    },
    {
        "filename": "HIVE-9141.json",
        "creation_time": "2014-12-17T07:23:05.000+0000",
        "stack_trace": "```\njava.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork\n        at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:361)\n        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)\n        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)\n        at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)\n        at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)\n        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)\n        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)\n        at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)\n        at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez2(TestMiniTezCliDriver.java:120)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n```"
    },
    {
        "filename": "HIVE-10010.json",
        "creation_time": "2015-03-18T17:48:46.000+0000",
        "stack_trace": "```\njava.lang.NullPointerException\n    at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)\n    at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)\n    at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)\n    at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)\n    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)\n    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)\n    at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)\n```"
    },
    {
        "filename": "HIVE-7763.json",
        "creation_time": "2014-08-18T09:35:09.000+0000",
        "stack_trace": "```\njava.lang.RuntimeException: Map operator initialization failed\n        at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:127)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:52)\n        at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:30)\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)\n        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)\n        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n        at org.apache.spark.scheduler.Task.run(Task.scala:54)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.java:199)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent\n        at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:404)\n        at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:93)\n        ... 16 more\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent\n        at org.apache.hadoop.hive.ql.exec\n```"
    },
    {
        "filename": "HIVE-12083.json",
        "creation_time": "2015-10-09T22:45:47.000+0000",
        "stack_trace": "```\norg.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)\n        at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)\n        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)\n        at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n```"
    },
    {
        "filename": "HIVE-14784.json",
        "creation_time": "2016-09-17T02:00:28.000+0000",
        "stack_trace": "```\njava.io.IOException: No such file or directory\n\tat java.io.UnixFileSystem.createFileExclusively(Native Method)\n\tat java.io.File.createNewFile(File.java:1012)\n\tat org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)\n\tat org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:385)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\norg.apache.hive.service.cli.HiveSQLException: Couldn't find log associated with operation handle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d35414f7-2418-426c-8489-c6f643ca4599]\n\tat org.apache.hive.service.cli.operation.OperationManager.getOperationLogRowSet(OperationManager.java:259)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:701)\n\tat org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:451)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:676)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)\n\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n```"
    }
]