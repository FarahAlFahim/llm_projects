[
    {
        "filename": "HDFS-4558.json",
        "creation_time": "2013-03-06T11:10:08.000+0000",
        "bug_report": {
            "BugID": "HDFS-4558",
            "Title": "NullPointerException when starting the balancer due to replication policy compatibility check",
            "Description": "The balancer fails to start and throws a NullPointerException (NPE) when checking the replication policy compatibility. This issue needs to be addressed to ensure the balancer can operate correctly without encountering this error.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)"
            ],
            "StepsToReproduce": [
                "1. Start the balancer using the command line.",
                "2. Observe the logs for any errors during the startup process."
            ],
            "ExpectedBehavior": "The balancer should start successfully without throwing any exceptions.",
            "ObservedBehavior": "The balancer fails to start and logs a NullPointerException related to the replication policy compatibility check.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-13039.json",
        "creation_time": "2018-01-19T18:52:12.000+0000",
        "bug_report": {
            "BugID": "HDFS-13039",
            "Title": "DataNode Socket Leak Leading to 'Too Many Open Files' Exception",
            "Description": "When running Erasure Coding (EC) on a cluster, the DataNode experiences a significant number of {{CLOSE_WAIT}} connections, leading to the inability to open new files or sockets. This is evidenced by the log message indicating a 'Too many open files' exception.",
            "StackTrace": [
                "java.io.IOException: Too many open files",
                "at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)",
                "at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)",
                "at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)",
                "at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)",
                "at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)",
                "at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop cluster with DataNode configured for Erasure Coding.",
                "2. Initiate a large number of file operations that would lead to socket creation.",
                "3. Monitor the DataNode's socket connections using a command like 'lsof'.",
                "4. Observe the number of connections in the CLOSE_WAIT state."
            ],
            "ExpectedBehavior": "The DataNode should manage socket connections efficiently without reaching the limit of open files, allowing for normal file operations.",
            "ObservedBehavior": "The DataNode reaches a state where it has millions of CLOSE_WAIT connections, resulting in the inability to open new files or sockets and throwing a 'Too many open files' exception.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-13023.json",
        "creation_time": "2018-01-16T20:27:39.000+0000",
        "bug_report": {
            "BugID": "HDFS-13023",
            "Title": "Authorization Exception during Journal Sync on Secure Cluster",
            "Description": "The JournalNodeSyncer fails to sync with the journal on a secure cluster due to an authorization issue. The error indicates that the user is not authorized for the QJournalProtocol interface, which is only accessible by a specific user.",
            "StackTrace": [
                "2018-01-10 01:15:40,517 INFO server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(235)) - Syncing Journal /0.0.0.0:8485 with xxx, journal id: mycluster",
                "2018-01-10 01:15:40,583 ERROR server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(259)) - Could not sync with Journal at xxx/xxx:8485",
                "com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Set up a secure Hadoop cluster with Kerberos authentication.",
                "2. Attempt to sync the journal using the JournalNodeSyncer.",
                "3. Monitor the logs for any authorization errors."
            ],
            "ExpectedBehavior": "The JournalNodeSyncer should successfully sync with the journal without any authorization errors.",
            "ObservedBehavior": "The JournalNodeSyncer fails to sync with the journal, logging an authorization exception indicating that the user is not authorized for the QJournalProtocol interface.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-3157.json",
        "creation_time": "2012-03-28T14:35:53.000+0000",
        "bug_report": {
            "BugID": "HDFS-3157",
            "Title": "IOException during block deletion in DataNode after block report",
            "Description": "An IOException is thrown in the DataNode when attempting to delete a block that has already been reported as corrupted. This occurs even after the block report and directory scanning have been completed, leading to inconsistencies in block management.",
            "StackTrace": [
                "java.io.IOException: Error in deleting blocks.",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with 1 NameNode and 3 DataNodes (DN1, DN2, DN3) with a replication factor of 2.",
                "2. Write a file named 'a.txt' and do not close it immediately (keep it open with sync).",
                "3. Manually delete the blocks from one of the DataNodes (e.g., DN1) that were replicated.",
                "4. Close the file.",
                "5. Observe the logs on the DataNode where the blocks were deleted."
            ],
            "ExpectedBehavior": "The DataNode should successfully delete the specified blocks without throwing an IOException, as the blocks should be marked for deletion after the block report and directory scanning.",
            "ObservedBehavior": "An IOException is thrown indicating an error in deleting blocks, with warnings logged about the block not being found in the volume map.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-4850.json",
        "creation_time": "2013-05-25T00:18:15.000+0000",
        "bug_report": {
            "BugID": "HDFS-4850",
            "Title": "NegativeArraySizeException in OfflineImageViewer when processing fsimages with empty files",
            "Description": "When using the OfflineImageViewer to process fsimages that contain empty files, a NegativeArraySizeException is thrown, causing the image loading to fail. This issue occurs after creating an empty file in HDFS and forcing a checkpoint.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NegativeArraySizeException",
                "\tat org.apache.hadoop.io.Text.readString(Text.java:458)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)",
                "\tat org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)"
            ],
            "StepsToReproduce": [
                "Deploy hadoop-trunk HDFS.",
                "Create a directory: /user/schu.",
                "Force a checkpoint and fetch the fsimage.",
                "Run the OfflineImageViewer on the fsimage: `hdfs oiv -i fsimage_0000000000000000004 -o oiv_out_1`.",
                "Create an empty file: `hadoop fs -touchz /user/schu/testFile1`.",
                "Force another checkpoint and fetch the new fsimage.",
                "Run the OfflineImageViewer again on the new fsimage: `hdfs oiv -i fsimage_0000000000000000008 -o oiv_out_2`."
            ],
            "ExpectedBehavior": "The OfflineImageViewer should successfully process the fsimage and include the empty file in the output.",
            "ObservedBehavior": "The OfflineImageViewer throws a NegativeArraySizeException and fails to load the fsimage, resulting in an error message indicating that the image loading failed at a specific offset.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-3415.json",
        "creation_time": "2012-05-13T13:44:43.000+0000",
        "bug_report": {
            "BugID": "HDFS-3415",
            "Title": "NullPointerException during NameNode startup due to inconsistent storage layout versions",
            "Description": "When starting the NameNode after modifying the layout version of one of the storage directories, a NullPointerException is thrown. This occurs if the layout versions of the storage directories are inconsistent, leading to the NameNode picking the wrong storage directory inspector.",
            "StackTrace": [
                "2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)"
            ],
            "StepsToReproduce": [
                "1. Start the NameNode and DataNode with three configured storage directories.",
                "2. Write 10 files to the NameNode.",
                "3. Edit the version file of one of the storage directories to set the layout version to 123, which is different from the default (-40).",
                "4. Stop the NameNode.",
                "5. Start the NameNode again."
            ],
            "ExpectedBehavior": "The NameNode should start successfully without throwing any exceptions, regardless of the layout version differences.",
            "ObservedBehavior": "The NameNode throws a NullPointerException during startup due to inconsistent layout versions in the storage directories.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-2245.json",
        "creation_time": "2011-08-10T22:55:33.000+0000",
        "bug_report": {
            "BugID": "HDFS-2245",
            "Title": "NullPointerException in BlockManager.chooseTarget() during addBlock operation",
            "Description": "A NullPointerException is thrown in the BlockManager.chooseTarget() method when attempting to add a block to the Hadoop HDFS. This issue occurs when the system is unable to find suitable DataNodes for block placement, leading to a failure in the addBlock operation.",
            "StackTrace": [
                "java.io.IOException: java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS cluster.",
                "2. Attempt to add a block using the addBlock method with a source path that does not have available DataNodes.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The system should log an appropriate error message indicating that no suitable DataNodes are available for block placement, without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the BlockManager.chooseTarget() method, causing the addBlock operation to fail.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-10320.json",
        "creation_time": "2016-04-20T23:02:09.000+0000",
        "bug_report": {
            "BugID": "HDFS-10320",
            "Title": "InvalidTopologyException in BlockPlacementPolicy during Rack Failures",
            "Description": "When there are rack failures that leave only one rack available, the method `BlockPlacementPolicyDefault#chooseRandom` may throw an `InvalidTopologyException` when calling `NetworkTopology#chooseRandom`. This exception propagates up to the `BlockManager`'s `ReplicationMonitor` thread, potentially causing the NameNode (NN) to terminate unexpectedly.",
            "StackTrace": [
                "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Simulate rack failures in the Hadoop cluster.",
                "2. Ensure that only one rack remains available.",
                "3. Trigger a block replication operation that requires choosing a datanode from the available racks."
            ],
            "ExpectedBehavior": "The system should handle rack failures gracefully and continue to operate without terminating the NameNode.",
            "ObservedBehavior": "The NameNode terminates unexpectedly due to an `InvalidTopologyException` thrown during the block replication process.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-4201.json",
        "creation_time": "2012-11-16T21:28:57.000+0000",
        "bug_report": {
            "BugID": "HDFS-4201",
            "Title": "NullPointerException in BPServiceActor#sendHeartBeat due to uninitialized DataNode",
            "Description": "A NullPointerException (NPE) occurs in the BPServiceActor's sendHeartBeat method, likely caused by the DataNode (dn) or its FSDataset being null. This issue may arise from a configuration error or a failure in the local directory setup, preventing the DataNode from properly initializing.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Start the DataNode service with an incorrect or incomplete configuration.",
                "2. Ensure that the DataNode cannot connect to the NameNode.",
                "3. Monitor the logs for any NPE related to the sendHeartBeat method."
            ],
            "ExpectedBehavior": "The DataNode should handle connection failures gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the sendHeartBeat method, causing the DataNode to fail in its operation.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-6904.json",
        "creation_time": "2014-08-21T10:17:21.000+0000",
        "bug_report": {
            "BugID": "HDFS-6904",
            "Title": "YARN Fails to Renew WebHDFS Delegation Token Due to Incorrect Service Port",
            "Description": "YARN is unable to renew delegation tokens obtained via the WebHDFS REST API. The issue arises when a user creates a delegation token using the WebHDFS REST API and subsequently submits an application to YARN with this token. When YARN attempts to renew the token, it fails because the token service is incorrectly pointing to the RPC port instead of the WebHDFS service port.",
            "StackTrace": [
                "2014-08-19 03:12:54,733 WARN  security.DelegationTokenRenewer (DelegationTokenRenewer.java:handleDTRenewerAppSubmitEvent(661)) - Unable to add the application to the delegation token renewer.",
                "java.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$5(DelegationTokenRenewer.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:657)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:638)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:331)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:598)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:448)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:477)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:473)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.renewDelegationToken(WebHdfsFileSystem.java:1318)",
                "at org.apache.hadoop.hdfs.web.TokenAspect$TokenManager.renew(TokenAspect.java:73)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:477)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:1)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:392)",
                "... 6 more",
                "Caused by: java.io.IOException: The error stream is null.",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:304)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:329)"
            ],
            "StepsToReproduce": [
                "1. Create a delegation token using the WebHDFS REST API.",
                "2. Submit an application to YARN using the created delegation token via the YARN REST API.",
                "3. Observe the logs for errors related to token renewal."
            ],
            "ExpectedBehavior": "YARN should successfully renew the WebHDFS delegation token without errors.",
            "ObservedBehavior": "YARN fails to renew the WebHDFS delegation token, resulting in an IOException indicating an unexpected HTTP response.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-13721.json",
        "creation_time": "2018-07-05T20:11:57.000+0000",
        "bug_report": {
            "BugID": "HDFS-13721",
            "Title": "NullPointerException in DataNode#getDiskBalancerStatus() during DataNode startup",
            "Description": "A NullPointerException (NPE) occurs in the DataNode#getDiskBalancerStatus() method when the DataNode is restarted. This issue arises due to an uninitialized DiskBalancer, which leads to the failure of the JMXJsonServlet when attempting to retrieve the DiskBalancerStatus attribute.",
            "StackTrace": [
                "2018-06-28 05:11:47,650 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception",
                "javax.management.RuntimeMBeanException: java.lang.NullPointerException",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)",
                "at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)",
                "at org.eclipse.jetty.server.session.SessionHandler.handle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.handle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)"
            ],
            "StepsToReproduce": [
                "1. Start the DataNode service.",
                "2. Restart the DataNode service.",
                "3. Access the JMX endpoint for the DataNode.",
                "4. Attempt to retrieve the DiskBalancerStatus attribute."
            ],
            "ExpectedBehavior": "The DiskBalancerStatus attribute should return a valid JSON response without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, resulting in an error response when attempting to access the DiskBalancerStatus attribute.",
            "Resolution": "The issue can be resolved by ensuring that the DiskBalancer is properly initialized before it is accessed in the getDiskBalancerStatus() method. Changing the NPE to an IOException will allow JMX to return an empty string correctly for getDiskBalancerStatus."
        }
    },
    {
        "filename": "HDFS-7180.json",
        "creation_time": "2014-10-02T03:07:04.000+0000",
        "bug_report": {
            "BugID": "HDFS-7180",
            "Title": "NFSv3 Gateway Becomes Unresponsive After Extended Use",
            "Description": "The NFSv3 gateway frequently becomes unresponsive after approximately one day of operation, particularly after significant data uploads (hundreds of GBs). The issue manifests as the NFS daemon getting stuck, while HDFS operations continue to function normally. The logs indicate that the NFS server is not responding, and attempts to list the mounted directory or check disk usage result in hangs.",
            "StackTrace": [
                "java.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:828)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop 2.5.0 cluster with HDFS and NFSv3 gateway.",
                "2. Start the NFSv3 daemon on one node in the cluster.",
                "3. Mount the NFS on the same node.",
                "4. Use rsync to upload several hundreds of GBs of data to the NFS mount.",
                "5. Monitor the system for approximately 24 hours."
            ],
            "ExpectedBehavior": "The NFSv3 gateway should remain responsive and allow for continuous data uploads without becoming unresponsive.",
            "ObservedBehavior": "After about one day of operation, the NFSv3 daemon becomes unresponsive, and operations such as 'ls' on the mounted directory hang. The logs indicate repeated messages stating 'nfs: server localhost not responding'.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-6102.json",
        "creation_time": "2014-03-13T18:27:36.000+0000",
        "bug_report": {
            "BugID": "HDFS-6102",
            "Title": "Protocol Buffer Size Limit Exceeded When Loading Large FSImage",
            "Description": "During testing, an error occurs when attempting to load a very large fsimage file due to exceeding the Protocol Buffer size limit. This issue arises when creating a large number of directories, which inflates the fsimage size beyond the allowable limit.",
            "StackTrace": [
                "2014-03-13 13:57:59,038 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Failed to load image from FSImageFile(file=/dfs/nn/current/fsimage_0000000000024532742, cpktTxId=0000000000024532742)",
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large. May be malicious. Use CodedInputStream.setSizeLimit() to increase the size limit.",
                "at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)",
                "at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)",
                "at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)",
                "at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)",
                "at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)"
            ],
            "StepsToReproduce": [
                "1. Create a large number of directories within a single directory in HDFS.",
                "2. Attempt to load the fsimage file that has been generated.",
                "3. Observe the error message indicating that the Protocol Buffer size limit has been exceeded."
            ],
            "ExpectedBehavior": "The fsimage should load successfully without exceeding the Protocol Buffer size limit.",
            "ObservedBehavior": "An error occurs indicating that the Protocol message was too large, preventing the fsimage from loading.",
            "Resolution": "The default maximum items per directory should be lowered to prevent exceeding the Protocol Buffer size limit when loading large fsimages."
        }
    },
    {
        "filename": "HDFS-6250.json",
        "creation_time": "2014-04-16T16:14:32.000+0000",
        "bug_report": {
            "BugID": "HDFS-6250",
            "Title": "AssertionError in TestBalancerWithNodeGroup.testBalancerWithRackLocality due to incorrect expected value",
            "Description": "The test case `testBalancerWithRackLocality` in the `TestBalancerWithNodeGroup` class is failing with an `AssertionError`. The test expects the total used capacity across racks to be equal, but the actual value differs, indicating a potential issue in the balancer logic or test setup.",
            "StackTrace": [
                "java.lang.AssertionError: expected:<1800> but was:<1810>",
                "at org.junit.Assert.fail(Assert.java:93)",
                "at org.junit.Assert.failNotEquals(Assert.java:647)",
                "at org.junit.Assert.assertEquals(Assert.java:128)",
                "at org.junit.Assert.assertEquals(Assert.java:147)",
                "at org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup.testBalancerWithRackLocality(TestBalancerWithNodeGroup.java:253)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop HDFS environment with the necessary configurations.",
                "2. Execute the test suite that includes `TestBalancerWithNodeGroup`.",
                "3. Observe the failure in the `testBalancerWithRackLocality` test case."
            ],
            "ExpectedBehavior": "The balancer should distribute the data evenly across the racks, resulting in equal used capacity for each rack.",
            "ObservedBehavior": "The test fails with an `AssertionError`, indicating that the expected used capacity (1800) does not match the actual used capacity (1810).",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-11377.json",
        "creation_time": "2017-01-26T23:40:53.000+0000",
        "bug_report": {
            "BugID": "HDFS-11377",
            "Title": "Balancer Hangs When No Mover Threads Are Available in Large Clusters",
            "Description": "When running the balancer on a large cluster with more than 3000 Datanodes, the process may hang due to a lack of available mover threads. The stack trace indicates that the balancer is waiting indefinitely, leading to a state where no block moves can be completed. This issue is exacerbated by the condition where `DDatanode.isPendingQEmpty()` returns false, causing the balancer to hang.",
            "StackTrace": [
                "\"main\" #1 prio=5 os_prio=0 tid=0x00007ff6cc014800 nid=0x6b2c waiting on condition [0x00007ff6d1bad000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with more than 3000 Datanodes.",
                "2. Initiate the balancer process.",
                "3. Monitor the logs for warnings related to mover threads."
            ],
            "ExpectedBehavior": "The balancer should successfully move blocks between Datanodes without hanging, even when the number of Datanodes is high.",
            "ObservedBehavior": "The balancer hangs indefinitely with the stack trace indicating it is waiting for move completion, and logs show multiple warnings about 'No mover threads available'.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-6753.json",
        "creation_time": "2014-07-25T11:36:43.000+0000",
        "bug_report": {
            "BugID": "HDFS-6753",
            "Title": "DataNode Fails to Shutdown on Full Disk with Permission Denied",
            "Description": "The DataNode does not shut down when all configured volumes are marked as failed due to full disk space and permission issues. This leads to potential data loss and inconsistency in the Hadoop HDFS environment.",
            "StackTrace": [
                "2014-07-21 14:10:52,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: XX1.XX1.XX1.XX1:50010:DataXceiver error processing WRITE_BLOCK operation src: /XX2.XX2.XX2.XX2:10106 dst: /XX1.XX1.XX1.XX1:50010",
                "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)",
                "2014-07-21 14:13:00,180 WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception occurred while compiling report:",
                "java.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized",
                "at org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)"
            ],
            "StepsToReproduce": [
                "1. Set the permissions of /mnt/tmp_Datanode to root.",
                "2. Fill the disk at /mnt/tmp_Datanode to capacity.",
                "3. Attempt to perform write operations from a client."
            ],
            "ExpectedBehavior": "The DataNode should shut down when all configured volumes are marked as failed due to full disk space and permission issues.",
            "ObservedBehavior": "The DataNode remains operational despite all configured volumes being marked as failed, leading to write operations being processed incorrectly.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-3443.json",
        "creation_time": "2012-05-18T11:30:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-3443",
            "Title": "NullPointerException during NameNode transition to active state on startup",
            "Description": "A NullPointerException (NPE) occurs when the NameNode attempts to transition to the active state during startup, specifically when the editLogTailer is initialized. This issue arises in the `startActiveServices()` method of the `FSNamesystem` class, where the `catchupDuringFailover()` method is called without proper checks, leading to the NPE.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)",
                "at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)"
            ],
            "StepsToReproduce": [
                "Start the NameNode service.",
                "Allow the standby NameNode services to initialize.",
                "Before the editLogTailer is initialized, start the ZKFC (ZooKeeper Failover Controller).",
                "Observe the logs for any exceptions during the transition to active state."
            ],
            "ExpectedBehavior": "The NameNode should transition to the active state without throwing any exceptions, and the editLogTailer should be initialized correctly.",
            "ObservedBehavior": "A NullPointerException is thrown during the transition to active state, specifically in the `startActiveServices()` method when calling `editLogTailer.catchupDuringFailover()`.",
            "Resolution": "Add a check in the `startActiveServices()` method to ensure that the editLogTailer is properly initialized before calling `catchupDuringFailover()`. This will prevent the NullPointerException from occurring."
        }
    },
    {
        "filename": "HDFS-11479.json",
        "creation_time": "2017-03-01T11:08:07.000+0000",
        "bug_report": {
            "BugID": "HDFS-11479",
            "Title": "UDP Server Fails to Start Due to Bind Exception in SimpleUdpServer",
            "Description": "The NFS gateway restart can fail because of a bind error in the SimpleUdpServer. The server fails to bind to the specified port when it is in the TIME_WAIT state. This issue can be resolved by enabling the SO_REUSEADDR option in the SimpleUdpServer to allow binding to the port even when it is in the TIME_WAIT state.",
            "StackTrace": [
                "2017-02-28 04:19:53,495 FATAL mount.MountdBase (MountdBase.java:startUDPServer(66)) - Failed to start the UDP server.",
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)",
                "at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)",
                "at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "at sun.nio.ch.Net.bind0(Native Method)",
                "at sun.nio.ch.Net.bind(Net.java:433)",
                "at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)",
                "at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)",
                "at org.jboss.netty.channel.Channels.bind(Channels.java:561)",
                "at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "1. Attempt to restart the NFS gateway.",
                "2. Observe the logs for any fatal errors related to the UDP server.",
                "3. Check if the port 4242 is already in use by another process."
            ],
            "ExpectedBehavior": "The UDP server should start successfully and bind to the specified port (4242) without any errors.",
            "ObservedBehavior": "The UDP server fails to start, resulting in a fatal error due to a bind exception indicating that the address is already in use.",
            "Resolution": "Implement the SO_REUSEADDR option in the SimpleUdpServer to allow binding to the port even when it is in the TIME_WAIT state."
        }
    },
    {
        "filename": "HDFS-8055.json",
        "creation_time": "2015-04-02T23:21:55.000+0000",
        "bug_report": {
            "BugID": "HDFS-8055",
            "Title": "NullPointerException in DatanodeManager when topology script is missing",
            "Description": "A NullPointerException (NPE) is thrown in the DatanodeManager when the topology script is missing. This issue affects the ability to retrieve block locations, leading to potential disruptions in HDFS operations. The stack trace indicates that the error occurs during the sorting of located blocks, which is critical for data retrieval and management in HDFS.",
            "StackTrace": [
                "2015-02-06 23:02:12,250 ERROR [pool-4-thread-1] util.HFileV1Detector: Got exception while reading trailer for file:hdfs://hqhd02nm01.pclc0.merkle.local:8020/hbase/.META./1028785192/info/1490a396aea448b693da563f76a28486",
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1468)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1399)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1220)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)",
                "at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)",
                "at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)",
                "at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)",
                "at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)",
                "at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)",
                "at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:320)",
                "at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:300)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the topology script is missing from the configuration.",
                "2. Attempt to retrieve block locations from the HDFS using the NameNode.",
                "3. Observe the logs for any NullPointerException errors."
            ],
            "ExpectedBehavior": "The system should handle the absence of the topology script gracefully, providing a clear error message indicating the issue without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the DatanodeManager, causing disruptions in the retrieval of block locations.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-6533.json",
        "creation_time": "2014-06-14T16:15:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-6533",
            "Title": "Intermittent Failure in TestBPOfferService#testBasicFunctionality Due to Mock Interaction",
            "Description": "The test method `testBasicFunctionality` in `TestBPOfferService` fails intermittently, indicating that the expected interaction with the mock `datanodeProtocolClientSideTranslatorPB.registerDatanode` was not invoked during the test execution. This issue was observed in the CI environment but not during local reruns.",
            "StackTrace": [
                "org.mockito.exceptions.verification.WantedButNotInvoked: ",
                "Wanted but not invoked:",
                "datanodeProtocolClientSideTranslatorPB.registerDatanode(<any>);",
                "-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)",
                "Actually, there were zero interactions with this mock."
            ],
            "StepsToReproduce": [
                "Run the test suite for Hadoop HDFS, specifically targeting `TestBPOfferService`.",
                "Observe the results of `testBasicFunctionality` in a CI environment.",
                "Rerun the same test locally to confirm that it passes without issues."
            ],
            "ExpectedBehavior": "The `testBasicFunctionality` should successfully verify that both NameNodes register the DataNode without any issues, and the expected interactions with the mock should occur.",
            "ObservedBehavior": "The test fails with a verification error indicating that `registerDatanode` was not invoked, despite the expectation that it should have been called during the test execution.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-10609.json",
        "creation_time": "2016-07-11T17:14:39.000+0000",
        "bug_report": {
            "BugID": "HDFS-10609",
            "Title": "Uncaught InvalidEncryptionKeyException during pipeline recovery aborts downstream applications",
            "Description": "During normal operations, if SASL negotiation fails due to an InvalidEncryptionKeyException, the exception is typically handled gracefully. However, if this exception occurs during pipeline recovery, it is not caught properly, leading to an uncaught exception that aborts downstream applications such as SOLR. This behavior is contrary to the expected retry mechanism in place for handling such exceptions.",
            "StackTrace": [
                "2016-07-06 12:12:51,992 ERROR org.apache.solr.update.HdfsTransactionLog: Exception closing tlog.",
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop cluster with HDFS enabled.",
                "2. Initiate a data transfer that requires encryption.",
                "3. Simulate an expired encryption key scenario to trigger an InvalidEncryptionKeyException during pipeline recovery.",
                "4. Observe the behavior of downstream applications (e.g., SOLR) during this process."
            ],
            "ExpectedBehavior": "The InvalidEncryptionKeyException should be caught and retried, allowing the data transfer to continue without aborting downstream applications.",
            "ObservedBehavior": "The InvalidEncryptionKeyException is not caught during pipeline recovery, leading to an uncaught exception that aborts operations in downstream applications such as SOLR.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-2310.json",
        "creation_time": "2011-09-05T15:31:00.000+0000",
        "bug_report": {
            "BugID": "HDFS-2310",
            "Title": "JournalProtocol Not Registered Causing IOException in IPC Server",
            "Description": "The system logs indicate an IOException due to the JournalProtocol not being registered with the server. This issue arises when the IPC server attempts to start a log segment but fails to recognize the protocol, leading to a failure in the operation.",
            "StackTrace": [
                "java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "2011-09-05 10:44:36,813 ERROR namenode.FSEditLog (FSEditLog.java:mapJournalsAndReportErrors(1048)) - Error starting log segment 3 (journal JournalAndStream(mgr=org.apache.hadoop.hdfs.server.namenode.BackupJournalManager@2876b359, stream=null))"
            ],
            "StepsToReproduce": [
                "1. Start the IPC server with the current configuration.",
                "2. Attempt to initiate a log segment using the command: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3).",
                "3. Observe the server logs for any errors related to protocol registration."
            ],
            "ExpectedBehavior": "The IPC server should successfully start the log segment without any IOException related to protocol registration.",
            "ObservedBehavior": "The IPC server throws an IOException indicating that the JournalProtocol is unknown, preventing the log segment from starting.",
            "Resolution": "The issue was resolved by ensuring that the JournalProtocol is properly registered with the server before attempting to start log segments."
        }
    },
    {
        "filename": "HDFS-3385.json",
        "creation_time": "2012-05-08T12:45:31.000+0000",
        "bug_report": {
            "BugID": "HDFS-3385",
            "Title": "ClassCastException Occurs When Appending a File in HDFS",
            "Description": "A ClassCastException is thrown when attempting to append a file in HDFS due to an incorrect type cast in the FSNamesystem class. The issue arises when the system tries to recover a lease for a file that is not in the expected state.",
            "StackTrace": [
                "2012-05-08 18:13:40,506 WARN  util.KerberosName (KerberosName.java:<clinit>(87)) - Kerberos krb5 configuration not found, setting default realm to empty",
                "Exception in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)",
                "...",
                "at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)",
                "at org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)"
            ],
            "StepsToReproduce": [
                "1. Ensure HDFS is running and accessible.",
                "2. Attempt to append data to a file that is already open for writing.",
                "3. Observe the logs for any warnings or errors."
            ],
            "ExpectedBehavior": "The file should be successfully appended without any exceptions.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating that the system is trying to cast a BlockInfo object to BlockInfoUnderConstruction, which fails.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-4006.json",
        "creation_time": "2012-10-04T22:05:14.000+0000",
        "bug_report": {
            "BugID": "HDFS-4006",
            "Title": "NullPointerException in SecondaryNameNode during checkpointing causes test failures",
            "Description": "The test 'TestCheckpoint#testSecondaryHasVeryOutOfDateImage' occasionally fails due to an unexpected exit caused by a NullPointerException (NPE) during the checkpointing process. This issue arises when the background checkpointing conflicts with explicit checkpoints initiated by the tests, leading to an unexpected exit of the MiniDFSCluster.",
            "StackTrace": [
                "2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit",
                "org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Set up a MiniDFSCluster environment.",
                "2. Run the test 'TestCheckpoint#testSecondaryHasVeryOutOfDateImage'.",
                "3. Observe the test execution and monitor for any unexpected exits."
            ],
            "ExpectedBehavior": "The test 'TestCheckpoint#testSecondaryHasVeryOutOfDateImage' should complete successfully without any unexpected exits or exceptions.",
            "ObservedBehavior": "The test occasionally fails with a NullPointerException during the checkpointing process, leading to an unexpected exit of the MiniDFSCluster.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-6715.json",
        "creation_time": "2014-07-21T21:26:25.000+0000",
        "bug_report": {
            "BugID": "HDFS-6715",
            "Title": "WebHDFS Fails to Handle Namenode Startup Mode Exception Gracefully",
            "Description": "During high availability (HA) testing of MapReduce jobs using WebHDFS, the system encounters an IOException indicating that the Namenode is in startup mode. This results in job failures and prevents the system from recovering gracefully, leading to potential data loss and operational disruptions.",
            "StackTrace": [
                "2014-04-17 05:08:06,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1397710493213_0001_r_000008_0: Container killed by the ApplicationMaster.",
                "Container killed on request. Exit code is 143",
                "Container exited with a non-zero exit code 143",
                "2014-04-17 05:08:10,205 ERROR [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job",
                "java.io.IOException: Namenode is in startup mode",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with high availability enabled.",
                "2. Start the Namenode and put it into startup mode.",
                "3. Submit a MapReduce job that utilizes WebHDFS as the file system.",
                "4. Monitor the job execution and observe the logs for any exceptions."
            ],
            "ExpectedBehavior": "The WebHDFS should handle the Namenode startup mode gracefully, allowing the MapReduce job to either retry or failover to a standby Namenode without crashing.",
            "ObservedBehavior": "The job fails with an IOException indicating that the Namenode is in startup mode, and the system does not recover, leading to job termination.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that WebHDFS can handle the Namenode startup mode without causing job failures."
        }
    },
    {
        "filename": "HDFS-2392.json",
        "creation_time": "2011-09-30T20:29:09.000+0000",
        "bug_report": {
            "BugID": "HDFS-2392",
            "Title": "DistCp Fails with IOException When Using HFTP Source",
            "Description": "The DistCp tool fails to copy files from an HFTP source, resulting in an IOException. The job reports that no files were copied, skipped, or failed, yet the task fails with an error indicating that the job has failed due to exceeding the allowed limit of failed map tasks.",
            "StackTrace": [
                "java.io.IOException: Copied: 0 Skipped: 0 Failed: 1",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "1. Execute the command: `hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3`.",
                "2. Monitor the job status in the Hadoop job tracker or logs."
            ],
            "ExpectedBehavior": "The DistCp command should successfully copy the specified file from the HFTP source to the HDFS destination without any errors.",
            "ObservedBehavior": "The DistCp command fails with an IOException, indicating that no files were copied, skipped, or failed, yet the job fails due to exceeding the allowed limit of failed map tasks.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-11472.json",
        "creation_time": "2017-02-28T18:39:13.000+0000",
        "bug_report": {
            "BugID": "HDFS-11472",
            "Title": "Inconsistent Replica Size After Data Pipeline Failure",
            "Description": "A critical issue has been identified where a replica's on-disk length is less than the acknowledged length, violating assumptions in the recovery code. This inconsistency can lead to data integrity issues during recovery processes. The problem arises when exceptions are thrown within the `BlockReceiver#receivePacket` method, causing the in-memory replica on disk length to remain outdated despite data being written to disk.",
            "StackTrace": [
                "java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW",
                "  getNumBytes()     = 27530",
                "  getBytesOnDisk()  = 27006",
                "  getVisibleLength()= 27268",
                "  getVolume()       = /data/6/hdfs/datanode/current",
                "  getBlockFile()    = /data/6/hdfs/datanode/current/BP-947993742-10.204.0.136-1362248978912/current/rbw/blk_2526438952",
                "  bytesAcked=27268",
                "  bytesOnDisk=27006",
                "        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)",
                "        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop HDFS cluster with multiple datanodes.",
                "2. Initiate a data write operation that involves multiple packets.",
                "3. Simulate a failure during the write operation (e.g., interrupt the data transfer).",
                "4. Observe the logs for any exceptions thrown during the packet reception."
            ],
            "ExpectedBehavior": "The on-disk length of the replica should always be equal to or greater than the acknowledged length after a write operation, ensuring data integrity during recovery.",
            "ObservedBehavior": "The on-disk length of the replica is less than the acknowledged length, leading to potential data loss or corruption during recovery processes.",
            "Resolution": "The recovery code should be improved to handle cases where the on-disk size is less than the acknowledged size, ensuring that the in-memory checksum is updated accordingly."
        }
    },
    {
        "filename": "HDFS-10760.json",
        "creation_time": "2016-08-12T08:29:24.000+0000",
        "bug_report": {
            "BugID": "HDFS-10760",
            "Title": "DataXceiver#run() Logs InvalidToken Exception as Error Instead of Warning",
            "Description": "The DataXceiver#run() method currently logs the InvalidToken exception as an error when a client attempts to access a block with an expired token. This behavior is misleading since the checkAccess() method already logs this as a warning. The logging of the InvalidToken exception as an error can lead to confusion and unnecessary alarm in the logs.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)",
                "org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)",
                "org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)",
                "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)",
                "java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS cluster.",
                "2. Use a client to request a block read operation with an expired token.",
                "3. Observe the logs generated by the DataXceiver."
            ],
            "ExpectedBehavior": "The InvalidToken exception should be logged as a warning, consistent with the behavior of the checkAccess() method.",
            "ObservedBehavior": "The InvalidToken exception is logged as an error, which is misleading and causes unnecessary alarm in the logs.",
            "Resolution": "Modify the DataXceiver#run() method to catch the InvalidToken exception and log it as a warning instead of an error. This will align the logging behavior with the existing checkAccess() method."
        }
    },
    {
        "filename": "HDFS-13635.json",
        "creation_time": "2018-05-29T19:54:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-13635",
            "Title": "Incorrect Error Message When Block is Not Found in DataNode",
            "Description": "When a client attempts to open a file, the DataNode checks the visible length of the blocks. If a block is not found on the DataNode, it incorrectly throws a 'Cannot append to a non-existent replica' error message. This is misleading, as the method `getReplicaVisibleLength()` is used for purposes other than appending to a block. The expected behavior is to return a message indicating that the block is not found.",
            "StackTrace": [
                "2018-05-29 09:23:41,966 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getReplicaVisibleLength from 10.0.0.14:53217 Call#38334117 Retry#0",
                "org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop DataNode service.",
                "2. Attempt to open a file that references a block not present on the DataNode.",
                "3. Observe the error message returned."
            ],
            "ExpectedBehavior": "The system should return a message indicating that the block is not found.",
            "ObservedBehavior": "The system returns an incorrect error message: 'Cannot append to a non-existent replica'.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-11608.json",
        "creation_time": "2017-03-31T17:37:20.000+0000",
        "bug_report": {
            "BugID": "HDFS-11608",
            "Title": "HDFS Write Operation Fails with Out of Memory Exception for Block Sizes Greater than 2 GB",
            "Description": "When attempting to write files larger than 2 GB using HDFS with a block size greater than 2 GB, the HDFS client encounters an OutOfMemoryError. This is followed by an IOException from the DataNode indicating an incorrect packet payload size. The issue arises due to the handling of large block sizes in the DataXceiver component of HDFS.",
            "StackTrace": [
                "2017-03-30 16:34:33,828 ERROR datanode.DataNode (DataXceiver.java:run(278)) - c6401.ambari.apache.org:50010:DataXceiver error processing WRITE_BLOCK operation src: /192.168.64.101:47167 dst: /192.168.64.101:50010",
                "java.io.IOException: Incorrect value for packet payload size: 2147483128",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:898)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up an HDFS cluster with default configurations.",
                "2. Attempt to write a file larger than 2 GB with a block size set to 3 GB.",
                "3. Monitor the logs for any exceptions thrown during the write operation."
            ],
            "ExpectedBehavior": "The HDFS client should successfully write the file without encountering memory issues or exceptions related to packet payload size.",
            "ObservedBehavior": "The HDFS client throws an OutOfMemoryError, and the DataNode logs an IOException indicating an incorrect packet payload size.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HDFS-12638.json",
        "creation_time": "2017-10-11T11:29:04.000+0000",
        "bug_report": {
            "BugID": "HDFS-12638",
            "Title": "NullPointerException in ReplicationMonitor due to null BlockCollection",
            "Description": "The Active NameNode encounters a NullPointerException (NPE) when attempting to process replication work. The issue arises because the BlockCollection passed to the ReplicationWork is null. This situation leads to a failure in the ReplicationMonitor thread, which is critical for maintaining data replication in HDFS. The root cause appears to be related to changes made in HDFS-9754, which removed checks for null BlockCollections.",
            "StackTrace": [
                "2017-10-11 16:29:06,161 ERROR [ReplicationMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)",
                "    at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": [
                "1. Start the HDFS NameNode.",
                "2. Trigger a replication event that requires the ReplicationMonitor to process blocks.",
                "3. Observe the logs for any NullPointerExceptions related to BlockCollection."
            ],
            "ExpectedBehavior": "The ReplicationMonitor should process replication work without encountering a NullPointerException, ensuring that all blocks are replicated as needed.",
            "ObservedBehavior": "The ReplicationMonitor throws a NullPointerException when it attempts to choose targets for replication due to a null BlockCollection, leading to a failure in the replication process.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-12383.json",
        "creation_time": "2017-08-31T20:43:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-12383",
            "Title": "Re-encryption Updater Fails to Handle Canceled Tasks Gracefully",
            "Description": "The re-encryption updater in the Hadoop HDFS fails to handle canceled tasks properly, leading to an exit of the updater thread and preventing subsequent tasks from executing. This issue was observed during the re-encryption process of an encryption zone.",
            "StackTrace": [
                "java.util.concurrent.CancellationException",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:121)",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:192)",
                "at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)",
                "at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up an encryption zone in Hadoop HDFS.",
                "2. Initiate the re-encryption process for the zone.",
                "3. Cancel the re-encryption task while it is in progress.",
                "4. Observe the behavior of the re-encryption updater."
            ],
            "ExpectedBehavior": "The re-encryption updater should handle canceled tasks without exiting, allowing subsequent tasks to continue processing.",
            "ObservedBehavior": "The re-encryption updater exits upon encountering a canceled task, preventing any further tasks from executing.",
            "Resolution": "The re-encryption updater should be modified to handle canceled tasks more gracefully, ensuring that it does not exit unexpectedly."
        }
    },
    {
        "filename": "HDFS-5322.json",
        "creation_time": "2013-10-07T22:52:54.000+0000",
        "bug_report": {
            "BugID": "HDFS-5322",
            "Title": "HDFS Delegation Token Not Found in Cache Errors on Secure HA Clusters",
            "Description": "During the execution of High Availability (HA) tests, we encountered errors indicating that the HDFS delegation token could not be found in the cache. This issue leads to job failures, which significantly impacts the reliability of the system in secure HA configurations.",
            "StackTrace": [
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1300)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)"
            ],
            "StepsToReproduce": [
                "1. Set up a secure High Availability (HA) cluster with HDFS.",
                "2. Run a job that requires HDFS delegation tokens.",
                "3. Monitor the logs for any errors related to delegation tokens."
            ],
            "ExpectedBehavior": "The HDFS delegation token should be successfully retrieved from the cache, allowing the job to complete without errors.",
            "ObservedBehavior": "The job fails with an error indicating that the HDFS delegation token cannot be found in the cache, leading to job failures.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-11741.json",
        "creation_time": "2017-05-02T17:57:25.000+0000",
        "bug_report": {
            "BugID": "HDFS-11741",
            "Title": "Long Running Balancer Fails Due to Expired DataEncryptionKey",
            "Description": "A long-running balancer may fail to move blocks due to the KeyManager returning an expired DataEncryptionKey. This results in an InvalidEncryptionKeyException being thrown, which interrupts the block movement process.",
            "StackTrace": [
                "2017-04-30 05:03:58,661 WARN  [pool-1464-thread-10] balancer.Dispatcher (Dispatcher.java:dispatch(325)) - Failed to move blk_1067352712_3913241 with size=546650 from 10.0.0.134:50010:DISK to 10.0.0.98:50010:DISK through 10.0.0.134:50010",
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with Kerberos authentication and Data transfer encryption enabled.",
                "2. Start a long-running balancer process using a keytab for authentication.",
                "3. Allow the balancer to run for more than 20 hours.",
                "4. Monitor the logs for any warnings or exceptions related to block movement."
            ],
            "ExpectedBehavior": "The balancer should successfully move blocks without encountering any exceptions related to encryption keys.",
            "ObservedBehavior": "The balancer fails to move blocks after a prolonged period due to an InvalidEncryptionKeyException, indicating that the required block key does not exist.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-3936.json",
        "creation_time": "2012-09-14T05:35:12.000+0000",
        "bug_report": {
            "BugID": "HDFS-3936",
            "Title": "Race Condition During MiniDFSCluster Shutdown Causes NullPointerException in BlockManager",
            "Description": "During the shutdown of MiniDFSCluster, a race condition occurs between the BlockManager and the NameNode, leading to a NullPointerException. The BlockManager attempts to access block collections while the NameNode is holding a lock, resulting in an unexpected exit of the test.",
            "StackTrace": [
                "2012-09-13 18:54:12,526 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit",
                "org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start a MiniDFSCluster instance.",
                "2. Trigger a shutdown of the MiniDFSCluster while replication tasks are pending.",
                "3. Observe the logs for any fatal exceptions or unexpected exits."
            ],
            "ExpectedBehavior": "The MiniDFSCluster should shut down gracefully without throwing any exceptions, and all replication tasks should complete successfully.",
            "ObservedBehavior": "The MiniDFSCluster experiences a fatal exit due to a NullPointerException caused by a race condition between the BlockManager and the NameNode during shutdown.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-6348.json",
        "creation_time": "2014-05-07T12:34:54.000+0000",
        "bug_report": {
            "BugID": "HDFS-6348",
            "Title": "SecondaryNameNode Fails to Terminate on RuntimeException During Startup",
            "Description": "The SecondaryNameNode process does not terminate when a RuntimeException occurs during startup due to invalid configuration. This issue arises because the RMI thread remains alive, preventing the JVM from exiting. The attached thread dump provides further details on the state of the threads at the time of the error.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)",
                "Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)",
                "\tat ... 6 more",
                "Caused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)",
                "\tat ... 7 more"
            ],
            "StepsToReproduce": [
                "1. Configure the SecondaryNameNode with an invalid configuration that leads to a RuntimeException.",
                "2. Start the SecondaryNameNode process.",
                "3. Observe that the process does not terminate despite the RuntimeException."
            ],
            "ExpectedBehavior": "The SecondaryNameNode should terminate gracefully when a RuntimeException occurs during startup due to invalid configuration.",
            "ObservedBehavior": "The SecondaryNameNode process remains alive, and the JVM does not exit because the RMI thread is not a daemon thread.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-7884.json",
        "creation_time": "2015-03-04T15:47:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-7884",
            "Title": "NullPointerException in BlockSender during Block Read Operation",
            "Description": "A NullPointerException occurs in the BlockSender class when attempting to read a block. This issue arises from a failure to properly handle a null reference when obtaining a volume reference for the block.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS datanode service.",
                "2. Attempt to read a block that is not properly initialized or does not exist.",
                "3. Observe the logs for a NullPointerException in the BlockSender class."
            ],
            "ExpectedBehavior": "The system should handle the block read operation gracefully, either by returning an appropriate error message or by ensuring that the block reference is valid.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the datanode to fail in processing the read request.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-7996.json",
        "creation_time": "2015-03-26T21:19:55.000+0000",
        "bug_report": {
            "BugID": "HDFS-7996",
            "Title": "ReplicaNotFoundException Thrown When Removing Disk from Active DataNode",
            "Description": "When a disk is removed from an actively writing DataNode, the BlockReceiver encounters a ReplicaNotFoundException due to the removal of replicas from memory. This occurs because the FsVolumeList#removeVolume method waits for all threads to release the FsVolumeReference on the volume being removed, but the PacketResponder#finalizeBlock method calls close() on the BlockReceiver before finalizing the block, leading to inconsistencies.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)",
                "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)",
                "org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)",
                "org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)",
                "java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a DataNode and ensure it is actively writing data.",
                "2. Remove a disk from the DataNode while it is writing.",
                "3. Monitor the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The DataNode should handle the removal of the disk gracefully without throwing a ReplicaNotFoundException.",
            "ObservedBehavior": "The DataNode throws a ReplicaNotFoundException, indicating that it cannot append to a non-existent replica after the disk removal.",
            "Resolution": "A fix for this issue has been checked into the tree and tested, ensuring that the BlockReceiver properly handles disk removal without throwing exceptions."
        }
    },
    {
        "filename": "HDFS-4302.json",
        "creation_time": "2012-12-11T19:44:59.000+0000",
        "bug_report": {
            "BugID": "HDFS-4302",
            "Title": "Fatal Exception During NameNode Startup Due to Early Precondition Check in EditLogFileInputStream",
            "Description": "When starting a NameNode in standby mode with DEBUG logging enabled, an IllegalStateException is thrown because the precondition in the EditLogFileInputStream's length() method is checked before the advertisedSize is initialized. This occurs when the EditLogFileInputStream is created but the HTTP client has not yet connected to the remote edit log server, leading to a failure in the NameNode startup process.",
            "StackTrace": [
                "2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join",
                "java.lang.IllegalStateException: must get input stream before length is available",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)"
            ],
            "StepsToReproduce": [
                "1. Start the NameNode in standby mode with DEBUG logging enabled.",
                "2. Ensure that the EditLogFileInputStream is created but the HTTP client has not yet connected to the remote edit log server.",
                "3. Observe the logs for the IllegalStateException."
            ],
            "ExpectedBehavior": "The NameNode should start successfully without throwing an exception, and the EditLogFileInputStream should correctly initialize its advertisedSize before being accessed.",
            "ObservedBehavior": "The NameNode fails to start and throws an IllegalStateException due to the precondition check in the EditLogFileInputStream's length() method being triggered before the advertisedSize is initialized.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-11849.json",
        "creation_time": "2017-05-18T11:31:48.000+0000",
        "bug_report": {
            "BugID": "HDFS-11849",
            "Title": "JournalNode Fails to Start Due to Kerberos Login Error Not Logged",
            "Description": "The JournalNode fails to start because of a Kerberos login failure, but the exception is not recorded in the log file. This issue can lead to confusion and difficulty in diagnosing startup problems.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)",
                "at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)"
            ],
            "StepsToReproduce": [
                "1. Configure the JournalNode with an incorrect Kerberos keytab file.",
                "2. Attempt to start the JournalNode.",
                "3. Observe the startup process and check the log files."
            ],
            "ExpectedBehavior": "The JournalNode should start successfully, and any login errors should be logged in the log file for troubleshooting.",
            "ObservedBehavior": "The JournalNode fails to start due to a Kerberos login error, but the error is not logged, making it difficult to diagnose the issue.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-4841.json",
        "creation_time": "2013-05-22T17:01:36.000+0000",
        "bug_report": {
            "BugID": "HDFS-4841",
            "Title": "Warning on ShutdownHook 'ClientFinalizer' Failure When Using FsShell with Secure WebHDFS",
            "Description": "When executing FsShell commands with the webhdfs:// URI while security is enabled, a warning is generated indicating that the ShutdownHook 'ClientFinalizer' has failed. This occurs despite the command completing successfully. The issue does not manifest when security is disabled.",
            "StackTrace": [
                "java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook",
                "at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)",
                "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)",
                "at org.apache.hadoop.security.token.Token.cancel(Token.java:382)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)",
                "at org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)",
                "at org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)",
                "at org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "StepsToReproduce": [
                "1. Ensure Hadoop security is enabled.",
                "2. Execute the command: `hadoop fs -ls webhdfs://<your-hdfs-uri>`.",
                "3. Observe the output for the warning message regarding the ShutdownHook 'ClientFinalizer'."
            ],
            "ExpectedBehavior": "The command should execute successfully without any warnings related to ShutdownHook failures.",
            "ObservedBehavior": "The command completes successfully but generates a warning: 'ShutdownHook 'ClientFinalizer' failed'.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-3384.json",
        "creation_time": "2012-05-08T09:01:10.000+0000",
        "bug_report": {
            "BugID": "HDFS-3384",
            "Title": "DataStreamer Thread Fails to Close Immediately on Pipeline Setup Failure",
            "Description": "When attempting to append data to a file in HDFS, if a block becomes corrupted and the DataStreamer fails to set up a pipeline for appending or recovery, it does not close the DataStreamer thread immediately. This can lead to further exceptions and resource leaks.",
            "StackTrace": [
                "2012-04-19 09:33:10,776 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1059)) - Exception in createBlockOutputStream",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1511)) - Error while syncing",
                "java.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)"
            ],
            "StepsToReproduce": [
                "1. Write a file to HDFS.",
                "2. Manually corrupt a block of the file.",
                "3. Call the append method on the file."
            ],
            "ExpectedBehavior": "The DataStreamer thread should close immediately upon failing to set up a pipeline for appending or recovery, preventing further exceptions and resource leaks.",
            "ObservedBehavior": "The DataStreamer thread continues to run, leading to a NullPointerException and an IOException indicating that all datanodes are bad.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-5657.json",
        "creation_time": "2013-12-11T21:59:46.000+0000",
        "bug_report": {
            "BugID": "HDFS-5657",
            "Title": "Race Condition in NFS Gateway Causes Writeback State Error",
            "Description": "A race condition between the NFS gateway's writeback executor thread and the new write handler thread can lead to a writeback state check failure. This issue manifests as an IllegalStateException when the asynchronous write context is in an invalid state, causing disruptions in data handling.",
            "StackTrace": [
                "2013-11-26 10:34:07,859 DEBUG nfs3.RpcProgramNfs3 (Nfs3Utils.java:writeChannel(113)) - WRITE_RPC_CALL_END______957880843",
                "2013-11-26 10:34:07,863 DEBUG nfs3.OpenFileCtx (OpenFileCtx.java:offerNextToWrite(832)) - The async write task has no pending writes, fileId: 30938",
                "2013-11-26 10:34:07,871 ERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-11-26 10:34:07,901 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:write(707)) - requested offset=917504 and current filesize=917504",
                "2013-11-26 10:34:07,902 DEBUG nfs3.WriteManager (WriteManager.java:handleWrite(131)) - handleWrite fileId: 30938 offset: 917504 length:65536 stableHow:0"
            ],
            "StepsToReproduce": [
                "1. Initiate a write operation on the NFS gateway.",
                "2. Simultaneously trigger multiple write tasks that interact with the same fileId.",
                "3. Monitor the logs for any IllegalStateException related to the async status of the openFileCtx."
            ],
            "ExpectedBehavior": "The NFS gateway should handle concurrent write operations without throwing an IllegalStateException, ensuring that the writeback state is correctly maintained.",
            "ObservedBehavior": "The NFS gateway throws an IllegalStateException indicating that the openFileCtx has a false async status, leading to a failure in the writeback process.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the race condition is resolved and the writeback state is correctly managed."
        }
    },
    {
        "filename": "HDFS-11827.json",
        "creation_time": "2017-05-16T06:28:49.000+0000",
        "bug_report": {
            "BugID": "HDFS-11827",
            "Title": "NullPointerException in BlockPlacementPolicyDefault.chooseRandom() when changing log level",
            "Description": "A NullPointerException (NPE) is thrown in the BlockPlacementPolicyDefault class when the log level is changed using the 'hadoop daemonlog' command. This issue occurs specifically in the chooseRandom() method, which lacks necessary null checks. The problem was identified during a log level change by a colleague.",
            "StackTrace": [
                "2017-05-16 13:36:51,053 ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Change the log level of BlockPlacementPolicy using the command: 'hadoop daemonlog -setlevel <daemon> DEBUG'.",
                "2. Monitor the logs for any exceptions thrown.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The log level should change without causing any exceptions in the BlockPlacementPolicyDefault class.",
            "ObservedBehavior": "A NullPointerException is thrown, leading to potential instability in the BlockManager's replication process.",
            "Resolution": "Add null checks in the chooseRandom() method of BlockPlacementPolicyDefault to prevent NullPointerExceptions."
        }
    },
    {
        "filename": "HDFS-6804.json",
        "creation_time": "2014-08-01T05:37:23.000+0000",
        "bug_report": {
            "BugID": "HDFS-6804",
            "Title": "Checksum Error During Block Transfer Between Datanodes",
            "Description": "A checksum error occurs during the transfer of blocks between datanodes, leading to the source datanode's replica being incorrectly marked as corrupted. This issue arises when concurrent append and read operations are performed, resulting in an unexpected checksum mismatch.",
            "StackTrace": [
                "java.io.IOException: Terminating due to a checksum error.",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with multiple datanodes.",
                "2. Initiate a block write operation on one datanode while simultaneously reading from the same block.",
                "3. Monitor the logs for checksum errors during the block transfer."
            ],
            "ExpectedBehavior": "The block should be transferred successfully without any checksum errors, and the source datanode's replica should remain valid.",
            "ObservedBehavior": "The block transfer fails with a checksum error, causing the source datanode's replica to be marked as corrupted, even though it is valid.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-5843.json",
        "creation_time": "2014-01-28T06:05:16.000+0000",
        "bug_report": {
            "BugID": "HDFS-5843",
            "Title": "IOException Thrown by DFSClient.getFileChecksum() When Checksum is Disabled",
            "Description": "When a file is created with checksum disabled (using ChecksumOpt.disabled()), calling DFSClient.getFileChecksum() results in an IOException. The underlying issue appears to be related to incorrect arithmetic operations in the datanode, specifically due to a division by zero error when processing block checksums.",
            "StackTrace": [
                "java.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)",
                "2014-01-27 21:58:46,329 ERROR datanode.DataNode (DataXceiver.java:run(225)) - 127.0.0.1:52398:DataXceiver error processing BLOCK_CHECKSUM operation src: /127.0.0.1:52407 dest: /127.0.0.1:52398",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "1. Create a file in HDFS with checksum disabled using ChecksumOpt.disabled().",
                "2. Attempt to retrieve the file's checksum using DFSClient.getFileChecksum().",
                "3. Observe the resulting IOException."
            ],
            "ExpectedBehavior": "The system should return a valid checksum or indicate that the checksum is not available without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating failure to get block MD5, along with an ArithmeticException due to division by zero in the datanode.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-8070.json",
        "creation_time": "2015-04-06T19:57:58.000+0000",
        "bug_report": {
            "BugID": "HDFS-8070",
            "Title": "ShortCircuitCache fails to release shared memory slot due to network errors",
            "Description": "The HDFS ShortCircuitShm layer encounters issues during multi-threaded split-generation, leading to failures in releasing short-circuit shared memory slots. This problem appears to be exacerbated when a DataNode running version 2.8.0 interacts with a client running version 2.7.0, potentially due to incompatibilities in the ShortCircuitShim wire protocol.",
            "StackTrace": [
                "java.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38",
                "at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Upgrade the DataNode to version 2.8.0.",
                "2. Connect a client running version 2.7.0 to the upgraded DataNode.",
                "3. Execute a multi-threaded operation that requires split-generation."
            ],
            "ExpectedBehavior": "The ShortCircuitCache should successfully release shared memory slots without errors, allowing for efficient data access.",
            "ObservedBehavior": "The ShortCircuitCache fails to release shared memory slots, resulting in IOException errors and potential data access issues.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-1085.json",
        "creation_time": "2010-04-06T22:49:40.000+0000",
        "bug_report": {
            "BugID": "HDFS-1085",
            "Title": "HFTP Read Fails Silently with File Size Mismatch",
            "Description": "During a large data transfer using DistCp over HFTP, several tasks encountered failures due to a mismatch in the expected file size. The error indicates that the copied file size does not match the expected size, leading to incomplete data transfers without any clear error messages for the user.",
            "StackTrace": [
                "2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)",
                "but expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)",
                "at org.apache.hadoop.mapred.Child.main(Child.main:159)"
            ],
            "StepsToReproduce": [
                "1. Initiate a large data transfer using DistCp over HFTP.",
                "2. Monitor the logs for any file transfer failures.",
                "3. Observe the error messages indicating file size mismatches."
            ],
            "ExpectedBehavior": "The data transfer should complete successfully, with the copied file size matching the expected size.",
            "ObservedBehavior": "The data transfer fails with an IOException indicating that the copied file size does not match the expected size, resulting in incomplete data transfers.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-12339.json",
        "creation_time": "2017-08-23T03:16:49.000+0000",
        "bug_report": {
            "BugID": "HDFS-12339",
            "Title": "NFS Gateway Fails to Unregister with rpcbind Portmapper on Shutdown",
            "Description": "When stopping the NFS Gateway, an error is thrown in the NFS gateway role logs indicating an unregistration failure with the rpcbind portmapper. This issue prevents the NFS Gateway from properly unregistering, which may lead to resource leaks or conflicts in future operations.",
            "StackTrace": [
                "2017-08-17 18:09:16,529 ERROR org.apache.hadoop.oncrpc.RpcProgram: Unregistration failure with localhost:2049, portmap entry: (PortmapMapping-100003:3:6:2049)",
                "2017-08-17 18:09:16,531 WARN org.apache.hadoop.util.ShutdownHookManager: ShutdownHook 'NfsShutdownHook' failed, java.lang.RuntimeException: Unregistration failure",
                "java.lang.RuntimeException: Unregistration failure",
                "Caused by: java.net.SocketException: Socket is closed",
                "at java.net.DatagramSocket.send(DatagramSocket.java:641)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)"
            ],
            "StepsToReproduce": [
                "1. Start the NFS Gateway service.",
                "2. Attempt to stop the NFS Gateway service.",
                "3. Check the NFS gateway role logs for any errors."
            ],
            "ExpectedBehavior": "The NFS Gateway should unregister successfully with the rpcbind portmapper without throwing any errors.",
            "ObservedBehavior": "An error is logged indicating a failure to unregister with the rpcbind portmapper, specifically a SocketException stating 'Socket is closed'.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-6520.json",
        "creation_time": "2014-06-12T02:32:29.000+0000",
        "bug_report": {
            "BugID": "HDFS-6520",
            "Title": "HDFS fsck -move fails with 'Expected empty end-of-read packet' error",
            "Description": "When executing the command 'fsck -move' on a corrupted file in HDFS, an IOException is thrown indicating an expected empty end-of-read packet. This occurs after the fsck command identifies a corrupt block and attempts to move the corrupted file to the /lost+found directory.",
            "StackTrace": [
                "java.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536 seqno: 1 lastPacketInBlock: false dataLen: 65536",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)",
                "at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)"
            ],
            "StepsToReproduce": [
                "1. Set up a pseudo cluster.",
                "2. Copy a file to HDFS.",
                "3. Corrupt a block of the file.",
                "4. Run the command 'fsck' to check the file system.",
                "5. Execute 'fsck -move' to move the corrupted file to /lost+found."
            ],
            "ExpectedBehavior": "The corrupted file should be moved to the /lost+found directory without errors.",
            "ObservedBehavior": "An IOException is thrown with the message 'Expected empty end-of-read packet' when attempting to move the corrupted file.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-10715.json",
        "creation_time": "2016-08-02T08:10:23.000+0000",
        "bug_report": {
            "BugID": "HDFS-10715",
            "Title": "NullPointerException in AvailableSpaceBlockPlacementPolicy when choosing DataNode",
            "Description": "The implementation of the AvailableSpaceBlockPlacementPolicy introduced in HDFS-8131 is causing a NullPointerException (NPE) under certain conditions. The issue arises when the method `chooseDataNode` attempts to compare DataNode descriptors, and one of the descriptors is null. This leads to a failure in the block placement process, impacting the stability of the HDFS system.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)",
                "    at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)",
                "    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)",
                "    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)",
                "    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)"
            ],
            "StepsToReproduce": [
                "1. Deploy HDFS with the AvailableSpaceBlockPlacementPolicy enabled.",
                "2. Attempt to add a block to a file when the DataNode descriptors are not fully populated.",
                "3. Monitor the namenode logs for any NullPointerExceptions."
            ],
            "ExpectedBehavior": "The system should successfully choose a DataNode for block placement without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the `chooseDataNode` method attempts to compare a null DataNode descriptor, causing the block placement operation to fail.",
            "Resolution": "The issue can be resolved by adding a null check in the `compareDataNode` method to handle cases where one of the DataNode descriptors is null. Additionally, implementing a retry mechanism for block placement could mitigate the impact of this bug."
        }
    },
    {
        "filename": "HDFS-3332.json",
        "creation_time": "2012-04-27T04:42:38.000+0000",
        "bug_report": {
            "BugID": "HDFS-3332",
            "Title": "NullPointerException in DataNode during DirectoryScanner's Bad Block Reporting",
            "Description": "A NullPointerException (NPE) occurs in the DataNode when the DirectoryScanner attempts to report bad blocks. This issue arises during the execution of the DirectoryScanner's run method, specifically when it calls the reportBadBlocks method, which leads to the NPE due to uninitialized DatanodeID.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)",
                "at org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)",
                "at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop cluster with 1 NameNode (NN) and 1 DataNode (DN) using HA configuration.",
                "2. Corrupt a block in the HDFS.",
                "3. Monitor the logs of the DataNode during the block reporting process."
            ],
            "ExpectedBehavior": "The DataNode should successfully report bad blocks without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown in the DataNode when the DirectoryScanner attempts to report bad blocks, causing the reporting process to fail.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-6130.json",
        "creation_time": "2014-03-20T07:15:47.000+0000",
        "bug_report": {
            "BugID": "HDFS-6130",
            "Title": "NullPointerException when initializing shared edits during Namenode upgrade with HA enabled",
            "Description": "When attempting to upgrade an old Hadoop cluster (version 0.20.2-cdh3u1) to a trunk instance with High Availability (HA) enabled, a NullPointerException (NPE) occurs during the execution of the command 'hdfs namenode -initializeSharedEdits'. This issue arises specifically when the Namenode tries to load the fsimage, leading to a failure in the upgrade process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)"
            ],
            "StepsToReproduce": [
                "1. Set up an old Hadoop cluster running version 0.20.2-cdh3u1.",
                "2. Configure the cluster for High Availability (HA).",
                "3. Execute the command 'hdfs namenode -initializeSharedEdits'."
            ],
            "ExpectedBehavior": "The command 'hdfs namenode -initializeSharedEdits' should execute successfully without any exceptions, allowing the upgrade process to complete.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the upgrade process to fail.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-2827.json",
        "creation_time": "2012-01-23T09:34:51.000+0000",
        "bug_report": {
            "BugID": "HDFS-2827",
            "Title": "Checkpointing Fails When Renaming Directory Above a File with Open Lease",
            "Description": "When executing a series of file system operations, including creating a directory and a file, followed by renaming the directory containing the file, the checkpointing process fails with an IOException. The error indicates that the system cannot find a matching entry in the namespace for the file that is still under construction.",
            "StackTrace": [
                "2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\\HDFS-1623\\hadoop-hdfs-project\\hadoop-hdfs\\build\\test\\data\\dfs\\name3",
                "java.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage.java:789)",
                "at java.lang.Thread.run(Unknown Source)"
            ],
            "StepsToReproduce": [
                "1. Execute the following operations in the Hadoop file system:",
                "   - fs.mkdirs(new Path('/test1'));",
                "   - FSDataOutputStream create = fs.create(new Path('/test/abc.txt')); // Do not close the stream",
                "   - fs.rename(new Path('/test/'), new Path('/test1/'));",
                "2. Wait for the checkpointing process to complete."
            ],
            "ExpectedBehavior": "The checkpointing process should complete successfully, saving the namespace without errors.",
            "ObservedBehavior": "The checkpointing process fails with an IOException indicating that it cannot find a matching entry in the namespace for the file that is still under construction.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-11056.json",
        "creation_time": "2016-10-25T23:00:33.000+0000",
        "bug_report": {
            "BugID": "HDFS-11056",
            "Title": "Checksum Error During Concurrent Append and Read Operations",
            "Description": "When two clients perform concurrent operations on the same file\u2014one continuously appending data while the other reads\u2014the reader eventually encounters a checksum error. This issue has been observed specifically with HTTPFS clients, but it is likely to affect other append clients as well. The error manifests as a mismatch between the expected and computed checksums, leading to data integrity concerns.",
            "StackTrace": [
                "Exception in thread \"Thread-144\" java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C",
                "at org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:309)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C",
                "at org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)",
                "at org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)",
                "at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)"
            ],
            "StepsToReproduce": [
                "1. Start two clients on the same machine.",
                "2. Have one client continuously open, append to, and close a file (e.g., /tmp/bar.txt).",
                "3. Simultaneously, have the second client continuously open, read from, and close the same file.",
                "4. Monitor the output for checksum errors."
            ],
            "ExpectedBehavior": "The reader should be able to read the data without encountering any checksum errors, regardless of concurrent write operations.",
            "ObservedBehavior": "The reader encounters a checksum error after a few minutes of concurrent operations, indicating a mismatch between the expected and computed checksums.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-6825.json",
        "creation_time": "2014-08-06T01:31:34.000+0000",
        "bug_report": {
            "BugID": "HDFS-6825",
            "Title": "FileNotFoundException during block synchronization due to delayed block removal",
            "Description": "A FileNotFoundException is thrown when attempting to synchronize a block for a file that has already been deleted. This occurs when a client attempts to append to a file after the lease has expired, leading to a race condition where the file is deleted but pending blocks are not properly cleaned up.",
            "StackTrace": [
                "java.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)",
                "at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)"
            ],
            "StepsToReproduce": [
                "1. Create a file at the path /solr/hierarchy/core_node1/data/tlog/tlog.xyz.",
                "2. Attempt to append data to the file after the lease has expired.",
                "3. Observe that the file is deleted during lease recovery.",
                "4. Trigger a block synchronization for the deleted file."
            ],
            "ExpectedBehavior": "The system should handle the deletion of the file gracefully and not attempt to synchronize blocks for a non-existent file.",
            "ObservedBehavior": "A FileNotFoundException is thrown when the system attempts to synchronize blocks for a file that has already been deleted.",
            "Resolution": "The issue has been fixed in version 2.6.0."
        }
    },
    {
        "filename": "HDFS-5710.json",
        "creation_time": "2014-01-01T04:06:03.000+0000",
        "bug_report": {
            "BugID": "HDFS-5710",
            "Title": "NullPointerException in FSDirectory#getFullPathName due to unhandled null inodes",
            "Description": "A NullPointerException occurs in the FSDirectory#getFullPathName method when the getRelativePathINodes() method returns null. The getFullPathName method does not check if the inodes are null before attempting to access them, leading to a runtime exception in the ReplicationMonitor thread.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)",
                "at org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop HDFS environment.",
                "2. Trigger a replication event that leads to the invocation of the FSDirectory#getFullPathName method.",
                "3. Ensure that the getRelativePathINodes() method returns null.",
                "4. Observe the logs for a NullPointerException in the ReplicationMonitor thread."
            ],
            "ExpectedBehavior": "The FSDirectory#getFullPathName method should handle null inodes gracefully and not throw a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the FSDirectory#getFullPathName method when inodes are null, causing the ReplicationMonitor thread to fail.",
            "Resolution": "A fix has been implemented to check for null inodes in the FSDirectory#getFullPathName method before accessing them."
        }
    },
    {
        "filename": "HDFS-3555.json",
        "creation_time": "2012-06-20T17:36:48.000+0000",
        "bug_report": {
            "BugID": "HDFS-3555",
            "Title": "Datanode logs SocketTimeoutException at ERROR level instead of INFO",
            "Description": "The Datanode service is logging a java.net.SocketTimeoutException at the ERROR level when the client stops reading data. This behavior is misleading as the exception does not indicate a critical failure and should be logged at the INFO level instead.",
            "StackTrace": [
                "2012-06-18 17:47:13 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode DatanodeRegistration(x.x.x.x:50010, storageID=DS-196671195-10.10.120.67-50010-1334328338972, infoPort=50075, ipcPort=50020):DataXceiver",
                "java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]",
                "at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)",
                "at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)",
                "at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)"
            ],
            "StepsToReproduce": [
                "1. Start the Datanode service.",
                "2. Initiate a data transfer from a client to the Datanode.",
                "3. Stop the client from reading data before the transfer completes.",
                "4. Observe the Datanode logs for any ERROR level messages."
            ],
            "ExpectedBehavior": "The Datanode should log the SocketTimeoutException at the INFO level, indicating that the client has stopped reading data without implying a critical error.",
            "ObservedBehavior": "The Datanode logs the SocketTimeoutException at the ERROR level, which may cause unnecessary alarm and misinterpretation of the system's health.",
            "Resolution": "The logging level for SocketTimeoutException in the Datanode should be changed from ERROR to INFO to reflect the non-critical nature of the event."
        }
    },
    {
        "filename": "HDFS-10962.json",
        "creation_time": "2016-10-05T04:31:45.000+0000",
        "bug_report": {
            "BugID": "HDFS-10962",
            "Title": "Flaky Test in TestRequestHedgingProxyProvider: Missing Invocation of getStats()",
            "Description": "The test method `testHedgingWhenOneFails` in `TestRequestHedgingProxyProvider` occasionally fails with a verification error indicating that the expected method `getStats()` was not invoked on the mock object. This suggests that the test is not reliably simulating the intended behavior of the `RequestHedgingProxyProvider` under failure conditions.",
            "StackTrace": [
                "org.mockito.exceptions.verification.WantedButNotInvoked: ",
                "Wanted but not invoked:",
                "namenodeProtocols.getStats();",
                "-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)",
                "Actually, there were zero interactions with this mock."
            ],
            "StepsToReproduce": [
                "Run the test suite for the Hadoop HDFS project.",
                "Observe the intermittent failure of the `testHedgingWhenOneFails` test case."
            ],
            "ExpectedBehavior": "The `testHedgingWhenOneFails` test should successfully invoke `getStats()` on both the good and bad mock instances of `NamenodeProtocols`, verifying that the proxy provider correctly handles the failure of one of the mocks.",
            "ObservedBehavior": "The test fails with an error indicating that `getStats()` was not invoked on the mock, leading to a verification failure.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-12363.json",
        "creation_time": "2017-08-28T05:34:21.000+0000",
        "bug_report": {
            "BugID": "HDFS-12363",
            "Title": "NullPointerException in BlockManager$StorageInfoDefragmenter.scanAndCompactStorages",
            "Description": "A NullPointerException (NPE) occurs in the BlockManager's StorageInfoDefragmenter during the execution of the scanAndCompactStorages method. This issue leads to the NameNode going down unexpectedly, impacting the stability of the Hadoop HDFS system.",
            "StackTrace": [
                "ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-08-21 22:14:05,303 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1",
                "2017-08-21 22:14:05,313 INFO org.apache.hadoop.hdfs.server.namenode.NameNode:"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS NameNode.",
                "2. Trigger a block report from the DataNodes.",
                "3. Monitor the logs for any exceptions thrown during the block management process."
            ],
            "ExpectedBehavior": "The NameNode should process block reports without throwing exceptions, maintaining stability and availability.",
            "ObservedBehavior": "The NameNode crashes with a NullPointerException, leading to service disruption.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-7916.json",
        "creation_time": "2015-03-11T10:40:00.000+0000",
        "bug_report": {
            "BugID": "HDFS-7916",
            "Title": "Infinite Loop in Reporting Bad Blocks to Standby Node in BPServiceActor",
            "Description": "When a bad block is detected, the BPServiceActor for the Standby Node enters an infinite loop while attempting to report the bad block. This occurs due to repeated failures in reporting the bad block to the Namenode, leading to excessive resource consumption and potential service degradation.",
            "StackTrace": [
                "2015-03-11 19:43:41,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode: stobdtserver3/10.224.54.70:18010",
                "org.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:",
                "at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop HDFS cluster with a Standby Node.",
                "2. Introduce a bad block in the data storage.",
                "3. Monitor the logs for the DataNode attempting to report the bad block to the Standby Node."
            ],
            "ExpectedBehavior": "The DataNode should successfully report the bad block to the Standby Node without entering an infinite loop.",
            "ObservedBehavior": "The DataNode fails to report the bad block, resulting in repeated warnings in the logs and an infinite loop in the BPServiceActor.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-9549.json",
        "creation_time": "2015-12-11T16:25:08.000+0000",
        "bug_report": {
            "BugID": "HDFS-9549",
            "Title": "Flaky Test: TestCacheDirectives.testExceedsCapacity Fails Due to Non-Empty Pending Cached List",
            "Description": "The test method `TestCacheDirectives.testExceedsCapacity` fails intermittently in Jenkins environments, specifically when checking if the pending cached list is empty. The error message indicates that the pending cached list is not empty, which suggests a potential issue with cache management in the Hadoop HDFS system.",
            "StackTrace": [
                "java.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.junit.Assert.assertTrue(Assert.java:41)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)"
            ],
            "StepsToReproduce": [
                "1. Set up a Jenkins environment with Hadoop HDFS.",
                "2. Run the test suite that includes `TestCacheDirectives`.",
                "3. Observe the test `testExceedsCapacity` for intermittent failures."
            ],
            "ExpectedBehavior": "The test `testExceedsCapacity` should pass without any assertion errors, indicating that the pending cached list is empty after the operations.",
            "ObservedBehavior": "The test `testExceedsCapacity` fails with an AssertionError indicating that the pending cached list is not empty, which suggests that the cache management is not functioning as expected.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-11164.json",
        "creation_time": "2016-11-22T05:58:03.000+0000",
        "bug_report": {
            "BugID": "HDFS-11164",
            "Title": "Mover Should Avoid Unnecessary Retries for Pinned Blocks",
            "Description": "When the Mover attempts to move a pinned block to another datanode, it encounters an IOException indicating that the block cannot be moved due to its pinned status. The Mover continues to retry moving the block until it reaches the maximum retry attempts, which is unnecessary since pinned blocks cannot be relocated. This behavior leads to wasted resources and delays in processing other block movements.",
            "StackTrace": [
                "2016-11-22 10:56:10,537 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: Failed to move blk_1073741825_1001 with size=52 from 127.0.0.1:19501:DISK to 127.0.0.1:19758:ARCHIVE through 127.0.0.1:19501",
                "java.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed",
                "\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Configure a datanode with a block that is pinned.",
                "2. Initiate a block movement using the Mover.",
                "3. Monitor the logs for any warnings or errors related to block movement."
            ],
            "ExpectedBehavior": "The Mover should recognize that the block is pinned and avoid retrying the movement, logging a message indicating that the block cannot be moved due to its pinned status.",
            "ObservedBehavior": "The Mover attempts to move the pinned block multiple times, resulting in repeated IOException warnings in the logs until the maximum retry attempts are reached.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-5291.json",
        "creation_time": "2013-10-02T23:52:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-5291",
            "Title": "Active NameNode enters SafeMode immediately after transition, causing client timeouts",
            "Description": "During testing, it was observed that the Active NameNode (NN) immediately enters SafeMode after transitioning from Standby to Active state. This behavior leads to HBase region servers timing out and subsequently killing themselves. The system should allow clients to retry operations when High Availability (HA) is enabled and the Active NameNode is in SafeMode.",
            "StackTrace": [
                "java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:356)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:296)"
            ],
            "StepsToReproduce": [
                "1. Set up a High Availability (HA) configuration with two NameNodes: one Active and one Standby.",
                "2. Trigger a transition from Standby to Active state for the NameNode.",
                "3. Monitor the logs for the Active NameNode during the transition.",
                "4. Observe the behavior of HBase region servers during this transition."
            ],
            "ExpectedBehavior": "Clients should be able to retry operations when the Active NameNode is in SafeMode, without causing timeouts or failures in HBase region servers.",
            "ObservedBehavior": "The Active NameNode enters SafeMode immediately after transitioning from Standby, leading to HBase region servers timing out and killing themselves.",
            "Resolution": "A fix for this issue has been implemented and tested, allowing clients to retry operations when the Active NameNode is in SafeMode."
        }
    },
    {
        "filename": "HDFS-12836.json",
        "creation_time": "2017-11-17T20:26:32.000+0000",
        "bug_report": {
            "BugID": "HDFS-12836",
            "Title": "In-progress Edit Log Tailing Can Cause Transaction ID Mismatch",
            "Description": "When the configuration parameter {{dfs.ha.tail-edits.in-progress}} is set to true, the edit log tailer attempts to process in-progress edit log segments. However, there is a potential issue in the code where the starting transaction ID of the remote log can exceed the ending transaction ID, leading to a mismatch and causing a premature end-of-file exception. This can result in the following error message:\n\n```\n2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576. Expected transaction ID was 87\nRecent opcode offsets: 1048576\norg.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85\n```\n\nThis issue arises from the following code snippet:\n\n```java\nif (onlyDurableTxns && inProgressOk) {\n    endTxId = Math.min(endTxId, committedTxnId);\n}\nEditLogInputStream elis = EditLogFileInputStream.fromUrl(\n    connectionFactory, url, remoteLog.getStartTxId(),\n    endTxId, remoteLog.isInProgress());\n```\n\nIn this code, if {{remoteLog.getStartTxId()}} is greater than {{endTxId}}, it leads to the aforementioned error.",
            "StackTrace": [
                "2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576. Expected transaction ID was 87",
                "Recent opcode offsets: 1048576",
                "org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)"
            ],
            "StepsToReproduce": [
                "Set the configuration parameter {{dfs.ha.tail-edits.in-progress}} to true.",
                "Start the edit log tailer.",
                "Ensure that the starting transaction ID of the remote log is greater than the ending transaction ID.",
                "Observe the logs for the error message indicating a transaction ID mismatch."
            ],
            "ExpectedBehavior": "The edit log tailer should process in-progress edit log segments without causing transaction ID mismatches or premature end-of-file exceptions.",
            "ObservedBehavior": "The edit log tailer throws a premature end-of-file exception when the starting transaction ID exceeds the ending transaction ID, leading to errors in processing the edit logs.",
            "Resolution": "A fix has been implemented to ensure that the starting transaction ID does not exceed the ending transaction ID when processing in-progress edit logs."
        }
    },
    {
        "filename": "HDFS-8113.json",
        "creation_time": "2015-04-09T11:29:31.000+0000",
        "bug_report": {
            "BugID": "HDFS-8113",
            "Title": "NullPointerException in BlockInfoContiguous Constructor When BlockCollection is Null",
            "Description": "The BlockInfoContiguous copy constructor can throw a NullPointerException if the BlockCollection (bc) is null. This issue has been observed in DataNodes failing to report blocks to the NameNode, leading to operational disruptions.",
            "StackTrace": [
                "2015-03-08 19:28:13,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService",
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)",
                "at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26382)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)"
            ],
            "StepsToReproduce": [
                "1. Ensure that a DataNode is configured to report blocks to the NameNode.",
                "2. Simulate a scenario where the BlockCollection (bc) is null during the block reporting process.",
                "3. Trigger the block report from the DataNode to the NameNode."
            ],
            "ExpectedBehavior": "The DataNode should successfully report blocks to the NameNode without throwing any exceptions.",
            "ObservedBehavior": "The DataNode throws a NullPointerException, causing it to fail in reporting blocks to the NameNode.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-10512.json",
        "creation_time": "2016-06-09T14:34:04.000+0000",
        "bug_report": {
            "BugID": "HDFS-10512",
            "Title": "NullPointerException in VolumeScanner due to missing volume reference in DataNode.reportBadBlocks",
            "Description": "The VolumeScanner may terminate unexpectedly due to a NullPointerException thrown in the DataNode's reportBadBlocks method. This issue has been observed in a production CDH 5.5.1 cluster and persists in the upstream trunk. The NPE appears to occur when the volume variable is not properly initialized or is null, leading to a failure in reporting bad blocks.",
            "StackTrace": [
                "2016-04-07 20:30:53,830 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-1800173197-10.204.68.5-1444425156296:blk_1170134484_96468685 on /dfs/dn",
                "2016-04-07 20:30:53,831 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting because of exception",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)",
                "at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS cluster with a DataNode configured.",
                "2. Trigger a volume scan that includes blocks marked as bad.",
                "3. Monitor the logs for any NullPointerException related to the VolumeScanner."
            ],
            "ExpectedBehavior": "The VolumeScanner should successfully report bad blocks without terminating unexpectedly.",
            "ObservedBehavior": "The VolumeScanner terminates with a NullPointerException when attempting to report bad blocks due to a missing volume reference.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-10729.json",
        "creation_time": "2016-08-08T02:36:05.000+0000",
        "bug_report": {
            "BugID": "12995526",
            "Title": "Standby NameNode Crashes Due to NullPointerException When Loading Edits Exceeding Directory Item Limit",
            "Description": "The Standby NameNode crashes with a NullPointerException (NPE) when attempting to load edits from the edit log, specifically when the number of items in a directory exceeds the configured limit. This issue arises during the execution of the `applyEditLogOp` method in the `FSEditLogLoader` class, leading to a failure in the NameNode's operation.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)",
                "    at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)",
                "    at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)",
                "    at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)"
            ],
            "StepsToReproduce": [
                "1. Configure a directory in HDFS with a large number of items (e.g., exceeding 1,048,576).",
                "2. Attempt to load edits into the Standby NameNode.",
                "3. Observe the logs for any NullPointerException related to file encryption info."
            ],
            "ExpectedBehavior": "The Standby NameNode should successfully load edits without crashing, even when the directory item limit is exceeded.",
            "ObservedBehavior": "The Standby NameNode crashes with a NullPointerException when attempting to load edits, preventing it from restarting.",
            "Resolution": "A potential workaround is to increase the value of `dfs.namenode.fs-limits.max-directory-items` to a higher threshold (e.g., 6,400,000). However, further investigation is needed to ensure this does not introduce side effects."
        }
    },
    {
        "filename": "HDFS-13040.json",
        "creation_time": "2018-01-19T19:47:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-13040",
            "Title": "Kerberized iNotify Client Fails to Retrieve Edits After Kerberos Ticket Expiration",
            "Description": "The iNotify client fails to retrieve edits from the NameNode after the Kerberos ticket has expired, even when valid credentials are present. This issue arises in a high-availability (HA) cluster setup where the client uses a different principal than the active NameNode, leading to authentication failures during edit log retrieval.",
            "StackTrace": [
                "18/01/19 11:23:02 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs@GCE.CLOUDERA.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3. During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one! The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684. If you continue, metadata will be lost forever!",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)",
                "at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)"
            ],
            "StepsToReproduce": [
                "1. Set up a Kerberized HA cluster with two NameNodes.",
                "2. Ensure that the iNotify client is configured to use a principal that is different from the active NameNode's principal.",
                "3. Start the NameNodes and allow them to run until the Kerberos ticket expires.",
                "4. Attempt to retrieve edits from the NameNode using the iNotify client."
            ],
            "ExpectedBehavior": "The iNotify client should successfully retrieve edits from the active NameNode, even after the Kerberos ticket has expired, by using the appropriate principal for authentication.",
            "ObservedBehavior": "The iNotify client fails to retrieve edits, resulting in a PrivilegedActionException due to authentication issues, as the NameNode cannot re-login on behalf of the client with a different principal.",
            "Resolution": "A patch has been created to allow the NameNode to use a proxy user to retrieve edits on behalf of the client, ensuring that the correct principal is used for authentication. This patch has been verified to work in a CDH5.10.2 cluster."
        }
    },
    {
        "filename": "HDFS-3374.json",
        "creation_time": "2012-05-04T22:08:19.000+0000",
        "bug_report": {
            "BugID": "HDFS-3374",
            "Title": "Race Condition in TestDelegationToken Causes Intermittent Failures",
            "Description": "The test case for TestDelegationToken fails intermittently due to a race condition where the MiniDFSCluster is shut down before the DelegationTokenSecretManager can update the master key, leading to a fatal exit with no edit streams available.",
            "StackTrace": [
                "2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1",
                "2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible",
                "java.lang.Exception: No edit streams are accessible",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)",
                "at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Set up a MiniDFSCluster for testing.",
                "2. Run the TestDelegationToken test case.",
                "3. Observe the test case failing intermittently due to the race condition."
            ],
            "ExpectedBehavior": "The TestDelegationToken should complete successfully without any fatal errors related to edit streams.",
            "ObservedBehavior": "The test case fails intermittently with a fatal error indicating that no edit streams are accessible, leading to a system exit.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-2359.json",
        "creation_time": "2011-09-23T00:20:53.000+0000",
        "bug_report": {
            "BugID": "HDFS-2359",
            "Title": "NullPointerException in Datanode Log During Disk Failure in HDFS Operation",
            "Description": "A NullPointerException is thrown in the Datanode log when a disk failure occurs during a distcp operation. This happens when three disks are intentionally failed on a Datanode by changing their permissions to 000, while a distcp job is running. The Datanode logs show multiple warnings and an error related to block deletion and processing commands.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Set up a cluster with 4 Datanodes, each having 12 disks.",
                "2. Configure 'dfs.datanode.failed.volumes.tolerated' to 3 in hdfs-site.xml.",
                "3. Start a distcp job using the command: $hadoop distcp /user/$HADOOPQA_USER/data1 /user/$HADOOPQA_USER/data3.",
                "4. In another terminal, fail 3 disks on one Datanode by executing: $ chmod 000 /xyz/{0,1,2}/hadoop/var/hdfs/data."
            ],
            "ExpectedBehavior": "The distcp job should complete successfully without any exceptions in the Datanode logs.",
            "ObservedBehavior": "The distcp job completes successfully, but the Datanode logs contain multiple warnings about block deletion failures and a NullPointerException.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-10986.json",
        "creation_time": "2016-10-08T00:50:36.000+0000",
        "bug_report": {
            "BugID": "HDFS-10986",
            "Title": "DFSAdmin Command Errors Suppressed Without Detailed Logging",
            "Description": "Certain subcommands in the DFSAdmin tool suppress IOException and provide minimal error messages to stderr, making it difficult for users to diagnose issues. For example, when attempting to connect to an unreachable DataNode, the error message is vague, and users cannot access the exception stack even with DEBUG logging enabled. This lack of detailed error reporting is not user-friendly and can lead to confusion.",
            "StackTrace": [
                "java.net.ConnectException: Connection refused",
                "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "\tat org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "\tat org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)"
            ],
            "StepsToReproduce": [
                "1. Start the HDFS service.",
                "2. Run the command: `hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9866`.",
                "3. Observe the output for error messages.",
                "4. Run the command: `hdfs dfsadmin -getDatanodeInfo localhost:9866`.",
                "5. Observe the output for error messages.",
                "6. Run the command: `hdfs dfsadmin -evictWriters 127.0.0.1:9866`.",
                "7. Check the exit status using `echo $?`."
            ],
            "ExpectedBehavior": "When a command fails due to an unreachable DataNode, the user should receive a detailed error message that includes the exception stack trace, allowing for easier diagnosis of the issue.",
            "ObservedBehavior": "The commands return vague error messages such as 'Datanode unreachable' without providing any stack trace or detailed error information, making it difficult for users to understand the underlying issue.",
            "Resolution": "Enhance the error handling in the DFSAdmin tool to ensure that exceptions are not suppressed and that detailed error messages, including stack traces, are logged appropriately. This can be achieved by modifying the `DFSAdmin.run` method to log exceptions at the DEBUG level and provide meaningful output to the user."
        }
    },
    {
        "filename": "HDFS-6455.json",
        "creation_time": "2014-05-28T18:01:15.000+0000",
        "bug_report": {
            "BugID": "HDFS-6455",
            "Title": "NFS: Missing Error Logging for Invalid Separator in dfs.nfs.exports.allowed.hosts",
            "Description": "When an invalid separator is used in the 'dfs.nfs.exports.allowed.hosts' property, the NFS server fails to start without logging the error in the NFS log file. Instead, the error is only printed in the nfs.out file, which can lead to confusion and difficulty in troubleshooting.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'",
                "at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)",
                "at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)",
                "at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)"
            ],
            "StepsToReproduce": [
                "1. Set the 'dfs.nfs.exports.allowed.hosts' property with an invalid separator.",
                "   Example: <property><name>dfs.nfs.exports.allowed.hosts</name><value>host1  ro:host2 rw</value></property>",
                "2. Restart the NFS server using the command: ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null host1 \"sudo su - -c \"/usr/lib/hadoop/sbin/hadoop-daemon.sh start nfs3\" hdfs\".",
                "3. Observe the output and logs."
            ],
            "ExpectedBehavior": "The NFS server should log an error message in the NFS log file indicating that the 'dfs.nfs.exports.allowed.hosts' property is incorrectly formatted.",
            "ObservedBehavior": "The NFS server fails to start, and the error is only printed in the nfs.out file, not in the NFS log file. This leads to a lack of visibility into the error.",
            "Resolution": "The issue has been fixed in version 2.6.0, where appropriate error logging has been added to the NFS log file for invalid entries in the 'dfs.nfs.exports.allowed.hosts' property."
        }
    },
    {
        "filename": "HDFS-2882.json",
        "creation_time": "2012-02-02T18:11:20.000+0000",
        "bug_report": {
            "BugID": "HDFS-2882",
            "Title": "DataNode Startup Fails to Halt on Block Pool Initialization Error",
            "Description": "When starting a DataNode (DN) on a machine that is completely out of space on one of its drives, the initialization of the block pool fails, but the DataNode continues to run. This leads to subsequent NullPointerExceptions (NPEs) when the DataNode attempts to perform block reports. The issue was observed on the HDFS-1623 branch and may also affect the trunk.",
            "StackTrace": [
                "2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-1297842002148)",
                "java.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)"
            ],
            "StepsToReproduce": [
                "1. Ensure the machine is completely out of space on one of its drives.",
                "2. Start the DataNode service.",
                "3. Observe the logs for initialization errors related to block pool."
            ],
            "ExpectedBehavior": "The DataNode should not start if the block pool initialization fails due to insufficient disk space.",
            "ObservedBehavior": "The DataNode starts despite the block pool initialization failure, leading to NPEs during block reports.",
            "Resolution": "A fix has been implemented to ensure that the DataNode does not start if the block pool initialization fails due to disk space issues."
        }
    },
    {
        "filename": "HDFS-5185.json",
        "creation_time": "2013-09-11T14:38:23.000+0000",
        "bug_report": {
            "BugID": "HDFS-5185",
            "Title": "DataNode Fails to Start Up When One Data Directory is Full",
            "Description": "The DataNode fails to initialize if one of the configured data directories is out of space, resulting in a fatal exception. This prevents the DataNode from starting up, even if other data directories are available for use.",
            "StackTrace": [
                "2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110",
                "java.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Configure a DataNode with multiple data directories.",
                "2. Fill one of the data directories to its maximum capacity.",
                "3. Attempt to start the DataNode service."
            ],
            "ExpectedBehavior": "The DataNode should start successfully, utilizing the available data directories, even if one is full.",
            "ObservedBehavior": "The DataNode fails to start, throwing a fatal exception due to the inability to create necessary directories in the full data directory.",
            "Resolution": "A fix has been implemented to allow the DataNode to start up using available data directories, even if one or more are full."
        }
    },
    {
        "filename": "HDFS-13164.json",
        "creation_time": "2018-02-17T00:40:21.000+0000",
        "bug_report": {
            "BugID": "HDFS-13164",
            "Title": "File Not Closed When Streamer Fails with DSQuotaExceededException",
            "Description": "When a file creation operation exceeds the disk space quota, the DataStreamer may fail with a DSQuotaExceededException. This results in the file being left in an open-for-write state, which can lead to resource leaks, such as unclosed streams and potential lease renewals. The close operation on the stream does not complete successfully due to the state of the streamer thread, which may not have reached the quota exception at the time of the close call.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB",
                "at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)",
                "at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1833)"
            ],
            "StepsToReproduce": [
                "1. Set a disk space quota on a directory in HDFS.",
                "2. Attempt to create a file that exceeds the set quota.",
                "3. Observe the behavior of the DataStreamer and the resulting file state."
            ],
            "ExpectedBehavior": "The file should be closed properly, and no open-for-write state should remain if the DataStreamer fails due to a quota exception.",
            "ObservedBehavior": "The file remains in an open-for-write state, leading to potential resource leaks and unclosed streams.",
            "Resolution": "A fix has been implemented to ensure that the close operation handles DSQuotaExceededException correctly, allowing for proper cleanup of resources."
        }
    },
    {
        "filename": "HDFS-11508.json",
        "creation_time": "2017-03-07T04:55:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-11508",
            "Title": "Bind Failure in SimpleTCPServer & Portmap Due to TIME_WAIT State",
            "Description": "The SimpleTCPServer and Portmap fail to bind to the specified port when the socket is in the TIME_WAIT state. This issue arises because the server attempts to bind to a port that is already in use, leading to a BindException. The socket options should be modified to include the setReuseAddress option to allow the server to reuse the port.",
            "StackTrace": [
                "2017-03-05 08:37:13,283 INFO  oncrpc.SimpleUdpServer (SimpleUdpServer.java:run(73)) - Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1",
                "2017-03-05 08:37:13,298 FATAL mount.MountdBase (MountdBase.java:startTCPServer(85)) - Failed to start the TCP server.",
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)",
                "at org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)",
                "at org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "at sun.nio.ch.Net.bind0(Native Method)",
                "at sun.nio.ch.Net.bind(Net.java:433)",
                "at sun.nio.ch.Net.bind(Net.java:425)",
                "at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)",
                "at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)",
                "at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)",
                "at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-03-05 08:37:13,304 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1",
                "2017-03-05 08:37:13,308 INFO  nfs3.Nfs3Base (LogAdapter.java:info(45)) - SHUTDOWN_MSG: \n/************************************************************"
            ],
            "StepsToReproduce": [
                "1. Start the SimpleTCPServer on port 4242.",
                "2. Ensure that the server attempts to bind to the same port while it is still in the TIME_WAIT state.",
                "3. Observe the logs for the bind failure message."
            ],
            "ExpectedBehavior": "The SimpleTCPServer should successfully bind to the specified port and start listening for incoming TCP requests.",
            "ObservedBehavior": "The SimpleTCPServer fails to bind to the specified port, resulting in a BindException due to the address already being in use.",
            "Resolution": "Modify the socket options in the SimpleTcpServer to include the setReuseAddress option, allowing the server to reuse the port even if it is in the TIME_WAIT state."
        }
    },
    {
        "filename": "HDFS-2991.json",
        "creation_time": "2012-02-23T02:06:51.000+0000",
        "bug_report": {
            "BugID": "HDFS-2991",
            "Title": "ClassCastException during Edit Log Replay in FSEditLogLoader",
            "Description": "During scale testing of the Hadoop trunk at revision r1291606, an IOException occurred while attempting to replay the edit log. The root cause was identified as a ClassCastException when trying to cast an INodeFile to an INodeFileUnderConstruction.",
            "StackTrace": [
                "java.io.IOException: Error replaying edit log at offset 1354251",
                "Recent opcode offsets: 1350014 1350176 1350312 1354251",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with the trunk version at r1291606.",
                "2. Perform scale testing that involves writing to the edit log.",
                "3. Attempt to replay the edit log."
            ],
            "ExpectedBehavior": "The edit log should replay successfully without any exceptions.",
            "ObservedBehavior": "An IOException occurs with a ClassCastException indicating a failure to cast INodeFile to INodeFileUnderConstruction.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-4404.json",
        "creation_time": "2013-01-15T01:43:17.000+0000",
        "bug_report": {
            "BugID": "HDFS-4404",
            "Title": "SocketTimeoutException when creating a file on a down NameNode",
            "Description": "When attempting to create a file in HDFS, if the first NameNode is down, a SocketTimeoutException occurs, preventing the file creation process. This issue arises due to the inability to establish a connection to the NameNode within the specified timeout period.",
            "StackTrace": [
                "java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1156)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)",
                "at $Proxy10.create(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)",
                "at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)",
                "at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)",
                "at test.TestLease.main(TestLease.java:45)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop HDFS environment with two NameNodes (NN1 and NN2) and three DataNodes (DN1, DN2, DN3).",
                "2. Bring down the first NameNode (NN1).",
                "3. Attempt to create a file in HDFS using the command or API that targets the NameNode.",
                "4. Observe the error message indicating a SocketTimeoutException."
            ],
            "ExpectedBehavior": "The system should automatically failover to the second NameNode (NN2) and allow the file creation to proceed without errors.",
            "ObservedBehavior": "The file creation fails with a SocketTimeoutException, indicating that the system could not connect to the down NameNode within the specified timeout.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-8276.json",
        "creation_time": "2015-04-28T10:11:35.000+0000",
        "bug_report": {
            "BugID": "HDFS-8276",
            "Title": "Namenode Startup Fails When Scrub Interval is Configured to Zero",
            "Description": "The Namenode fails to start when the configuration parameter *dfs.namenode.lazypersist.file.scrub.interval.sec* is set to zero. This is due to an IllegalArgumentException being thrown, indicating that the scrub interval must be non-zero. The expected behavior is that the scrubber should be disabled when the interval is set to zero, preventing the startup failure.",
            "StackTrace": [
                "2015-04-27 23:47:31,744 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.",
                "java.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)"
            ],
            "StepsToReproduce": [
                "1. Set the configuration parameter dfs.namenode.lazypersist.file.scrub.interval.sec to 0.",
                "2. Attempt to start the Namenode.",
                "3. Observe the startup failure and the error message in the logs."
            ],
            "ExpectedBehavior": "The Namenode should start successfully without any errors when the scrub interval is set to zero, as the scrubber should be disabled.",
            "ObservedBehavior": "The Namenode fails to start, throwing an IllegalArgumentException indicating that the scrub interval must be non-zero.",
            "Resolution": "A fix has been implemented to ensure that the scrubber is disabled when the scrub interval is set to zero, allowing the Namenode to start successfully."
        }
    },
    {
        "filename": "HDFS-12369.json",
        "creation_time": "2017-08-28T22:56:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-12369",
            "Title": "FileNotFoundException during NameNode startup due to unclosed file with snapshots",
            "Description": "The NameNode fails to start with a FileNotFoundException when it encounters a file that was not properly closed before deletion. This issue arises from the hard lease recovery mechanism that does not log a close operation for files that have snapshots, leading to inconsistencies in the edit log.",
            "StackTrace": [
                "2017-08-17 14:32:18,418 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.",
                "java.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS NameNode.",
                "2. Ensure there is a file in the HDFS that was not properly closed and has snapshots.",
                "3. Observe the logs for the FileNotFoundException."
            ],
            "ExpectedBehavior": "The NameNode should start successfully without throwing a FileNotFoundException.",
            "ObservedBehavior": "The NameNode fails to start and throws a FileNotFoundException indicating that the specified file does not exist.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-6462.json",
        "creation_time": "2014-05-29T00:19:03.000+0000",
        "bug_report": {
            "BugID": "HDFS-6462",
            "Title": "NFS fsstat Request Fails in Secure HDFS Environment Due to Kerberos Authentication Issues",
            "Description": "The fsstat request fails when executed in a secure HDFS environment, resulting in an IOException related to Kerberos authentication. This issue occurs when the NFS server is started as a user with insufficient Kerberos credentials.",
            "StackTrace": [
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)",
                "at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)",
                "at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal(RpcProgramNfs3.java:1961)"
            ],
            "StepsToReproduce": [
                "1. Create two users: UserA and UserB.",
                "2. Create a group named GroupB.",
                "3. Add UserB and root to GroupB, ensuring UserA is not a member.",
                "4. Configure the following properties in hdfs-site.xml:",
                "   <property>",
                "       <name>dfs.nfs.keytab.file</name>",
                "       <value>/tmp/keytab/UserA.keytab</value>",
                "   </property>",
                "   <property>",
                "       <name>dfs.nfs.kerberos.principal</name>",
                "       <value>UserA@EXAMPLE.COM</value>",
                "   </property>",
                "5. Configure the following properties in core-site.xml:",
                "   <property>",
                "       <name>hadoop.proxyuser.UserA.groups</name>",
                "       <value>GroupB</value>",
                "   </property>",
                "   <property>",
                "       <name>hadoop.proxyuser.UserA.hosts</name>",
                "       <value>*</value>",
                "   </property>",
                "6. Start the NFS server as UserA.",
                "7. Mount the NFS as the root user.",
                "8. Execute the command: df /tmp/tmp_mnt/"
            ],
            "ExpectedBehavior": "The command 'df /tmp/tmp_mnt/' should return the disk status without any errors.",
            "ObservedBehavior": "The command fails with an 'Input/output error' and the NFS logs indicate a failure in Kerberos authentication.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-5425.json",
        "creation_time": "2013-10-25T06:53:28.000+0000",
        "bug_report": {
            "BugID": "HDFS-5425",
            "Title": "NameNode Failure on Restart After Snapshot Operations",
            "Description": "When performing snapshot operations such as createSnapshot or renameSnapshot, the NameNode fails to restart properly, resulting in a fatal exception. This issue occurs specifically after the NameNode has been restarted following these operations.",
            "StackTrace": [
                "2013-10-24 21:07:03,040 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.IllegalStateException",
                "\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat$Loader.java:855)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat$Loader.java:350)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)",
                "2013-10-24 21:07:03,050 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1",
                "2013-10-24 21:07:03,052 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:"
            ],
            "StepsToReproduce": [
                "1. Perform snapshot operations such as createSnapshot or renameSnapshot on the NameNode.",
                "2. Restart the NameNode.",
                "3. Observe the logs for any fatal exceptions during the restart process."
            ],
            "ExpectedBehavior": "The NameNode should restart successfully without any fatal exceptions after performing snapshot operations.",
            "ObservedBehavior": "The NameNode fails to restart and logs a fatal exception related to an IllegalStateException during the join process.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-13145.json",
        "creation_time": "2018-02-13T19:22:56.000+0000",
        "bug_report": {
            "BugID": "HDFS-13145",
            "Title": "SBN Crash During Transition to ANN with In-Progress Edit Log Tailing Enabled",
            "Description": "When the Standby NameNode (SBN) transitions to the Active NameNode (ANN) while in-progress edit log tailing is enabled, it can lead to a crash due to an IllegalStateException. This occurs because the commit transaction ID lags behind the end transaction ID, causing the check in the openForWrite method to fail.",
            "StackTrace": [
                "java.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)",
                "at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:249)"
            ],
            "StepsToReproduce": [
                "1. Enable in-progress edit log tailing in the Hadoop configuration.",
                "2. Start the Standby NameNode (SBN).",
                "3. Trigger a transition from SBN to Active NameNode (ANN).",
                "4. Observe the logs for any IllegalStateException related to transaction IDs."
            ],
            "ExpectedBehavior": "The Standby NameNode should transition to the Active NameNode without crashing, and the commit transaction ID should be updated correctly.",
            "ObservedBehavior": "The Standby NameNode crashes with an IllegalStateException indicating that it cannot start writing at a specific transaction ID due to an available stream for reading.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HDFS-8807.json",
        "creation_time": "2015-07-22T20:45:55.000+0000",
        "bug_report": {
            "BugID": "HDFS-8807",
            "Title": "DataNode Fails to Start Due to Improper Handling of Spaces in dfs.datanode.data.dir Configuration",
            "Description": "When configuring the DataNode's storage directory, if a space is inadvertently added between the storage type and the file URI, the DataNode fails to start, resulting in a parsing error. This issue arises from the way the configuration is parsed, leading to an IllegalArgumentException.",
            "StackTrace": [
                "2015-07-22 13:34:36,812 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain",
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:204)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:170)",
                "at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)",
                "Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at java.net.URI$Parser.fail(URI.java:2829)",
                "at java.net.URI$Parser.checkChars(URI.java:3002)",
                "at java.net.URI$Parser.checkChar(URI.java:3012)",
                "at java.net.URI.parse(URI.java:3028)",
                "at java.net.URI.<init>(URI.java:753)",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:201)",
                "... 7 more"
            ],
            "StepsToReproduce": [
                "1. Open the configuration file for the DataNode.",
                "2. Add a space between the storage type and the file URI in the dfs.datanode.data.dir property.",
                "   Example: <value>[DISK] file://tmp/hadoop-aengineer/disk1/dfs/data</value>",
                "3. Start the DataNode.",
                "4. Observe the logs for any errors."
            ],
            "ExpectedBehavior": "The DataNode should start successfully without any parsing errors, regardless of the formatting of the dfs.datanode.data.dir property.",
            "ObservedBehavior": "The DataNode fails to start and logs an IllegalArgumentException due to an improperly formatted URI caused by the space in the configuration.",
            "Resolution": "A fix has been implemented to handle spaces correctly in the dfs.datanode.data.dir configuration, ensuring that the DataNode can start without errors."
        }
    },
    {
        "filename": "HDFS-3436.json",
        "creation_time": "2012-05-17T10:36:31.000+0000",
        "bug_report": {
            "BugID": "HDFS-3436",
            "Title": "Failure to Append to File When Adding New DataNode to Existing Pipeline",
            "Description": "When attempting to append to a file in a Hadoop cluster with an existing pipeline, the operation fails if one of the DataNodes in the pipeline is stopped. This issue arises during the execution of the `addDatanode2ExistingPipeline` method, leading to an IOException due to a bad connection acknowledgment.",
            "StackTrace": [
                "java.io.IOException: Bad connect ack with firstBadLink as *******:50010",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with 4 DataNodes.",
                "2. Write a file to 3 DataNodes (e.g., DN1, DN2, DN3).",
                "3. Stop DN3.",
                "4. Attempt to append to the file."
            ],
            "ExpectedBehavior": "The append operation should succeed, and the new DataNode should be added to the existing pipeline without errors.",
            "ObservedBehavior": "The append operation fails with an IOException indicating a bad connection acknowledgment and premature EOF.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    }
]