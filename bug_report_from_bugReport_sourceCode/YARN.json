[
    {
        "filename": "YARN-5918.json",
        "creation_time": "2016-11-20T14:19:00.000+0000",
        "bug_report": {
            "BugID": "YARN-5918",
            "Title": "NullPointerException during Opportunistic Container Allocation when NodeManager is Lost",
            "Description": "A NullPointerException occurs in the OpportunisticContainerAllocatorAMService when attempting to allocate resources after a NodeManager has been lost. This issue arises during the conversion of NodeId to RemoteNode, specifically when the node is not found in the scheduler context.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)",
                "at org.apache.hadoop.ipc.Server.call(Server.java:846)",
                "at org.apache.hadoop.ipc.Server.run(Server.java:789)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN cluster with multiple NodeManagers.",
                "2. Submit an application that requires resource allocation.",
                "3. Forcefully stop one of the NodeManagers while the application is running.",
                "4. Monitor the logs for the ResourceManager and look for allocation requests."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the loss of a NodeManager gracefully and continue to allocate resources from the remaining NodeManagers without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown in the OpportunisticContainerAllocatorAMService, causing the resource allocation process to fail.",
            "Resolution": "[Provide additional details about the fix or workaround]"
        }
    },
    {
        "filename": "YARN-8629.json",
        "creation_time": "2018-08-07T00:14:14.000+0000",
        "bug_report": {
            "BugID": "YARN-8629",
            "Title": "Container Cleanup Fails Due to Missing Cgroup Tasks File",
            "Description": "When an application fails to launch a container successfully, the cleanup process for the container also fails, resulting in a warning message indicating that the cgroup tasks file could not be found. This issue occurs in the CGroupsHandlerImpl class when attempting to read the tasks file for the container's cgroup.",
            "StackTrace": [
                "2018-08-06 03:28:20,351 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file.",
                "java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:93)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Attempt to launch a container that fails to start.",
                "2. Observe the logs for the warning message regarding the cgroup tasks file.",
                "3. Check the specified path for the tasks file to confirm it does not exist."
            ],
            "ExpectedBehavior": "The cleanup process for the container should successfully delete the cgroup and its associated tasks file, even if the container fails to launch.",
            "ObservedBehavior": "The cleanup process fails with a warning indicating that the cgroup tasks file could not be found, leading to incomplete cleanup of resources.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-4431.json",
        "creation_time": "2015-12-07T18:31:36.000+0000",
        "bug_report": {
            "BugID": "YARN-4431",
            "Title": "Unnecessary NodeManager Unregistration on Connection Failure to ResourceManager",
            "Description": "When the NodeManager (NM) fails to connect to the ResourceManager (RM), it retries the connection according to a defined policy. After exhausting the maximum retries, the NM attempts to unregister itself from the RM, which is unnecessary and leads to repeated connection attempts and potential resource leaks. This behavior can cause performance issues and delays in resource management.",
            "StackTrace": [
                "java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at sun.reflect.Constructor.newInstance(Constructor.java:408)",
                "at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1452)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1385)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)",
                "at com.sun.proxy.$Proxy74.unRegisterNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)",
                "at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)",
                "at sun.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:255)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)",
                "at com.sun.proxy.$Proxy75.unRegisterNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop(NodeStatusUpdaterImpl.java:245)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:377)"
            ],
            "StepsToReproduce": [
                "1. Start the NodeManager service.",
                "2. Ensure that the ResourceManager is not running or is unreachable.",
                "3. Observe the NodeManager logs for connection attempts to the ResourceManager.",
                "4. Note the repeated attempts to unregister the NodeManager after maximum retries are reached."
            ],
            "ExpectedBehavior": "The NodeManager should not attempt to unregister itself from the ResourceManager if it is shutting down due to connection issues.",
            "ObservedBehavior": "The NodeManager attempts to unregister itself from the ResourceManager after failing to connect, leading to unnecessary retries and potential resource management issues.",
            "Resolution": "Modify the NodeManager's shutdown logic to skip the unregistration process when the shutdown is due to connection issues with the ResourceManager."
        }
    },
    {
        "filename": "YARN-2273.json",
        "creation_time": "2014-07-10T18:38:53.000+0000",
        "bug_report": {
            "BugID": "YARN-2273",
            "Title": "NullPointerException in ContinuousScheduling when Node is Lost",
            "Description": "A NullPointerException occurs in the ContinuousScheduling thread of the FairScheduler when a node is lost, leading to a failure in scheduling containers. This issue arises when the ResourceManager attempts to compare available resources of nodes that may no longer exist in the cluster.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)",
                "at java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)",
                "at java.util.TimSort.sort(TimSort.java:203)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN cluster with multiple nodes.",
                "2. Allow a node to experience memory errors and enter a cycle of rebooting and rejoining the cluster.",
                "3. Monitor the ResourceManager logs for any NullPointerExceptions during the continuous scheduling process."
            ],
            "ExpectedBehavior": "The ResourceManager should gracefully handle the loss of a node and continue scheduling containers without throwing exceptions.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException in the ContinuousScheduling thread, causing scheduling to fail and containers not to be assigned, leading to a halt in job progress.",
            "Resolution": "A fix has been implemented in version 2.6.0 to handle node loss more gracefully and prevent NullPointerExceptions during scheduling."
        }
    },
    {
        "filename": "YARN-2834.json",
        "creation_time": "2014-11-09T06:07:01.000+0000",
        "bug_report": {
            "BugID": "YARN-2834",
            "Title": "ResourceManager Crashes with Null Pointer Exception During Application Attempt Recovery",
            "Description": "The ResourceManager fails to recover application attempts after a restart, resulting in a Null Pointer Exception. This issue occurs when the ResourceManager attempts to add an application attempt that has not been properly initialized, leading to a crash.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1005)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:843)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:701)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1091)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1226)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Submit an application to the ResourceManager.",
                "3. Allow the application to fail and trigger a recovery attempt.",
                "4. Restart the ResourceManager.",
                "5. Observe the logs for Null Pointer Exception during application attempt recovery."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover application attempts without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a Null Pointer Exception during the recovery of application attempts.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-370.json",
        "creation_time": "2013-02-01T04:02:58.000+0000",
        "bug_report": {
            "BugID": "YARN-370",
            "Title": "CapacityScheduler Application Submission Fails Due to Resource Allocation Mismatch",
            "Description": "When running the CapacityScheduler with a minimum allocation size that is not a multiple of the Application Master (AM) size, the application submission fails. The error indicates an unauthorized request to start the container due to a mismatch in expected and actual resource allocations.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unauthorized request to start container. Expected resource <memory:2048, vCores:1> but found <memory:1536, vCores:1>",
                "at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:383)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:400)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:68)",
                "at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1735)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1731)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1729)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)",
                "at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)",
                "at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:123)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:109)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:255)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Configure the CapacityScheduler with a minimum allocation size of 1G.",
                "2. Set the Application Master size to 1.5G.",
                "3. Ensure that no resource calculator is specified, allowing the DefaultResourceCalculator to be used.",
                "4. Submit an application to the CapacityScheduler."
            ],
            "ExpectedBehavior": "The application should launch successfully without any resource allocation errors.",
            "ObservedBehavior": "The application fails to launch with an error indicating a mismatch in expected and actual resource allocations, specifically stating 'Unauthorized request to start container.'",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-3675.json",
        "creation_time": "2015-05-18T22:38:39.000+0000",
        "bug_report": {
            "BugID": "12830853",
            "Title": "NullPointerException in FairScheduler when handling APP_ATTEMPT_REMOVED event during node removal",
            "Description": "The FairScheduler encounters a NullPointerException when attempting to handle the APP_ATTEMPT_REMOVED event while a node is being removed. This issue arises specifically when continuous scheduling is enabled, leading to the ResourceManager quitting unexpectedly.",
            "StackTrace": [
                "12:28:53.782 AM FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                "Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserve(FSAppAttempt.java:469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:815)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:763)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Enable continuous scheduling in the FairScheduler.",
                "2. Add a node to the cluster.",
                "3. Submit an application that reserves resources on the added node.",
                "4. Remove the node while the application is still running and resources are reserved."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the APP_ATTEMPT_REMOVED event gracefully without throwing a NullPointerException, allowing the system to continue functioning normally.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when handling the APP_ATTEMPT_REMOVED event during node removal, leading to a complete shutdown of the service.",
            "Resolution": "A fix has been implemented in the code to ensure that the unreserve method checks for null references before proceeding with resource unreservation. This change has been tested and is included in the upcoming release."
        }
    },
    {
        "filename": "YARN-4763.json",
        "creation_time": "2016-03-04T10:03:56.000+0000",
        "bug_report": {
            "BugID": "YARN-4763",
            "Title": "NullPointerException on RMApps Page Rendering",
            "Description": "The RMApps page crashes with a NullPointerException when attempting to render application data. This issue occurs when the application state is NEW and the application attempts are empty, leading to a failure in accessing the blacklisted nodes.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)",
                "at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet._(Hamlet.java:30354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics.render(AppsBlockWithMetrics.java:30)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:848)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:156)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)"
            ],
            "StepsToReproduce": [
                "1. Access the RMApps page in the YARN web UI.",
                "2. Ensure that there are applications in the NEW state with no attempts.",
                "3. Observe the page behavior."
            ],
            "ExpectedBehavior": "The RMApps page should render without errors, displaying application information correctly, even if some applications are in the NEW state with no attempts.",
            "ObservedBehavior": "The RMApps page crashes with a NullPointerException, preventing the display of application data.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8202.json",
        "creation_time": "2018-04-24T15:52:00.000+0000",
        "bug_report": {
            "BugID": "YARN-8202",
            "Title": "Invalid Resource Request Exception Due to Incorrect Resource Unit Handling in DefaultAMSProcessor",
            "Description": "When executing a YARN job with specific resource requests, an InvalidResourceRequestException is thrown due to the DefaultAMSProcessor not properly validating custom resource types against their minimum and maximum allocation limits. This issue arises particularly when the resource units are not taken into account during validation.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation. Requested resource=<memory:200, vCores:1, resource1: 500M>, maximum allowed allocation=<memory:6144, vCores:8, resource1: 5G>",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)",
                "at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:433)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)"
            ],
            "StepsToReproduce": [
                "1. Execute a YARN job with the following arguments: -Dmapreduce.map.resource.memory-mb=200 -Dmapreduce.map.resource.resource1=500M 1 1000.",
                "2. Ensure that there is one node with 5GB of resource1 available.",
                "3. Observe the job execution and note the exception thrown."
            ],
            "ExpectedBehavior": "The job should execute successfully without throwing an InvalidResourceRequestException, as the requested resources are within the limits of the available resources.",
            "ObservedBehavior": "The job hangs and throws an InvalidResourceRequestException, indicating that the requested resource type is less than 0 or greater than the maximum allowed allocation.",
            "Resolution": "The issue is related to the validation logic in the DefaultAMSProcessor, specifically in the SchedulerUtils.validateResourceRequest method, which does not account for resource units correctly. A fix should ensure that resource units are properly validated against the maximum allowed allocation."
        }
    },
    {
        "filename": "YARN-7118.json",
        "creation_time": "2017-08-29T12:04:01.000+0000",
        "bug_report": {
            "BugID": "YARN-7118",
            "Title": "NullPointerException in AHS REST API when fetching application history",
            "Description": "The ApplicationHistoryService REST API returns a NullPointerException when attempting to fetch application history. This issue occurs when the API is called without the necessary parameters or when the application history is not available, leading to an internal server error.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN application history service.",
                "2. Execute the following curl command: curl --negotiate -u: 'http://<ATS IP>:8188/ws/v1/applicationhistory/apps?queue=test'",
                "3. Observe the response from the API."
            ],
            "ExpectedBehavior": "The API should return a valid JSON response containing the application history or an appropriate error message indicating that no applications are found.",
            "ObservedBehavior": "The API returns a NullPointerException, resulting in an internal server error.",
            "Resolution": "A fix for this issue has been checked into the tree and tested. Ensure that the API handles cases where application history is not available to prevent NullPointerExceptions."
        }
    },
    {
        "filename": "YARN-4743.json",
        "creation_time": "2016-02-27T09:12:28.000+0000",
        "bug_report": {
            "BugID": "YARN-4743",
            "Title": "FairSharePolicy Violates TimSort Contract Leading to IllegalArgumentException",
            "Description": "The FairSharePolicy implementation in Hadoop YARN is causing a fatal error due to a violation of the general contract of the comparison method used in TimSort. This occurs when both memorySize and weight are zero, resulting in a NaN value during the sorting process.",
            "StackTrace": [
                "2016-02-26 14:08:50,821 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:868)",
                "at java.util.TimSort.mergeAt(TimSort.java:485)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:410)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-02-26 14:08:50,822 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN environment with Fair Scheduler enabled.",
                "2. Configure a queue with zero memory size and zero weight.",
                "3. Trigger a NODE_UPDATE event to the scheduler."
            ],
            "ExpectedBehavior": "The scheduler should handle the NODE_UPDATE event without throwing an exception, allowing for normal operation.",
            "ObservedBehavior": "The scheduler throws an IllegalArgumentException due to a violation of the comparison method contract in TimSort, causing the ResourceManager to exit unexpectedly.",
            "Resolution": "The issue has been identified and fixed in version 2.9.0. Ensure to update to this version or later to avoid this problem."
        }
    },
    {
        "filename": "YARN-2414.json",
        "creation_time": "2014-08-12T23:48:48.000+0000",
        "bug_report": {
            "BugID": "YARN-2414",
            "Title": "NullPointerException in RM Web UI when accessing app page for failed applications",
            "Description": "The RM web UI crashes with a NullPointerException when attempting to access the application page for an application that has failed before any attempts have been created. This issue occurs in the AppBlock rendering logic.",
            "StackTrace": [
                "2014-08-12 16:45:13,573 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/app/application_1407887030038_0001",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:84)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:55)",
                "... 44 more"
            ],
            "StepsToReproduce": [
                "1. Deploy the Hadoop YARN application.",
                "2. Submit an application that fails before any attempts are created.",
                "3. Attempt to access the application page in the RM web UI."
            ],
            "ExpectedBehavior": "The RM web UI should display the application details without crashing, even if the application has failed before any attempts.",
            "ObservedBehavior": "The RM web UI crashes with a NullPointerException when trying to render the application page for a failed application.",
            "Resolution": "A fix has been implemented and tested in the codebase."
        }
    },
    {
        "filename": "YARN-3878.json",
        "creation_time": "2015-07-02T00:20:59.000+0000",
        "bug_report": {
            "BugID": "YARN-3878",
            "Title": "AsyncDispatcher Hangs During Shutdown When Draining Events",
            "Description": "The AsyncDispatcher can hang indefinitely during the shutdown process if it is configured to drain events. This occurs when the ResourceManager (RM) is stopped while attempting to post an event to the RMStateStore's AsyncDispatcher, leading to an InterruptedException. The dispatcher then waits indefinitely for the event queue to drain, causing the JVM to exit without completing the shutdown process properly.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:838)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager (RM).",
                "2. Submit an application that generates events.",
                "3. While the application is running, stop the ResourceManager.",
                "4. Observe the logs for any InterruptedException and the state of the AsyncDispatcher."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should complete the shutdown process without hanging, allowing the ResourceManager to exit gracefully.",
            "ObservedBehavior": "The AsyncDispatcher hangs indefinitely while waiting for the event queue to drain, resulting in the JVM not exiting properly.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-6683.json",
        "creation_time": "2017-06-02T00:29:13.000+0000",
        "bug_report": {
            "BugID": "YARN-6683",
            "Title": "Invalid State Transition: COLLECTOR_UPDATE Event in KILLED State",
            "Description": "An InvalidStateTransitionException occurs when attempting to handle a COLLECTOR_UPDATE event while the application is in the KILLED state. This indicates a flaw in the event handling logic of the ResourceManager, specifically in the RMAppImpl class.",
            "StackTrace": [
                "2017-06-01 20:01:22,686 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(905)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the ResourceManager.",
                "2. Allow the application to transition to the KILLED state.",
                "3. Attempt to send a COLLECTOR_UPDATE event to the application."
            ],
            "ExpectedBehavior": "The application should handle the COLLECTOR_UPDATE event gracefully, regardless of its state.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the event cannot be handled in the KILLED state.",
            "Resolution": "Consider modifying the RMAppImpl class to handle COLLECTOR_UPDATE events directly through a method call instead of dispatching an event, which would avoid state machine transitions. Additionally, review the implications of handling such events in terminal states."
        }
    },
    {
        "filename": "YARN-2910.json",
        "creation_time": "2014-11-27T06:19:00.000+0000",
        "bug_report": {
            "BugID": "YARN-2910",
            "Title": "ConcurrentModificationException in FSLeafQueue due to unsynchronized access to app lists",
            "Description": "The FSLeafQueue class is using standard ArrayLists to maintain runnable and non-runnable applications. This can lead to ConcurrentModificationException when multiple threads attempt to modify the lists simultaneously. The exception is thrown during resource allocation, indicating that the application is not thread-safe. To resolve this, we should replace ArrayList with a thread-safe alternative, such as CopyOnWriteArrayList.",
            "StackTrace": [
                "2014-11-12 02:29:01,169 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.util.ConcurrentModificationException",
                "at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)",
                "at java.util.ArrayList$Itr.next(ArrayList.java:831)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager with multiple applications submitting requests concurrently.",
                "2. Monitor the logs for any ConcurrentModificationException errors.",
                "3. Observe the stack trace indicating the source of the exception."
            ],
            "ExpectedBehavior": "The FSLeafQueue should handle concurrent access to the runnable and non-runnable application lists without throwing exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown when multiple threads attempt to access and modify the application lists simultaneously.",
            "Resolution": "Replace ArrayList with CopyOnWriteArrayList in FSLeafQueue to ensure thread-safe operations."
        }
    },
    {
        "filename": "YARN-192.json",
        "creation_time": "2012-11-01T05:00:41.000+0000",
        "bug_report": {
            "BugID": "YARN-192",
            "Title": "NullPointerException in Fair Scheduler on Node Update with Unknown NodeId",
            "Description": "A NullPointerException occurs in the Fair Scheduler when the 'unreserve' method is called on an FSSchedulerApp with a NodeId that is not recognized. This discrepancy between the ResourceManager and the scheduler regarding reserved applications leads to a failure in handling NODE_UPDATE events.",
            "StackTrace": [
                "2012-10-29 22:30:52,901 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.unreserve(FSSchedulerApp.java:356)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.unreserve(AppSchedulable.java:214)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:266)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:330)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueSchedulable.assignContainer(FSQueueSchedulable.java:161)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:759)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:836)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:329)",
                "    at java.lang.Thread.run(Thread.java:662)",
                "2012-10-29 22:30:52,903 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager and Fair Scheduler.",
                "2. Attempt to update a node with a NodeId that is not recognized by the scheduler.",
                "3. Observe the logs for a NullPointerException during the NODE_UPDATE event handling."
            ],
            "ExpectedBehavior": "The Fair Scheduler should handle NODE_UPDATE events gracefully without throwing exceptions, even if the NodeId is unknown.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the ResourceManager to log a fatal error and exit.",
            "Resolution": "[Provide additional details about the fix or workaround]"
        }
    },
    {
        "filename": "YARN-4581.json",
        "creation_time": "2016-01-12T03:37:40.000+0000",
        "bug_report": {
            "BugID": "YARN-4581",
            "Title": "Application History Writer Thread Leak Causes ResourceManager Crash During Recovery",
            "Description": "Enabling the ApplicationHistoryWriter results in thousands of errors related to history file access, leading to a ResourceManager (RM) crash due to an OutOfMemoryError. This issue manifests as a thread leak in the RM after several failovers, causing significant instability in the system.",
            "StackTrace": [
                "2016-01-08 03:13:03,441 ERROR org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore: Error when opening history file of application application_1451878591907_0197",
                "java.io.IOException: Output file not at zero offset.",
                "at org.apache.hadoop.io.file.tfile.BCFile$Writer.<init>(BCFile.java:288)",
                "at org.apache.hadoop.io.file.tfile.TFile$Writer.<init>(TFile.java:288)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:728)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-01-08 03:13:08,335 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:714)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.start(DFSOutputStream.java:2033)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForAppend(DFSOutputStream.java:1652)",
                "at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1573)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1603)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1591)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:328)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:324)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:324)",
                "at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:723)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Enable ApplicationHistoryWriter in the Hadoop YARN configuration.",
                "2. Submit multiple applications to the YARN ResourceManager.",
                "3. Monitor the logs for errors related to history file access.",
                "4. Observe the ResourceManager's behavior during application recovery."
            ],
            "ExpectedBehavior": "The ResourceManager should handle application history events without errors, and should not crash during recovery.",
            "ObservedBehavior": "The ResourceManager crashes with an OutOfMemoryError due to a thread leak caused by repeated failures in handling application history events.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7786.json",
        "creation_time": "2018-01-22T14:29:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7786",
            "Title": "NullPointerException Occurs When Sending Kill Command Before Launching ApplicationMaster",
            "Description": "A NullPointerException is thrown when a kill command is sent to the job before the ApplicationMaster is launched. This issue occurs in the AMLauncher class during the setup of tokens for the application attempt.",
            "StackTrace": [
                "2017-11-25 21:27:25,333 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1511616410268_0001_000001. Got exception: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN application.",
                "2. Immediately send a kill command to the job before the ApplicationMaster has launched.",
                "3. Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The application should handle the kill command gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application attempt to fail during the setup of tokens.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8035.json",
        "creation_time": "2018-03-16T12:02:04.000+0000",
        "bug_report": {
            "BugID": "YARN-8035",
            "Title": "MetricsException: Tag ContainerPid already exists during container relaunch",
            "Description": "During a container relaunch event, the ContainersMonitorImpl encounters a MetricsException due to the reuse of the container ID and the spawning of a new process. The existing PID for the container is retained, leading to a conflict when attempting to initialize the process tree monitoring with a new PID. This results in the following warning being logged: 'Uncaught exception in ContainersMonitorImpl while monitoring resource of container_1521201379995_0001_01_000002'. The MetricsRegistry's checkTagName method throws an exception when it detects that a tag with the same name already exists.",
            "StackTrace": [
                "2018-03-16 11:59:02,563 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Uncaught exception in ContainersMonitorImpl while monitoring resource of container_1521201379995_0001_01_000002",
                "org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448)"
            ],
            "StepsToReproduce": [
                "1. Start a container using the YARN framework.",
                "2. Allow the container to run until it fails.",
                "3. Trigger a relaunch of the failed container.",
                "4. Monitor the logs for the ContainersMonitorImpl."
            ],
            "ExpectedBehavior": "The ContainersMonitorImpl should successfully initialize the process tree monitoring for the new container PID without throwing a MetricsException.",
            "ObservedBehavior": "The ContainersMonitorImpl throws a MetricsException indicating that the tag 'ContainerPid' already exists, preventing proper monitoring of the new container process.",
            "Resolution": "Consider updating the value of the existing 'ContainerPid' tag instead of attempting to create a new one. This can be achieved by modifying the MetricsRegistry's tag method to allow for overriding existing tags."
        }
    },
    {
        "filename": "YARN-4152.json",
        "creation_time": "2015-09-12T15:02:22.000+0000",
        "bug_report": {
            "BugID": "YARN-4152",
            "Title": "NullPointerException in LogAggregationService when stopping absent container",
            "Description": "The NodeManager crashes with a NullPointerException when the LogAggregationService's stopContainer method is called for a container that does not exist. This occurs during log aggregation when an event to kill a container is sent, but the container is absent.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Run a Pi job with 500 containers.",
                "2. Kill the application while it is still running.",
                "3. Observe the logs for the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should handle the absence of a container gracefully without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when attempting to stop a container that is not present.",
            "Resolution": "The issue has been fixed by adding a null check for the container in the stopContainer method to prevent the NullPointerException."
        }
    },
    {
        "filename": "YARN-3697.json",
        "creation_time": "2015-05-21T18:05:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3697",
            "Title": "FairScheduler: ContinuousSchedulingThread Fails to Shutdown Properly",
            "Description": "The ContinuousSchedulingThread in the FairScheduler can fail to shut down properly after a stop command is issued. This issue arises due to an InterruptedException being blocked in the continuousSchedulingAttempt method, preventing the thread from terminating as expected.",
            "StackTrace": [
                "2015-05-17 23:30:43,065 WARN  [FairSchedulerContinuousScheduling] event.AsyncDispatcher (AsyncDispatcher.java:handle(247)) - AsyncDispatcher thread interrupted",
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)"
            ],
            "StepsToReproduce": [
                "1. Start the FairScheduler with a configured ContinuousSchedulingThread.",
                "2. Issue a stop command to the FairScheduler.",
                "3. Observe the logs for any warnings or errors related to thread interruption."
            ],
            "ExpectedBehavior": "The ContinuousSchedulingThread should shut down cleanly without any warnings or errors in the logs.",
            "ObservedBehavior": "The ContinuousSchedulingThread fails to shut down properly, resulting in an InterruptedException being logged and the thread remaining active.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-2340.json",
        "creation_time": "2014-07-23T15:18:38.000+0000",
        "bug_report": {
            "BugID": "YARN-2340",
            "Title": "NullPointerException on ResourceManager Restart After Queue State Change to STOPPED",
            "Description": "When the ResourceManager (RM) is restarted after changing the state of a queue to STOPPED, it fails to come up as active, resulting in a NullPointerException (NPE). This issue occurs specifically when there are applications in progress, leading to a failure in handling the event type APP_ATTEMPT_ADDED to the scheduler.",
            "StackTrace": [
                "2014-07-23 18:43:24,432 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1406116264351_0014_000002 State change from NEW to SUBMITTED",
                "2014-07-23 18:43:24,433 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:568)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:916)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:602)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2014-07-23 18:43:24,434 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with a CapacityScheduler configured with queues 'a' and 'b'.",
                "2. Submit a job to the ResourceManager.",
                "3. While the job is in progress, change the state of the queue to STOPPED.",
                "4. Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully and recover the application's state without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to come up as active and throws a NullPointerException during the handling of the APP_ATTEMPT_ADDED event.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8022.json",
        "creation_time": "2018-03-10T19:29:27.000+0000",
        "bug_report": {
            "BugID": "YARN-8022",
            "Title": "NullPointerException when rendering ResourceManager UI for application attempts",
            "Description": "The ResourceManager UI fails to render the attempts of an application, resulting in a NullPointerException. The error message displayed is 'Failed to read the attempts of the application'. This issue occurs when the application ID is not found or is null, leading to a failure in the rendering process.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.webapp.AppBlock: Failed to read the attempts of the application application_1520597233415_0002.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:283)",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:280)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock.render(AppBlock.java:279)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock.render(RMAppBlock.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:54)"
            ],
            "StepsToReproduce": [
                "1. Access the ResourceManager UI.",
                "2. Navigate to the application attempts page for a specific application ID (e.g., application_1520597233415_0002).",
                "3. Observe the error message displayed on the page."
            ],
            "ExpectedBehavior": "The ResourceManager UI should display the attempts of the specified application without errors.",
            "ObservedBehavior": "The ResourceManager UI displays a message indicating failure to read the attempts, along with a NullPointerException in the logs.",
            "Resolution": "A fix for this issue has been implemented and tested. Ensure that the application ID is valid and exists before attempting to render the attempts."
        }
    },
    {
        "filename": "YARN-3793.json",
        "creation_time": "2015-06-10T20:52:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3793",
            "Title": "Null Pointer Exceptions during Local File Deletion on NodeManager Recovery",
            "Description": "When the NodeManager (NM) work-preserving restart is enabled, multiple Null Pointer Exceptions (NPEs) occur during the recovery process. These exceptions appear to be related to attempts to delete sub-directories that are not being tracked correctly, potentially leading to resource leaks. This issue needs investigation and resolution to ensure proper handling of file deletions.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "    at org.apache.hadoop.fs.FileContext.delete(FileContext.java:755)",
                "    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)",
                "    at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)"
            ],
            "StepsToReproduce": [
                "1. Enable work-preserving restart on the NodeManager.",
                "2. Start the NodeManager and allow it to run for a period of time.",
                "3. Trigger a recovery event (e.g., restart the NodeManager).",
                "4. Monitor the logs for any Null Pointer Exceptions related to file deletions."
            ],
            "ExpectedBehavior": "The NodeManager should successfully delete local files and sub-directories during recovery without throwing any exceptions.",
            "ObservedBehavior": "During recovery, the NodeManager throws multiple Null Pointer Exceptions, indicating that it is attempting to delete files or directories with null paths.",
            "Resolution": "[Provide additional details about the fix or resolution applied]"
        }
    },
    {
        "filename": "YARN-6102.json",
        "creation_time": "2017-01-17T09:36:29.000+0000",
        "bug_report": {
            "BugID": "YARN-6102",
            "Title": "Dispatcher Fails to Handle Events After ResourceManager Failover",
            "Description": "During the ResourceManager failover process, the dispatcher may not be properly initialized before events are sent to it, leading to a fatal error. This issue occurs specifically when a node heartbeat is sent to the ResourceTrackerService and a failover is triggered, causing the dispatcher to reset before the new dispatcher is fully registered for events.",
            "StackTrace": [
                "2017-01-17 16:42:17,911 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(200)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-17 16:42:17,914 INFO  [AsyncDispatcher ShutDown handler] event.AsyncDispatcher (AsyncDispatcher.java:run(303)) - Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager and ensure it is running.",
                "2. Send a node heartbeat to the ResourceTrackerService.",
                "3. Trigger a failover of the ResourceManager while the heartbeat is being processed.",
                "4. Observe the logs for the fatal error indicating no handler registered for the event."
            ],
            "ExpectedBehavior": "The dispatcher should handle the RMNodeEventType without throwing an exception, even after a failover occurs.",
            "ObservedBehavior": "The dispatcher throws a fatal exception indicating that there is no handler registered for the RMNodeEventType, leading to abnormal termination of the dispatcher thread.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8409.json",
        "creation_time": "2018-06-08T20:36:32.000+0000",
        "bug_report": {
            "BugID": "YARN-8409",
            "Title": "NullPointerException in ActiveStandbyElectorBasedElectorService during ResourceManager failover",
            "Description": "In a ResourceManager High Availability (RM-HA) environment, killing the ZooKeeper (ZK) leader and subsequently performing a ResourceManager failover can lead to a NullPointerException (NPE) in the ActiveStandbyElectorBasedElectorService, preventing the active ResourceManager from starting successfully.",
            "StackTrace": [
                "2018-06-08 10:31:03,007 INFO client.ZooKeeperSaslClient (ZooKeeperSaslClient.java:run(289)) - Client will use GSSAPI as SASL mechanism.",
                "2018-06-08 10:31:03,008 INFO zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server xxx/xxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'",
                "2018-06-08 10:31:03,009 WARN zookeeper.ClientCnxn (ClientCnxn.java:run(1146)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect",
                "java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)",
                "at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)",
                "2018-06-08 10:31:03,344 INFO service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService failed in state INITED",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)",
                "2018-06-08 10:31:03,345 INFO ha.ActiveStandbyElector (ActiveStandbyElector.java:quitElection(409)) - Yielding from election"
            ],
            "StepsToReproduce": [
                "1. Set up a ResourceManager in a High Availability (RM-HA) environment.",
                "2. Kill the ZooKeeper leader node.",
                "3. Attempt to perform a ResourceManager failover."
            ],
            "ExpectedBehavior": "The active ResourceManager should successfully recover and start without throwing a NullPointerException.",
            "ObservedBehavior": "The active ResourceManager fails to start and throws a NullPointerException in the ActiveStandbyElectorBasedElectorService.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8223.json",
        "creation_time": "2018-04-27T11:49:02.000+0000",
        "bug_report": {
            "BugID": "YARN-8223",
            "Title": "ClassNotFoundException when loading auxiliary service from HDFS",
            "Description": "When attempting to load an auxiliary service from HDFS, a ClassNotFoundException is thrown, indicating that the class cannot be found in the classpath. This issue does not occur when loading the same service from a local jar file.",
            "StackTrace": [
                "java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal",
                "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)",
                "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)",
                "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)",
                "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)",
                "\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)",
                "\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)",
                "\tat java.lang.Class.forName0(Native Method)",
                "\tat java.lang.Class.forName(Class.java:348)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)"
            ],
            "StepsToReproduce": [
                "1. Configure the auxiliary service to load from HDFS.",
                "2. Start the NodeManager.",
                "3. Observe the logs for ClassNotFoundException."
            ],
            "ExpectedBehavior": "The auxiliary service should load successfully from HDFS without any ClassNotFoundException.",
            "ObservedBehavior": "A ClassNotFoundException is thrown when attempting to load the auxiliary service from HDFS, indicating that the class is not found in the classpath.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8331.json",
        "creation_time": "2018-05-21T05:19:35.000+0000",
        "bug_report": {
            "BugID": "YARN-8331",
            "Title": "Race Condition in Container Launch State Transition Leading to Invalid Event Handling",
            "Description": "A race condition occurs during the container launch process in the NodeManager, where a container's state transitions from SCHEDULED to KILLING and then to DONE. If a CONTAINER_LAUNCHED event is sent after the container has transitioned to DONE, it results in an InvalidStateTransitionException. This exception indicates that the system cannot handle the CONTAINER_LAUNCHED event in the DONE state, leading to potential resource leaks as the container processes are not cleaned up properly.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start a container in the NodeManager.",
                "2. Ensure the container transitions to the SCHEDULED state.",
                "3. Send a kill event to the container while it is still in the SCHEDULED state.",
                "4. Observe the state transition to KILLING and then to DONE.",
                "5. Attempt to send a CONTAINER_LAUNCHED event after the container has transitioned to DONE."
            ],
            "ExpectedBehavior": "The system should handle the CONTAINER_LAUNCHED event appropriately, regardless of the container's current state, without throwing an InvalidStateTransitionException.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when attempting to handle a CONTAINER_LAUNCHED event for a container that is in the DONE state, indicating that the event cannot be processed in this state.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-2931.json",
        "creation_time": "2014-12-08T21:09:13.000+0000",
        "bug_report": {
            "BugID": "YARN-2931",
            "Title": "PublicLocalizer Fails Due to Missing Local Directory Initialization",
            "Description": "When the data directory is cleaned up and the NodeManager (NM) is started with an existing recovery state, the local directories are not recreated due to YARN-90. This results in a PublicLocalizer failure until the getInitializedLocalDirs method is called by a LocalizeRunner for private localization.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)",
                "\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)",
                "\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)",
                "\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)",
                "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)",
                "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:720)",
                "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)",
                "\tat org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)",
                "\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)",
                "\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)",
                "\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Clean up the data directory used by the NodeManager.",
                "2. Start the NodeManager with an existing recovery state.",
                "3. Attempt to localize resources that require the local directories."
            ],
            "ExpectedBehavior": "The NodeManager should recreate the necessary local directories upon startup, allowing the PublicLocalizer to function correctly without errors.",
            "ObservedBehavior": "The PublicLocalizer fails with a FileNotFoundException indicating that the local directory '/data/yarn/nm/filecache' does not exist.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-6837.json",
        "creation_time": "2017-07-18T11:17:55.000+0000",
        "bug_report": {
            "BugID": "YARN-6837",
            "Title": "Null LocalResource Visibility Causes NodeManager Crash",
            "Description": "When a LocalResource is created without setting its visibility, it leads to a NullPointerException in the NodeManager, causing it to shut down unexpectedly. This issue arises specifically when the visibility is set to null, which is not handled properly in the ResourceSet class.",
            "StackTrace": [
                "2017-07-18 17:54:09,292 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.addResources(ResourceSet.java:84)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:868)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:819)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1684)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:96)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1418)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1411)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Create a LocalResource without setting its visibility.",
                "2. Submit the yarn application.",
                "3. Observe the NodeManager logs for NullPointerException."
            ],
            "ExpectedBehavior": "The NodeManager should handle the LocalResource creation gracefully without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when a LocalResource is created with null visibility.",
            "Resolution": "The issue was resolved by ensuring that the visibility of LocalResource is set to a valid value before submission."
        }
    },
    {
        "filename": "YARN-4762.json",
        "creation_time": "2016-03-04T02:24:47.000+0000",
        "bug_report": {
            "BugID": "YARN-4762",
            "Title": "NodeManager Fails to Initialize with LinuxContainerExecutor Enabled",
            "Description": "The NodeManager crashes during initialization when the LinuxContainerExecutor is enabled due to an inability to set up cgroups. This results in a YarnRuntimeException indicating a failure to initialize the container executor.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)",
                "Caused by: java.io.IOException: Failed to initialize linux container runtime(s)!",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the LinuxContainerExecutor is enabled in the configuration.",
                "2. Start the NodeManager service.",
                "3. Observe the logs for any initialization errors."
            ],
            "ExpectedBehavior": "The NodeManager should initialize successfully without crashing, allowing it to manage containers as expected.",
            "ObservedBehavior": "The NodeManager fails to initialize, resulting in a YarnRuntimeException and subsequent crash.",
            "Resolution": "The issue was fixed by modifying the CgroupHandler's creation and usage to ensure proper initialization of the NodeManager when the LinuxContainerExecutor is enabled."
        }
    },
    {
        "filename": "YARN-2823.json",
        "creation_time": "2014-11-06T21:38:47.000+0000",
        "bug_report": {
            "BugID": "YARN-2823",
            "Title": "NullPointerException in ResourceManager during Application Attempt Recovery in HA Enabled Cluster",
            "Description": "In a 3-node cluster with ResourceManager High Availability (HA) enabled, a NullPointerException (NPE) occurs when the ResourceManager attempts to transfer the state from a previous application attempt during recovery. This issue leads to the ResourceManagers going down and failing to restart, impacting the availability of the cluster.",
            "StackTrace": [
                "2014-09-16 01:36:28,037 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(612)) - Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Set up a 3-node cluster with ResourceManager High Availability (HA) enabled using Ambari.",
                "2. Install HBase using Slider on the cluster.",
                "3. Allow the ResourceManagers to run for a period of time.",
                "4. Simulate a failure that causes the ResourceManagers to go down.",
                "5. Attempt to restart the ResourceManagers."
            ],
            "ExpectedBehavior": "The ResourceManagers should recover from the failure and resume normal operations without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManagers fail to restart and log a NullPointerException when handling the APP_ATTEMPT_ADDED event, leading to a complete halt in cluster operations.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-5098.json",
        "creation_time": "2016-05-17T00:43:08.000+0000",
        "bug_report": {
            "BugID": "YARN-5098",
            "Title": "YARN Application Log Aggregation Fails Due to Missing HDFS Delegation Token",
            "Description": "In a high-availability (HA) cluster environment, the YARN application logs for a long-running application could not be gathered because the NodeManager failed to communicate with HDFS. The error encountered indicates that the HDFS delegation token could not be found in the cache, leading to a failure in log aggregation.",
            "StackTrace": [
                "2016-05-16 18:18:28,533 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(555)) - Application just finished : application_1463170334122_0002",
                "2016-05-16 18:18:28,545 WARN  ipc.Client (Client.java:run(705)) - Exception encountered while connecting to the server :",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:583)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:398)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:752)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:748)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:747)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1597)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1439)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1386)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:282)",
                "at org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:1018)",
                "at org.apache.hadoop.fs.Hdfs.getServerDefaults(Hdfs.java:156)",
                "at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:550)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:687)"
            ],
            "StepsToReproduce": [
                "1. Set up a high-availability (HA) YARN cluster.",
                "2. Submit a long-running YARN application.",
                "3. Attempt to gather application logs after the application has finished."
            ],
            "ExpectedBehavior": "The application logs should be successfully aggregated and accessible after the application completes its execution.",
            "ObservedBehavior": "The log aggregation fails with an error indicating that the HDFS delegation token cannot be found in the cache, preventing the NodeManager from communicating with HDFS.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-3971.json",
        "creation_time": "2015-07-24T10:17:05.000+0000",
        "bug_report": {
            "BugID": "YARN-3971",
            "Title": "IOException during Node Label Recovery in ResourceManager",
            "Description": "When attempting to recover node labels in the ResourceManager, an IOException is thrown if a label is still in use by a queue. This prevents the ResourceManager from transitioning to an active state, causing both ResourceManagers to become standby.",
            "StackTrace": [
                "java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue(RMNodeLabelsManager.java:104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels(RMNodeLabelsManager.java:118)",
                "at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.recover(FileSystemNodeLabelsStore.java:221)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:232)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:245)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:964)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1005)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1001)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1666)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1001)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:832)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:422)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "StepsToReproduce": [
                "1. Create label x,y.",
                "2. Delete label x,y.",
                "3. Create label x,y and add capacity scheduler XML for labels x and y.",
                "4. Restart ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover node labels and transition to an active state without throwing exceptions.",
            "ObservedBehavior": "Both ResourceManagers become standby due to an IOException indicating that a label cannot be removed because it is still in use by a queue.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-6948.json",
        "creation_time": "2017-08-04T08:23:46.000+0000",
        "bug_report": {
            "BugID": "YARN-6948",
            "Title": "InvalidStateTransitionException when sending kill command to a running job",
            "Description": "When attempting to kill a running job in Hadoop YARN, an InvalidStateTransitionException is thrown, indicating that the system cannot handle the event due to the current state of the application attempt.",
            "StackTrace": [
                "2017-08-03 01:35:20,485 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a job in Hadoop YARN.",
                "2. While the job is running, send a kill command to the job.",
                "3. Check the logs for any exceptions."
            ],
            "ExpectedBehavior": "The job should be terminated successfully without throwing any exceptions.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the event cannot be handled in the current state.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-1409.json",
        "creation_time": "2013-11-13T11:25:56.000+0000",
        "bug_report": {
            "BugID": "YARN-1409",
            "Title": "RejectedExecutionException in NonAggregatingLogHandler during APPLICATION_FINISHED event handling",
            "Description": "The NonAggregatingLogHandler can throw a RejectedExecutionException when handling APPLICATION_FINISHED events after the shutdown process has been initiated. This issue arises due to the scheduling of log deletion tasks when the thread pool is already shutting down, leading to task rejection.",
            "StackTrace": [
                "2013-11-13 10:53:06,970 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(166)) - Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Start an application that utilizes the NonAggregatingLogHandler.",
                "2. Trigger the application to finish, which will generate an APPLICATION_FINISHED event.",
                "3. Ensure that the shutdown process for the log handler is initiated before the event is processed."
            ],
            "ExpectedBehavior": "The log deletion task should be scheduled successfully without throwing any exceptions, even if the log handler is in the process of shutting down.",
            "ObservedBehavior": "A RejectedExecutionException is thrown, indicating that the task was rejected because the ScheduledThreadPoolExecutor is shutting down.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-5545.json",
        "creation_time": "2016-08-21T12:57:35.000+0000",
        "bug_report": {
            "BugID": "YARN-5545",
            "Title": "Application Submission Failure in Capacity Scheduler with Zero Default Capacity",
            "Description": "When configuring the capacity scheduler with a default capacity of zero, applications cannot be submitted to the default queue, even when other queues have available capacity. This issue arises due to an AccessControlException indicating that the default queue cannot accept submissions.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)",
                "at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:301)"
            ],
            "StepsToReproduce": [
                "Configure the capacity scheduler with the following settings:",
                "yarn.scheduler.capacity.root.default.capacity=0",
                "yarn.scheduler.capacity.root.queue1.accessible-node-labels.labelx.capacity=50",
                "yarn.scheduler.capacity.root.default.accessible-node-labels.labelx.capacity=50",
                "Submit an application using the command:",
                "./yarn jar ../share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-alpha2-SNAPSHOT-tests.jar sleep -Dmapreduce.job.node-label-expression=labelx -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 10000000 -rt 1"
            ],
            "ExpectedBehavior": "The application should be submitted successfully to the default queue or another available queue, given that other queues have capacity.",
            "ObservedBehavior": "The application submission fails with an AccessControlException, indicating that the default queue cannot accept submissions due to its configured capacity of zero.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-301.json",
        "creation_time": "2013-01-01T05:40:18.000+0000",
        "bug_report": {
            "BugID": "YARN-301",
            "Title": "ConcurrentModificationException in Fair Scheduler during Node Update",
            "Description": "In a test cluster, the Fair Scheduler encounters a ConcurrentModificationException when handling node updates, leading to a ResourceManager crash. This issue arises specifically during the assignment of containers to applications, which is critical for resource management in Hadoop YARN.",
            "StackTrace": [
                "2012-12-30 17:14:17,171 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.util.ConcurrentModificationException",
                "at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)",
                "at java.util.TreeMap$KeyIterator.next(TreeMap.java:1154)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:181)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:780)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:842)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:340)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Set up a test cluster with the Fair Scheduler enabled.",
                "2. Trigger a node update event in the ResourceManager.",
                "3. Monitor the ResourceManager logs for any exceptions."
            ],
            "ExpectedBehavior": "The Fair Scheduler should handle node updates without throwing exceptions, allowing for proper resource allocation and management.",
            "ObservedBehavior": "The Fair Scheduler throws a ConcurrentModificationException during the handling of node update events, causing the ResourceManager to crash.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-7942.json",
        "creation_time": "2018-02-16T19:09:39.000+0000",
        "bug_report": {
            "BugID": "YARN-7942",
            "Title": "ServiceClient Fails to Delete ZNode from Secure ZooKeeper Due to NoPathPermissionsException",
            "Description": "When attempting to delete a ZNode from ZooKeeper using the Yarn ServiceClient, the operation fails with a NoPathPermissionsException, despite the appropriate ACLs being set. This issue occurs during the actionDestroy method call, which is expected to remove the specified service entry from the registry.",
            "StackTrace": [
                "2018-02-16 15:49:29,691 WARN  client.ServiceClient (ServiceClient.java:actionDestroy(470)) - Error deleting registry entry /users/hbase/services/yarn-service/hbase-app-test",
                "org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:412)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:390)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:722)",
                "at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.delete(RegistryOperationsService.java:162)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionDestroy(ServiceClient.java:462)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:253)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.deleteService(ApiServer.java:223)",
                "Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test"
            ],
            "StepsToReproduce": [
                "1. Ensure that the ZK node exists at the path `/users/hbase/services/yarn-service/hbase-app-test`.",
                "2. Set the ACLs for the ZK node to allow access for the service account.",
                "3. Call the actionDestroy method on the ServiceClient with the service name corresponding to the ZK node.",
                "4. Observe the logs for any warnings or errors related to the deletion process."
            ],
            "ExpectedBehavior": "The ZNode at `/users/hbase/services/yarn-service/hbase-app-test` should be successfully deleted without any permission errors.",
            "ObservedBehavior": "The deletion fails with a NoPathPermissionsException, indicating that the service is not authorized to access the specified path, despite the ACLs being set correctly.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the ServiceClient can successfully delete ZNodes from secure ZooKeeper when the appropriate permissions are set."
        }
    },
    {
        "filename": "YARN-7692.json",
        "creation_time": "2017-12-29T06:00:34.000+0000",
        "bug_report": {
            "BugID": "YARN-7692",
            "Title": "Resource Manager Crashes During Application Recovery Due to ACL Validation Failure",
            "Description": "When the Resource Manager attempts to recover applications after enabling ACLs, it crashes due to an AccessControlException. This occurs specifically when a user who is not included in the priority ACL tries to submit a job, leading to a failure in the recovery process.",
            "StackTrace": [
                "2017-12-27 10:52:30,432 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(776)) - Failed to load/recover state",
                "org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2348)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:358)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:567)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1390)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)"
            ],
            "StepsToReproduce": [
                "1. Create a YARN cluster without any ACLs.",
                "2. Submit jobs using an existing user (e.g., 'user_a').",
                "3. Enable ACLs and create a priority ACL entry via the property 'yarn.scheduler.capacity.priority-acls', excluding 'user_a'.",
                "4. Attempt to submit a job with 'user_a'.",
                "5. Observe the Resource Manager's behavior during application recovery."
            ],
            "ExpectedBehavior": "The Resource Manager should reject the job submission from 'user_a' without crashing, and it should successfully recover previous applications.",
            "ObservedBehavior": "The job submission from 'user_a' is rejected as expected, but the Resource Manager crashes during the recovery process, failing to recover any applications.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-3917.json",
        "creation_time": "2015-07-11T00:41:28.000+0000",
        "bug_report": {
            "BugID": "YARN-3917",
            "Title": "UnsupportedOperationException when initializing ResourceCalculatorPlugin due to OS detection failure",
            "Description": "The NodeManager fails to initialize the ResourceCalculatorPlugin because it cannot determine the operating system, resulting in an UnsupportedOperationException. This issue occurs when the default resource calculator is instantiated without a specific plugin configuration.",
            "StackTrace": [
                "2015-07-10 08:16:18,445 INFO org.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.UnsupportedOperationException: Could not determine OS",
                "java.lang.UnsupportedOperationException: Could not determine OS",
                "at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN NodeManager without a specific ResourceCalculatorPlugin configured.",
                "2. Observe the logs during the initialization process."
            ],
            "ExpectedBehavior": "The NodeManager should initialize the ResourceCalculatorPlugin without throwing an exception, allowing it to monitor container resources correctly.",
            "ObservedBehavior": "The NodeManager fails to initialize due to an UnsupportedOperationException indicating that the operating system could not be determined.",
            "Resolution": "A fix has been implemented to ensure that the NodeManager can handle cases where the operating system cannot be determined, allowing it to proceed with a default behavior."
        }
    },
    {
        "filename": "YARN-3537.json",
        "creation_time": "2015-04-23T11:34:23.000+0000",
        "bug_report": {
            "BugID": "YARN-3537",
            "Title": "NullPointerException in NodeManager during serviceStop when stopRecoveryStore is invoked",
            "Description": "A NullPointerException occurs in the NodeManager when the serviceStop method is called, specifically during the execution of stopRecoveryStore. This issue arises when the NodeManager is in a STOPPED state and attempts to stop the recovery store, leading to a failure in the service initialization process.",
            "StackTrace": [
                "2015-04-23 19:30:34,961 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service NodeManager failed in state STOPPED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore(NodeManager.java:181)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:326)",
                "\tat org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:106)"
            ],
            "StepsToReproduce": [
                "1. Start the NodeManager service.",
                "2. Trigger a service stop while the NodeManager is in a STOPPED state.",
                "3. Observe the logs for any NullPointerException related to stopRecoveryStore."
            ],
            "ExpectedBehavior": "The NodeManager should stop gracefully without throwing a NullPointerException during the execution of stopRecoveryStore.",
            "ObservedBehavior": "A NullPointerException is thrown when stopRecoveryStore is invoked, causing the NodeManager to fail during the service stop process.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-7962.json",
        "creation_time": "2018-02-22T22:32:20.000+0000",
        "bug_report": {
            "BugID": "YARN-7962",
            "Title": "Race Condition in DelegationTokenRenewer Shutdown Causes ResourceManager Crash",
            "Description": "A race condition occurs when stopping the DelegationTokenRenewer, leading to a crash of the ResourceManager during failover. The issue arises because the `serviceStop` method does not set the `isServiceStarted` flag to false before shutting down the `renewerService` thread pool, which can result in tasks being rejected when the service is already terminated.",
            "StackTrace": [
                "2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with the DelegationTokenRenewer enabled.",
                "2. Submit an application to the ResourceManager.",
                "3. Allow the application to finish execution.",
                "4. Trigger a failover of the ResourceManager while the application is finishing."
            ],
            "ExpectedBehavior": "The ResourceManager should gracefully handle the application finishing and stop the DelegationTokenRenewer without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a RejectedExecutionException due to the DelegationTokenRenewer attempting to execute a task after the service has been stopped.",
            "Resolution": "Update the `serviceStop` method to acquire the `serviceStateLock` and set `isServiceStarted` to false before shutting down the `renewerService` thread pool."
        }
    },
    {
        "filename": "YARN-8357.json",
        "creation_time": "2018-05-24T16:46:57.000+0000",
        "bug_report": {
            "BugID": "YARN-8357",
            "Title": "NullPointerException in ServiceClient when starting a service without an application ID",
            "Description": "A NullPointerException (NPE) occurs in the ServiceClient when attempting to start a service that has not been properly initialized with an application ID. This issue arises specifically when the service is saved first and then started, leading to a failure in the actionStart method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:644)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:498)",
                "    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)"
            ],
            "StepsToReproduce": [
                "1. Save a service using the ServiceClient without an application ID.",
                "2. Attempt to start the saved service using the actionStart method.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The service should start successfully without throwing a NullPointerException, even if it was saved without an application ID.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to start the service, indicating that the service state is null.",
            "Resolution": "[Provide additional details about the resolution, if applicable]"
        }
    },
    {
        "filename": "YARN-6534.json",
        "creation_time": "2017-04-26T21:43:52.000+0000",
        "bug_report": {
            "BugID": "YARN-6534",
            "Title": "ResourceManager Fails to Start Due to SSLFactory Initialization in Non-Secured Cluster",
            "Description": "In a non-secured cluster, the ResourceManager fails to start consistently because the TimelineServiceV1Publisher attempts to initialize the TimelineClient with an SSLFactory without checking if HTTPS is enabled. This results in a FileNotFoundException when the system tries to access a non-existent keystore file.",
            "StackTrace": [
                "2017-04-26 21:09:10,683 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(1457)) - Error starting ResourceManager",
                "org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher.serviceInit(AbstractSystemMetricsPublisher.java:59)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.serviceInit(TimelineServiceV1Publisher.java:67)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1453)",
                "Caused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:168)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:86)",
                "at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:219)",
                "at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:179)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.getSSLFactory(TimelineConnector.java:176)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit(TimelineConnector.java:106)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "1. Set up a non-secured Hadoop YARN cluster.",
                "2. Attempt to start the ResourceManager.",
                "3. Observe the logs for any fatal errors related to SSL initialization."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any errors related to SSL initialization.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing a FileNotFoundException for the keystore file '/etc/security/clientKeys/all.jks'.",
            "Resolution": "A fix has been implemented to check if HTTPS is enabled before attempting to initialize the SSLFactory, preventing the ResourceManager from failing in non-secured environments."
        }
    },
    {
        "filename": "YARN-4227.json",
        "creation_time": "2015-10-06T04:59:10.000+0000",
        "bug_report": {
            "BugID": "YARN-4227",
            "Title": "NullPointerException in FairScheduler when handling expired containers from removed nodes",
            "Description": "Under certain conditions, when a node is removed before an expired container event is processed, the ResourceManager (RM) crashes due to a NullPointerException. This issue has been observed in versions 2.3.0, 2.5.0, and 2.6.0.",
            "StackTrace": [
                "2015-10-04 21:14:01,063 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type CONTAINER_EXPIRED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:849)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1273)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:122)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:585)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with FairScheduler enabled.",
                "2. Submit an application that allocates containers.",
                "3. Remove a node from the cluster while there are active containers.",
                "4. Wait for the containers to expire."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the expired container events gracefully without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when trying to process expired containers from a removed node.",
            "Resolution": "A fix has been implemented to ensure that expired containers from removed nodes are ignored, preventing the NullPointerException."
        }
    },
    {
        "filename": "YARN-2649.json",
        "creation_time": "2014-10-06T22:57:46.000+0000",
        "bug_report": {
            "BugID": "YARN-2649",
            "Title": "Flaky Test Failure in TestAMRMRPCNodeUpdates Due to Incorrect AppAttempt State",
            "Description": "The test 'testAMRMUnusableNodes' in the class 'TestAMRMRPCNodeUpdates' intermittently fails with an assertion error indicating that the application attempt state is not as expected. The test expects the state to be 'ALLOCATED', but it is 'SCHEDULED'. This issue arises when the SchedulerEventType.NODE_UPDATE is processed before the RMAppAttemptEvent.ATTEMPT_ADDED event, leading to a race condition in state transitions.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>",
                "at junit.framework.Assert.fail(Assert.java:50)",
                "at junit.framework.Assert.failNotEquals(Assert.java:287)",
                "at junit.framework.Assert.assertEquals(Assert.java:67)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)",
                "at org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN environment with the necessary configurations.",
                "2. Run the test suite that includes 'TestAMRMRPCNodeUpdates'.",
                "3. Observe the test 'testAMRMUnusableNodes' for intermittent failures."
            ],
            "ExpectedBehavior": "The application attempt state should transition to 'ALLOCATED' after the application is launched and the necessary events are processed.",
            "ObservedBehavior": "The application attempt state is 'SCHEDULED' instead of 'ALLOCATED', leading to a test failure.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-4288.json",
        "creation_time": "2015-10-22T12:30:16.000+0000",
        "bug_report": {
            "BugID": "YARN-4288",
            "Title": "NodeManager Fails to Retry Registration with ResourceManager After Connection Reset",
            "Description": "When the NodeManager (NM) is restarted, it attempts to register with the ResourceManager (RM). If the RM is also restarting at the same time, the NM encounters a connection reset error, which prevents it from successfully registering. This issue leads to a failure in the NM restart process, impacting the overall cluster stability and resource management.",
            "StackTrace": [
                "2015-08-17 14:35:59,434 ERROR nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:rebootNodeStatusUpdaterAndRegisterWithRM(222)) - Unexpected error rebooting NodeStatusUpdater",
                "java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1473)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)",
                "Caused by: java.io.IOException: Connection reset by peer",
                "at sun.nio.ch.FileDispatcherImpl.read0(Native Method)",
                "at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)",
                "at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)",
                "at sun.nio.ch.IOUtil.read(IOUtil.java:197)",
                "at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)",
                "at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)",
                "at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)",
                "at java.io.FilterInputStream.read(FilterInputStream.java:133)",
                "at java.io.FilterInputStream.read(FilterInputStream.java:133)",
                "at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:514)",
                "at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1072)",
                "at org.apache.hadoop.ipc.Client$Connection.run(Client.java:967)"
            ],
            "StepsToReproduce": [
                "1. Restart the NodeManager service.",
                "2. Ensure that the ResourceManager is also restarted during the same time.",
                "3. Monitor the NodeManager logs for connection errors."
            ],
            "ExpectedBehavior": "The NodeManager should successfully retry registration with the ResourceManager even if a connection reset occurs during the RM's restart.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager due to a connection reset error, leading to a failure in the NM restart process.",
            "Resolution": "[Provide additional details on the fix or workaround]"
        }
    },
    {
        "filename": "YARN-1032.json",
        "creation_time": "2013-08-05T21:10:46.000+0000",
        "bug_report": {
            "BugID": "YARN-1032",
            "Title": "NullPointerException in RackResolver during Host Address Resolution",
            "Description": "A NullPointerException (NPE) occurs in the RackResolver class when attempting to resolve a host address. This issue arises from the coreResolve method failing to handle cases where the DNS resolution does not return a valid result, leading to an attempt to access an element from an empty list.",
            "StackTrace": [
                "2013-08-01 07:11:37,708 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)",
                "at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$400(RMContainerAllocator.java:681)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Initialize the RackResolver class without a valid DNS configuration.",
                "2. Call the resolve method with a hostname that cannot be resolved.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The RackResolver should handle unresolvable hostnames gracefully, either by returning a default value or throwing a meaningful exception.",
            "ObservedBehavior": "A NullPointerException is thrown when the coreResolve method attempts to access the first element of an empty list returned by the DNS resolution.",
            "Resolution": "A fix has been implemented to check if the rNameList is empty before attempting to access its elements in the coreResolve method."
        }
    },
    {
        "filename": "YARN-5837.json",
        "creation_time": "2016-11-04T16:06:59.000+0000",
        "bug_report": {
            "BugID": "YARN-5837",
            "Title": "NullPointerException when retrieving status of decommissioned node after ResourceManager restart",
            "Description": "When a node is decommissioned and the ResourceManager is restarted, attempting to retrieve the status of that node using the 'yarn node -status' command results in a NullPointerException. This occurs because the node's ID is incorrectly reported as '-1', leading to an invalid state being processed in the NodeCLI class.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)"
            ],
            "StepsToReproduce": [
                "1. Decommission a node using the command: `yarn node -decommission <node-id>`.",
                "2. Verify the node is decommissioned by running: `yarn node -list -all`.",
                "3. Restart the ResourceManager using the command: `yarn resourcemanager -restart`.",
                "4. Attempt to retrieve the status of the decommissioned node using: `yarn node -status <node-id>`."
            ],
            "ExpectedBehavior": "The command `yarn node -status <node-id>` should return the status of the decommissioned node without throwing an exception.",
            "ObservedBehavior": "The command `yarn node -status <node-id>` throws a NullPointerException when the ResourceManager is restarted and the node ID is reported as '-1'.",
            "Resolution": "A fix has been implemented to handle the case where a decommissioned node's ID is reported as '-1' after a ResourceManager restart, preventing the NullPointerException."
        }
    },
    {
        "filename": "YARN-6827.json",
        "creation_time": "2017-07-15T05:14:25.000+0000",
        "bug_report": {
            "BugID": "YARN-6827",
            "Title": "NullPointerException during Application Recovery in ResourceManager",
            "Description": "A NullPointerException (NPE) occurs when attempting to publish recovering applications into the Application Timeline Service (ATS) during the ResourceManager (RM) restart. This issue arises because the ATS services are not fully initialized before the active services attempt to recover applications, leading to a failure in publishing the entities.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:178)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.putEntity(TimelineServiceV1Publisher.java:368)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager in a non-HA configuration.",
                "2. Trigger a restart of the ResourceManager.",
                "3. Observe the logs during the recovery phase."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover applications and publish their entities to the ATS without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when the ResourceManager attempts to publish recovering applications into ATS, resulting in failure to publish the entities.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the ATS services are fully initialized before any attempts to publish entities during the recovery phase."
        }
    },
    {
        "filename": "YARN-3832.json",
        "creation_time": "2015-06-19T13:31:18.000+0000",
        "bug_report": {
            "BugID": "YARN-3832",
            "Title": "Resource Localization Fails Due to Existing Cache Directories",
            "Description": "The resource localization process fails on a Hadoop cluster when attempting to rename a directory that is not empty. This issue was observed in Hadoop version 2.7.0 and was previously fixed in version 2.6.0 (YARN-2624). The error message indicates that the application failed due to an IOException caused by attempting to rename a non-empty destination directory.",
            "StackTrace": [
                "java.io.IOException: Rename cannot overwrite non empty destination directory /opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39",
                "at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:735)",
                "at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:244)",
                "at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:678)",
                "at org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop cluster with version 2.7.0.",
                "2. Submit an application that requires resource localization.",
                "3. Ensure that the destination directory for the resource is not empty.",
                "4. Monitor the application logs for errors related to resource localization."
            ],
            "ExpectedBehavior": "The resource localization process should successfully rename the necessary directories and complete without errors.",
            "ObservedBehavior": "The resource localization process fails with an IOException indicating that the rename operation cannot overwrite a non-empty destination directory.",
            "Resolution": "This issue has been resolved in later versions of Hadoop. Users are advised to upgrade to at least version 2.6.1 or later to avoid this problem."
        }
    },
    {
        "filename": "YARN-2409.json",
        "creation_time": "2014-08-12T10:53:06.000+0000",
        "bug_report": {
            "BugID": "YARN-2409",
            "Title": "Invalid State Transition in ResourceManager Causes AsyncDispatcher Thread Leak",
            "Description": "The ResourceManager in Hadoop YARN fails to handle certain events (STATUS_UPDATE and CONTAINER_ALLOCATED) when in the LAUNCHED state, leading to an InvalidStateTransitionException. This results in a thread leak in the AsyncDispatcher, as the dispatcher does not stop properly during the Active to Standby transition.",
            "StackTrace": [
                "2014-08-12 07:03:00,839 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager in Active state.",
                "2. Submit an application that transitions to the LAUNCHED state.",
                "3. Trigger a STATUS_UPDATE event for the application.",
                "4. Observe the logs for InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle STATUS_UPDATE and CONTAINER_ALLOCATED events correctly, transitioning states without throwing exceptions.",
            "ObservedBehavior": "The ResourceManager throws InvalidStateTransitionException for STATUS_UPDATE and CONTAINER_ALLOCATED events when in the LAUNCHED state, leading to a thread leak in the AsyncDispatcher.",
            "Resolution": "[Provide additional details about the fix or workaround]"
        }
    },
    {
        "filename": "YARN-8116.json",
        "creation_time": "2018-04-04T15:30:52.000+0000",
        "bug_report": {
            "BugID": "YARN-8116",
            "Title": "NodeManager Fails to Start Due to NumberFormatException on Empty String Input",
            "Description": "The NodeManager fails to start when the configuration for the debug delay is set, leading to a NumberFormatException caused by an empty string input. This issue occurs during the recovery process of the NodeManager.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)",
                "at java.lang.Long.parseLong(Long.java:601)",
                "at java.lang.Long.parseLong(Long.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)"
            ],
            "StepsToReproduce": [
                "1. Update the NodeManager debug delay configuration in the configuration file:",
                "   <property>",
                "       <name>yarn.nodemanager.delete.debug-delay-sec</name>",
                "       <value>350</value>",
                "   </property>",
                "2. Launch the distributed shell application multiple times using the command:",
                "   /usr/hdp/current/hadoop-yarn-client/bin/yarn jar hadoop-yarn-applications-distributedshell-*.jar -shell_command \"sleep 120\" -num_containers 1 -shell_env YARN_CONTAINER_RUNTIME_TYPE=docker -shell_env YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=centos/httpd-24-centos7:latest -shell_env YARN_CONTAINER_RUNTIME_DOCKER_DELAYED_REMOVAL=true -jar hadoop-yarn-applications-distributedshell-*.jar",
                "3. Restart the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully without any exceptions.",
            "ObservedBehavior": "The NodeManager fails to start and logs a NumberFormatException due to an empty string input.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8403.json",
        "creation_time": "2018-06-06T22:34:42.000+0000",
        "bug_report": {
            "BugID": "YARN-8403",
            "Title": "Nodemanager logs incorrectly log file download failures at INFO/WARN level instead of ERROR",
            "Description": "The Nodemanager is logging stack traces related to failed file downloads at INFO or WARN levels, which should ideally be logged at the ERROR level. This misclassification can lead to difficulties in diagnosing issues as critical errors are not highlighted appropriately in the logs.",
            "StackTrace": [
                "2018-06-06 03:10:40,077 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:writeCredentials(1312)) - Writing credentials to the nmPrivate file /grid/0/hadoop/yarn/local/nmPrivate/container_e02_1528246317583_0048_01_000001.tokens",
                "2018-06-06 03:10:40,087 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(975)) - Failed to download resource { { hdfs://mycluster.example.com:8020/user/hrt_qa/Streaming/InputDir, 1528254452720, FILE, null },pending,[(container_e02_1528246317583_0048_01_000001)],6074418082915225,DOWNLOADING}",
                "org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed",
                "at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)",
                "at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:409)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:66)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:408)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:399)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:381)",
                "at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:298)",
                "... 9 more"
            ],
            "StepsToReproduce": [
                "1. Start the YARN NodeManager.",
                "2. Attempt to download a resource that is not accessible due to permission issues.",
                "3. Check the NodeManager logs for the output."
            ],
            "ExpectedBehavior": "The NodeManager should log critical errors, such as failed file downloads due to permission issues, at the ERROR log level to ensure they are easily identifiable.",
            "ObservedBehavior": "The NodeManager logs failed file downloads at INFO or WARN levels, which may lead to oversight of critical issues.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-1458.json",
        "creation_time": "2013-11-29T03:31:39.000+0000",
        "bug_report": {
            "BugID": "YARN-1458",
            "Title": "FairScheduler: Zero weight can lead to livelock when processing multiple job submissions",
            "Description": "The ResourceManager's SchedulerEventDispatcher's EventProcessor becomes blocked when clients submit a large number of jobs. This issue is difficult to reproduce consistently, as it may take days of running the test cluster to observe the problem. The jstack output indicates that the FairScheduler's removeApplication method is waiting for a monitor entry, leading to a livelock situation.",
            "StackTrace": [
                "java.lang.Thread.State: BLOCKED (on object monitor)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplication(FairScheduler.java:671)",
                "- waiting to lock <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1023)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:440)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN cluster with FairScheduler enabled.",
                "2. Submit a large number of jobs simultaneously (exact number may vary).",
                "3. Monitor the ResourceManager's logs and thread states using jstack.",
                "4. Observe if the EventProcessor thread becomes blocked."
            ],
            "ExpectedBehavior": "The ResourceManager should handle job submissions efficiently without blocking, allowing all jobs to be processed in a timely manner.",
            "ObservedBehavior": "The ResourceManager's EventProcessor thread becomes blocked, leading to a livelock situation where no further job submissions can be processed.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8209.json",
        "creation_time": "2018-04-26T00:22:23.000+0000",
        "bug_report": {
            "BugID": "YARN-8209",
            "Title": "NullPointerException in DockerContainerDeletionTask during container removal",
            "Description": "A NullPointerException is thrown in the DeletionService when attempting to remove a Docker container. This issue occurs in the method 'writeCommandToTempFile' of the DockerClient class, indicating that a required object is not initialized properly.",
            "StackTrace": [
                "2018-04-25 23:38:41,039 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread DeletionService #1:",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile(DockerClient.java:109)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand(DockerCommandExecutor.java:85)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeStatusCommand(DockerCommandExecutor.java:192)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.getContainerStatus(DockerCommandExecutor.java:128)",
                "    at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer(LinuxContainerExecutor.java:935)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask.run(DockerContainerDeletionTask.java:61)",
                "    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "    at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN NodeManager with Docker support enabled.",
                "2. Deploy a Docker container through YARN.",
                "3. Trigger the deletion of the Docker container while it is still running."
            ],
            "ExpectedBehavior": "The Docker container should be removed without any exceptions, and the deletion task should complete successfully.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the deletion task to fail and log an error message.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-3804.json",
        "creation_time": "2015-06-15T08:54:42.000+0000",
        "bug_report": {
            "BugID": "YARN-3804",
            "Title": "ResourceManager Stuck in Standby State Due to Insufficient ACL Permissions for Kerberos User",
            "Description": "When configuring a YARN cluster in secure mode, if the Kerberos user is not included in the 'yarn.admin.acl' configuration, both ResourceManagers (RMs) remain in a standby state indefinitely. This occurs because the user lacks the necessary permissions to execute the 'refreshAdminAcls' operation, leading to repeated failures in transitioning to the active state.",
            "StackTrace": [
                "2015-06-15 12:20:21,556 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn OPERATION=refreshAdminAcls TARGET=AdminService RESULT=FAILURE DESCRIPTION=Unauthorized user PERMISSIONS=",
                "2015-06-15 12:20:21,556 WARN org.apache.hadoop.ha.ActiveStandbyElector: Exception handling the winning of election",
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:645)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:518)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Can not execute refreshAdminAcls",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "... 4 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAdminAcls(AdminService.java:465)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:295)",
                "... 5 more",
                "Caused by: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:182)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:148)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAccess(AdminService.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:228)",
                "... 7 more"
            ],
            "StepsToReproduce": [
                "1. Configure the YARN cluster in secure mode.",
                "2. Set 'yarn.admin.acl' to 'dsperf'.",
                "3. Set 'yarn.resourcemanager.principal' to 'yarn'.",
                "4. Start both ResourceManagers."
            ],
            "ExpectedBehavior": "The ResourceManager should transition to the active state after a few retries or at least on the first attempt, even if the user does not have the necessary permissions.",
            "ObservedBehavior": "Both ResourceManagers remain in standby state indefinitely due to repeated failures in executing 'refreshAdminAcls' because the user 'yarn' lacks the required permissions.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1839.json",
        "creation_time": "2014-03-14T23:52:29.000+0000",
        "bug_report": {
            "BugID": "YARN-1839",
            "Title": "InvalidToken Exception when AM attempt 2 is launched after preemption in Capacity Scheduler",
            "Description": "When using a single-node cluster with the capacity scheduler preemption enabled, an application master (AM) attempt fails to launch a task container due to an InvalidToken exception. This occurs after the first application is preempted by a second application that has completed its job. The error message indicates that no NMToken was sent, which prevents the AM from successfully launching the task container.",
            "StackTrace": [
                "2014-03-13 20:13:50,254 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1394741557066_0001_m_000000_1009: Container launch failed for container_1394741557066_0001_02_000021 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Set up a single-node cluster.",
                "2. Enable capacity scheduler preemption.",
                "3. Run a MapReduce sleep job as application 1, occupying the entire cluster.",
                "4. Run another MapReduce sleep job as application 2.",
                "5. Allow application 2 to preempt application 1.",
                "6. Wait for application 2 to finish.",
                "7. Observe the logs for application 1's AM attempt 2."
            ],
            "ExpectedBehavior": "The application master for application 1 should successfully launch its task containers after being preempted by application 2.",
            "ObservedBehavior": "The application master for application 1 fails to launch its task containers, resulting in an InvalidToken exception due to the absence of an NMToken.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-6714.json",
        "creation_time": "2017-06-15T09:56:15.000+0000",
        "bug_report": {
            "BugID": "YARN-6714",
            "Title": "IllegalStateException during APP_ATTEMPT_REMOVED event handling in CapacityScheduler with async-scheduling enabled",
            "Description": "In the async-scheduling mode of the CapacityScheduler, after an Application Master (AM) failover and the unreservation of all reserved containers, there is a risk of committing an outdated reserve proposal from a failed application attempt. This issue leads to an IllegalStateException being thrown, which causes the ResourceManager (RM) to crash. The error occurs when the application attempts to unreserve resources that are still reserved for a different application attempt.",
            "StackTrace": [
                "java.lang.IllegalStateException: Trying to unreserve for application appattempt_1495188831758_0121_000002 when currently reserved for application application_1495188831758_0121 on node host: node1:45454 #containers=2 available=... used=...",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:152)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": [
                "1. Enable async-scheduling mode in the CapacityScheduler.",
                "2. Submit an application to the YARN cluster.",
                "3. Force a failover of the Application Master (AM) for the submitted application.",
                "4. Observe the unreservation of all reserved containers for the failed application attempt.",
                "5. Monitor the ResourceManager logs for IllegalStateException during the handling of APP_ATTEMPT_REMOVED events."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the APP_ATTEMPT_REMOVED event without throwing an IllegalStateException, allowing for proper resource management and stability of the cluster.",
            "ObservedBehavior": "The ResourceManager crashes with an IllegalStateException when attempting to unreserve resources for a failed application attempt, leading to instability in the cluster.",
            "Resolution": "A fix has been implemented to ensure that both CapacityScheduler#doneApplicationAttempt and CapacityScheduler#tryCommit acquire a write lock before execution, allowing for proper state checks during the commit process to prevent outdated proposals from being committed."
        }
    },
    {
        "filename": "YARN-3351.json",
        "creation_time": "2015-03-16T14:19:59.000+0000",
        "bug_report": {
            "BugID": "YARN-3351",
            "Title": "AppMaster Tracking URL Fails to Resolve in High Availability (HA) Mode",
            "Description": "After the implementation of YARN-2713, the AppMaster tracking URL is broken when running in High Availability (HA) mode. This issue occurs specifically when the first ResourceManager (RM) is not active, leading to a failure in binding the socket for the proxy link.",
            "StackTrace": [
                "java.net.BindException: Cannot assign requested address",
                "at java.net.PlainSocketImpl.socketBind(Native Method)",
                "at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)",
                "at java.net.Socket.bind(Socket.java:631)",
                "at java.net.Socket.<init>(Socket.java:423)",
                "at java.net.Socket.<init>(Socket.java:280)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)",
                "at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)"
            ],
            "StepsToReproduce": [
                "1. Set up ResourceManager (RM) High Availability (HA) with two RMs.",
                "2. Ensure that the first RM is not active.",
                "3. Submit a long-running job (e.g., a sleep job).",
                "4. Navigate to the RM applications page and attempt to view the tracking URL."
            ],
            "ExpectedBehavior": "The AppMaster tracking URL should resolve correctly and allow access to the job tracking information.",
            "ObservedBehavior": "The tracking URL fails to resolve, resulting in a 'Cannot assign requested address' error.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the AppMaster tracking URL functions correctly in HA mode."
        }
    },
    {
        "filename": "YARN-2813.json",
        "creation_time": "2014-11-05T22:29:46.000+0000",
        "bug_report": {
            "BugID": "YARN-2813",
            "Title": "NullPointerException in MemoryTimelineStore.getDomains during TimelineWebServices invocation",
            "Description": "A NullPointerException is thrown when the getDomains method of MemoryTimelineStore is invoked, leading to an INTERNAL_SERVER_ERROR response in the TimelineWebServices. This issue occurs when the server attempts to retrieve domain information but encounters a null reference.",
            "StackTrace": [
                "2014-11-04 20:50:05,146 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:606)",
                "    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "    at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "    at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "    at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "    at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:96)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:572)",
                "    at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:269)",
                "    at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:542)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1204)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "    at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "    at org.mortbay.jetty.Server.handle(Server.java:326)",
                "    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)",
                "    at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:713)",
                "    at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)",
                "    at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)",
                "    at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:353)",
                "    ... 54 more"
            ],
            "StepsToReproduce": [
                "1. Start the YARN Timeline Server.",
                "2. Send a request to the TimelineWebServices endpoint that triggers the getDomains method.",
                "3. Observe the server logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The TimelineWebServices should return a list of domains without throwing an exception.",
            "ObservedBehavior": "The server throws a NullPointerException, resulting in an INTERNAL_SERVER_ERROR response.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1550.json",
        "creation_time": "2013-12-30T03:58:32.000+0000",
        "bug_report": {
            "BugID": "YARN-1550",
            "Title": "NullPointerException in FairSchedulerAppsBlock#render Method",
            "Description": "A NullPointerException (NPE) occurs in the FairSchedulerAppsBlock#render method when attempting to render the scheduler page after submitting an application. This issue arises when the application ID already exists in the ResourceManager context, leading to an error in handling the rendering logic.",
            "StackTrace": [
                "2013-12-30 11:51:43,795 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/scheduler",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock.render(FairSchedulerAppsBlock.java:96)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:66)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:76)"
            ],
            "StepsToReproduce": [
                "1. Debug at RMAppManager#submitApplication after the code block that checks for existing applications.",
                "2. Submit an application using the command: hadoop jar ~/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.0.0-ydh2.2.0-tests.jar sleep -Dhadoop.job.ugi=test2,#111111 -Dmapreduce.job.queuename=p1 -m 1 -mt 1 -r 1",
                "3. Navigate to the scheduler page at http://ip:50030/cluster/scheduler.",
                "4. Observe the 500 ERROR displayed on the page."
            ],
            "ExpectedBehavior": "The scheduler page should render correctly without any errors, displaying the current applications and their statuses.",
            "ObservedBehavior": "A 500 ERROR is displayed on the scheduler page, indicating a server-side issue due to a NullPointerException.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the rendering logic handles existing application IDs correctly."
        }
    },
    {
        "filename": "YARN-5006.json",
        "creation_time": "2016-04-28T08:26:38.000+0000",
        "bug_report": {
            "BugID": "YARN-5006",
            "Title": "ResourceManager Crashes Due to ApplicationStateData Exceeding Znode Size Limit in Zookeeper",
            "Description": "When a client submits a job that adds a large number of files (e.g., 10,000) into the DistributedCache, the ResourceManager attempts to store the ApplicationStateData in Zookeeper. If the size of this data exceeds the maximum allowed size for a znode, the ResourceManager crashes with a ConnectionLossException. This issue is critical as it leads to the failure of the ResourceManager, impacting the entire cluster's ability to manage resources effectively.",
            "StackTrace": [
                "2016-04-20 11:26:35,732 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore AsyncDispatcher event handler: Maxed out ZK retries. Giving up!",
                "2016-04-20 11:26:35,732 ERROR org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore AsyncDispatcher event handler: Error storing app: application_1461061795989_17671",
                "org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:936)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1075)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1096)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:626)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:138)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:123)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that adds a large number of files (e.g., 10,000) to the DistributedCache.",
                "2. Monitor the ResourceManager logs for any errors related to Zookeeper.",
                "3. Observe that the ResourceManager crashes with a ConnectionLossException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the ApplicationStateData appropriately without exceeding the znode size limit, allowing it to continue operating normally.",
            "ObservedBehavior": "The ResourceManager crashes with a ConnectionLossException when the ApplicationStateData exceeds the znode size limit in Zookeeper.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-5728.json",
        "creation_time": "2016-10-13T05:16:28.000+0000",
        "bug_report": {
            "BugID": "YARN-5728",
            "Title": "Timeout Exception in TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization",
            "Description": "The test case `TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization` is failing due to a timeout exception. The test is expected to complete within 60 seconds but is exceeding this limit, leading to a failure.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 60000 milliseconds",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.processWaitTimeAndRetryInfo(RetryInvocationHandler.java:130)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:107)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy85.nodeHeartbeat(Unknown Source)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:113)"
            ],
            "StepsToReproduce": [
                "1. Set up the Hadoop YARN environment.",
                "2. Run the test suite that includes `TestMiniYARNClusterNodeUtilization`.",
                "3. Observe the execution of `testUpdateNodeUtilization`."
            ],
            "ExpectedBehavior": "The test `testUpdateNodeUtilization` should complete successfully within the specified timeout of 60 seconds.",
            "ObservedBehavior": "The test fails with a timeout exception after 60 seconds, indicating that the expected operations did not complete in the allotted time.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-2805.json",
        "creation_time": "2014-11-04T20:37:09.000+0000",
        "bug_report": {
            "BugID": "YARN-2805",
            "Title": "ResourceManager fails to start due to Kerberos login failure in HA setup",
            "Description": "The ResourceManager (RM) in a High Availability (HA) setup attempts to log in using the Kerberos principal of the previous RM, leading to a login failure. This issue prevents the RM from initializing properly, causing service disruptions.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)",
                "Caused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)"
            ],
            "StepsToReproduce": [
                "1. Set up a High Availability (HA) configuration for the ResourceManager.",
                "2. Start the first ResourceManager (RM1) and ensure it is running correctly.",
                "3. Stop RM1 and start the second ResourceManager (RM2).",
                "4. Observe the logs for any login failures related to Kerberos."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully and log in using its own Kerberos principal without any errors.",
            "ObservedBehavior": "The ResourceManager fails to start, logging a YarnRuntimeException indicating a login failure due to an incorrect Kerberos principal being used.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "YARN-4744.json",
        "creation_time": "2016-02-29T10:08:57.000+0000",
        "bug_report": {
            "BugID": "YARN-4744",
            "Title": "Container Failure Due to Excessive Signals in LCE Mode",
            "Description": "When running a MapReduce application (terasort/teragen) in a secure HA cluster with LCE enabled, the system throws an exception due to too many signals sent to the container. This issue occurs specifically when the application is submitted with the user 'yarn' or 'dsperf'.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=9:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer(DefaultLinuxContainerRuntime.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.signalContainer(DelegatingLinuxContainerRuntime.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.signalContainer(LinuxContainerExecutor.java:513)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Install a secure HA cluster.",
                "2. Enable LCE with cgroups.",
                "3. Start the server with the dsperf user.",
                "4. Submit a MapReduce application (terasort/teragen) using the yarn or dsperf user.",
                "5. Observe the logs for exceptions related to container signaling."
            ],
            "ExpectedBehavior": "The MapReduce application should run successfully without throwing exceptions related to container signaling.",
            "ObservedBehavior": "The application fails with an exception indicating too many signals sent to the container, resulting in a PrivilegedOperationException with exit code 9.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-1752.json",
        "creation_time": "2014-02-22T05:51:42.000+0000",
        "bug_report": {
            "BugID": "YARN-1752",
            "Title": "Invalid State Transition: UNREGISTERED Event at LAUNCHED State",
            "Description": "An error occurs when the ResourceManager attempts to handle an UNREGISTERED event while in the LAUNCHED state, resulting in an InvalidStateTransitionException. This issue impairs the application's ability to transition states correctly, potentially leading to resource management failures.",
            "StackTrace": [
                "2014-02-21 14:56:03,453 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED",
                "  at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "  at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "  at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)",
                "  at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "  at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "  at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "1. Launch an application in the YARN ResourceManager.",
                "2. Trigger an event that causes the application to transition to the LAUNCHED state.",
                "3. Attempt to send an UNREGISTERED event to the application while it is in the LAUNCHED state."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the UNREGISTERED event gracefully, allowing for a valid state transition or logging an appropriate error without crashing.",
            "ObservedBehavior": "The ResourceManager throws an InvalidStateTransitionException, indicating that it cannot handle the UNREGISTERED event while in the LAUNCHED state, leading to potential application failures.",
            "Resolution": "[Provide additional details on the fix or workaround]"
        }
    },
    {
        "filename": "YARN-6629.json",
        "creation_time": "2017-05-22T08:31:16.000+0000",
        "bug_report": {
            "BugID": "YARN-6629",
            "Title": "NullPointerException during Container Allocation Proposal Application",
            "Description": "A NullPointerException (NPE) occurs when a container allocation proposal is applied after its resource requests have been removed. This issue arises in the Hadoop YARN resource manager's scheduling process, specifically when handling events related to node updates and resource allocation.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:446)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply(FiCaSchedulerApp.java:516)",
                "at org.apache.hadoop.yarn.client.TestNegativePendingResource$1.answer(TestNegativePendingResource.java:225)",
                "at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:31)",
                "at org.mockito.internal.MockHandler.handle(MockHandler.java:97)",
                "at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp$$EnhancerByMockitoWithCGLIB$$29eb8afc.apply(<generated>)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit(CapacityScheduler.java:2396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.submitResourceCommitRequest(CapacityScheduler.java:2281)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1247)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1236)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1325)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:987)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1367)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1437)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Application Master (AM) and request 1 container with schedulerRequestKey#1.",
                "2. The CapacityScheduler allocates 1 container for this request and accepts the proposal.",
                "3. The AM removes this request.",
                "4. The scheduler applies the proposal, leading to the NPE when trying to access the removed resource requests."
            ],
            "ExpectedBehavior": "The system should handle the removal of resource requests gracefully without throwing a NullPointerException during the allocation proposal application.",
            "ObservedBehavior": "A NullPointerException is thrown when the scheduler attempts to allocate resources after the corresponding requests have been removed.",
            "Resolution": "[Provide additional details about the resolution, if applicable]"
        }
    },
    {
        "filename": "YARN-3493.json",
        "creation_time": "2015-04-15T22:03:19.000+0000",
        "bug_report": {
            "BugID": "YARN-3493",
            "Title": "ResourceManager Fails to Start with Invalid Memory Configuration",
            "Description": "The ResourceManager (RM) fails to start when the memory settings in yarn-site.xml are modified incorrectly. Specifically, if yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb are set to 4000, and a job is submitted with mapreduce.map.memory.mb=4000, the RM cannot recover after the yarn-site.xml is reverted to a lower memory limit (2048). This results in an InvalidResourceRequestException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)"
            ],
            "StepsToReproduce": [
                "1. Modify yarn-site.xml to set yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb to 4000.",
                "2. Start a random text writer job with mapreduce.map.memory.mb=4000 in the background and wait for the job to reach the running state.",
                "3. Revert yarn-site.xml to set yarn.scheduler.maximum-allocation-mb back to 2048 before the job completes.",
                "4. Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without errors, even after the memory settings are changed and reverted.",
            "ObservedBehavior": "The ResourceManager fails to start and logs an error indicating an invalid resource request due to the requested memory exceeding the maximum configured memory.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7645.json",
        "creation_time": "2017-12-12T21:19:53.000+0000",
        "bug_report": {
            "BugID": "YARN-7645",
            "Title": "Flaky Test: TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers Fails with FairScheduler",
            "Description": "The test method `testUsageAfterAMRestartWithMultipleContainers` in the `TestContainerResourceUsage` class exhibits flakiness when executed with the `FairScheduler`. The test fails intermittently, leading to inconsistent results during CI/CD pipelines.",
            "StackTrace": [
                "java.lang.AssertionError: Attempt state is not correct (timeout). expected:<ALLOCATED> but was:<SCHEDULED>",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.amRestartTests(TestContainerResourceUsage.java:275)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.testUsageAfterAMRestartWithMultipleContainers(TestContainerResourceUsage.java:254)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN environment with FairScheduler configured.",
                "2. Execute the test suite containing `TestContainerResourceUsage`.",
                "3. Observe the results of the `testUsageAfterAMRestartWithMultipleContainers` test."
            ],
            "ExpectedBehavior": "The test should pass without any assertion errors, confirming that the application master (AM) state transitions correctly to ALLOCATED after a restart.",
            "ObservedBehavior": "The test fails intermittently with an AssertionError indicating that the expected state was ALLOCATED, but the actual state was SCHEDULED.",
            "Resolution": "A fix for this issue has been checked into the tree and tested, resolving the flakiness of the test."
        }
    },
    {
        "filename": "YARN-6054.json",
        "creation_time": "2017-01-04T20:58:59.000+0000",
        "bug_report": {
            "BugID": "YARN-6054",
            "Title": "TimelineServer Fails to Start Due to Missing LevelDB State Files",
            "Description": "The TimelineServer fails to start when certain LevelDB state files are missing, leading to a ServiceStateException. This issue arises from the inability of the ApplicationHistoryServer to initialize properly when it detects missing files in the LevelDB store.",
            "StackTrace": [
                "2016-11-21 20:46:43,134 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer failed in state INITED; cause: org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:182)",
                "Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)",
                "at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)",
                "at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)",
                "at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:229)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "... 5 more"
            ],
            "StepsToReproduce": [
                "1. Ensure that the LevelDB state files for the TimelineServer are missing.",
                "2. Attempt to start the TimelineServer.",
                "3. Observe the logs for any errors related to service initialization."
            ],
            "ExpectedBehavior": "The TimelineServer should start successfully, even if some LevelDB state files are missing, by implementing graceful degradation.",
            "ObservedBehavior": "The TimelineServer fails to start and logs a ServiceStateException due to missing LevelDB state files.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-196.json",
        "creation_time": "2012-01-16T09:52:45.000+0000",
        "bug_report": {
            "BugID": "YARN-196",
            "Title": "NodeManager Fails to Start When ResourceManager is Not Running",
            "Description": "When the NodeManager (NM) is started before the ResourceManager (RM), it fails to start and shuts down with an error indicating a connection refusal. This behavior can lead to confusion and disrupt the expected operation of the YARN cluster.",
            "StackTrace": [
                "ERROR org.apache.hadoop.yarn.service.CompositeService: Error starting services org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)",
                "... 3 more",
                "Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:131)",
                "at $Proxy23.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "... 5 more",
                "Caused by: java.net.ConnectException: Connection refused",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:857)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1141)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1100)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:128)",
                "... 7 more"
            ],
            "StepsToReproduce": [
                "1. Ensure that the ResourceManager (RM) is not running.",
                "2. Start the NodeManager (NM).",
                "3. Observe the logs for any errors."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and handle the absence of the ResourceManager gracefully, possibly by retrying the connection or logging a warning without shutting down.",
            "ObservedBehavior": "The NodeManager fails to start and shuts down with a connection refused error, indicating it cannot connect to the ResourceManager.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "YARN-8508.json",
        "creation_time": "2018-07-09T23:37:49.000+0000",
        "bug_report": {
            "BugID": "YARN-8508",
            "Title": "GPU Resource Allocation Failure During Container Launch",
            "Description": "The system fails to allocate the requested number of GPUs for a new container, leading to a failure in launching the container. This occurs even when the previous container using the GPU has been killed, indicating a potential issue with GPU resource cleanup.",
            "StackTrace": [
                "2018-07-06 05:22:39,048 ERROR nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:handleLaunchForLaunchType(550)) - ResourceHandlerChain.preStart() failed!",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.internalAssignGpus(GpuResourceAllocator.java:225)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.assignGpus(GpuResourceAllocator.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl.preStart(GpuResourceHandlerImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.preStart(ResourceHandlerChain.java:75)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:509)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:479)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:494)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:306)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:103)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Launch a container that requests 2 GPUs.",
                "2. Monitor the container's lifecycle until it transitions to KILLING state.",
                "3. Attempt to launch a new container that also requests 2 GPUs immediately after the previous container is killed.",
                "4. Observe the logs for GPU allocation errors."
            ],
            "ExpectedBehavior": "The system should successfully allocate the requested number of GPUs for the new container after the previous container has been killed.",
            "ObservedBehavior": "The system fails to allocate the requested GPUs, resulting in an error indicating insufficient available GPUs, despite the previous container being killed.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-2308.json",
        "creation_time": "2014-07-17T10:01:57.000+0000",
        "bug_report": {
            "BugID": "YARN-2308",
            "Title": "NullPointerException during ResourceManager restart after CapacityScheduler queue configuration changes",
            "Description": "A NullPointerException (NPE) occurs when the ResourceManager (RM) is restarted after changes to the CapacityScheduler queue configuration. This issue arises when queues are removed or modified, leading to failures in recovering historical applications, which results in the RM being unable to restart successfully.",
            "StackTrace": [
                "2014-07-16 07:22:46,957 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "    at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Modify the CapacityScheduler queue configuration by removing existing queues and adding new ones.",
                "2. Restart the ResourceManager.",
                "3. Observe the logs for any NullPointerException during the restart process."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully and recover all historical applications without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to restart and throws a NullPointerException due to missing queue configurations for historical applications.",
            "Resolution": "The issue has been fixed in version 2.6.0. Ensure that queue configurations are properly managed before restarting the ResourceManager."
        }
    },
    {
        "filename": "YARN-933.json",
        "creation_time": "2013-07-17T12:29:28.000+0000",
        "bug_report": {
            "BugID": "YARN-933",
            "Title": "InvalidStateTransitionException Occurs During Application Attempt Retry",
            "Description": "When an application attempt fails due to a connection loss, the ResourceManager attempts to retry the application attempt. However, this leads to an InvalidStateTransitionException because the application attempt is already in a failed state, causing the client to exit unexpectedly while the job is still running.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: LAUNCH_FAILED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:630)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:99)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:495)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:476)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Install a cluster with NodeManager (NM) on two machines.",
                "2. Ensure that the ApplicationMaster (AM) is configured with a maximum of 3 retries.",
                "3. Make a successful ping from the ResourceManager (RM) machine to NM1, but ensure that the hostname resolution fails.",
                "4. Execute a job that allocates AppAttempt_1 to NM1.",
                "5. Simulate a connection loss after AppAttempt_1 is allocated.",
                "6. Observe the behavior of the ResourceManager and the client."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the failure of AppAttempt_1 gracefully, retrying the application attempt without throwing an InvalidStateTransitionException, and the client should remain active while the job is still running.",
            "ObservedBehavior": "After AppAttempt_1 fails, the ResourceManager attempts to retry it, leading to an InvalidStateTransitionException. The client exits prematurely even though other application attempts are still running.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-1374.json",
        "creation_time": "2013-10-30T11:49:49.000+0000",
        "bug_report": {
            "BugID": "YARN-1374",
            "Title": "Resource Manager Fails to Start Due to ConcurrentModificationException",
            "Description": "The Resource Manager fails to start, throwing a ConcurrentModificationException during the initialization process. This issue occurs when the service attempts to refresh the hosts list, leading to a failure in the service initialization state.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "\tat java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)",
                "\tat java.util.AbstractList$Itr.next(AbstractList.java:343)",
                "\tat java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN Resource Manager.",
                "2. Ensure that the configuration is set to refresh the hosts list.",
                "3. Observe the logs for any errors during the initialization process."
            ],
            "ExpectedBehavior": "The Resource Manager should start successfully without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start and logs a ConcurrentModificationException, indicating a failure in the service initialization.",
            "Resolution": "A fix for this issue has been implemented and tested in version 2.3.0."
        }
    },
    {
        "filename": "YARN-174.json",
        "creation_time": "2012-10-19T17:25:40.000+0000",
        "bug_report": {
            "BugID": "YARN-174",
            "Title": "NodeManager Fails to Start Due to Invalid Log Directory Path",
            "Description": "The NodeManager fails to start when the log directory path is not specified correctly. The error message indicates that the path should either have a file scheme or be without a scheme. This issue leads to a fatal error in the NodeManager, causing it to exit unexpectedly.",
            "StackTrace": [
                "2012-10-19 12:18:23,941 FATAL [Node Status Updater] nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(277)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService.init(NodeHealthCheckerService.java:48)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged(NodeManager.java:256)",
                "at org.apache.hadoop.yarn.service.AbstractService.changeState(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:112)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.reboot(NodeStatusUpdaterImpl.java:157)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$900(NodeStatusUpdaterImpl.java:63)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:357)"
            ],
            "StepsToReproduce": [
                "1. Set the yarn.log.dir configuration to an invalid path (e.g., '${yarn.log.dir}/userlogs').",
                "2. Start the NodeManager.",
                "3. Observe the logs for the fatal error message."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully without any fatal errors.",
            "ObservedBehavior": "The NodeManager fails to start and logs a fatal error indicating that the specified log directory path is invalid.",
            "Resolution": "Ensure that the yarn.log.dir configuration is set to a valid path with the correct file scheme."
        }
    },
    {
        "filename": "YARN-6448.json",
        "creation_time": "2017-04-05T18:39:49.000+0000",
        "bug_report": {
            "BugID": "YARN-6448",
            "Title": "Continuous Scheduling Thread Crashes Due to IllegalArgumentException in Node Sorting",
            "Description": "The continuous scheduling thread in the FairScheduler crashes when attempting to sort nodes, resulting in an IllegalArgumentException. This occurs because the comparison method used for sorting violates its general contract, particularly when nodes are modified during the sorting process.",
            "StackTrace": [
                "2017-04-04 23:42:26,123 FATAL org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler: Critical thread FairSchedulerContinuousScheduling crashed!",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:899)",
                "at java.util.TimSort.mergeAt(TimSort.java:516)",
                "at java.util.TimSort.mergeForceCollapse(TimSort.java:457)",
                "at java.util.TimSort.sort(TimSort.java:254)",
                "at java.util.Arrays.sort(Arrays.java:1512)",
                "at java.util.ArrayList.sort(ArrayList.java:1454)",
                "at java.util.Collections.sort(Collections.java:175)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN resource manager with the FairScheduler enabled.",
                "2. Modify the nodes in the cluster while the continuous scheduling thread is running.",
                "3. Observe the logs for any exceptions related to node sorting."
            ],
            "ExpectedBehavior": "The continuous scheduling thread should sort the nodes without crashing, even if nodes are modified during the sorting process.",
            "ObservedBehavior": "The continuous scheduling thread crashes with an IllegalArgumentException when attempting to sort nodes, indicating that the comparison method violates its general contract.",
            "Resolution": "A fix has been implemented to ensure that the sorting operation does not violate the comparison contract, preventing the crash."
        }
    },
    {
        "filename": "YARN-4530.json",
        "creation_time": "2015-12-30T15:19:19.000+0000",
        "bug_report": {
            "BugID": "YARN-4530",
            "Title": "NullPointerException in NodeManager due to LocalizedResource download failure",
            "Description": "In our cluster, a failure to download a LocalizedResource triggers a NullPointerException (NPE) that causes the NodeManager to shut down. This issue arises when the resource's modification time changes unexpectedly during the download process, leading to an IOException, which is not handled properly, resulting in an NPE.",
            "StackTrace": [
                "java.lang.NullPointerException at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:176)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:276)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:50)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Deploy a YARN cluster with NodeManager configured.",
                "2. Attempt to download a LocalizedResource that has its modification time changed during the download process.",
                "3. Monitor the NodeManager logs for any download failures."
            ],
            "ExpectedBehavior": "The NodeManager should handle the IOException gracefully without shutting down, allowing for retries or alternative handling of the resource download failure.",
            "ObservedBehavior": "The NodeManager shuts down due to a NullPointerException triggered by an unhandled IOException when the resource's modification time changes during the download.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-7737.json",
        "creation_time": "2018-01-11T19:35:01.000+0000",
        "bug_report": {
            "BugID": "YARN-7737",
            "Title": "FileNotFoundException for prelaunch.err during container failure in YARN",
            "Description": "An exception is thrown when a container fails to launch due to the absence of the prelaunch error log file. The system attempts to access a non-existent file, leading to a FileNotFoundException.",
            "StackTrace": [
                "2018-01-11 19:04:08,036 ERROR org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Failed to get tail of the container's prelaunch error log file",
                "java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Launch a YARN application that is expected to fail during the container prelaunch phase.",
                "2. Monitor the logs for the container launch process.",
                "3. Observe the error message indicating that the prelaunch.err file could not be found."
            ],
            "ExpectedBehavior": "The system should handle the absence of the prelaunch.err file gracefully, either by creating the file or by providing a meaningful error message without throwing an exception.",
            "ObservedBehavior": "The system throws a FileNotFoundException when attempting to access the prelaunch.err file, which does not exist, leading to a failure in handling the container exit.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-5136.json",
        "creation_time": "2016-05-24T15:34:28.000+0000",
        "bug_report": {
            "BugID": "YARN-5136",
            "Title": "IllegalStateException when removing application attempt in Fair Scheduler",
            "Description": "An IllegalStateException is thrown when attempting to remove an application attempt that does not exist in the queue. This issue occurs during the handling of the APP_ATTEMPT_REMOVED event in the Fair Scheduler, leading to a fatal error and subsequent exit of the ResourceManager.",
            "StackTrace": [
                "2016-05-24 23:20:47,202 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Upgrade the Hadoop YARN from version 2.2.0 to 2.4.1-SNAP.",
                "2. Submit an application to the YARN ResourceManager.",
                "3. Trigger the removal of an application attempt that does not exist in the queue.",
                "4. Observe the logs for the IllegalStateException."
            ],
            "ExpectedBehavior": "The application attempt should be removed without throwing an exception, and the ResourceManager should continue to operate normally.",
            "ObservedBehavior": "An IllegalStateException is thrown, indicating that the application attempt does not exist in the queue, leading to a fatal error and exit of the ResourceManager.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8211.json",
        "creation_time": "2018-04-26T02:13:22.000+0000",
        "bug_report": {
            "BugID": "YARN-8211",
            "Title": "BufferUnderflowException in Yarn Registry DNS during TCP Client Operations",
            "Description": "The Yarn registry DNS server is encountering a BufferUnderflowException when processing TCP client requests. This issue arises during the reading of data from the socket channel, specifically when attempting to retrieve the message length from the ByteBuffer.",
            "StackTrace": [
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(50)) - Execution exception when running task in RegistryDNS 76",
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread RegistryDNS 76:",
                "java.nio.BufferUnderflowException",
                "at java.nio.Buffer.nextGetIndex(Buffer.java:500)",
                "at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the Yarn registry DNS server.",
                "2. Send a TCP request to the server that is expected to return a response.",
                "3. Monitor the server logs for any exceptions."
            ],
            "ExpectedBehavior": "The Yarn registry DNS server should successfully process TCP requests and return the appropriate responses without throwing exceptions.",
            "ObservedBehavior": "The server throws a BufferUnderflowException, indicating that it is attempting to read more data from the ByteBuffer than is available.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-2124.json",
        "creation_time": "2014-06-05T07:44:27.000+0000",
        "bug_report": {
            "BugID": "YARN-2124",
            "Title": "NullPointerException in ProportionalCapacityPreemptionPolicy due to premature initialization",
            "Description": "When using the ProportionalCapacityPreemptionPolicy in the YARN scheduler, a NullPointerException (NPE) is raised during the ResourceManager startup. This occurs because the ProportionalCapacityPreemptionPolicy is initialized before the CapacityScheduler, leading to a null ResourceCalculator reference.",
            "StackTrace": [
                "2014-06-05 11:01:33,201 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[SchedulingMonitor (ProportionalCapacityPreemptionPolicy),5,main] threw an Exception.",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)",
                "    at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager with the ProportionalCapacityPreemptionPolicy enabled.",
                "2. Observe the logs during startup."
            ],
            "ExpectedBehavior": "The ResourceManager should start without throwing any exceptions, and the ProportionalCapacityPreemptionPolicy should be initialized correctly.",
            "ObservedBehavior": "A NullPointerException is thrown during the initialization of the ProportionalCapacityPreemptionPolicy, causing the ResourceManager to fail to start properly.",
            "Resolution": "A fix for this issue has been checked into the tree and tested. The initialization order of the ProportionalCapacityPreemptionPolicy and CapacityScheduler has been corrected to ensure that the ResourceCalculator is not null."
        }
    },
    {
        "filename": "YARN-7849.json",
        "creation_time": "2018-01-29T23:49:33.000+0000",
        "bug_report": {
            "BugID": "YARN-7849",
            "Title": "TestMiniYarnClusterNodeUtilization#testUpdateNodeUtilization fails due to incorrect container utilization propagation",
            "Description": "The test case `testUpdateNodeUtilization` in `TestMiniYarnClusterNodeUtilization` is failing because the expected container utilization values are not being propagated correctly to the ResourceManager (RMNode). The test expects specific utilization values but receives null instead, indicating a potential issue in the heartbeat synchronization process.",
            "StackTrace": [
                "java.lang.AssertionError: Containers Utilization not propagated to RMNode expected:<<pmem:1024, vmem:2048, vCores:11.0>> but was:<null>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.verifySimulatedUtilization(TestMiniYarnClusterNodeUtilization.java:227)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:116)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN cluster with the necessary configurations.",
                "2. Run the test suite for `TestMiniYarnClusterNodeUtilization`.",
                "3. Observe the failure in the `testUpdateNodeUtilization` test case."
            ],
            "ExpectedBehavior": "The container utilization values should be correctly propagated to the ResourceManager and SchedulerNode, matching the expected values of <<pmem:1024, vmem:2048, vCores:11.0>>.",
            "ObservedBehavior": "The test fails with an assertion error indicating that the expected container utilization values are null instead of the expected values.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8591.json",
        "creation_time": "2018-07-27T05:56:26.000+0000",
        "bug_report": {
            "BugID": "YARN-8591",
            "Title": "[ATSv2] Null Pointer Exception when checking entity ACL in non-secure cluster",
            "Description": "A Null Pointer Exception (NPE) occurs in the TimelineReaderWebServices when attempting to access entity ACLs in a non-secure cluster environment. This issue arises specifically when the method checkAccess is invoked without proper validation of the entity's access rights, leading to a failure in handling requests appropriately.",
            "StackTrace": [
                "2018-07-27 05:32:03,468 WARN  webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter.doFilter(TimelineReaderWhitelistAuthorizationFilter.java:85)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccess(TimelineReaderWebServices.java:3536)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccessForGenericEntities(TimelineReaderWebServices.java:3513)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:622)"
            ],
            "StepsToReproduce": [
                "1. Set up a non-secure Hadoop YARN cluster.",
                "2. Send a GET request to the TimelineReaderWebServices endpoint for entity ACLs: GET http://<your-timeline-server>:<port>/ws/v2/timeline/apps/application_<app-id>/entities/YARN_CONTAINER?fields=ALL",
                "3. Observe the server logs for any warnings or errors."
            ],
            "ExpectedBehavior": "The server should return the entity ACLs without throwing an exception, even in a non-secure cluster.",
            "ObservedBehavior": "The server throws a Null Pointer Exception, resulting in an INTERNAL_SERVER_ERROR response.",
            "Resolution": "A fix has been implemented to ensure that access checks are properly validated before invoking methods that may lead to a Null Pointer Exception."
        }
    },
    {
        "filename": "YARN-6649.json",
        "creation_time": "2017-05-25T20:36:08.000+0000",
        "bug_report": {
            "BugID": "YARN-6649",
            "Title": "RuntimeException in RollingLevelDBTimelineServer during Object Decoding",
            "Description": "When using the Tez UI, which makes REST API calls to the Timeline Service REST API, some calls return a 500 Internal Server Error. The root cause is related to object decoding failures, specifically a RuntimeException thrown when the system is unable to encode a value class from a specific code. This issue needs to be addressed to prevent internal server errors and instead return a partial message to the client.",
            "StackTrace": [
                "2017-05-30 12:47:10,670 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)",
                "at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:636)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:294)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:588)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:95)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1352)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.util.FSTUtil.rethrow(FSTUtil.java:122)",
                "at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:879)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:414)",
                "at org.apache.hadoop.yarn.server.timeline.EntityFileTimelineStore.getEntity(EntityFileTimelineStore.java:911)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.doGetEntity(TimelineDataManager.java:215)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getEntity(TimelineDataManager.java:202)",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:155)",
                "... 52 more",
                "Caused by: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:240)",
                "at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:877)",
                "... 58 more",
                "Caused by: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTClazzNameRegistry.decodeClass(FSTClazzNameRegistry.java:173)",
                "at org.nustaq.serialization.coders.FSTStreamDecoder.readClass(FSTStreamDecoder.java:431)",
                "at org.nustaq.serialization.FSTObjectInput.readClass(FSTObjectInput.java:853)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:338)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)",
                "at org.nustaq.serialization.serializers.FSTArrayListSerializer.instantiate(FSTArrayListSerializer.java:63)",
                "at org.nustaq.serialization.FSTObjectInput.instantiateAndReadWithSer(FSTObjectInput.java:459)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:354)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:304)",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:238)",
                "... 59 more"
            ],
            "StepsToReproduce": [
                "1. Launch the Tez UI.",
                "2. Perform an action that triggers a REST API call to the Timeline Service.",
                "3. Observe the response from the server."
            ],
            "ExpectedBehavior": "The server should return a valid response without any internal server errors, even if there are issues with object decoding.",
            "ObservedBehavior": "The server returns a 500 Internal Server Error with a RuntimeException indicating an inability to encode a value class.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3742.json",
        "creation_time": "2015-05-29T06:00:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3742",
            "Title": "ResourceManager Shutdown on ZKClient Creation Timeout",
            "Description": "The ResourceManager (RM) shuts down when the ZKClient fails to create a connection, instead of transitioning to StandBy mode. This behavior prevents the RM from continuing to operate and allows another RM to take over, which is critical for high availability in a YARN cluster.",
            "StackTrace": [
                "2015-04-19 01:22:20,513  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED. Cause:",
                "java.io.IOException: Wait for ZKClient creation timed out",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager.",
                "2. Simulate a failure in the ZKClient connection (e.g., by stopping the Zookeeper service).",
                "3. Observe the behavior of the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should transition to StandBy mode and allow another ResourceManager to take over instead of shutting down.",
            "ObservedBehavior": "The ResourceManager shuts down completely when the ZKClient creation times out, leading to a loss of service.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-4984.json",
        "creation_time": "2016-04-21T19:16:03.000+0000",
        "bug_report": {
            "BugID": "YARN-4984",
            "Title": "LogAggregationService Swallows Exception Leading to Thread Leak During App Initialization",
            "Description": "The LogAggregationService fails to handle exceptions properly when attempting to create application directories for log aggregation. This results in stale applications persisting in the NodeManager's state store, causing application initialization failures due to invalid tokens. The exception is swallowed, leading to the creation of aggregator threads for invalid applications, which can lead to resource leaks.",
            "StackTrace": [
                "2016-04-19 23:38:33,039 ERROR logaggregation.LogAggregationService (LogAggregationService.java:run(300)) - Failed to setup application log directory for application_1448060878692_11842",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be found in cache",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1427)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1358)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)",
                "at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)",
                "at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.access$100(LogAggregationService.java:67)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createAppDir(LogAggregationService.java:261)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator(LogAggregationService.java:367)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:320)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:447)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)"
            ],
            "StepsToReproduce": [
                "1. Start a YARN application that requires log aggregation.",
                "2. Ensure that the application encounters a token invalidation issue during initialization.",
                "3. Observe the logs for errors related to log directory setup."
            ],
            "ExpectedBehavior": "The LogAggregationService should properly handle exceptions during the creation of application directories, preventing the creation of aggregator threads for invalid applications.",
            "ObservedBehavior": "The LogAggregationService swallows exceptions, leading to the creation of aggregator threads for applications that fail to initialize due to invalid tokens, resulting in resource leaks.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-4584.json",
        "creation_time": "2016-01-12T09:08:31.000+0000",
        "bug_report": {
            "BugID": "YARN-4584",
            "Title": "ResourceManager Fails to Start After Multiple Application Master Preemptions",
            "Description": "When multiple Application Masters (AMs) are preempted due to resource limits in the default queue, the ResourceManager (RM) fails to restart after a ResourceManager restart. This issue is triggered when the AM is preempted approximately 20 times, leading to a NullPointerException during the recovery process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover(RMAppAttemptImpl.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:953)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:946)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recoverApplication(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recover(RMAppManager.java:464)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1232)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:594)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1022)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1062)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1058)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1705)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1058)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:323)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:127)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:877)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "StepsToReproduce": [
                "Configure 3 queues in the cluster with the following resource allocations: Queue 1 - 40%, Queue 2 - 50%, Default Queue - 10%.",
                "Submit applications to all 3 queues with a container size of 1024MB (e.g., a sleep job with 50 containers on all queues).",
                "Ensure that the Application Master (AM) assigned to the default queue is preempted multiple times (approximately 20 times).",
                "Restart the ResourceManager (RM)."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully restart and recover all applications without throwing exceptions.",
            "ObservedBehavior": "The ResourceManager fails to restart, throwing a NullPointerException during the recovery process of the Application Master attempts.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-2846.json",
        "creation_time": "2014-11-11T15:30:08.000+0000",
        "bug_report": {
            "BugID": "YARN-2846",
            "Title": "Incorrect Exit Code Persistence for Running Containers During NodeManager Restart",
            "Description": "The NodeManager (NM) restart feature is causing running Application Master (AM) containers to be marked as LOST and killed when the NM daemon is stopped. This occurs due to an IOException thrown in the reacquireContainer() method of the ContainerExecutor class, which fails to generate an ExitCodeFile for the running container. The exit code is incorrectly recorded as LOST, leading to incorrect container state after NM restart.",
            "StackTrace": [
                "2014-11-11 00:48:35,214 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(408)) - Memory usage of ProcessTree 22140 for container-id container_1415666714233_0001_01_000084: 53.8 MB of 512 MB physical memory used; 931.3 MB of 1.0 GB virtual memory used",
                "2014-11-11 00:48:35,223 ERROR nodemanager.NodeManager (SignalLogger.java:handle(60)) - RECEIVED SIGNAL 15: SIGTERM",
                "2014-11-11 00:48:35,299 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50060",
                "2014-11-11 00:48:35,337 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:cleanUpApplicationsOnNMShutDown(512)) - Applications still running : [application_1415666714233_0001]",
                "2014-11-11 00:48:35,338 INFO  ipc.Server (Server.java:stop(2437)) - Stopping server on 45454",
                "2014-11-11 00:48:35,344 INFO  ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 45454",
                "2014-11-11 00:48:35,346 INFO  logaggregation.LogAggregationService (LogAggregationService.java:serviceStop(141)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit",
                "2014-11-11 00:48:35,347 INFO  ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder",
                "2014-11-11 00:48:35,347 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(502)) - Aborting log aggregation for application_1415666714233_0001",
                "2014-11-11 00:48:35,348 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(382)) - Aggregation did not complete for application application_1415666714233_0001",
                "2014-11-11 00:48:35,358 WARN  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(476)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.",
                "2014-11-11 00:48:35,406 ERROR launcher.RecoveredContainerLaunch (RecoveredContainerLaunch.java:call(87)) - Unable to recover container container_1415666714233_0001_01_000001",
                "java.io.IOException: Interrupted while waiting for process 20001 to exit",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:46)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)"
            ],
            "StepsToReproduce": [
                "1. Start a NodeManager with an active Application Master (AM) container.",
                "2. Trigger a stop signal (SIGTERM) to the NodeManager.",
                "3. Observe the logs for the container's exit code after the NodeManager restarts."
            ],
            "ExpectedBehavior": "The running AM container should maintain its state and exit code correctly after a NodeManager restart.",
            "ObservedBehavior": "The running AM container is marked as LOST with an incorrect exit code after the NodeManager restart.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7890.json",
        "creation_time": "2018-02-03T21:10:43.000+0000",
        "bug_report": {
            "BugID": "YARN-7890",
            "Title": "NullPointerException during Container Relaunch in YARN NodeManager",
            "Description": "A NullPointerException occurs when attempting to relaunch a container in the YARN NodeManager. This issue arises from the method `getFilecacheDirs()` returning an unmodifiable list that is not properly initialized, leading to a failure in the container relaunch process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)",
                "at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)",
                "at java.util.Collections.unmodifiableList(Collections.java:1287)",
                "at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:49)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start a YARN NodeManager instance.",
                "2. Submit a job that requires container relaunch.",
                "3. Monitor the logs for the NodeManager during the container relaunch process."
            ],
            "ExpectedBehavior": "The container should relaunch successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the container relaunch to fail.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-139.json",
        "creation_time": "2012-10-01T19:51:20.000+0000",
        "bug_report": {
            "BugID": "YARN-139",
            "Title": "InterruptedException during AsyncDispatcher shutdown causes user confusion",
            "Description": "During the shutdown process of the AsyncDispatcher, an InterruptedException is logged, which, while harmless, leads to confusion among users. This issue arises from the way the stop method is implemented in the AsyncDispatcher class, where the InterruptedException is not handled gracefully.",
            "StackTrace": [
                "2012-09-28 14:50:12,477 WARN [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Interrupted Exception while stopping",
                "java.lang.InterruptedException",
                "\tat java.lang.Object.wait(Native Method)",
                "\tat java.lang.Thread.join(Thread.java:1143)",
                "\tat java.lang.Thread.join(Thread.java:1196)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:105)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:437)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:402)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "\tat java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop YARN application using the MRAppMaster.",
                "2. Allow the application to run until completion.",
                "3. Observe the shutdown process of the AsyncDispatcher."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should shut down without logging any warnings or errors, providing a clean exit without user confusion.",
            "ObservedBehavior": "During the shutdown process, a warning message is logged indicating an InterruptedException, which may confuse users as it suggests a problem occurred.",
            "Resolution": "Consider implementing a more graceful handling of InterruptedExceptions in the stop method of AsyncDispatcher to prevent unnecessary logging during normal shutdown."
        }
    },
    {
        "filename": "YARN-42.json",
        "creation_time": "2012-05-14T11:38:55.000+0000",
        "bug_report": {
            "BugID": "YARN-42",
            "Title": "NodeManager Fails to Start Due to NullPointerException on Missing Permissions",
            "Description": "The NodeManager fails to start when it does not have the necessary permissions on the local directory, resulting in a NullPointerException (NPE) during the shutdown process. This issue is critical as it prevents the NodeManager from initializing properly, impacting the overall functionality of the Hadoop YARN framework.",
            "StackTrace": [
                "2012-05-14 16:32:13,468 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:166)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:268)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:284)",
                "Caused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed",
                "\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)",
                "\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)",
                "\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)",
                "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)",
                "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)",
                "\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)",
                "\t... 6 more",
                "2012-05-14 16:32:13,472 INFO org.apache.hadoop.yarn.service.CompositeService: Error stopping org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop(NonAggregatingLogHandler.java:82)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stop(ContainerManagerImpl.java:266)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:182)",
                "\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)",
                "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the NodeManager's local directory does not have the necessary permissions for the user running the NodeManager.",
                "2. Start the NodeManager service.",
                "3. Observe the logs for any errors related to initialization."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and initialize the LocalizationService without errors.",
            "ObservedBehavior": "The NodeManager fails to start, throwing a YarnException due to a failure in initializing the LocalizationService, followed by a NullPointerException during the shutdown process.",
            "Resolution": "[Provide additional details on the resolution, if applicable]"
        }
    },
    {
        "filename": "YARN-7453.json",
        "creation_time": "2017-11-07T09:46:28.000+0000",
        "bug_report": {
            "BugID": "YARN-7453",
            "Title": "ResourceManager Fails to Transition to ACTIVE State After Initial Start",
            "Description": "The ResourceManager (RM) fails to switch to the ACTIVE state after the first successful start, resulting in a continuous loop of state transitions between ACTIVE and STANDBY. This issue is triggered by an authentication error when attempting to recover the RM state from ZooKeeper.",
            "StackTrace": [
                "2017-11-07 15:08:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state",
                "2017-11-07 15:08:11,669 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started",
                "2017-11-07 15:08:11,669 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.5",
                "2017-11-07 15:08:11,670 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state",
                "org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1006)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:910)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:159)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.access$200(CuratorTransactionImpl.java:44)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:129)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:125)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:122)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction.commit(ZKCuratorManager.java:403)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager.safeSetData(ZKCuratorManager.java:372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.getAndIncrementEpoch(ZKRMStateStore.java:493)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1202)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1198)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1198)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:607)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager in a High Availability (HA) configuration.",
                "Observe the logs during the transition from STANDBY to ACTIVE state.",
                "Note the occurrence of the NoAuthException in the logs."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the ACTIVE state after the first successful start without any errors.",
            "ObservedBehavior": "The ResourceManager fails to transition to the ACTIVE state and continuously loops between ACTIVE and STANDBY states due to a NoAuthException.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-3369.json",
        "creation_time": "2015-03-18T23:29:06.000+0000",
        "bug_report": {
            "BugID": "YARN-3369",
            "Title": "NullPointerException in AppSchedulingInfo.checkForDeactivation() due to missing null check",
            "Description": "The method checkForDeactivation() in AppSchedulingInfo.java does not handle the case where getResourceRequest() returns null. This leads to a NullPointerException when the method attempts to dereference the request object without checking for null, causing the ResourceManager to crash.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)",
                "    at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with a configured application.",
                "2. Trigger a NODE_UPDATE event that leads to the invocation of checkForDeactivation().",
                "3. Ensure that the getResourceRequest() method returns null for a priority.",
                "4. Observe the ResourceManager logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the null case gracefully without crashing, allowing the application to continue running.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when checkForDeactivation() attempts to dereference a null ResourceRequest.",
            "Resolution": "A null check should be added in the checkForDeactivation() method to ensure that the ResourceRequest is not null before dereferencing it."
        }
    },
    {
        "filename": "YARN-945.json",
        "creation_time": "2013-07-19T22:59:06.000+0000",
        "bug_report": {
            "BugID": "YARN-945",
            "Title": "AM Registration Fails Due to Missing AMRM Token",
            "Description": "The Application Master (AM) fails to register with the Resource Manager (RM) due to an AccessControlException indicating that SIMPLE authentication is not enabled. This issue arises when the AM attempts to read and process the connection, leading to an authentication failure.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]",
                "at org.apache.hadoop.ipc.Server$Connection.initializeAuthContext(Server.java:1531)",
                "at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1482)",
                "at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:788)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:587)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:562)"
            ],
            "StepsToReproduce": [
                "1. Start the Resource Manager (RM) with SIMPLE authentication disabled.",
                "2. Attempt to launch an Application Master (AM) that requires registration with the RM.",
                "3. Observe the logs for any authentication-related exceptions."
            ],
            "ExpectedBehavior": "The Application Master should successfully register with the Resource Manager without any authentication errors.",
            "ObservedBehavior": "The Application Master fails to register, and the following error is logged: 'SIMPLE authentication is not enabled. Available:[TOKEN]'.",
            "Resolution": "[Provide additional details on the resolution, such as enabling SIMPLE authentication or modifying the AM configuration to use TOKEN authentication.]"
        }
    },
    {
        "filename": "YARN-6072.json",
        "creation_time": "2017-01-08T09:21:12.000+0000",
        "bug_report": {
            "BugID": "YARN-6072",
            "Title": "ResourceManager Fails to Start in Secure Mode Due to NullPointerException",
            "Description": "The ResourceManager fails to start in secure mode, resulting in a NullPointerException during the refresh of service ACLs. This issue occurs because the AdminService's server is null when refreshAll() is invoked, leading to a failure in transitioning to the active state.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:552)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "StepsToReproduce": [
                "1. Configure the ResourceManager to run in secure mode.",
                "2. Start the ResourceManager service.",
                "3. Observe the logs for any errors during the startup process."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully in secure mode without any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing a NullPointerException during the refresh of service ACLs, which prevents it from transitioning to the active state.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-7663.json",
        "creation_time": "2017-12-15T01:52:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7663",
            "Title": "InvalidStateTransitionException when sending START event to KILLED application",
            "Description": "When attempting to send a START event to an application that is already in the KILLED state, an InvalidStateTransitionException is thrown. This occurs in the ResourceManager's application handling logic, specifically within the RMAppImpl class. The issue can be reproduced by introducing a delay before the START event is created.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: START at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:805)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:885)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the ResourceManager.",
                "2. Kill the application before it can transition to a RUNNING state.",
                "3. Attempt to send a START event to the application after it has been killed.",
                "4. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the START event gracefully without throwing an exception, even if the application is in the KILLED state.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the START event cannot be processed for an application that is in the KILLED state.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-5873.json",
        "creation_time": "2016-11-12T09:54:20.000+0000",
        "bug_report": {
            "BugID": "YARN-5873",
            "Title": "NullPointerException in ResourceManager when Generic Application History is Enabled",
            "Description": "The ResourceManager crashes with a NullPointerException (NPE) when the generic application history feature is enabled. This issue occurs during the handling of container start events, specifically when the hashCode method is invoked on a null object.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:217)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:210)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.doAllocation(RegularContainerAllocator.java:746)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:832)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:865)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:931)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1044)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:690)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:508)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1475)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1470)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1559)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1346)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1221)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1601)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:149)",
                "    at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "    at java.lang.Thread.run(Thread.java:745)",
                "2016-11-12 14:22:07,153 INFO SchedulerEventDispatcher:Event Processor org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:79) org.apache.hadoop.yarn.event.EventDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Enable the generic application history feature in the ResourceManager configuration.",
                "2. Start a new application that triggers container start events.",
                "3. Monitor the ResourceManager logs for any NullPointerExceptions."
            ],
            "ExpectedBehavior": "The ResourceManager should handle container start events without crashing, even when the generic application history feature is enabled.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when handling container start events, leading to service disruption.",
            "Resolution": "A fix has been implemented and tested, ensuring that the hashCode method is called on a valid object to prevent NullPointerExceptions."
        }
    },
    {
        "filename": "YARN-3227.json",
        "creation_time": "2015-02-19T16:58:01.000+0000",
        "bug_report": {
            "BugID": "YARN-3227",
            "Title": "Renewal of Timeline Delegation Token Fails When ResourceManager User's TGT is Expired",
            "Description": "When the ResourceManager (RM) user's Kerberos Ticket Granting Ticket (TGT) expires, the operation to renew the delegation token fails during job submission. The expected behavior is for the RM to automatically re-login and obtain a new TGT to continue the renewal process.",
            "StackTrace": [
                "2015-02-06 18:54:05,617 [DelegationTokenRenewer #25954] WARN security.DelegationTokenRenewer: Unable to add the application to the delegation token renewer.",
                "java.io.IOException: Failed to renew token: Kind: TIMELINE_DELEGATION_TOKEN, Service: timelineserver.example.com:4080, Ident: (owner=user, renewer=rmuser, realUser=oozie, issueDate=1423248845528, maxDate=1423853645528, sequenceNumber=9716, masterKeyId=9)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:443)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:808)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:789)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: java.io.IOException: HTTP status [401], message [Unauthorized]",
                "at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:286)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.renewDelegationToken(DelegationTokenAuthenticator.java:211)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.renewDelegationToken(DelegationTokenAuthenticatedURL.java:414)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:374)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:360)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$4.run(TimelineClientImpl.java:429)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:161)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.operateDelegationToken(TimelineClientImpl.java:444)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.renewDelegationToken(TimelineClientImpl.java:378)",
                "at org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier$Renewer.renew(TimelineDelegationTokenIdentifier.java:81)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:532)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:529)"
            ],
            "StepsToReproduce": [
                "1. Ensure the ResourceManager user's Kerberos TGT is expired.",
                "2. Submit a job that requires a timeline delegation token.",
                "3. Observe the logs for warnings or errors related to token renewal."
            ],
            "ExpectedBehavior": "The ResourceManager should automatically re-login to obtain a new TGT and successfully renew the timeline delegation token.",
            "ObservedBehavior": "The job submission fails with an IOException indicating that the token renewal could not be completed due to an expired TGT, resulting in an HTTP 401 Unauthorized error.",
            "Resolution": "A fix for this issue has been implemented and tested in the codebase."
        }
    },
    {
        "filename": "YARN-4235.json",
        "creation_time": "2015-10-07T19:26:24.000+0000",
        "bug_report": {
            "BugID": "YARN-4235",
            "Title": "NPE in FairScheduler when handling empty user groups",
            "Description": "The FairScheduler encounters a NullPointerException (NPE) when it attempts to process application events for users with empty groups. This results in the ResourceManager crashing, as indicated by the stack trace below. The issue arises specifically in the `getQueueForApp` method when it tries to access an index in an empty list.",
            "StackTrace": [
                "2015-09-22 16:51:52,780  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ADDED to the scheduler",
                "java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Configure a user in the system with no associated groups.",
                "2. Submit an application as this user.",
                "3. Monitor the ResourceManager logs for errors."
            ],
            "ExpectedBehavior": "The FairScheduler should handle the case of empty user groups gracefully without throwing an exception, allowing the ResourceManager to continue functioning normally.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when it attempts to process application events for users with empty groups.",
            "Resolution": "[Provide additional details about the fix or workaround]"
        }
    },
    {
        "filename": "YARN-4833.json",
        "creation_time": "2016-03-17T13:22:23.000+0000",
        "bug_report": {
            "BugID": "YARN-4833",
            "Title": "YARN Client Retries Excessively on AccessControlException During Application Submission",
            "Description": "When submitting an application to a YARN queue with ACLs enabled, if the submitting user does not have the necessary permissions, the client retries the submission multiple times (up to 10) despite receiving an AccessControlException. This behavior leads to unnecessary load on the ResourceManager and can cause delays in job processing.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.call(ProtobufRpcEngine.java:637)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2360)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2356)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2356)"
            ],
            "StepsToReproduce": [
                "1. Configure a YARN queue with ACLs enabled.",
                "2. Attempt to submit an application using a user that does not have permission to submit to the specified queue.",
                "3. Observe the client behavior as it retries the submission multiple times."
            ],
            "ExpectedBehavior": "The YARN client should handle the AccessControlException gracefully and not retry the submission multiple times.",
            "ObservedBehavior": "The YARN client retries the application submission up to 10 times, leading to unnecessary load and delays.",
            "Resolution": "As per discussion with [Provide additional details], the AccessControlException should be handled in the RetryPolicy, and the client should switch to a fallback policy. Additionally, wrap the AccessControlException in a YarnException in the RMAppManager#submitApplication method."
        }
    },
    {
        "filename": "YARN-1689.json",
        "creation_time": "2014-02-05T19:16:00.000+0000",
        "bug_report": {
            "BugID": "YARN-1689",
            "Title": "NullPointerException in ResourceManager when registering ApplicationMaster",
            "Description": "When running Hive on Tez jobs, the ResourceManager (RM) becomes unusable after a period of time, preventing any jobs from running. The RM logs indicate a NullPointerException occurring during the registration of the ApplicationMaster, which leads to an invalid state transition error.",
            "StackTrace": [
                "2014-02-04 20:28:08,553 WARN  ipc.Server (Server.java:run(1978)) - IPC Server handler 0 on 8030, call org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster from 172.18.145.156:40474 Call#0 Retry#0: error: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)",
                "    at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)",
                "    at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)",
                "    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:396)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)",
                "2014-02-04 20:28:08,544 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(626)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:624)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:81)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:656)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:640)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "    at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager and submit a Hive on Tez job.",
                "2. Monitor the ResourceManager logs for any exceptions.",
                "3. Wait for a period of time until the ResourceManager becomes unresponsive.",
                "4. Observe the NullPointerException in the logs when the ApplicationMaster attempts to register."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully register the ApplicationMaster and continue to manage jobs without entering an unusable state.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException during the ApplicationMaster registration process, leading to an invalid state transition and preventing any jobs from running.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-5594.json",
        "creation_time": "2016-08-30T15:14:19.000+0000",
        "bug_report": {
            "BugID": "YARN-5594",
            "Title": "InvalidProtocolBufferException during ResourceManager recovery after upgrade from v2.5.1 to v2.7.0",
            "Description": "After upgrading the cluster from version 2.5.1 to 2.7.0, the ResourceManager fails to load/recover its state due to an InvalidProtocolBufferException. This issue arises from incompatible formats of the RMDelegationToken files used in the different Hadoop versions.",
            "StackTrace": [
                "2016-08-25 17:20:33,293 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state",
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).",
                "at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4644)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4740)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4735)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:5075)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:4955)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:337)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:267)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:210)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadState(FileSystemRMStateStore.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1007)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1044)"
            ],
            "StepsToReproduce": [
                "Upgrade the Hadoop cluster from version 2.5.1 to 2.7.0.",
                "Start the ResourceManager service.",
                "Observe the logs for any errors related to state recovery."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover its state without any errors after the upgrade.",
            "ObservedBehavior": "The ResourceManager fails to recover its state and logs an InvalidProtocolBufferException due to incompatible file formats.",
            "Resolution": "Implement a fix to handle the old data format during ResourceManager recovery when an InvalidProtocolBufferException occurs."
        }
    },
    {
        "filename": "YARN-7511.json",
        "creation_time": "2017-11-16T11:41:43.000+0000",
        "bug_report": {
            "BugID": "YARN-7511",
            "Title": "NullPointerException in ContainerLocalizer During Resource Localization Failure",
            "Description": "A NullPointerException (NPE) occurs in the ContainerLocalizer when resource localization fails for a running container. This issue arises when a null LocalResourceRequest is sent during the localization failure process, leading to an attempt to remove a null request from the pending resources.",
            "StackTrace": [
                "2017-09-30 20:14:32,839 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "    at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)",
                "    at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "    at java.lang.Thread.run(Thread.java:834)",
                "2017-09-30 20:14:32,842 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start a container and ensure it is running.",
                "2. Trigger a localization failure by calling ContainerManagerImpl#localize for the running container.",
                "3. Observe that the localization fails in ResourceLocalizationService$LocalizerRunner#run, which sends out a ContainerResourceFailedEvent with a null LocalResourceRequest.",
                "4. The system attempts to handle the failure, leading to a NullPointerException in ResourceSet.resourceLocalizationFailed."
            ],
            "ExpectedBehavior": "The system should handle resource localization failures gracefully without throwing a NullPointerException, ensuring that no null requests are processed.",
            "ObservedBehavior": "A NullPointerException is thrown when the system attempts to remove a null LocalResourceRequest from the pending resources, causing a failure in the dispatcher thread.",
            "Resolution": "Implement a check to ensure that the LocalResourceRequest is not null before attempting to remove it from the pending resources in ResourceSet.resourceLocalizationFailed."
        }
    },
    {
        "filename": "YARN-3790.json",
        "creation_time": "2015-06-10T04:53:40.000+0000",
        "bug_report": {
            "BugID": "YARN-3790",
            "Title": "AssertionError in TestWorkPreservingRMRestart.testSchedulerRecovery due to stale usedResource metrics",
            "Description": "The test case `testSchedulerRecovery` in `TestWorkPreservingRMRestart` fails with an `AssertionError` indicating that the expected used resource value is 6144, but the actual value is 8192. This discrepancy suggests that the resource metrics may not be updated correctly after recovering the container, leading to stale data being used in assertions.",
            "StackTrace": [
                "java.lang.AssertionError: expected:<6144> but was:<8192>",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.junit.Assert.failNotEquals(Assert.java:743)",
                "\tat org.junit.Assert.assertEquals(Assert.java:118)",
                "\tat org.junit.Assert.assertEquals(Assert.java:555)",
                "\tat org.junit.Assert.assertEquals(Assert.java:542)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.assertMetrics(TestWorkPreservingRMRestart.java:853)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.checkFSQueue(TestWorkPreservingRMRestart.java:342)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.testSchedulerRecovery(TestWorkPreservingRMRestart.java:241)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN environment with the necessary configurations.",
                "2. Run the test suite that includes `TestWorkPreservingRMRestart`.",
                "3. Observe the failure in the `testSchedulerRecovery` test case."
            ],
            "ExpectedBehavior": "The test should pass, confirming that the used resource metrics are correctly updated and reflect the expected values after container recovery.",
            "ObservedBehavior": "The test fails with an `AssertionError`, indicating a mismatch between the expected and actual used resource values.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-6068.json",
        "creation_time": "2017-01-07T03:16:07.000+0000",
        "bug_report": {
            "BugID": "YARN-6068",
            "Title": "Log Aggregation Fails After NodeManager Restart Despite Recovery Mechanism",
            "Description": "The log aggregation process fails to complete after a NodeManager (NM) restart, even when recovery mechanisms are in place. This issue is indicated by the following exception in the logs:\n\n```\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING\n```\n\nThe stack trace shows that the application is unable to handle the event due to an invalid state transition, which suggests a flaw in the state management logic of the application.",
            "StackTrace": [
                "2017-01-05 19:16:36,352 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(527)) - Aborting log aggregation for application_1483640789847_0001",
                "2017-01-05 19:16:36,352 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(399)) - Aggregation did not complete for application application_1483640789847_0001",
                "2017-01-05 19:16:36,353 WARN  application.ApplicationImpl (ApplicationImpl.java:handle(461)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-05 19:16:36,355 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1483640789847_0001 transitioned from RUNNING to null"
            ],
            "StepsToReproduce": [
                "1. Start a YARN application and ensure it is running.",
                "2. Restart the NodeManager while the application is still running.",
                "3. Observe the logs for the application after the NodeManager restart."
            ],
            "ExpectedBehavior": "The log aggregation process should complete successfully after a NodeManager restart, utilizing the recovery mechanisms in place.",
            "ObservedBehavior": "The log aggregation fails with an InvalidStateTransitonException, indicating that the application cannot handle the log handling event due to being in an invalid state.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-903.json",
        "creation_time": "2013-07-07T08:35:30.000+0000",
        "bug_report": {
            "BugID": "YARN-903",
            "Title": "YARN DistributedShell Logs Errors Despite Successful Container Execution",
            "Description": "When running the DistributedShell application, the logs indicate errors related to container management, even though the application completes successfully. This behavior is misleading and may cause confusion for users monitoring the application logs.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000002 is not handled by this NodeManager",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)",
                "at org.apache.hadoop.yarn.proto.ContainerManagementProtocol$ContainerManagementProtocolService$2.callBlockingMethod(ContainerManagementProtocol.java:85)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1868)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1864)"
            ],
            "StepsToReproduce": [
                "1. Deploy the DistributedShell application on a YARN cluster.",
                "2. Monitor the logs of the NodeManager and ApplicationMaster during the execution.",
                "3. Observe the logs for any error messages related to container management."
            ],
            "ExpectedBehavior": "The application should run without logging any errors if it completes successfully.",
            "ObservedBehavior": "The application logs contain errors indicating that certain containers are not handled by the NodeManager, despite the application completing successfully.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-8236.json",
        "creation_time": "2018-04-29T16:28:11.000+0000",
        "bug_report": {
            "BugID": "YARN-8236",
            "Title": "NullPointerException in ServiceClient due to Invalid Kerberos Principal File Name",
            "Description": "A NullPointerException (NPE) occurs in the ServiceClient class when attempting to add a keytab resource if the Kerberos principal file name is invalid. This issue arises during the application submission process, leading to a failure in the service client operations.",
            "StackTrace": [
                "2018-04-29 16:22:54,266 WARN webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the Kerberos principal file name is invalid or incorrectly configured.",
                "2. Attempt to submit an application using the ServiceClient.",
                "3. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The application should be submitted successfully without any exceptions, even if the Kerberos principal file name is invalid.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application submission to fail.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-2857.json",
        "creation_time": "2014-10-24T20:47:51.000+0000",
        "bug_report": {
            "BugID": "YARN-2857",
            "Title": "ConcurrentModificationException in ContainerLogAppender during log closure",
            "Description": "A ConcurrentModificationException is thrown when the ContainerLogAppender attempts to close while iterating over its log events. This issue occurs in the context of a Hadoop job submitted via Oozie, specifically when running a Pig script. The exception disrupts the normal shutdown process of the logging system.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)",
                "at org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)",
                "at org.apache.log4j.Category.removeAllAppenders(Category.java:891)",
                "at org.apache.log4j.Hierarchy.shutdown(Hierarchy.java:471)",
                "at org.apache.log4j.LogManager.shutdown(LogManager.java:267)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogsShutdown(TaskLog.java:286)",
                "at org.apache.hadoop.mapred.TaskLog$2.run(TaskLog.java:339)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with Oozie and Pig.",
                "2. Submit a Pig script as a job through Oozie.",
                "3. Monitor the logs during the job execution.",
                "4. Observe the logs for any ConcurrentModificationException messages."
            ],
            "ExpectedBehavior": "The logging system should close gracefully without throwing any exceptions, allowing for proper cleanup of resources.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, indicating that the log closure process is attempting to modify a collection while it is being iterated over, leading to potential resource leaks and incomplete logging.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the logging system can close without encountering ConcurrentModificationExceptions."
        }
    },
    {
        "filename": "YARN-2416.json",
        "creation_time": "2014-08-13T22:36:31.000+0000",
        "bug_report": {
            "BugID": "YARN-2416",
            "Title": "InvalidStateTransitionException in ResourceManager when AMLauncher fails to receive timely response from startContainers() call",
            "Description": "The ResourceManager encounters an InvalidStateTransitionException when the AMLauncher does not receive a timely response from the startContainers() call. This leads to the RMAppAttempt remaining in the ALLOCATED state, causing subsequent events like REGISTERED, STATUS_UPDATE, and CONTAINER_ALLOCATED to fail with the same exception.",
            "StackTrace": [
                "2014-07-05 08:59:05,021 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: REGISTERED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Launch an application using the AMLauncher.",
                "2. Ensure that the startContainers() call is delayed or does not return in a timely manner.",
                "3. Observe the state of the RMAppAttempt in the ResourceManager."
            ],
            "ExpectedBehavior": "The RMAppAttempt should transition from ALLOCATED to LAUNCHED state upon receiving a response from the startContainers() call.",
            "ObservedBehavior": "The RMAppAttempt remains in the ALLOCATED state, leading to InvalidStateTransitionException when the application master sends REGISTERED, STATUS_UPDATE, or CONTAINER_ALLOCATED events.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-345.json",
        "creation_time": "2013-01-17T12:57:46.000+0000",
        "bug_report": {
            "BugID": "YARN-345",
            "Title": "InvalidStateTransitionException Errors in Application State Handling",
            "Description": "The Node Manager is encountering multiple instances of InvalidStateTransitionException when processing application events, particularly during state transitions. This issue is causing the application to fail to handle certain events correctly, leading to potential resource management problems.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the YARN Node Manager.",
                "2. Monitor the application state transitions in the logs.",
                "3. Observe the occurrence of InvalidStateTransitionException errors during state transitions."
            ],
            "ExpectedBehavior": "The Node Manager should handle application events without throwing InvalidStateTransitionException, allowing for smooth state transitions and resource management.",
            "ObservedBehavior": "The Node Manager throws InvalidStateTransitionException for various events (e.g., FINISH_APPLICATION, INIT_CONTAINER) when the application is in certain states, leading to warnings in the logs and potential application failures.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3894.json",
        "creation_time": "2015-07-08T07:00:51.000+0000",
        "bug_report": {
            "BugID": "YARN-3894",
            "Title": "ResourceManager Fails to Start Due to NodeLabel Capacity Configuration Mismatch",
            "Description": "When the ResourceManager (RM) is configured with an incorrect capacity for NodeLabels in the Capacity Scheduler, it fails to start. This issue occurs specifically when the NodeLabel capacity configuration is inconsistent with the existing NodeLabels. The initialization process does not handle the mismatch correctly, leading to an IOException during the queue reinitialization phase.",
            "StackTrace": [
                "2015-07-07 19:18:25,655 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=0.5, absoluteCapacity=0.5, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0",
                "2015-07-07 19:18:25,656 WARN org.apache.hadoop.yarn.server.resourcemanager.AdminService: Exception refresh queues.",
                "java.io.IOException: Failed to re-init queues",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:383)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:376)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:605)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "Caused by: java.lang.IllegalArgumentException: Illegal capacity of 0.5 for children of queue root for label=node2",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues(ParentQueue.java:159)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:639)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:503)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:379)",
                "    ... 8 more"
            ],
            "StepsToReproduce": [
                "1. Configure ResourceManager with the Capacity Scheduler.",
                "2. Add one or two NodeLabels using the 'rmadmin' command.",
                "3. Configure the capacity XML with NodeLabels but introduce an issue with the capacity configuration for the already added labels.",
                "4. Restart both ResourceManager instances.",
                "5. Check the service initialization logs to see if the Capacity Scheduler's NodeLabel list is populated."
            ],
            "ExpectedBehavior": "The ResourceManager should fail to start due to the incorrect NodeLabel capacity configuration.",
            "ObservedBehavior": "The ResourceManager attempts to start but throws an IOException indicating a failure to reinitialize queues due to illegal capacity settings.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-1903.json",
        "creation_time": "2014-04-04T20:51:24.000+0000",
        "bug_report": {
            "BugID": "YARN-1903",
            "Title": "Container Status Not Updated After Killing Container in NEW and LOCALIZING States",
            "Description": "The container status after stopping a container in the NEW and LOCALIZING states does not reflect the expected exit code and diagnostics. This issue leads to an assertion failure in the test case, indicating that the container's state is not being updated correctly upon termination.",
            "StackTrace": [
                "java.lang.AssertionError: 4:",
                "at org.junit.Assert.fail(Assert.java:93)",
                "at org.junit.Assert.assertTrue(Assert.java:43)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testGetContainerStatus(TestNMClient.java:382)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testContainerManagement(TestNMClient.java:346)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient(TestNMClient.java:226)"
            ],
            "StepsToReproduce": [
                "1. Start a container in the NEW state.",
                "2. Attempt to kill the container while it is still in the NEW state.",
                "3. Check the container status after the kill operation."
            ],
            "ExpectedBehavior": "The container status should reflect the exit code and diagnostics after the container is killed, indicating that it has transitioned to a TERMINATED state with appropriate exit information.",
            "ObservedBehavior": "The container status remains unchanged, and the exit code and diagnostics are not set, leading to an assertion failure in the test case.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4347.json",
        "creation_time": "2015-11-11T22:32:59.000+0000",
        "bug_report": {
            "BugID": "YARN-4347",
            "Title": "Resource Manager Null Pointer Exception During Application Recovery",
            "Description": "The Resource Manager fails with a Null Pointer Exception (NPE) when attempting to load or recover a finished application. This issue occurs specifically in the CapacityScheduler's addApplicationAttempt method, which is invoked during the recovery process of application attempts.",
            "StackTrace": [
                "2015-11-11 17:53:22,351 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(597)) - Failed to load/recover state",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:755)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:839)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:854)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:844)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:411)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1219)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:593)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1026)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1067)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1063)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)"
            ],
            "StepsToReproduce": [
                "1. Start the Resource Manager with a YARN application.",
                "2. Allow the application to finish its execution.",
                "3. Restart the Resource Manager.",
                "4. Observe the logs for any errors during the recovery process."
            ],
            "ExpectedBehavior": "The Resource Manager should successfully recover the state of finished applications without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager throws a Null Pointer Exception during the recovery process, preventing it from loading the state of finished applications.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-1692.json",
        "creation_time": "2014-02-07T02:01:17.000+0000",
        "bug_report": {
            "BugID": "YARN-1692",
            "Title": "ConcurrentModificationException in Fair Scheduler during Demand Update",
            "Description": "A ConcurrentModificationException is thrown in the Fair Scheduler's UpdateThread when iterating over application priorities. This occurs due to unsynchronized access to a shared resource, specifically the map returned by FSSchedulerApp.getResourceRequests().",
            "StackTrace": [
                "2014-02-07 01:40:01,978 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Exception in fair scheduler UpdateThread",
                "java.util.ConcurrentModificationException",
                "    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)",
                "    at java.util.HashMap$ValueIterator.next(HashMap.java:954)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)",
                "    at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN resource manager with the Fair Scheduler enabled.",
                "2. Submit multiple applications that require resource allocation.",
                "3. Monitor the logs for the Fair Scheduler during the demand update phase."
            ],
            "ExpectedBehavior": "The Fair Scheduler should update the demand for resources without throwing any exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, causing the Fair Scheduler to fail in updating resource demands.",
            "Resolution": "The issue has been fixed by ensuring proper synchronization when accessing shared resources in the Fair Scheduler."
        }
    },
    {
        "filename": "YARN-7697.json",
        "creation_time": "2018-01-03T19:28:50.000+0000",
        "bug_report": {
            "BugID": "YARN-7697",
            "Title": "OutOfMemoryError in Log Aggregation Service Causes NodeManager Shutdown",
            "Description": "The NodeManager experiences a fatal shutdown due to an OutOfMemoryError triggered during the log aggregation process. This issue arises when the log aggregation service attempts to load indexed logs metadata, leading to excessive memory consumption.",
            "StackTrace": [
                "2017-12-29 01:43:50,601 FATAL yarn.YarnUncaughtExceptionHandler (YarnUncaughtExceptionHandler.java:uncaughtException(51)) - Thread Thread[LogAggregationService #0,5,main] threw an Error.  Shutting down now...",
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:823)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:840)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling(LogAggregationIndexedFileController.java:293)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.access$600(LogAggregationIndexedFileController.java:98)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:216)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:205)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:312)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:284)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN NodeManager.",
                "2. Submit a job that generates a significant amount of log data.",
                "3. Monitor the NodeManager logs for log aggregation activity.",
                "4. Observe the memory usage of the NodeManager process.",
                "5. Wait for the log aggregation service to attempt to load indexed logs metadata."
            ],
            "ExpectedBehavior": "The NodeManager should successfully aggregate logs without running out of memory, allowing it to continue operating normally.",
            "ObservedBehavior": "The NodeManager crashes with an OutOfMemoryError, leading to a shutdown of the log aggregation service.",
            "Resolution": "[Provide additional details on the fix or workaround applied]"
        }
    },
    {
        "filename": "YARN-7382.json",
        "creation_time": "2017-10-23T23:36:59.000+0000",
        "bug_report": {
            "BugID": "YARN-7382",
            "Title": "NoSuchElementException in FairScheduler after ResourceManager Failover Causes Crash",
            "Description": "During the execution of a MapReduce job, a failover of the ResourceManager (RM) occurs. Once the map tasks reach 100% completion, the newly active RM crashes due to a NoSuchElementException when attempting to handle a NODE_UPDATE event. This issue leaves the cluster without any active ResourceManagers.",
            "StackTrace": [
                "java.util.NoSuchElementException",
                "at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)",
                "at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:128)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job (e.g., a simple sleep job) on the Hadoop cluster.",
                "2. Simulate a failover of the ResourceManager during the job execution.",
                "3. Monitor the ResourceManager logs for any exceptions after the map tasks reach 100% completion."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the NODE_UPDATE event gracefully and continue to manage the cluster without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NoSuchElementException when processing the NODE_UPDATE event after a failover, leaving the cluster without any active ResourceManagers.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1094.json",
        "creation_time": "2013-08-23T19:06:17.000+0000",
        "bug_report": {
            "BugID": "YARN-1094",
            "Title": "Null Pointer Exception during ResourceManager Restart in Secure Environment",
            "Description": "When attempting to restart the ResourceManager while a job is running, a Null Pointer Exception occurs, preventing the ResourceManager from starting successfully. This issue arises specifically in a secure environment, leading to application recovery failures.",
            "StackTrace": [
                "2013-08-23 17:57:40,705 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(370)) - Recovering application application_1377280618693_0001",
                "2013-08-23 17:57:40,763 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(617)) - Failed to load/recover state",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.setTimerForTokenRenewal(DelegationTokenRenewer.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplication(DelegationTokenRenewer.java:307)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:819)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:613)",
                "        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:832)",
                "2013-08-23 17:57:40,766 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1"
            ],
            "StepsToReproduce": [
                "1. Enable the rmrestart feature in a secure environment.",
                "2. Start a job using the ResourceManager.",
                "3. Attempt to restart the ResourceManager while the job is still running."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully without throwing any exceptions, allowing for application recovery.",
            "ObservedBehavior": "The ResourceManager fails to start and throws a Null Pointer Exception, preventing recovery of the application state.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-7269.json",
        "creation_time": "2017-09-28T23:56:42.000+0000",
        "bug_report": {
            "BugID": "YARN-7269",
            "Title": "Redirection Failure for ApplicationMaster Tracking URL in Running Applications",
            "Description": "The application fails to redirect the tracking URL to the ApplicationMaster for running applications, resulting in a ServletException. This issue occurs when the system cannot determine the appropriate proxy server for redirection.",
            "StackTrace": [
                "javax.servlet.ServletException: Could not determine the proxy server for redirection",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:199)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:141)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1426)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(ServletHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(ServletHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ServletHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(ServletHandler.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "1. Start a running application in the Hadoop YARN environment.",
                "2. Attempt to access the tracking URL for the application.",
                "3. Observe the resulting error message indicating a failure to redirect."
            ],
            "ExpectedBehavior": "The tracking URL should successfully redirect to the ApplicationMaster for running applications.",
            "ObservedBehavior": "The application fails to redirect the tracking URL, resulting in a ServletException stating 'Could not determine the proxy server for redirection'.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-7249.json",
        "creation_time": "2017-09-25T16:49:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7249",
            "Title": "NullPointerException in CapacityScheduler when preempting container on node removal",
            "Description": "A NullPointerException occurs in the CapacityScheduler when a container is preempted while the node it is running on is being removed. This issue arises under specific conditions: \n1) A node is being removed from the scheduler.\n2) A container running on that node is being preempted.\n3) A rare race condition leads to the scheduler passing a null node to the leaf queue.\n\nThe proposed fix is to add a null node check inside the CapacityScheduler to prevent this exception.",
            "StackTrace": [
                "2017-08-31 02:51:24,748 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(714)) - Error in handling event type KILL_RESERVED_CONTAINER to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager with a node that has running containers.",
                "2. Trigger the removal of the node from the scheduler.",
                "3. Simultaneously, initiate the preemption of a container running on that node.",
                "4. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The CapacityScheduler should handle the removal of the node and the preemption of the container without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the CapacityScheduler, causing the ResourceManager to log a fatal error and potentially disrupt scheduling operations.",
            "Resolution": "A fix has been implemented to add a null node check in the CapacityScheduler to prevent this issue from occurring."
        }
    },
    {
        "filename": "YARN-4598.json",
        "creation_time": "2016-01-15T06:48:48.000+0000",
        "bug_report": {
            "BugID": "YARN-4598",
            "Title": "Invalid State Transition: RESOURCE_FAILED Event Not Handled in CONTAINER_CLEANEDUP_AFTER_KILL State",
            "Description": "In the Hadoop YARN cluster, an invalid state transition occurs when a container in the CONTAINER_CLEANEDUP_AFTER_KILL state receives a RESOURCE_FAILED event. This results in an InvalidStateTransitionException being thrown, indicating that the event cannot be processed in the current state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:83)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1071)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Deploy a YARN cluster with multiple containers.",
                "2. Forcefully kill a container to transition it to the CONTAINER_CLEANEDUP_AFTER_KILL state.",
                "3. Trigger a RESOURCE_FAILED event for the killed container.",
                "4. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The container should handle the RESOURCE_FAILED event gracefully, transitioning to an appropriate state without throwing an exception.",
            "ObservedBehavior": "The container throws an InvalidStateTransitionException when it receives a RESOURCE_FAILED event while in the CONTAINER_CLEANEDUP_AFTER_KILL state.",
            "Resolution": "[Provide additional details on the fix or workaround]"
        }
    },
    {
        "filename": "YARN-1149.json",
        "creation_time": "2013-09-04T21:46:58.000+0000",
        "bug_report": {
            "BugID": "YARN-1149",
            "Title": "InvalidStateTransitionException Thrown When Application Log Handling Finishes During NodeManager Shutdown",
            "Description": "When the NodeManager receives a kill signal after an application has finished execution but before log aggregation has completed, an InvalidStateTransitionException is thrown. This occurs specifically when the application is in the RUNNING state and the event APPLICATION_LOG_HANDLING_FINISHED is processed.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:689)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start an application in the YARN cluster.",
                "2. Allow the application to finish execution.",
                "3. Send a kill signal to the NodeManager before log aggregation has completed.",
                "4. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The NodeManager should handle the application log handling completion gracefully without throwing an exception.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the application cannot handle the APPLICATION_LOG_HANDLING_FINISHED event while in the RUNNING state.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "YARN-7818.json",
        "creation_time": "2018-01-25T18:42:55.000+0000",
        "bug_report": {
            "BugID": "YARN-7818",
            "Title": "Container Launch Fails with Exit Code 143 After NodeManager Restart",
            "Description": "When running a Dshell application with multiple containers, the application fails to launch new containers after the NodeManager (NM) is restarted. The existing containers remain in a RUNNING state, but the new attempt does not start, leading to an exit code of 143 for the failed container launch.",
            "StackTrace": [
                "2018-01-24 09:48:30,547 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1516787230461_0001_01_000003 transitioned from RUNNING to KILLING",
                "2018-01-24 09:48:30,547 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1516787230461_0001_01_000003",
                "2018-01-24 09:48:30,552 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 143.",
                "2018-01-24 09:48:30,553 WARN  runtime.DefaultLinuxContainerRuntime (DefaultLinuxContainerRuntime.java:launchContainer(127)) - Launch container failed. Exception:",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.launchContainer(DefaultLinuxContainerRuntime.java:124)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.launchContainer(DelegatingLinuxContainerRuntime.java:152)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:549)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:285)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1) Run the Dshell Application using the following command:",
                "{code:java}",
                "yarn org.apache.hadoop.yarn.applications.distributedshell.Client -jar /usr/hdp/3.0.0.0-751/hadoop-yarn/hadoop-yarn-applications-distributedshell-*.jar -keep_containers_across_application_attempts -timeout 900000 -shell_command \"sleep 110\" -num_containers 4",
                "{code}",
                "2) Identify the host where the Application Master (AM) is running.",
                "3) Locate the containers launched by the application.",
                "4) Restart the NodeManager where the AM is running.",
                "5) Validate that a new attempt is not started and that the previously launched containers are still in RUNNING state."
            ],
            "ExpectedBehavior": "After restarting the NodeManager, a new attempt should start, and all previously launched containers should remain in a RUNNING state without any errors.",
            "ObservedBehavior": "The new attempt does not start, and the previously launched containers remain in RUNNING state. The container launch fails with exit code 143.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    }
]