[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6633",
            "Title": "Application Master Fails to Retry Map Tasks on Compression Errors",
            "Description": "When a reduce task encounters compression-related errors, the Application Master (AM) does not retry the corresponding map task. This can lead to job failures, especially when the failure is due to a bad drive on the node where the map task was executed. The expected behavior is for the AM to retry the map task on a different node, which would likely allow the job to succeed.",
            "StackTrace": [
                "2016-01-27 13:44:28,915 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)",
                "at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)",
                "at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)",
                "at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)",
                "at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop MapReduce job with compression enabled.",
                "2. Ensure that one of the nodes has a bad drive.",
                "3. Run the job and monitor the logs for errors related to compression during the reduce phase."
            ],
            "ExpectedBehavior": "The Application Master should retry the failed map task on a different node when a compression-related error occurs during the reduce phase.",
            "ObservedBehavior": "The Application Master does not retry the map task, leading to job failure when encountering compression-related errors.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "bug_report": {
            "BugID": "12922612",
            "Title": "MR AM Fails to Load Native Library When MR_AM_ADMIN_USER_ENV is Not Set",
            "Description": "The MapReduce Application Master (MR AM) fails to load the native Hadoop library if the environment variable 'yarn.app.mapreduce.am.admin.user.env' (or 'yarn.app.mapreduce.am.env') is not configured to include 'LD_LIBRARY_PATH'. This results in a failure for any code that requires the native library, such as when using lz4 compression in an uberized MapReduce task.",
            "StackTrace": [
                "2015-12-15 21:29:22,473 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable",
                "2015-12-15 21:30:17,575 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: native lz4 library not available",
                "\tat org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)",
                "\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)",
                "\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)",
                "\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)",
                "\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)",
                "\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)",
                "\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)",
                "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)",
                "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)",
                "\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)",
                "\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)",
                "\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)",
                "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the environment variable 'yarn.app.mapreduce.am.admin.user.env' is not set or does not include 'LD_LIBRARY_PATH'.",
                "2. Submit a MapReduce job that uses lz4 compression.",
                "3. Monitor the logs for warnings related to the native library loading."
            ],
            "ExpectedBehavior": "The MR AM should successfully load the native Hadoop library and execute the MapReduce job without errors related to the native lz4 library.",
            "ObservedBehavior": "The MR AM fails to load the native library, resulting in a RuntimeException indicating that the native lz4 library is not available, which prevents the MapReduce job from executing correctly.",
            "Resolution": "[Provide additional details on the resolution or workaround, if applicable]"
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "bug_report": {
            "BugID": "12641646",
            "Title": "500 Error When Accessing Map Task in AM Web UI",
            "Description": "When navigating to the MapReduce application master web UI and attempting to access a specific map task, a 500 Internal Server Error is encountered. This issue does not occur in version 0.23.6, indicating a regression in the subsequent release.",
            "StackTrace": [
                "2013-04-09 13:53:01,587 DEBUG [1088374@qtp-13877033-2 - /mapreduce/task/task_1365457322543_0004_m_000000] org.apache.hadoop.yarn.webapp.GenericExceptionHandler: GOT EXCEPTION",
                "com.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:123)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1069)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce application and ensure it is running.",
                "2. Navigate to the application master web UI.",
                "3. Click on the job listed in the UI.",
                "4. Click on the 'MAP' task type to view the list of map tasks.",
                "5. Attempt to click on a specific map task."
            ],
            "ExpectedBehavior": "The specific map task details should be displayed without any errors.",
            "ObservedBehavior": "A 500 Internal Server Error is displayed instead of the map task details.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4008",
            "Title": "ResourceManager Fails to Start Due to Existing QueueMetrics MBean",
            "Description": "The ResourceManager fails to start and throws a MetricsException indicating that the QueueMetrics MBean already exists. This issue occurs during the initialization phase of the ResourceManager, specifically when attempting to register the MBean for the default queue.",
            "StackTrace": [
                "2012-03-14 15:22:23,089 WARN org.apache.hadoop.metrics2.util.MBeans: Error creating MBean object name: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default",
                "org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)",
                "at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:93)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)",
                "at $Proxy6.postStart(Unknown Source)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop ResourceManager.",
                "2. Observe the logs for any warnings or errors during startup."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any errors related to MBean registration.",
            "ObservedBehavior": "The ResourceManager fails to start and logs a MetricsException indicating that the QueueMetrics MBean already exists.",
            "Resolution": "[Provide additional details on the resolution, if applicable]"
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6259",
            "Title": "IllegalArgumentException Triggered by Missing Job Submit Time in Job History",
            "Description": "An IllegalArgumentException occurs when parsing the job history file name due to a missing job submit time, which defaults to -1. This issue arises during the job initialization process, leading to incorrect job state handling and potential job failures.",
            "StackTrace": [
                "2015-02-10 04:54:01,863 WARN org.apache.hadoop.mapreduce.v2.hs.PartialJob: Exception while parsing job state. Defaulting to KILLED",
                "java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0",
                "at java.lang.Enum.valueOf(Enum.java:236)",
                "at org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)"
            ],
            "StepsToReproduce": [
                "1. Create a new job using MRAppMaster#serviceStart.",
                "2. Ensure the job is in the JobStateInternal.NEW state.",
                "3. Trigger JobEventType.JOB_INIT to the JobImpl instance.",
                "4. During the job setup in InitTransition#transition, an IOException occurs.",
                "5. Observe that the job submit time remains -1 due to the exception.",
                "6. Check the job history file name for the IllegalArgumentException."
            ],
            "ExpectedBehavior": "The job submit time should be correctly set during job initialization, allowing the job to transition to the appropriate state without errors.",
            "ObservedBehavior": "The job submit time is set to -1, leading to an IllegalArgumentException when parsing the job state, causing the job to default to KILLED state.",
            "Resolution": "A fix has been implemented to ensure that the job submit time is correctly updated during the job initialization process, preventing the IllegalArgumentException."
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3333",
            "Title": "OutOfMemoryError during Container Launch in MR Application Master",
            "Description": "The MapReduce Application Master (AM) experiences an OutOfMemoryError when attempting to launch containers in a sort job on a 350-node cluster. This issue leads to prolonged job execution times, with the job hanging for over an hour instead of completing in the usual 20 minutes.",
            "StackTrace": [
                "2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002_01_001434 : java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)",
                "at $Proxy20.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)",
                "... 4 more",
                "Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1089)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)",
                "... 6 more",
                "Caused by: java.io.IOException: Couldn't set up IO streams",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1065)",
                "... 7 more",
                "Caused by: java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:597)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)",
                "... 10 more"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with 350 nodes.",
                "2. Submit a sort job to the cluster using the MapReduce framework.",
                "3. Monitor the Application Master logs for errors during container launch."
            ],
            "ExpectedBehavior": "The Application Master should successfully launch containers for the sort job without encountering OutOfMemory errors, completing the job within the usual time frame.",
            "ObservedBehavior": "The Application Master hangs and eventually fails to launch containers due to an OutOfMemoryError, resulting in job execution times exceeding one hour.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7059",
            "Title": "Compatibility Issue: MR Job Fails Due to Unknown setErasureCodingPolicy Method in HDFS 2.x",
            "Description": "When running the teragen job using Hadoop 3.1 with an HDFS server version 2.8, the job fails due to the absence of the setErasureCodingPolicy method in the HDFS 2.x API. This results in a RemoteException being thrown, indicating that the method is unknown. The issue arises during the resource upload phase of the MapReduce job.",
            "StackTrace": [
                "2018-02-26 11:22:53,178 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1518615699369_0006",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol protocol.",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1437)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:359)",
                "at com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)",
                "at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)",
                "at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)",
                "at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)",
                "at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:304)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:218)"
            ],
            "StepsToReproduce": [
                "1. Set up an HDFS server with version 2.8.",
                "2. Use Hadoop version 3.1.",
                "3. Run the following command to execute the teragen job: bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0-SNAPSHOT.jar teragen 1000000 /teragen",
                "4. Observe the job failure due to the unknown method."
            ],
            "ExpectedBehavior": "The teragen job should complete successfully without any exceptions.",
            "ObservedBehavior": "The teragen job fails with a RemoteException indicating that the method setErasureCodingPolicy is unknown.",
            "Resolution": "[Provide additional details on the resolution or workaround]"
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4843",
            "Title": "Thread Safety Issue in JobLocalizer Causes Job Initialization Failures",
            "Description": "In our Hadoop cluster, jobs occasionally fail to initialize due to a DiskErrorException. The root cause is identified as a thread safety issue in the JobLocalizer class, which can lead to job configuration conflicts when multiple TaskLauncher threads attempt to initialize jobs simultaneously.",
            "StackTrace": [
                "2012-12-03 23:11:54,811 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing attempt_201212031626_1115_r_000023_0:",
                "org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)",
                "at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)",
                "at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)",
                "at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop cluster with multiple TaskTracker nodes.",
                "2. Submit multiple jobs that require simultaneous initialization.",
                "3. Monitor the TaskTracker logs for any DiskErrorException related to job initialization."
            ],
            "ExpectedBehavior": "Jobs should initialize successfully without any DiskErrorException, and job.xml files should be correctly located in the appropriate user directories.",
            "ObservedBehavior": "Jobs occasionally fail to initialize, resulting in a DiskErrorException indicating that the job.xml file could not be found in the configured local directories.",
            "Resolution": "The issue has been fixed in version 1.2.0 by ensuring that JobLocalizer is thread-safe, preventing configuration conflicts during job initialization."
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5028",
            "Title": "IOException: Spill failed when running WordCount with high io.sort.mb value",
            "Description": "When executing the WordCount example in Hadoop with a high value for io.sort.mb, the map tasks fail with an IOException indicating that the spill operation has failed. This issue appears to be related to the handling of large data sizes during the spill phase of the MapReduce process.",
            "StackTrace": [
                "java.io.IOException: Spill failed",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)",
                "at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "at org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)",
                "at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with the following configuration: Pseudo-dist mode with 2 maps and 1 reduce.",
                "2. Configure the following parameters: mapred.child.java.opts=-Xmx2048m, io.sort.mb=1280, dfs.block.size=2147483648.",
                "3. Run the teragen command to generate 4 GB of data.",
                "4. Execute the WordCount job on the generated data."
            ],
            "ExpectedBehavior": "The WordCount job should complete successfully without any IOException during the spill phase.",
            "ObservedBehavior": "The WordCount job fails with an IOException indicating that the spill operation has failed, leading to a termination of the map tasks.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4300",
            "Title": "OutOfMemoryError in Application Master leading to zombie state",
            "Description": "The Application Master (AM) encounters an OutOfMemoryError (OOM) which causes it to become unresponsive, potentially leading to a zombie state. This issue affects the stability of the Hadoop MapReduce framework.",
            "StackTrace": [
                "Exception in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space",
                "\tat com.google.protobuf.CodedInputStream.<init>(CodedInputStream.java:538)",
                "\tat com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)",
                "\tat com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)",
                "\tat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)",
                "\tat org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)",
                "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)",
                "Exception in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space",
                "\tat java.util.HashMap.resize(HashMap.java:462)",
                "\tat java.util.HashMap.addEntry(HashMap.java:755)",
                "\tat java.util.HashMap.put(HashMap.java:385)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)",
                "\tat java.lang.Thread.run(Thread.java:619)",
                "Exception in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space",
                "Exception in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop MapReduce job with a large dataset.",
                "2. Monitor the Application Master logs for memory usage.",
                "3. Observe the logs for any OutOfMemoryError messages."
            ],
            "ExpectedBehavior": "The Application Master should handle large datasets without running out of memory, maintaining responsiveness and stability.",
            "ObservedBehavior": "The Application Master encounters OutOfMemoryError, leading to unresponsiveness and potential zombie state.",
            "Resolution": "[Provide additional details on the fix or workaround]"
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3241",
            "Title": "IllegalArgumentException in TraceBuilder when processing job history files",
            "Description": "When executing the TraceBuilder tool, an IllegalArgumentException is thrown due to an unknown event type encountered while processing a job history file. This results in the TraceBuilder failing to output the expected map and reduce task information.",
            "StackTrace": [
                "2011-10-21 22:07:17,268 WARN  rumen.TraceBuilder (TraceBuilder.java:run(272)) - TraceBuilder got an error while processing the [possibly virtual] file job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist within Path hdfs://10.18.52.57:9000/user/root/null/history/done_intermediate/root/job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist",
                "java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type",
                "at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:189)"
            ],
            "StepsToReproduce": [
                "1. Ensure Hadoop is set up and running.",
                "2. Prepare a job history file that includes various event types.",
                "3. Execute the TraceBuilder tool with the command: `hadoop jar <path_to_rumen_jar> org.apache.hadoop.tools.rumen.TraceBuilder <path_to_job_history_file>`.",
                "4. Observe the output for any exceptions."
            ],
            "ExpectedBehavior": "The TraceBuilder should process the job history file without errors and output the map and reduce task information correctly.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, indicating an unknown event type, and the output does not contain the expected task information.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6002",
            "Title": "MR Task Reports Failure to AM During Shutdown Instead of Preemption",
            "Description": "When a MapReduce task is preempted, it should not be reported as failed. However, if the task encounters an error during the shutdown process (e.g., due to a FileSystem shutdown hook), it may incorrectly report a failure to the Application Master (AM) before the AM receives the completed container from the Resource Manager (RM). This leads to the task being marked as failed instead of preempted, which can cause confusion and incorrect handling of task states.",
            "StackTrace": [
                "2014-07-22 01:46:19,613 FATAL [IPC Server handler 10 on 56903] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1405985051088_0018_m_000025_0 - exited : java.io.IOException: Filesystem closed",
                "at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)",
                "at java.io.DataInputStream.readByte(DataInputStream.java:265)",
                "at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)",
                "at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)",
                "at org.apache.hadoop.io.Text.readString(Text.java:464)",
                "at org.apache.hadoop.io.Text.readString(Text.java:457)",
                "at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job that utilizes HDFS.",
                "2. Trigger a preemption of a running MapReduce task.",
                "3. Ensure that the FileSystem is in use during the preemption.",
                "4. Observe the logs for the task's exit status."
            ],
            "ExpectedBehavior": "The MapReduce task should report its status as preempted to the Application Master (AM) without indicating a failure.",
            "ObservedBehavior": "The MapReduce task reports a failure to the AM due to a 'Filesystem closed' error during the shutdown process, leading to it being marked as failed instead of preempted.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6452",
            "Title": "NullPointerException when enabling intermediate encryption in LocalJobRunner",
            "Description": "When running a MapReduce job with intermediate encryption enabled, a NullPointerException is thrown, causing the job to fail. This issue occurs specifically when the following properties are set:\n\n- `mapreduce.framework.name=local`\n- `mapreduce.job.encrypted-intermediate-data=true`\n\nThe stack trace indicates that the error originates from the `CryptoOutputStream` initialization, which fails due to a null reference.",
            "StackTrace": [
                "2015-08-14 16:27:25,248 WARN  [Thread-21] mapred.LocalJobRunner (LocalJobRunner.java:run(561)) - job_local473843898_0001",
                "java.lang.Exception: java.lang.NullPointerException",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)",
                "at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)",
                "at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Set the following properties in your MapReduce job configuration:",
                "1. `mapreduce.framework.name=local`",
                "2. `mapreduce.job.encrypted-intermediate-data=true`",
                "3. Run the MapReduce job.",
                "4. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The MapReduce job should run successfully without throwing any exceptions, even with intermediate encryption enabled.",
            "ObservedBehavior": "The job fails with a NullPointerException, indicating an issue with the initialization of the CryptoOutputStream.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6649",
            "Title": "getFailureInfo Method Fails to Return Error Details for Job Failures",
            "Description": "The `getFailureInfo` method does not provide any failure information when a job fails, making it difficult to diagnose issues. This is in contrast to other job failures that do return detailed error messages.",
            "StackTrace": [
                "2016-03-07 10:34:58,112 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1431)) - Job job_1457364518683_0004 failed with state FAILED due to:",
                "ExitCodeException exitCode=1:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Execute the following command to run a Hadoop job that is expected to fail:",
                "$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.jobtracker.split.metainfo.maxsize=10 -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1",
                "2. Observe the logs for failure messages."
            ],
            "ExpectedBehavior": "The `getFailureInfo` method should return detailed error messages indicating the reason for the job failure.",
            "ObservedBehavior": "The `getFailureInfo` method does not return any failure information, making it impossible to diagnose the cause of the job failure.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4048",
            "Title": "NullPointerException Occurs When Accessing Application Master UI for KILLED Jobs",
            "Description": "A NullPointerException is thrown when attempting to access the Application Master UI for jobs that have been killed. This issue arises during the handling of requests to the URI associated with the job attempts.",
            "StackTrace": [
                "2012-03-21 10:21:31,838 ERROR [2145015588@qtp-957250718-801] org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /mapreduce/attempts/job_1332261815858_2_8/m/KILLED",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at com.google.common.base.Joiner.toString(Joiner.java:317)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:97)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:127)",
                "at com.google.common.base.Joiner.join(Joiner.java:158)",
                "at com.google.common.base.Joiner.join(Joiner.java:166)",
                "at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)",
                "... 36 more"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job and allow it to run.",
                "2. Kill the job using the appropriate command or interface.",
                "3. Attempt to access the Application Master UI for the killed job using the URI: /mapreduce/attempts/job_<job_id>/m/KILLED."
            ],
            "ExpectedBehavior": "The Application Master UI should display the status of the killed job without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the UI from rendering properly and displaying an error message instead.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5198",
            "Title": "Race Condition During Task Tracker Reinitialization with LinuxTaskController",
            "Description": "A race condition occurs when the Job Tracker is restarted while jobs are still running, leading to a failure in the Task Tracker's reinitialization process. This issue manifests as a ClosedChannelException during IPC communication, which ultimately results in a Shell$ExitCodeException when attempting to delete user directories.",
            "StackTrace": [
                "2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075 caught: java.nio.channels.ClosedChannelException",
                "at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)",
                "at org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)",
                "at org.apache.hadoop.ipc.Server.access$2000(Server.java:98)",
                "at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)",
                "at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)",
                "2013-04-27 20:19:10,414 ERROR org.apache.hadoop.mapred.TaskTracker: Got fatal exception while reinitializing TaskTracker: org.apache.hadoop.util.Shell$ExitCodeException",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:182)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)",
                "at org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)",
                "at org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)",
                "at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)",
                "at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)",
                "at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)"
            ],
            "StepsToReproduce": [
                "1. Start the Job Tracker and submit a job.",
                "2. While the job is running, restart the Job Tracker.",
                "3. Observe the Task Tracker logs for errors during reinitialization."
            ],
            "ExpectedBehavior": "The Task Tracker should reinitialize successfully without any exceptions, allowing it to continue processing tasks.",
            "ObservedBehavior": "The Task Tracker fails to reinitialize, logging a ClosedChannelException followed by a Shell$ExitCodeException, indicating a failure in executing shell commands due to the race condition.",
            "Resolution": "A fix for this issue has been implemented and tested in version 1.2.0."
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7028",
            "Title": "NullPointerException in Application Master due to Concurrent Task Progress Updates",
            "Description": "The Application Master encounters a NullPointerException when multiple task progress updates are processed concurrently. This issue arises during high-load scenarios, such as large word count jobs, where task updates are frequent. The stack trace indicates that the error occurs in the transition method of the TaskAttemptImpl class.",
            "StackTrace": [
                "2017-12-20 06:49:42,369 INFO [IPC Server handler 9 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,369 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,383 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "    at java.lang.Thread.run(Thread.java:748)",
                "2017-12-20 06:49:42,385 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,386 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start a large word count job in the Hadoop MapReduce framework.",
                "2. Configure the job to update task progress at a high frequency.",
                "3. Monitor the Application Master logs for concurrent task progress updates."
            ],
            "ExpectedBehavior": "The Application Master should handle concurrent task progress updates without throwing exceptions, ensuring that all task states are updated correctly.",
            "ObservedBehavior": "The Application Master throws a NullPointerException when processing concurrent task progress updates, leading to potential job failures.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3790",
            "Title": "Streaming Job Success Despite Truncated Output Due to Broken Pipe",
            "Description": "When a streaming job does not consume all of its input, it can be incorrectly marked as successful, leading to truncated output. This issue arises from the handling of IOExceptions in the PipeMapRed class, specifically in the mapRedFinished() method, which does not properly account for the state of output threads before committing the job.",
            "StackTrace": [
                "2012-02-02 11:27:25,054 WARN [main] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Broken pipe",
                "2012-02-02 11:27:25,054 INFO [main] org.apache.hadoop.streaming.PipeMapRed: mapRedFinished",
                "2012-02-02 11:27:25,056 WARN [Thread-12] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Bad file descriptor",
                "2012-02-02 11:27:25,124 INFO [main] org.apache.hadoop.mapred.Task: Task:attempt_1328203555769_0001_m_000000_0 is done. And is in the process of committing",
                "2012-02-02 11:27:25,127 WARN [Thread-11] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: DFSOutputStream is closed",
                "2012-02-02 11:27:25,199 INFO [main] org.apache.hadoop.mapred.Task: Task attempt_1328203555769_0001_m_000000_0 is allowed to commit now",
                "2012-02-02 11:27:25,225 INFO [main] org.apache.hadoop.mapred.FileOutputCommitter: Saved output of task 'attempt_1328203555769_0001_m_000000_0' to hdfs://localhost:9000/user/somebody/out/_temporary/1",
                "2012-02-02 11:27:27,834 INFO [main] org.apache.hadoop.mapred.Task: Task 'attempt_1328203555769_0001_m_000000_0' done."
            ],
            "StepsToReproduce": [
                "1. Create a file in HDFS with a single line of input: 'foo'.",
                "2. Run the following command to execute a streaming job with the input file:\n   $ yarn jar ./share/hadoop/tools/lib/hadoop-streaming-0.24.0-SNAPSHOT.jar -Dmapred.map.tasks=1 -Dmapred.reduce.tasks=1 -mapper /bin/env -reducer NONE -input in -output out",
                "3. Check the output in HDFS to verify if it is truncated."
            ],
            "ExpectedBehavior": "The streaming job should fail if it does not consume all of its input, ensuring that the output is complete and not truncated.",
            "ObservedBehavior": "The streaming job is marked as successful even though the output is truncated due to not consuming all input, leading to potential data loss.",
            "Resolution": "A fix has been implemented to ensure that the job fails if not all input is consumed, preventing premature closure of output streams."
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6357",
            "Title": "Documentation Update Needed for MultipleOutputs.write() Regarding Absolute Paths",
            "Description": "The MultipleOutputs.write() method documentation does not clarify that using absolute paths can lead to improper execution of tasks during retries or when speculative execution is enabled. This oversight can cause confusion for users who expect the output committing feature to work correctly with absolute paths.",
            "StackTrace": [
                "2015-04-28 23:13:10,452 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File already exists: wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop job that uses MultipleOutputs.write() with an absolute output path.",
                "2. Run the job and observe the behavior during task retries or speculative execution.",
                "3. Check the logs for any IOException related to file existence."
            ],
            "ExpectedBehavior": "The job should execute successfully without throwing an IOException related to file existence, and the output should be committed correctly.",
            "ObservedBehavior": "The job fails with an IOException indicating that the file already exists, suggesting that the output committing mechanism is not utilized when an absolute path is provided.",
            "Resolution": "A fix for this issue has been checked into the tree and tested. The documentation for MultipleOutputs.write() has been updated to clarify the behavior when using absolute paths."
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "bug_report": {
            "BugID": "12741645",
            "Title": "YARNRunner.getJobStatus() Fails with ApplicationNotFoundException for Jobs Rolled Off RM View",
            "Description": "When querying the job status of a job that has rolled off the ResourceManager (RM) view, the system throws an ApplicationNotFoundException. This issue arises from changes made in YARN-873, which altered the behavior of ClientRMService to throw an exception for unknown application IDs instead of returning null. The previous behavior allowed fallback to the job history server, which is no longer possible in version 2.1.0 and later.",
            "StackTrace": [
                "2014-09-15 07:09:51,084 ERROR org.apache.pig.tools.grunt.Grunt: ERROR 6017: JobID: job_1410289045532_90542 Reason: java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)",
                "at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)",
                "at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)",
                "at java.lang.Thread.run(Thread.java:662)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to YARN.",
                "2. Allow the job to complete.",
                "3. Wait for the job to roll off the ResourceManager view.",
                "4. Attempt to query the job status using YARNRunner.getJobStatus() with the job ID."
            ],
            "ExpectedBehavior": "The system should retrieve the job status from the job history server even if the job has rolled off the ResourceManager view.",
            "ObservedBehavior": "The system throws an ApplicationNotFoundException indicating that the application with the specified ID does not exist in the ResourceManager.",
            "Resolution": "This issue has been fixed in version 2.6.0, which restores the fallback mechanism to the job history server."
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4825",
            "Title": "JobImpl.finished does not handle ERROR as a final job state, causing AsyncDispatcher to exit",
            "Description": "The method JobImpl.finished is not designed to handle the ERROR state as a final job state, leading to an unhandled exception in the AsyncDispatcher. This results in the application master exiting unexpectedly when a job encounters an error after transitioning to a completion state.",
            "StackTrace": [
                "2012-11-27 18:46:15,240 ERROR [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:internalError(665)) - Invalid event T_SCHEDULE on Task task_0_0000_m_000000",
                "2012-11-27 18:46:15,242 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(132)) - Error in dispatcher thread",
                "java.lang.IllegalArgumentException: Illegal job state: ERROR",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "\tat java.lang.Thread.run(Thread.java:662)",
                "2012-11-27 18:46:15,242 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(135)) - Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Submit a job that is expected to fail.",
                "2. Ensure that the job transitions to a completion state before failing.",
                "3. Observe the logs for the AsyncDispatcher."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should handle the ERROR state gracefully without exiting, allowing for proper error handling and logging.",
            "ObservedBehavior": "The AsyncDispatcher exits with a fatal error when encountering an ERROR state, leading to application instability.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3319",
            "Title": "ClassCastException in multifilewc example due to incorrect type handling in LongSumReducer",
            "Description": "When running the multifilewc example from Hadoop's examples, a ClassCastException occurs. The error arises because the LongSumReducer is attempting to cast an IntWritable to a LongWritable, which is not allowed. This issue prevents the job from completing successfully.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable",
                "at org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)",
                "at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "1. Execute the following command: /usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.205.0.22.jar multifilewc examples/text examples-output/multifilewc",
                "2. Monitor the job execution in the Hadoop console.",
                "3. Observe the error message indicating a ClassCastException."
            ],
            "ExpectedBehavior": "The multifilewc job should complete successfully, aggregating the word counts from the input files without any exceptions.",
            "ObservedBehavior": "The job fails with a ClassCastException, indicating that an IntWritable cannot be cast to a LongWritable, preventing successful execution.",
            "Resolution": "A fix has been implemented to ensure that the correct data types are used in the LongSumReducer. The issue has been resolved in version 1.0.0."
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6361",
            "Title": "NullPointerException in Shuffle due to Concurrent Access in Fetcher",
            "Description": "A NullPointerException occurs in the shuffle process when there is concurrent access between the copySucceeded() and copyFailed() methods in different threads on the same host. This issue arises during the fetching of map outputs, leading to instability in the MapReduce job execution.",
            "StackTrace": [
                "2015-05-08 21:00:00,513 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job that involves shuffling data between map and reduce tasks.",
                "2. Ensure that there are multiple threads attempting to fetch map outputs concurrently.",
                "3. Monitor the logs for any warnings or errors related to shuffle operations."
            ],
            "ExpectedBehavior": "The shuffle process should complete without errors, and all map outputs should be fetched successfully without any NullPointerExceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the shuffle process, causing the job to fail. The error occurs due to concurrent access issues between the copySucceeded() and copyFailed() methods.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4848",
            "Title": "ClassCastException during AM recovery in Hadoop MapReduce",
            "Description": "A ClassCastException occurs when attempting to recover a task in the Application Master (AM) due to an incorrect type cast between TaskAttemptContextImpl and TaskAttemptContext. This issue arises during the recovery process of failed task attempts, leading to a failure in the recovery mechanism.",
            "StackTrace": [
                "2012-12-05 02:33:36,752 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext",
                "at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job in Hadoop.",
                "2. Simulate a failure in the Application Master (AM) during task execution.",
                "3. Observe the recovery process initiated by the Resource Manager (RM).",
                "4. Check the logs for the ClassCastException during the recovery attempt."
            ],
            "ExpectedBehavior": "The Application Master should successfully recover the task without throwing a ClassCastException.",
            "ObservedBehavior": "The Application Master throws a ClassCastException, preventing the recovery of the task and leading to job failure.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3226",
            "Title": "Hanging Reducer Tasks in Gridmix Run Due to EventFetcher Issues",
            "Description": "During a Gridmix run with approximately 1000 jobs, several reducer tasks are hanging after downloading all map outputs. The issue appears to be related to the EventFetcher thread, which is stuck in a TIMED_WAITING state, causing the reducers to not complete their execution.",
            "StackTrace": [
                "EventFetcher for fetching Map Completion Events daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)",
                "main prio=10 tid=0x080ed400 nid=0x1c71 in Object.wait() [0xf73a2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "at java.lang.Thread.join(Thread.java:1196)",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "StepsToReproduce": [
                "1. Set up a Gridmix run with approximately 1000 jobs.",
                "2. Monitor the reducer tasks during execution.",
                "3. Observe the state of the reducer tasks and check for any hanging tasks."
            ],
            "ExpectedBehavior": "All reducer tasks should complete successfully without hanging, and the EventFetcher should fetch map completion events without issues.",
            "ObservedBehavior": "Several reducer tasks are hanging indefinitely after downloading all map outputs, leading to a failure in job completion.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3070",
            "Title": "NodeManager Fails to Register with ResourceManager After Restart Due to Duplicate Registration Error",
            "Description": "After gracefully stopping the NodeManager (NM) and then starting it again, the NM fails to register with the ResourceManager (RM) and throws a 'Duplicate registration from the node!' error. This issue prevents the NodeManager from functioning correctly after a restart.",
            "StackTrace": [
                "2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)",
                "at $Proxy13.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more"
            ],
            "StepsToReproduce": [
                "1. Stop the NodeManager gracefully.",
                "2. Start the NodeManager again.",
                "3. Observe the logs for any registration errors."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager without any errors after a restart.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager and logs a 'Duplicate registration from the node!' error.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3932",
            "Title": "Reduce Task Preemption Causes Application Master Crash Due to Zero Headroom",
            "Description": "[~karams] reported this issue where a reduce task gets preempted because of zero headroom, leading to a crash of the Application Master (AM). The logs indicate that the AM cannot handle the event when a container launch fails, resulting in an InvalidStateTransitionException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job with a configuration that leads to high resource contention.",
                "2. Monitor the resource allocation and ensure that the available headroom reaches zero.",
                "3. Observe the behavior of the reduce tasks as they get preempted.",
                "4. Check the logs for any InvalidStateTransitionException related to container launches."
            ],
            "ExpectedBehavior": "The Application Master should handle the preemption of reduce tasks gracefully without crashing, allowing for proper recovery or rescheduling of tasks.",
            "ObservedBehavior": "The Application Master crashes with an InvalidStateTransitionException when a reduce task is preempted due to zero headroom, leading to job failure.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4062",
            "Title": "AM Launcher Thread Hangs Indefinitely Due to Single Thread Limitation",
            "Description": "An issue was observed where the Application Master (AM) launcher thread becomes unresponsive, preventing the Resource Manager (RM) from launching new Application Masters. This occurs when the Node Manager (NM) experiences issues, leading to a hang in the launcher thread. The current implementation only allows for a single launcher thread, which exacerbates the problem when multiple nodes fail. This bug was observed to persist for approximately 9 hours.",
            "StackTrace": [
                "\"pool-1-thread-1\" prio=10 tid=0x000000004343e800 nid=0x3a4c in Object.wait()",
                "[0x000000004fad2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Object.wait(Object.java:485)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1076)",
                "- locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)",
                "at $Proxy76.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop cluster with a single Application Master launcher thread.",
                "2. Simulate a failure in one or more Node Managers.",
                "3. Attempt to launch new Application Masters while the Node Managers are down.",
                "4. Observe the state of the AM launcher thread."
            ],
            "ExpectedBehavior": "The Resource Manager should be able to launch new Application Masters even if some Node Managers are unresponsive, without any threads hanging indefinitely.",
            "ObservedBehavior": "The AM launcher thread hangs indefinitely, preventing the Resource Manager from launching new Application Masters, leading to a complete stall in application processing.",
            "Resolution": "[Provide additional details about the resolution, if applicable]"
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3066",
            "Title": "NodeManager Fails to Start Due to Invalid ResourceManager Address Configuration",
            "Description": "The NodeManager fails to start when the configuration for the ResourceManager's resource-tracker address is invalid. This issue arises when the address is not specified as a valid host:port pair, leading to a runtime exception during the NodeManager's startup process.",
            "StackTrace": [
                "2011-09-21 15:36:33,534 INFO  service.AbstractService (AbstractService.java:stop(71)) - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.",
                "2011-09-21 15:36:33,534 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more"
            ],
            "StepsToReproduce": [
                "1. Ensure that the configuration file for YARN is set up.",
                "2. Set the 'yarn.resourcemanager.resource-tracker.address' property to an invalid value (e.g., an empty string or a malformed address).",
                "3. Attempt to start the NodeManager using the command: `yarn nodemanager`.",
                "4. Observe the logs for the error message indicating failure to start."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and connect to the ResourceManager without any errors.",
            "ObservedBehavior": "The NodeManager fails to start, logging a fatal error due to an invalid host:port configuration for the ResourceManager's resource-tracker address.",
            "Resolution": "[Provide additional details on the resolution, if applicable]"
        }
    },
    {
        "filename": "MAPREDUCE-3123.json",
        "creation_time": "2011-09-29T19:03:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3123",
            "Title": "Container Launch Failure Due to Special Characters in Symbolic Links",
            "Description": "When attempting to launch a Hadoop streaming job with symbolic links that contain special characters, the job fails with a syntax error in the generated task script. This issue arises specifically when the input directory name includes characters such as `!`, `@`, `$`, `&`, `*`, `(`, `)`, `-`, `_`, `+`, and `=`.",
            "StackTrace": [
                "org.apache.hadoop.util.Shell$ExitCodeException: /tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:",
                "line 26: syntax error near unexpected token `-_+='",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:",
                "line 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir testlink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:"
            ],
            "StepsToReproduce": [
                "1. Create a directory with special characters in its name, e.g., 'InputDir#testlink!@$&*()-_+='.",
                "2. Run the following Hadoop streaming command:",
                "   hadoop jar hadoop-streaming.jar -Dmapreduce.job.acl-view-job=* -Dmapreduce.job.queuename=queue1 -files file:///homes/user/hadoop/Streaming/data/streaming-980//InputDir#testlink!@$&*()-_+= -input Streaming/streaming-980/input.txt -mapper 'xargs cat' -reducer cat -output Streaming/streaming-980/Output -jobconf mapred.job.name=streamingTest-980 -jobconf mapreduce.job.acl-view-job=*."
            ],
            "ExpectedBehavior": "The Hadoop streaming job should execute successfully without any syntax errors in the generated task script.",
            "ObservedBehavior": "The job fails with a syntax error in the task script due to the presence of special characters in the symbolic link, causing the container to exit with a non-zero exit code.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "bug_report": {
            "BugID": "12850372",
            "Title": "YarnRuntimeException during RM communication causes Application Master to fail prematurely",
            "Description": "The Application Master (AM) encounters a YarnRuntimeException when communicating with the Resource Manager (RM), leading it to incorrectly assume that it has exhausted its retry attempts. This results in the AM failing instead of retrying the allocation request. The error is logged as an InterruptedException, which is propagated back to the AM, causing it to terminate the heartbeat thread.",
            "StackTrace": [
                "2015-07-25 20:07:27,346 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Error communicating with RM: java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)"
            ],
            "StepsToReproduce": [
                "1. Start the Resource Manager (RM) and Application Master (AM) in a Hadoop cluster.",
                "2. Initiate a job that requires resource allocation from the RM.",
                "3. Simulate a shutdown of the RM during the allocation call.",
                "4. Observe the logs for any YarnRuntimeException or InterruptedException."
            ],
            "ExpectedBehavior": "The Application Master should handle the RM shutdown gracefully and retry the allocation request without failing.",
            "ObservedBehavior": "The Application Master fails and logs a YarnRuntimeException, leading it to believe it has exhausted its retry attempts.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4490",
            "Title": "JVM Reuse with LinuxTaskController Causes Task Failures Due to Missing Log Directories",
            "Description": "When using the LinuxTaskController with JVM reuse (mapred.job.reuse.jvm.num.tasks > 1), if there are more map tasks than available map slots, the second task in each JVM fails immediately due to a missing user log directory. This results in an ENOENT error when attempting to write the log index file, causing the JVM to exit prematurely.",
            "StackTrace": [
                "2012-07-24 14:29:11,914 INFO org.apache.hadoop.mapred.TaskLog: Starting logging for a new task attempt_201207241401_0013_m_000027_0 in the same JVM as that of the first task /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_000006_0",
                "2012-07-24 14:29:11,915 WARN org.apache.hadoop.mapred.Child: Error running child",
                "ENOENT: No such file or directory",
                "at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)",
                "at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)",
                "at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:229)"
            ],
            "StepsToReproduce": [
                "1. Configure the Hadoop cluster to use LinuxTaskController.",
                "2. Set the JVM reuse parameter: mapred.job.reuse.jvm.num.tasks > 1.",
                "3. Submit a job with more map tasks than available map slots.",
                "4. Monitor the logs for task attempts."
            ],
            "ExpectedBehavior": "All map tasks should execute successfully without any errors, and log directories should be created for each task attempt.",
            "ObservedBehavior": "The second task in each JVM fails with an ENOENT error due to the absence of the user log directory, causing the JVM to exit.",
            "Resolution": "A fix has been implemented to ensure that the log directories are created for each task attempt when using LinuxTaskController. This involves modifying the createLogDir method to invoke a new command in the task-controller to initialize task directories."
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3744",
            "Title": "FileNotFoundException when retrieving application logs via 'yarn logs' or 'mapred job -logs'",
            "Description": "When attempting to retrieve application logs using the 'yarn logs' command, a FileNotFoundException is thrown indicating that the specified log file does not exist. Additionally, using the 'mapred job -logs' command results in a warning about the Job History Server not being configured, despite the process running. This issue prevents users from accessing important log information for completed jobs.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)",
                "at org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)",
                "at org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)",
                "at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)",
                "at org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to the Hadoop MapReduce framework.",
                "2. Wait for the job to complete.",
                "3. Execute the command 'yarn logs -applicationId <application_id>' to retrieve logs.",
                "4. Observe the FileNotFoundException indicating the log file does not exist.",
                "5. Alternatively, execute 'mapred job -logs <job_id>' and observe the warning about the Job History Server."
            ],
            "ExpectedBehavior": "The application logs should be retrieved successfully without any errors, allowing users to view the logs of completed jobs.",
            "ObservedBehavior": "An error occurs stating that the log file does not exist when using 'yarn logs', and a warning is displayed about the Job History Server not being configured when using 'mapred job -logs'.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6815.json",
        "creation_time": "2016-12-02T00:30:37.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6815",
            "Title": "Flaky Test: Job State Assertion Failure in TestKill.testKillTask()",
            "Description": "The test case 'TestKill.testKillTask()' intermittently fails due to an assertion error indicating that the job state is not as expected. The test expects the job to succeed, but it sometimes reports an error state instead.",
            "StackTrace": [
                "java.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)"
            ],
            "StepsToReproduce": [
                "1. Set up the Hadoop MapReduce environment.",
                "2. Run the test suite that includes 'TestKill.testKillTask()'.",
                "3. Observe the test results for intermittent failures."
            ],
            "ExpectedBehavior": "The job should transition to the SUCCEEDED state after the first task is killed and the second task completes successfully.",
            "ObservedBehavior": "The job sometimes reports an ERROR state instead of SUCCEEDED, leading to an assertion failure in the test.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3053",
            "Title": "YARN ApplicationMaster Registration Fails with NullPointerException",
            "Description": "When attempting to register the ApplicationMaster with YARN's ResourceManager, a NullPointerException is thrown, causing the registration to fail. This issue appears to stem from the handling of the RegisterApplicationMasterRequest in the YARN RPC framework.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)",
                "at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)",
                "at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)",
                "at kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)",
                "Caused by: com.google.protobuf.ServiceException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1084)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager.",
                "2. Attempt to register an ApplicationMaster using the following parameters:",
                "   - appId: 1",
                "   - attemptId: 1",
                "   - timestamp: 1316556657998",
                "   - streamerClass: [Provide additional details]",
                "   - tasks: [Provide additional details]",
                "3. Observe the logs for the ApplicationMaster and ResourceManager."
            ],
            "ExpectedBehavior": "The ApplicationMaster should successfully register with the ResourceManager without any exceptions.",
            "ObservedBehavior": "The ApplicationMaster fails to register, throwing a NullPointerException in the ResourceManager logs.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5884",
            "Title": "AccessControlException when Cancelling Delegation Token due to User Name Mismatch",
            "Description": "When the owner of a delegation token attempts to cancel it, an AccessControlException is thrown due to a mismatch between the full principal name of the token owner and the short name of the user attempting the cancellation. This issue arises in the cancelToken method of the AbstractDelegationTokenSecretManager class.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)",
                "at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)"
            ],
            "StepsToReproduce": [
                "1. Obtain a delegation token as a user with a full principal name.",
                "2. Attempt to cancel the token using a user with a short name.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The user should be able to successfully cancel the delegation token if they are the owner, regardless of whether their user name is in short or full principal format.",
            "ObservedBehavior": "An AccessControlException is thrown indicating that the user is not authorized to cancel the token due to a mismatch in user name formats.",
            "Resolution": "Consider implementing one of the following options: \n1. Modify the cancelToken method to compare both short and full principal names for authorization. \n2. Ensure that all callers consistently use either short or full principal names when passing the canceller parameter."
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3706",
            "Title": "Circular Redirect Error on Job Attempts Page",
            "Description": "When attempting to access the job attempts page via the provided URL, a circular redirect error occurs, preventing users from viewing job details. This issue arises specifically when navigating directly to the job attempt URL without first visiting the proxy application page.",
            "StackTrace": [
                "org.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'",
                "at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)",
                "at org.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at org.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
            ],
            "StepsToReproduce": [
                "1. Submit a job in the Hadoop MapReduce application.",
                "2. Navigate to the job attempts page using the URL: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW.",
                "3. Observe the resulting HTTP ERROR 500."
            ],
            "ExpectedBehavior": "The job attempts page should load successfully, displaying the details of the job attempts without any errors.",
            "ObservedBehavior": "An HTTP ERROR 500 is encountered, indicating a circular redirect issue, which prevents access to the job attempts page.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "bug_report": {
            "BugID": "12650061",
            "Title": "Incorrect Handling of Native Library Path on Windows in MapReduce",
            "Description": "The MapReduce framework incorrectly relies on the LD_LIBRARY_PATH environment variable for loading native libraries on Windows, which leads to job failures. The framework should use the PATH variable instead, as LD_LIBRARY_PATH is not applicable in the Windows environment. This issue results in the following error when running MapReduce jobs on Windows:\n\n```\n2013-05-29 13:51:41,049 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n```\n\nThe root cause is that the configuration setting `mapreduce.admin.user.env` is not platform-dependent, leading to failures unless overridden.",
            "StackTrace": [
                "2013-05-29 13:51:41,049 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)",
                "at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)",
                "at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)",
                "at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)",
                "at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)",
                "at org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)",
                "at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop MapReduce environment on a Windows machine.",
                "2. Ensure that the `mapreduce.admin.user.env` configuration is set to use LD_LIBRARY_PATH.",
                "3. Submit a MapReduce job that requires access to native libraries.",
                "4. Observe the job failure with the UnsatisfiedLinkError in the logs."
            ],
            "ExpectedBehavior": "The MapReduce job should run successfully without errors related to native library access.",
            "ObservedBehavior": "The MapReduce job fails with a java.lang.UnsatisfiedLinkError indicating that the native library could not be accessed.",
            "Resolution": "The configuration for loading native libraries should be updated to use the PATH variable on Windows instead of LD_LIBRARY_PATH. This change should be made in the MapReduce framework to ensure compatibility with Windows environments."
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "bug_report": {
            "BugID": "12505901",
            "Title": "Job History Files Fail to Move to DONE Folder When Configured to HDFS Location",
            "Description": "When the configuration for 'mapreduce.jobtracker.jobhistory.location' is set to an HDFS path, the job history files fail to move to the DONE folder during the initialization of the Job Tracker or after job completion. This results in an IllegalArgumentException indicating a mismatch in the expected file system.",
            "StackTrace": [
                "2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.",
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///",
                "\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)",
                "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)",
                "\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)",
                "\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)",
                "\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)",
                "\tat org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)",
                "\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)",
                "\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)",
                "\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "\tat java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Set the configuration 'mapreduce.jobtracker.jobhistory.location' to an HDFS path.",
                "2. Start a job that generates job history files.",
                "3. Monitor the Job Tracker initialization or job completion process."
            ],
            "ExpectedBehavior": "The job history files should successfully move to the DONE folder without any exceptions.",
            "ObservedBehavior": "The job history files fail to move to the DONE folder, resulting in an IllegalArgumentException indicating a wrong file system.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3531",
            "Title": "IllegalArgumentException: Invalid key to HMAC computation during NODE_UPDATE causes ResourceManager to stop scheduling",
            "Description": "When a large sleep job is submitted to a 350-node cluster, the ResourceManager fails to allocate resources, resulting in the job not running. The logs indicate an IllegalArgumentException related to HMAC computation during a NODE_UPDATE event, which prevents the ResourceManager from scheduling containers properly.",
            "StackTrace": [
                "2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Invalid key to HMAC computation",
                "    at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)",
                "    at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)",
                "    at java.lang.Thread.run(Thread.java:619)",
                "Caused by: java.security.InvalidKeyException: Secret key expected",
                "    at com.sun.crypto.provider.HmacCore.a(DashoA13*..)",
                "    at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)",
                "    at javax.crypto.Mac.init(DashoA13*..)",
                "    at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)",
                "    ... 14 more"
            ],
            "StepsToReproduce": [
                "1. Start a 350-node Hadoop cluster.",
                "2. Submit a large sleep job to the cluster.",
                "3. Monitor the ResourceManager logs for errors."
            ],
            "ExpectedBehavior": "The ResourceManager should allocate resources for the submitted job, allowing it to run successfully.",
            "ObservedBehavior": "The ResourceManager fails to allocate resources, and the job does not run. The logs show an IllegalArgumentException related to HMAC computation during a NODE_UPDATE event.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3931",
            "Title": "Random Task Failures Due to Timestamp Mismatch on Resource Downloads",
            "Description": "During gridmix runs, tasks are failing intermittently due to a mismatch in expected and actual timestamps of resources being downloaded. This issue is likely caused by the resource being modified on the source filesystem after the job has started, leading to IOException.",
            "StackTrace": [
                "java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)",
                "at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)",
                "at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)",
                "at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)",
                "at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with gridmix enabled.",
                "2. Submit a job that utilizes resources from HDFS.",
                "3. Monitor the job execution and observe for task failures.",
                "4. Check the logs for IOException related to resource timestamp mismatches."
            ],
            "ExpectedBehavior": "Tasks should complete successfully without any IOException related to resource timestamps.",
            "ObservedBehavior": "Tasks fail intermittently with IOException indicating that the resource has changed on the source filesystem.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4467",
            "Title": "IndexCache Synchronization Error Leading to IllegalMonitorStateException",
            "Description": "The test case TestMRJobs.testSleepJob fails intermittently due to a synchronization error in the IndexCache class. The error occurs when the method getIndexInformation is called without proper synchronization, leading to an IllegalMonitorStateException when attempting to call wait(). This issue is related to the changes made in MAPREDUCE-4384, which removed the synchronized keyword from the relevant code block.",
            "StackTrace": [
                "2012-07-20 19:32:34,627 ERROR [New I/O server worker #2-1] mapred.ShuffleHandler (ShuffleHandler.java:exceptionCaught(528)) - Shuffle error:",
                "java.lang.IllegalMonitorStateException",
                "\tat java.lang.Object.wait(Native Method)",
                "\tat org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)",
                "\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)",
                "\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)",
                "\tat org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)",
                "\tat org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)",
                "\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)",
                "\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)",
                "\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)",
                "\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)",
                "\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)",
                "\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)",
                "\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)",
                "\tat org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)",
                "\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Set up the Hadoop MapReduce environment.",
                "2. Run the test case TestMRJobs.testSleepJob.",
                "3. Observe the intermittent failure due to synchronization issues."
            ],
            "ExpectedBehavior": "The test case should complete successfully without throwing an IllegalMonitorStateException.",
            "ObservedBehavior": "The test case fails intermittently with an IllegalMonitorStateException due to missing synchronization in the IndexCache class.",
            "Resolution": "The issue has been fixed by reintroducing the synchronized keyword in the relevant code block to ensure proper synchronization when calling wait()."
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3463",
            "Title": "Second Application Master Fails to Recover After First AM is Killed, Resulting in Job Loss",
            "Description": "When the first Application Master (AM) is forcefully terminated using 'kill -9', the second AM fails to recover properly, leading to a job loss. The error is caused by an IllegalArgumentException due to an invalid NodeId format, which prevents the second AM from processing task events correctly.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port",
                "at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Set 'yarn.resourcemanager.am.max-retries=5' in 'yarn-site.xml'.",
                "2. Start a YARN cluster with 4 nodes.",
                "3. Run a MapReduce job (e.g., RandomWriter or Sort) and let it progress to 50%.",
                "4. Forcefully kill the Application Master using 'kill -9'.",
                "5. Observe the client logs for errors related to job recovery."
            ],
            "ExpectedBehavior": "The second Application Master should successfully recover the job and continue processing without loss of data or job state.",
            "ObservedBehavior": "The second Application Master fails to recover, resulting in an IllegalArgumentException due to an invalid NodeId format, leading to the job being marked as lost.",
            "Resolution": "The issue has been fixed in version 0.23.1, where the handling of NodeId formats has been improved to prevent such exceptions."
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4164",
            "Title": "IOException during task completion leads to unexpected task re-execution in Hadoop MapReduce",
            "Description": "An IOException is thrown during the task completion phase, specifically a ClosedByInterruptException, which causes the task to be re-executed instead of completing successfully. This issue arises when the task attempts to communicate with the Task Tracker after the task has already been marked as done.",
            "StackTrace": [
                "java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1062)",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)",
                "at $Proxy0.statusUpdate(Unknown Source)",
                "at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)",
                "at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)",
                "at java.io.DataOutputStream.flush(DataOutputStream.java:106)",
                "at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1040)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job that involves multiple tasks.",
                "2. Monitor the task execution and completion process.",
                "3. Observe the logs for any IOException during the task completion phase."
            ],
            "ExpectedBehavior": "The task should complete successfully without throwing an IOException, and no re-execution should occur after the task is marked as done.",
            "ObservedBehavior": "An IOException is thrown during the task completion phase, leading to the task being re-executed instead of completing successfully.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "bug_report": {
            "BugID": "12648389",
            "Title": "NullPointerException in JvmManager during Task Initialization Failure",
            "Description": "In our Hadoop cluster, jobs are failing due to a NullPointerException occurring in the JvmManager when attempting to initialize tasks. This issue arises from the JvmManager entering an inconsistent state, leading to the TaskTracker failing to exit properly.",
            "StackTrace": [
                "java.lang.Throwable: Child Error",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)",
                "Caused by: java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)",
                "\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)",
                "\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop cluster with the current configuration.",
                "2. Submit a job that requires task initialization.",
                "3. Monitor the TaskTracker logs for errors during task initialization."
            ],
            "ExpectedBehavior": "The job should initialize tasks without errors, and the TaskTracker should exit cleanly after task completion.",
            "ObservedBehavior": "The job fails to initialize tasks, resulting in a NullPointerException in the JvmManager, causing the TaskTracker to hang and not exit properly.",
            "Resolution": "A fix for this issue has been checked into the tree and tested. Ensure that the JvmManager handles null references appropriately to prevent inconsistent states."
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4774",
            "Title": "Invalid State Transition Handling in JobImpl Causes Test Failures",
            "Description": "The test `org.apache.hadoop.mapred.TestClusterMRNotification.testMR` frequently fails in the mapred build due to an invalid state transition when handling asynchronous task events in the FAILED state. The test checks job status notifications received through an HTTP Servlet and expects notifications in a specific order. However, the actual notifications differ from the expected ones, leading to test failures.",
            "StackTrace": [
                "2012-11-06 12:22:02,335 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the test `org.apache.hadoop.mapred.TestClusterMRNotification.testMR` in the mapred build.",
                "Observe the job status notifications received through the HTTP Servlet.",
                "Check the order and type of notifications received against the expected values."
            ],
            "ExpectedBehavior": "The test should receive job status notifications in the expected order and with the expected types, allowing it to pass successfully.",
            "ObservedBehavior": "The test fails due to receiving an unexpected number and/or type of notifications, leading to an invalid state transition error.",
            "Resolution": "The root cause is identified as an invalid state transition when the job's task fails. The transition from RUNNING to FAILED should not allow the event `JOB_TASK_ATTEMPT_COMPLETED` to be processed. A fix is needed to prevent this transition from occurring."
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3005",
            "Title": "ResourceManager NPE Causes Application Hang During Node Update",
            "Description": "The application hangs intermittently due to a NullPointerException (NPE) in the ResourceManager during the handling of NODE_UPDATE events. This issue was observed during sort runs on a large cluster, affecting the application's performance and reliability.",
            "StackTrace": [
                "2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)",
                "    at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Deploy the application on a large Hadoop cluster.",
                "2. Execute sort runs multiple times (at least five times) to simulate load.",
                "3. Monitor the ResourceManager logs for NODE_UPDATE events.",
                "4. Observe the application behavior during the execution."
            ],
            "ExpectedBehavior": "The application should process NODE_UPDATE events without any errors, allowing for smooth resource allocation and scheduling.",
            "ObservedBehavior": "The application hangs intermittently, and the ResourceManager logs show a NullPointerException during the handling of NODE_UPDATE events, leading to resource allocation failures.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6895",
            "Title": "Job End Notification Not Sent Due to YarnRuntimeException in MRAppMaster",
            "Description": "The MRAppMaster fails to send job end notifications when a YarnRuntimeException occurs during the shutdown process. This issue is caused by a ClosedChannelException that arises when attempting to write job history events after the channel has been closed.",
            "StackTrace": [
                "2017-05-24 12:14:02,165 WARN [Thread-693] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Graceful stop failed",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)",
                "Caused by: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)",
                "at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:107)",
                "at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)",
                "at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)",
                "at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to the MRAppMaster.",
                "2. Allow the job to run until completion.",
                "3. Trigger a condition that causes a YarnRuntimeException during the shutdown process (e.g., simulate a closed channel).",
                "4. Observe the logs for the warning message indicating that the graceful stop failed."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully send a job end notification upon job completion, regardless of any exceptions that occur during the shutdown process.",
            "ObservedBehavior": "The MRAppMaster fails to send the job end notification due to a YarnRuntimeException caused by a ClosedChannelException, resulting in incomplete job termination handling.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4451",
            "Title": "FairScheduler Job Initialization Fails with Kerberos Authentication Due to Incorrect User Context",
            "Description": "When using FairScheduler in Hadoop 1.0.3 with Kerberos authentication configured, job initialization fails due to the JobInitializer running threads as the RPC user instead of the JobTracker user. This results in a failure to obtain valid Kerberos credentials, leading to an IOException during job submission.",
            "StackTrace": [
                "2012-07-17 15:15:09,220 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:",
                "java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1097)",
                "at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)",
                "at $Proxy7.getProtocolVersion(Unknown Source)",
                "at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)",
                "at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)",
                "at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)",
                "at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)",
                "at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)",
                "at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)",
                "at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Configure Hadoop 1.0.3 with Kerberos authentication.",
                "2. Submit a job using FairScheduler.",
                "3. Observe the job initialization process."
            ],
            "ExpectedBehavior": "The job should initialize successfully, utilizing the correct user context for Kerberos authentication.",
            "ObservedBehavior": "Job initialization fails with an IOException indicating that no valid Kerberos credentials were provided.",
            "Resolution": "[Provide additional details on the resolution or workaround]"
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4144",
            "Title": "NullPointerException in ResourceManager during NODE_UPDATE event handling",
            "Description": "The ResourceManager crashes with a NullPointerException (NPE) when processing a NODE_UPDATE event. This issue has occurred multiple times in the past few days, leading to instability in the cluster. The stack trace indicates that the error originates from the `allocateNodeLocal` method in the `AppSchedulingInfo` class.",
            "StackTrace": [
                "2012-04-12 02:09:01,672 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager on a Hadoop cluster.",
                "2. Trigger a NODE_UPDATE event by adding or updating a node in the cluster.",
                "3. Monitor the ResourceManager logs for any NullPointerException errors."
            ],
            "ExpectedBehavior": "The ResourceManager should handle NODE_UPDATE events without crashing, successfully updating the state of the nodes in the cluster.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException, leading to instability in the cluster and loss of resource management capabilities.",
            "Resolution": "A fix for this issue has been checked into the codebase and tested. Ensure that the ResourceManager is updated to the latest version where this issue is resolved."
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5924",
            "Title": "Sort Job Fails with InvalidStateTransitonException During Commit Phase",
            "Description": "The Sort job over 1GB of data fails with an InvalidStateTransitonException during the commit phase. The error occurs when the job attempts to transition to a commit-pending state, but the current state does not allow this transition. This results in the job being marked as ERROR in the JobHistory URL.",
            "StackTrace": [
                "2014-06-09 09:15:38,746 INFO [Socket Reader #1 for port 63415] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1402304714683_0002 (auth:SIMPLE)",
                "2014-06-09 09:15:38,750 INFO [IPC Server handler 13 on 63415] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1402304714683_0002_r_000015_1000",
                "2014-06-09 09:15:38,751 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1402304714683_0002_r_000015_1000",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:722)",
                "2014-06-09 09:15:38,753 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1402304714683_0002 Job Transitioned from RUNNING to ERROR"
            ],
            "StepsToReproduce": [
                "1. Submit a Sort job with over 1GB of data.",
                "2. Monitor the job execution through the JobHistory URL.",
                "3. Observe the transition of the job state and the error logs."
            ],
            "ExpectedBehavior": "The Sort job should successfully transition through all states and complete without errors.",
            "ObservedBehavior": "The Sort job fails with an InvalidStateTransitonException during the commit phase, resulting in the job being marked as ERROR.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3649",
            "Title": "Job End Notification Fails with UnknownServiceException",
            "Description": "When the job end notification is triggered for Oozie, the application master (AM) fails to send the notification due to an UnknownServiceException. This issue occurs specifically when the content-type is not specified in the response from the callback URL.",
            "StackTrace": [
                "2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&status=SUCCEEDED& failed",
                "java.net.UnknownServiceException: no content-type",
                "\tat java.net.URLConnection.getContentHandler(URLConnection.java:1192)",
                "\tat java.net.URLConnection.getContent(URLConnection.java:689)",
                "\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)",
                "\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to Oozie with a callback URL configured.",
                "2. Wait for the job to complete.",
                "3. Observe the logs for the job end notification process."
            ],
            "ExpectedBehavior": "The job end notification should successfully send a notification to the specified callback URL without any exceptions.",
            "ObservedBehavior": "The job end notification fails with an UnknownServiceException indicating 'no content-type'.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7077",
            "Title": "Permission Denied Error When Running Pipe MapReduce Job",
            "Description": "When executing a Pipe MapReduce job using the Hadoop framework, the application fails with a 'Permission denied' error while trying to create a local file for job token password. This issue occurs during the initialization of the application, specifically when attempting to write the password to a local file.",
            "StackTrace": [
                "java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)",
                "at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)",
                "at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)"
            ],
            "StepsToReproduce": [
                "Launch the wordcount example with the following command:",
                "/usr/hdp/current/hadoop-client/bin/hadoop pipes \"-Dhadoop.pipes.java.recordreader=true\" \"-Dhadoop.pipes.java.recordwriter=true\" -input pipeInput -output pipeOutput -program bin/wordcount"
            ],
            "ExpectedBehavior": "The Pipe MapReduce job should execute successfully and create the necessary local files without any permission issues.",
            "ObservedBehavior": "The job fails with a 'Permission denied' error when attempting to create the job token password file.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4102",
            "Title": "500 Error on Job History Web UI for Killed Jobs Due to NullPointerException in CountersBlock",
            "Description": "When a job is killed before completion, the Job History Web UI fails to display job counters, resulting in a '500 error'. This issue is caused by a NullPointerException in the CountersBlock constructor, which occurs when attempting to access counters for a job that has been killed.",
            "StackTrace": [
                "Caused by: com.google.inject.ProvisionException: Guice provision errors:",
                "2012-04-03 19:42:53,148 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /jobhistory/jobcounters/job_1333482028750_0001",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "...",
                "1) Error injecting constructor, java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)",
                "while locating org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)"
            ],
            "StepsToReproduce": [
                "1. Run a simple word count job or a sleep job in Hadoop.",
                "2. Kill the job before it finishes.",
                "3. Navigate to the Job History web UI.",
                "4. Click on the 'Counters' link for the killed job."
            ],
            "ExpectedBehavior": "The Job History web UI should display the job counters without errors, even for killed jobs.",
            "ObservedBehavior": "The Job History web UI displays a '500 error' when attempting to view counters for killed jobs.",
            "Resolution": "A fix for this issue has been implemented and tested in the codebase."
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6353",
            "Title": "ArithmeticException: Divide by Zero in ResourceCalculatorUtils.computeAvailableContainers",
            "Description": "An ArithmeticException occurs when the ResourceCalculatorUtils.computeAvailableContainers method is called with zero CPU vcores during a sleep job. This results in a divide by zero error, causing the Resource Manager (RM) to fail in contacting the Resource Manager.",
            "StackTrace": [
                "2015-04-30 06:41:06,954 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Configure a sleep job with zero CPU vcores.",
                "2. Run the job in the Hadoop MapReduce environment.",
                "3. Monitor the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The Resource Manager should handle the allocation of containers without throwing an ArithmeticException, even when zero CPU vcores are specified.",
            "ObservedBehavior": "An ArithmeticException is thrown, indicating a divide by zero error when attempting to compute available containers, leading to failure in contacting the Resource Manager.",
            "Resolution": "A fix has been implemented to handle cases where the available resources are zero, preventing the divide by zero error."
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4913",
            "Title": "AsyncDispatcher Fails to Handle AM_STARTED Event, Causing JVM Exit",
            "Description": "The test method `testMRAppMasterMissingStaging` occasionally causes the JVM to exit due to an unhandled event in the AsyncDispatcher. This results in a build failure as the test process exits without properly unregistering from the surefire plugin, which interprets this as a build error rather than a test failure.",
            "StackTrace": [
                "2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Run the test suite that includes `testMRAppMasterMissingStaging`.",
                "2. Observe the behavior of the AsyncDispatcher during the execution of the test.",
                "3. Note the occurrence of the JVM exit due to the unhandled AM_STARTED event."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should handle all registered event types, including AM_STARTED, without causing the JVM to exit.",
            "ObservedBehavior": "The AsyncDispatcher throws an exception indicating that there is no handler registered for the AM_STARTED event, leading to an unexpected JVM exit.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6492",
            "Title": "NullPointerException in AsyncDispatcher due to TaskAttemptImpl#sendJHStartEventForAssignedFailTask",
            "Description": "A NullPointerException (NPE) occurs in the AsyncDispatcher when handling task attempts that fail from the ASSIGNED state. This is caused by the method `sendJHStartEventForAssignedFailTask` being invoked with a null container, leading to an attempt to access `taskAttempt.container.getNodeHttpAddress()`, which results in an NPE.",
            "StackTrace": [
                "2015-09-28 18:01:48,656 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job with tasks that can fail.",
                "2. Ensure that a task is assigned but fails before it can be executed.",
                "3. Monitor the logs for the AsyncDispatcher."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should handle task failures gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "The AsyncDispatcher throws a NullPointerException when attempting to handle a task failure due to a null container reference.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5744",
            "Title": "Job Hangs Due to Comparator Contract Violation in RMContainerAllocator",
            "Description": "The job hangs because the method RMContainerAllocator$AssignedRequests.preemptReduce() fails repeatedly due to a violation of the comparator contract. This issue arises when the comparator used in sorting does not adhere to its general contract, specifically when the comparison result is zero (p == 0). This leads to an IllegalArgumentException being thrown, which prevents tasks from being assigned properly.",
            "StackTrace": [
                "2014-02-06 16:43:45,183 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeLo(TimSort.java:747)",
                "at java.util.TimSort.mergeAt(TimSort.java:483)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:408)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop MapReduce job with multiple tasks.",
                "2. Monitor the job execution and resource allocation.",
                "3. Observe the logs for any errors related to RMContainerAllocator.",
                "4. Note the IllegalArgumentException thrown due to the comparator violation."
            ],
            "ExpectedBehavior": "The job should execute without hanging, and tasks should be assigned properly without any exceptions being thrown.",
            "ObservedBehavior": "The job hangs indefinitely, and tasks are not assigned due to repeated IllegalArgumentException caused by a comparator contract violation.",
            "Resolution": "[Provide additional details about the fix or workaround]"
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3583",
            "Title": "NumberFormatException in ProcfsBasedProcessTree due to large process ID",
            "Description": "The HBase PreCommit builds frequently encounter a NumberFormatException when attempting to parse a process ID that exceeds the maximum value for a Java long. This issue arises specifically in the `ProcfsBasedProcessTree.constructProcessInfo` method, where a 64-bit positive integer is parsed as a signed long, leading to an exception when the value exceeds 2^63.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"18446743988060683582\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)",
                "at java.lang.Long.parseLong(Long.java:422)",
                "at java.lang.Long.parseLong(Long.java:468)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)",
                "at org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)",
                "at org.apache.hadoop.mapred.Task.initialize(Task.java:536)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "Run HBase PreCommit builds on a 64-bit Linux environment.",
                "Ensure that the process ID exceeds the maximum value for a Java long (2^63).",
                "Observe the build logs for any NumberFormatException related to process ID parsing."
            ],
            "ExpectedBehavior": "The system should handle large process IDs without throwing a NumberFormatException.",
            "ObservedBehavior": "A NumberFormatException is thrown when attempting to parse a process ID that exceeds the maximum value for a Java long.",
            "Resolution": "Propose changing the data structure from a long to a String for process IDs in the `ProcfsBasedProcessTree` to avoid parsing issues with large integers."
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-2238",
            "Title": "Build Directory Cleanup Failure Due to Permission Issues",
            "Description": "The Hudson job for Hadoop MapReduce is failing because a test is changing the permissions of a build directory, preventing the checkout process from cleaning the build directory. This issue is causing the build to fail, as the system is unable to delete certain files during the cleanup phase.",
            "StackTrace": [
                "hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7",
                "at hudson.FilePath.act(FilePath.java:749)",
                "at hudson.FilePath.act(FilePath.java:735)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)",
                "at hudson.model.AbstractProject.checkout(AbstractProject.java:1116)",
                "at hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild.java:479)",
                "at hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild.java:411)",
                "at hudson.model.Run.run(Run.java:1324)",
                "at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)",
                "at hudson.model.ResourceController.execute(ResourceController.java:88)",
                "at hudson.model.Executor.run(Executor.java:139)",
                "Caused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
            ],
            "StepsToReproduce": [
                "1. Trigger a build for the Hadoop MapReduce project on Hudson.",
                "2. Ensure that the build process includes tests that modify permissions of the build directory.",
                "3. Observe the build logs for any errors related to file operations."
            ],
            "ExpectedBehavior": "The build process should complete successfully, with all directories and files being cleaned up without permission errors.",
            "ObservedBehavior": "The build fails with an IOException indicating that it is unable to delete certain files due to permission issues.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6410",
            "Title": "GSSException Thrown During Log Deletion After Refreshing Log Retention Settings in Secure Cluster",
            "Description": "A GSSException is thrown every time log aggregation deletion is attempted after executing the command 'bin/mapred hsadmin -refreshLogRetentionSettings' in a secure cluster. This issue prevents the deletion of aggregated logs, which can lead to storage issues and hinder log management in a secure Hadoop environment.",
            "StackTrace": [
                "2015-06-04 14:14:40,070 | ERROR | Timer-3 | Error reading root log dir this deletion attempt is being aborted | AggregatedLogDeletionService.java:127",
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"vm-31/9.91.12.31\"; destination host is: \"vm-33\":25000;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy9.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy10.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystem filesys, Path path)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)",
                "at java.util.TimerThread.mainLoop(Timer.java:555)",
                "at java.util.TimerThread.run(Timer.java:505)"
            ],
            "StepsToReproduce": [
                "1. Start the history server in a secure cluster.",
                "2. Perform log deletion as per the expected behavior.",
                "3. Execute the command 'bin/mapred hsadmin -refreshLogRetentionSettings' to refresh the configuration value.",
                "4. Attempt to delete logs again."
            ],
            "ExpectedBehavior": "Log deletion should occur successfully after refreshing the log retention settings.",
            "ObservedBehavior": "Log deletion fails with a GSSException indicating that no valid credentials were provided.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6693",
            "Title": "ArrayIndexOutOfBoundsException when Job Name Exceeds Character Limit",
            "Description": "An ArrayIndexOutOfBoundsException occurs in the JobHistoryEventHandler when the length of the job name is equal to the configured limit defined by {{mapreduce.jobhistory.jobname.limit}}. This issue prevents the successful creation of job history files, leading to potential data loss and operational disruptions.",
            "StackTrace": [
                "java.lang.ArrayIndexOutOfBoundsException: 50",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set the job name to a string that is exactly equal to the value of {{mapreduce.jobhistory.jobname.limit}}.",
                "2. Submit the job to the Hadoop MapReduce framework.",
                "3. Monitor the job history files being created."
            ],
            "ExpectedBehavior": "The job history file should be created successfully without any exceptions, regardless of the job name length.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown, preventing the creation of the job history file.",
            "Resolution": "[Provide additional details on the resolution, if available]"
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "bug_report": {
            "BugID": "12718395",
            "Title": "Task.calculateOutputSize fails to handle Windows file paths correctly after MAPREDUCE-5196",
            "Description": "The method Task.calculateOutputSize does not correctly handle Windows file paths, leading to an IllegalArgumentException when attempting to retrieve the output size of local files. This issue arises due to the changes made in MAPREDUCE-5196, which altered the way file systems are resolved for local output files on Windows.",
            "StackTrace": [
                "2014-06-02 00:14:53,891 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)",
                "at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)",
                "at org.apache.hadoop.mapred.Task.done(Task.java:1048)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop MapReduce job on a Windows environment.",
                "2. Configure the job to output files to a local directory.",
                "3. Execute the job and monitor the logs for any warnings or errors."
            ],
            "ExpectedBehavior": "The method Task.calculateOutputSize should correctly retrieve the output size of local files without throwing an IllegalArgumentException.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the pathname is not a valid DFS filename when attempting to retrieve the output size of local files.",
            "Resolution": "The issue has been fixed in the latest version by ensuring that the getFileSystem method correctly identifies Windows file paths and returns the local file system instance."
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "bug_report": {
            "BugID": "12724589",
            "Title": "LocalContainerLauncher#renameMapOutputForReduce Misinterprets Map Output Directory Structure",
            "Description": "The method `renameMapOutputForReduce` in `LocalContainerLauncher` incorrectly assumes that there is a single map output directory. This leads to a `FileNotFoundException` when the expected output directory structure is not met, particularly when multiple reducers are involved. The javadoc for `renameMapOutputForReduce` should clarify that the behavior depends on the `LOCAL_DIRS` configuration.",
            "StackTrace": [
                "2014-06-30 14:48:35,574 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "1. Configure a MapReduce job with multiple reducers.",
                "2. Ensure that the `LOCAL_DIRS` configuration is set to a directory structure that includes multiple output directories.",
                "3. Run the MapReduce job.",
                "4. Observe the logs for any warnings or errors related to `renameMapOutputForReduce`."
            ],
            "ExpectedBehavior": "The `renameMapOutputForReduce` method should correctly handle multiple map output directories and not assume a single directory structure. It should successfully rename or access the required output files without throwing a `FileNotFoundException`.",
            "ObservedBehavior": "The method throws a `FileNotFoundException` indicating that the expected output file does not exist, which suggests that the method is incorrectly handling the directory structure for map outputs.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3306",
            "Title": "NoSuchElementException in Application Initialization Process",
            "Description": "A NoSuchElementException is thrown in the NodeManager logs when attempting to run applications after the MAPREDUCE-2989 update. This issue occurs during the application initialization phase, leading to a failure in processing application events.",
            "StackTrace": [
                "2011-10-28 21:40:21,264 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..",
                "java.util.NoSuchElementException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:822)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Update the Hadoop MapReduce framework to include the changes from MAPREDUCE-2989.",
                "2. Attempt to run a new application using the NodeManager.",
                "3. Check the NodeManager logs for any errors."
            ],
            "ExpectedBehavior": "The application should initialize successfully and process application events without throwing exceptions.",
            "ObservedBehavior": "The application fails to initialize, and a NoSuchElementException is logged, causing the dispatcher thread to exit.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6554",
            "Title": "NullPointerException during MRAppMaster Service Start in parsePreviousJobHistory",
            "Description": "The MRAppMaster fails to start due to a NullPointerException when attempting to recover the previous job history file. This occurs in the parsePreviousJobHistory method, specifically when the EventReader is initialized with a null input stream.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at java.io.StringReader.<init>(StringReader.java:50)",
                "    at org.apache.avro.Schema$Parser.parse(Schema.java:917)",
                "    at org.apache.avro.Schema.parse(Schema.java:966)",
                "    at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)",
                "    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)",
                "    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)"
            ],
            "StepsToReproduce": [
                "1. Create a scenario where the MRAppMaster is preempted.",
                "2. Attempt to recover the previous job history file by launching the MRAppMaster again.",
                "3. Observe the logs for the NullPointerException during the service start."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully recover from the previous job history and start without errors.",
            "ObservedBehavior": "The MRAppMaster fails to start, throwing a NullPointerException when trying to parse the previous job history file.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "bug_report": {
            "BugID": "12599348",
            "Title": "InvalidStateTransitionException in TaskAttemptImpl due to TA_TOO_MANY_FETCH_FAILURE event",
            "Description": "A job transitioned into the ERROR state due to an invalid state transition triggered by multiple TA_TOO_MANY_FETCH_FAILURE events. The system fails to handle the second event after the task has already transitioned to FAILED, leading to an InvalidStateTransitionException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_TOO_MANY_FETCH_FAILURE at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job that is expected to run successfully.",
                "2. Simulate conditions that would cause multiple TA_TOO_MANY_FETCH_FAILURE events to be generated.",
                "3. Monitor the job's state transitions in the logs."
            ],
            "ExpectedBehavior": "The job should handle TA_TOO_MANY_FETCH_FAILURE events gracefully without transitioning into an invalid state.",
            "ObservedBehavior": "The job transitions into the ERROR state and throws an InvalidStateTransitionException when a second TA_TOO_MANY_FETCH_FAILURE event is received after the task has already transitioned to FAILED.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-2716",
            "Title": "MR279: MRReliabilityTest Fails Due to Missing jobFile in ApplicationReport",
            "Description": "The ApplicationReport is missing the jobFile, which is essential for the MRReliabilityTest to execute successfully. The absence of the jobFile leads to an IllegalArgumentException when attempting to create a Path from an empty string, as the jobFile is hardcoded to an empty string in TypeConverter.java.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Can not create a Path from an empty string",
                "at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:96)",
                "at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)",
                "at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)",
                "at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)",
                "at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)",
                "at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)",
                "at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)",
                "at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)",
                "at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)",
                "at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with the necessary configurations.",
                "2. Attempt to run the MRReliabilityTest.",
                "3. Observe the failure due to the missing jobFile."
            ],
            "ExpectedBehavior": "The MRReliabilityTest should execute successfully without throwing an IllegalArgumentException.",
            "ObservedBehavior": "The MRReliabilityTest fails with an IllegalArgumentException indicating that a Path cannot be created from an empty string.",
            "Resolution": "[Provide additional details on the resolution, if applicable]"
        }
    },
    {
        "filename": "MAPREDUCE-6702.json",
        "creation_time": "2016-05-17T22:11:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6702",
            "Title": "Failures in TestMiniMRChildTask for Environment Configuration",
            "Description": "The unit tests `testTaskEnv` and `testTaskOldEnv` in the `TestMiniMRChildTask` class are failing due to an assertion error indicating that the environment checker job failed. This issue arises from the fact that YARN containers do not inherit the NodeManager's environment variables in Hadoop 3, which is causing the tests to fail when they expect certain environment variables to be present.",
            "StackTrace": [
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)",
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop 3 environment with YARN configured.",
                "2. Navigate to the `org.apache.hadoop.mapred` package.",
                "3. Run the unit tests `TestMiniMRChildTask.testTaskEnv` and `TestMiniMRChildTask.testTaskOldEnv`.",
                "4. Observe the test results."
            ],
            "ExpectedBehavior": "The tests should pass without any assertion errors, indicating that the environment variables are correctly inherited by the YARN containers.",
            "ObservedBehavior": "Both tests fail with an `AssertionError`, indicating that the environment checker job did not succeed due to missing environment variables.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4748",
            "Title": "Invalid State Transition Error When Handling Task Events in MapReduce",
            "Description": "An error occurs during the execution of a large Pig script, specifically when handling task events in the MapReduce framework. The error indicates that the system cannot handle the event 'T_ATTEMPT_SUCCEEDED' while in the 'SUCCEEDED' state, leading to an InvalidStateTransitionException.",
            "StackTrace": [
                "2012-10-23 22:45:24,986 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Can't handle this event at current state for task_1350837501057_21978_m_040453",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Enable speculative execution in the MapReduce configuration.",
                "2. Run a large Pig script that triggers multiple task attempts.",
                "3. Monitor the logs for any errors related to task event handling."
            ],
            "ExpectedBehavior": "The MapReduce framework should handle task events correctly without throwing an InvalidStateTransitionException, allowing tasks to transition through their states as expected.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when attempting to handle the 'T_ATTEMPT_SUCCEEDED' event while in the 'SUCCEEDED' state, causing task handling to fail.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3062",
            "Title": "ResourceManager Fails to Start Due to Invalid Configuration for Admin Address",
            "Description": "The ResourceManager fails to start because it encounters a configuration error related to the 'yarn.resourcemanager.admin.address' property. The error message indicates that the provided value is not a valid host:port pair.",
            "StackTrace": [
                "2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager",
                "java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the configuration file for YARN is set up.",
                "2. Set the 'yarn.resourcemanager.admin.address' property to an invalid value (e.g., 'localhost:') or leave it unset.",
                "3. Attempt to start the ResourceManager using the command: `yarn resourcemanager`."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any fatal errors.",
            "ObservedBehavior": "The ResourceManager fails to start, logging a fatal error indicating that the 'yarn.resourcemanager.admin.address' is not a valid host:port pair.",
            "Resolution": "[Provide additional details on the resolution, if applicable]"
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5724",
            "Title": "JobHistoryServer Fails to Start When HDFS is Not Running",
            "Description": "The JobHistoryServer fails to start if the Hadoop Distributed File System (HDFS) is not running, resulting in a connection error when attempting to create the necessary directories for job history storage.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)",
                "Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused"
            ],
            "StepsToReproduce": [
                "1. Ensure that HDFS is not running.",
                "2. Attempt to start the JobHistoryServer using the command: `hadoop-daemon.sh start historyserver`.",
                "3. Observe the logs for any errors related to directory creation."
            ],
            "ExpectedBehavior": "The JobHistoryServer should start successfully and handle the absence of HDFS gracefully, possibly logging a warning instead of failing.",
            "ObservedBehavior": "The JobHistoryServer fails to start and logs an error indicating that it cannot create the done directory due to a connection refusal to HDFS.",
            "Resolution": "[Provide additional details on the resolution, if applicable]"
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5358",
            "Title": "Invalid State Transition Errors in MRAppMaster During Job Execution",
            "Description": "The MRAppMaster is throwing InvalidStateTransitonException errors when handling job events, specifically when the job is in a SUCCEEDED state. This issue prevents the proper handling of job events and may lead to job failures or unexpected behavior.",
            "StackTrace": [
                "2013-06-26 11:39:50,128 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-06-26 11:39:50,129 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Submit a job to the MRAppMaster.",
                "Ensure the job transitions to the SUCCEEDED state.",
                "Trigger events such as JOB_TASK_ATTEMPT_COMPLETED or JOB_MAP_TASK_RESCHEDULED while the job is in the SUCCEEDED state."
            ],
            "ExpectedBehavior": "The MRAppMaster should handle job events appropriately without throwing InvalidStateTransitonException errors, regardless of the job's state.",
            "ObservedBehavior": "The MRAppMaster throws InvalidStateTransitonException errors when attempting to handle job events while the job is in the SUCCEEDED state, leading to potential job failures.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5837",
            "Title": "MRAppMaster Fails to Start Due to NoClassDefFoundError When Checking Uber Mode",
            "Description": "The MRAppMaster encounters a NoClassDefFoundError when attempting to determine if a job should run in uber mode. This occurs because the Class.forName() method is used to check for the presence of a class derived from ChainMapper, but it does not handle the NoClassDefFoundError that can arise if a required class is missing from the classpath.",
            "StackTrace": [
                "2014-04-15 11:52:55,877 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster",
                "java.lang.NoClassDefFoundError: scala/Function1",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)",
                "Caused by: java.lang.ClassNotFoundException: scala.Function1",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:358)"
            ],
            "StepsToReproduce": [
                "1. Configure a MapReduce job that requires the Scala library.",
                "2. Attempt to run the job using the MRAppMaster.",
                "3. Observe the logs for a NoClassDefFoundError related to scala.Function1."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully determine if the job can run in uber mode without throwing an error.",
            "ObservedBehavior": "The MRAppMaster fails to start and throws a NoClassDefFoundError due to a missing Scala class.",
            "Resolution": "Catch NoClassDefError in the isChainJob method to prevent the MRAppMaster from failing when a class is not found."
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3058",
            "Title": "Reducer Task Stuck for Extended Duration Despite Shutdown Signal",
            "Description": "While executing GridMixV3, a reducer task became unresponsive for over 15 hours. The syslog indicated that the task was supposed to shut down within 20 seconds, but it remained active. The logs revealed an IOException related to a bad connection with a datanode, leading to the task's indefinite hang.",
            "StackTrace": [
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)"
            ],
            "StepsToReproduce": [
                "1. Run GridMixV3 with a set of jobs that include reducer tasks.",
                "2. Monitor the job execution and check the syslog for any shutdown signals.",
                "3. Observe the reducer task that becomes unresponsive for an extended period."
            ],
            "ExpectedBehavior": "The reducer task should complete its execution and shut down within a reasonable time frame (e.g., 20 seconds) after receiving a shutdown signal.",
            "ObservedBehavior": "The reducer task remained active for over 15 hours despite the syslog indicating that it should have shut down within 20 seconds.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5088",
            "Title": "UninitializedMessageException: Missing 'renewer' field during Oozie job submission",
            "Description": "After the fix for HADOOP-9299, an UninitializedMessageException is thrown in Oozie when attempting to submit a job. The error indicates that a required field 'renewer' is missing, which is likely related to Kerberos authentication.",
            "StackTrace": [
                "org.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer",
                "at org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)",
                "at org.apache.oozie.command.XCommand.call(XCommand.java:277)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)",
                "at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer",
                "at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)",
                "at org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)",
                "at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the Oozie server is running and configured correctly.",
                "2. Submit a job to Oozie that requires Kerberos authentication.",
                "3. Observe the logs for the UninitializedMessageException related to the 'renewer' field."
            ],
            "ExpectedBehavior": "The job should be submitted successfully without any exceptions related to missing fields.",
            "ObservedBehavior": "An UninitializedMessageException is thrown indicating that the 'renewer' field is missing, preventing the job from starting.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    }
]