[
    {
        "filename": "HIVE-10992.json",
        "creation_time": "2015-06-12T19:52:35.000+0000",
        "bug_report": {
            "BugID": "HIVE-10992",
            "Title": "WebHCat Should Not Create Delegation Tokens When Kerberos is Not Enabled",
            "Description": "The `TempletonControllerJob.run()` method incorrectly creates delegation tokens even when Kerberos security is not enabled. This leads to issues with long-running jobs, as the tokens may be prematurely canceled, resulting in errors when attempting to find child jobs.",
            "StackTrace": [
                "2015-05-25 20:49:38,026 WARN [main] org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache",
                "2015-05-25 20:49:38,058 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child :",
                "java.lang.RuntimeException: Exception occurred while finding child jobs",
                "at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:204)",
                "at org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(WebHCatJTShim23.java:158)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(LaunchMapper.java:156)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(LaunchMapper.java:124)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(LaunchMapper.java:261)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "StepsToReproduce": [
                "1. Submit a long-running job via WebHCat without enabling Kerberos.",
                "2. Wait for the job to run for more than 24 hours.",
                "3. Observe the logs for warnings related to token invalidation."
            ],
            "ExpectedBehavior": "The system should not create delegation tokens when Kerberos is not enabled, preventing any token-related errors during job execution.",
            "ObservedBehavior": "The system creates delegation tokens, which are then canceled after 24 hours, leading to errors when attempting to find child jobs.",
            "Resolution": "A fix has been implemented to ensure that delegation tokens are only created when Kerberos security is enabled."
        }
    },
    {
        "filename": "HIVE-16450.json",
        "creation_time": "2017-04-14T13:59:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-16450",
            "Title": "Metastore Operations Fail to Retry on JDOException or NucleusException",
            "Description": "In the RetryingHMSHandler class, operations are expected to retry when a MetaException is caused by either a JDOException or a NucleusException. However, in the ObjectStore class, many instances throw a new MetaException without the original cause, leading to missed retry opportunities for certain exceptions. This issue was observed when attempting to retrieve statistics via JDO, resulting in a failure to retry under specific conditions.",
            "StackTrace": [
                "2017-04-04 17:28:21,602 ERROR metastore.ObjectStore (ObjectStore.java:getMTableColumnStatistics(6555)) - Error retrieving statistics via jdo",
                "javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(ObjectStore.java:6546)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.access$1200(ObjectStore.java:171)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6606)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6595)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.run(ObjectStore.java:2633)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(ObjectStore.java:6594)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatistics(ObjectStore.java:6588)",
                "at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:103)",
                "at com.sun.proxy.$Proxy0.getTableColumnStatistics(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:787)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:247)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3809)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3779)",
                "at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy3.alter_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9617)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9601)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Trigger a metastore operation that retrieves table column statistics.",
                "2. Ensure that the operation encounters a JDOException or NucleusException.",
                "3. Observe the behavior of the system regarding retries."
            ],
            "ExpectedBehavior": "The system should retry the metastore operation when a JDOException or NucleusException is encountered, allowing for successful completion of the operation.",
            "ObservedBehavior": "The system fails to retry the operation when a JDOException or NucleusException is thrown, leading to an error without recovery.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-6389.json",
        "creation_time": "2014-02-07T01:33:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-6389",
            "Title": "ClassCastException when querying null values in LazyBinaryColumnarSerDe-based RCFile tables",
            "Description": "When querying an RCFile table created with LazyBinaryColumnarSerDe, a ClassCastException occurs if the queried map-column contains null values. This issue arises specifically when attempting to access elements in a map-column that has null entries, leading to a runtime error during processing.",
            "StackTrace": [
                "2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":null,\"mymap\":null,\"isnull\":null}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)",
                "... 10 more"
            ],
            "StepsToReproduce": [
                "Create an RCFile table using LazyBinaryColumnarSerDe.",
                "Insert a row into the table where the map-column contains null values.",
                "Execute a query to select an element from the map-column, e.g., `SELECT mymap['1024'] FROM mytable;`.",
                "Observe the runtime error in the logs."
            ],
            "ExpectedBehavior": "The query should return null for the map-column lookup without throwing an exception.",
            "ObservedBehavior": "A ClassCastException is thrown when attempting to access elements in a map-column that contains null values.",
            "Resolution": "A patch is being developed to ensure that the LazyBinaryMapOI returns nulls if either the map or the lookup-key is null. This behavior is already correctly handled for Text data types."
        }
    },
    {
        "filename": "HIVE-2372.json",
        "creation_time": "2011-08-12T09:07:34.000+0000",
        "bug_report": {
            "BugID": "HIVE-2372",
            "Title": "IOException: Argument list too long when executing Perl reducer in Hive",
            "Description": "When executing a large query with a Perl reducer in Hive, the reducer fails with an IOException due to the argument list exceeding the system limit. This issue arises from the environment variable 'mapred.input.dir' being excessively large, which is caused by having too many input directories. The error message indicates that the argument list is too long, which is a limitation in Linux systems.",
            "StackTrace": [
                "2011-08-11 04:58:29,935 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator",
                "at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)",
                "at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)",
                "... 7 more",
                "Caused by: java.io.IOException: Cannot run program \"/usr/bin/perl\": java.io.IOException: error=7, Argument list too long",
                "at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)",
                "at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)",
                "... 15 more",
                "Caused by: java.io.IOException: java.io.IOException: error=7, Argument list too long",
                "at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)",
                "at java.lang.ProcessImpl.start(ProcessImpl.java:65)",
                "at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)",
                "... 16 more"
            ],
            "StepsToReproduce": [
                "1. Execute a large query on a Hive table with multiple 2-level partitions.",
                "2. Include a Perl reducer in the query.",
                "3. Observe the execution logs for the reducer."
            ],
            "ExpectedBehavior": "The reducer should execute successfully without throwing an IOException related to argument list length.",
            "ObservedBehavior": "The reducer fails with an IOException indicating that the argument list is too long, preventing successful execution.",
            "Resolution": "Consider reducing the size of the environment variable 'mapred.input.dir' by limiting the number of input directories or upgrading the Linux kernel to a version that supports larger argument lists."
        }
    },
    {
        "filename": "HIVE-2958.json",
        "creation_time": "2012-04-17T15:02:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-2958",
            "Title": "ClassCastException during GROUP BY operation in Hive with HBase Storage Handler",
            "Description": "When executing a GROUP BY query on an external table backed by HBase, a ClassCastException occurs due to an incompatible type cast between LazyDioInteger and LazyInteger. This issue arises specifically when the query attempts to aggregate data based on the 'data_resource_id' column.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:270)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:264)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:83)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)",
                "... 9 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(LazyIntObjectInspector.java:43)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:239)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:150)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:142)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.copyKey(KeyWrapperFactory.java:119)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:750)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:722)",
                "... 18 more"
            ],
            "StepsToReproduce": [
                "1. Create an external table in Hive using HBase as the storage handler.",
                "2. Insert sample data into the table, ensuring that some rows have null values for the 'scientific_name' column.",
                "3. Execute the following query: SELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id;",
                "4. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The query should return the count of records grouped by 'data_resource_id' without any errors.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that LazyDioInteger cannot be cast to LazyInteger.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-13392.json",
        "creation_time": "2016-03-30T22:32:50.000+0000",
        "bug_report": {
            "BugID": "HIVE-13392",
            "Title": "Speculative Execution Causes File Lease Conflicts in ACID Compactor",
            "Description": "When speculative execution is enabled for the ACID Compactor in Hive, it can lead to file lease conflicts, resulting in exceptions during file creation. This issue arises because the CompactorMR is not designed to handle speculative execution, which can cause multiple tasks to attempt to create the same file simultaneously.",
            "StackTrace": [
                "2016-02-08 22:56:38,256 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2451)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)"
            ],
            "StepsToReproduce": [
                "1. Enable speculative execution for the mappers and reducers in the Hive configuration.",
                "2. Run a job that triggers the ACID Compactor.",
                "3. Monitor the logs for any exceptions related to file creation."
            ],
            "ExpectedBehavior": "The ACID Compactor should successfully create files without any lease conflicts, regardless of whether speculative execution is enabled.",
            "ObservedBehavior": "The ACID Compactor fails to create files due to lease conflicts, resulting in 'AlreadyBeingCreatedException' errors in the logs.",
            "Resolution": "Disable speculative execution for the ACID Compactor jobs as a short-term workaround. A longer-term solution may involve modifying the CompactorMR to handle speculative execution by ensuring each task writes to a unique directory identified by a UUID."
        }
    },
    {
        "filename": "HIVE-11301.json",
        "creation_time": "2015-07-18T00:41:40.000+0000",
        "bug_report": {
            "BugID": "HIVE-11301",
            "Title": "Thrift Protocol Exception: Required Field 'colStats' is Unset in AggrStats",
            "Description": "When attempting to retrieve aggregate statistics from the Hive metastore, a Thrift protocol exception is thrown due to the required field 'colStats' being unset. This issue leads to a loss of connection between the client and the metastore, causing the client to hang while retrying the connection.",
            "StackTrace": [
                "2015-07-17 20:32:27,795 ERROR [pool-3-thread-150]: server.TThreadPoolServer (TThreadPoolServer.java:run(294)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-07-17 20:32:27,796 WARN  [pool-3-thread-150]: transport.TIOStreamTransport (TIOStreamTransport.java:close(112)) - Error closing output stream.",
                "java.net.SocketException: Socket closed",
                "at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)",
                "at java.net.SocketOutputStream.write(SocketOutputStream.java:153)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)",
                "at java.io.FilterOutputStream.close(FilterOutputStream.java:158)",
                "at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)",
                "at org.apache.thrift.transport.TSocket.close(TSocket.java:196)",
                "at org.apache.hadoop.hive.thrift.TFilterTransport.close(TFilterTransport.java:52)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-07-17 20:32:27,796 WARN  [main()]: metastore.RetryingMetaStoreClient (RetryingMetaStoreClient.java:invoke(187)) - MetaStoreClient lost connection. Attempting to reconnect.",
                "org.apache.thrift.transport.TTransportException",
                "at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_aggr_stats_for(ThriftHiveMetastore.java:3029)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_aggr_stats_for(ThriftHiveMetastore.java:3016)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2067)",
                "at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1991)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3124)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:252)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:137)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:125)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:111)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive metastore with partitions.",
                "2. Attempt to retrieve aggregate column statistics using the method getAggrColStatsFor with a table that has no partitions.",
                "3. Observe the logs for Thrift protocol exceptions."
            ],
            "ExpectedBehavior": "The aggregate column statistics should be retrieved successfully without any exceptions, and the client should maintain a stable connection to the metastore.",
            "ObservedBehavior": "A Thrift protocol exception is thrown indicating that the required field 'colStats' is unset, leading to a loss of connection and the client hanging while retrying.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11028.json",
        "creation_time": "2015-06-16T23:03:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-11028",
            "Title": "Tez: IndexOutOfBoundsException during self join and join with another table",
            "Description": "When executing a query that involves a self join on the table 'tez_self_join1' and a join with 'tez_self_join2', an IndexOutOfBoundsException is thrown, causing the query to fail. This issue occurs specifically in the Tez execution engine.",
            "StackTrace": [
                "2015-06-16 15:41:55,759 ERROR [main]: ql.Driver (SessionState.java:printError(979)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask.",
                "Vertex failed, vertexName=Reducer 3, vertexId=vertex_1434494327112_0002_4_04, diagnostics=[Task failed, taskId=task_1434494327112_0002_4_04_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.<init>(StandardStructObjectInspector.java:109)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:290)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:275)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:175)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:313)",
                "at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:71)",
                "at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:99)",
                "at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:362)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)",
                "... 13 more"
            ],
            "StepsToReproduce": [
                "1. Create the table 'tez_self_join1' with the specified schema.",
                "2. Insert the provided values into 'tez_self_join1'.",
                "3. Create the table 'tez_self_join2' with the specified schema.",
                "4. Insert the provided values into 'tez_self_join2'.",
                "5. Execute the provided SQL query that performs a self join on 'tez_self_join1' and a join with 'tez_self_join2'."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors.",
            "ObservedBehavior": "The query fails with an IndexOutOfBoundsException, indicating that the code attempted to access an index in a list that does not exist.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-14380.json",
        "creation_time": "2016-07-29T00:14:58.000+0000",
        "bug_report": {
            "BugID": "HIVE-14380",
            "Title": "IAException when querying tables with remote HDFS paths due to encryption checks",
            "Description": "When querying tables that have their locations set to remote HDFS paths, an IAException is thrown indicating that the system is unable to determine if the specified path is encrypted. This issue arises from the incorrect handling of the FileSystem instance in the SessionState class, which leads to a mismatch in expected and actual HDFS URIs.",
            "StackTrace": [
                "2016-07-26 01:16:27,471 ERROR parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1867)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to determine if hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table, expected: hdfs://bar.ygrid.yahoo.com:8020",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2204)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStrongestEncryptedTablePath(SemanticAnalyzer.java:2274)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive table with a location pointing to a remote HDFS path (e.g., hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table).",
                "2. Attempt to query the table using a Hive query.",
                "3. Observe the error message in the logs indicating an IAException related to encryption checks."
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing an IAException, and the system should correctly determine the encryption status of the specified HDFS path.",
            "ObservedBehavior": "An IAException is thrown, indicating that the system is unable to determine if the specified HDFS path is encrypted due to a mismatch in the expected and actual HDFS URIs.",
            "Resolution": "A fix is forthcoming to ensure that the FileSystem instance is correctly fetched based on the path being checked, rather than relying on the session configuration."
        }
    },
    {
        "filename": "HIVE-7799.json",
        "creation_time": "2014-08-20T09:45:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-7799",
            "Title": "NullPointerException in HiveKVResultCache during Spark transformation",
            "Description": "A NullPointerException occurs in the HiveKVResultCache class when executing a Spark transformation. The issue arises due to improper usage of RowContainer, which is not designed to allow writing after a row has been read. This bug needs to be investigated to determine if it is a Hive issue or specific to the Hive on Spark implementation.",
            "StackTrace": [
                "2014-08-20 01:14:36,594 ERROR executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.next(HiveKVResultCache.java:113)",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:124)",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:82)",
                "    at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)",
                "    at scala.collection.Iterator$class.foreach(Iterator.scala:727)",
                "    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)",
                "    at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.java:65)",
                "    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "    at org.apache.spark.scheduler.Task.run(Task.scala:54)",
                "    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "    at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive on Spark environment.",
                "2. Execute the transformation query 'transform_ppr1.q'.",
                "3. Monitor the logs for any exceptions during execution."
            ],
            "ExpectedBehavior": "The transformation should complete successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating a misuse of RowContainer in the HiveKVResultCache.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-6537.json",
        "creation_time": "2014-03-03T18:57:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-6537",
            "Title": "NullPointerException in HashTableLoader during MapJoin operation",
            "Description": "A NullPointerException occurs in the HashTableLoader when attempting to load a hashtable for a MapJoin operation. The error is triggered when the tables array is null, leading to a failure in the Arrays.fill method. This issue arises due to the improper initialization of the tables in the MapJoin context.",
            "StackTrace": [
                "2014-02-20 23:33:15,743 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.NullPointerException",
                "at java.util.Arrays.fill(Arrays.java:2685)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:81)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with a MapJoin operation.",
                "2. Ensure that the input data for the MapJoin operation is configured such that the tables array is not initialized properly.",
                "3. Execute the MapJoin query.",
                "4. Observe the logs for the NullPointerException in the HashTableLoader."
            ],
            "ExpectedBehavior": "The MapJoin operation should execute without errors, and the hashtable should be loaded successfully.",
            "ObservedBehavior": "A NullPointerException is thrown during the loading of the hashtable, indicating that the tables array is null.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-13691.json",
        "creation_time": "2016-05-04T23:40:03.000+0000",
        "bug_report": {
            "BugID": "HIVE-13691",
            "Title": "Compactor Fails to Record Entry in Completed Compaction Queue on Timeout",
            "Description": "When the compactor encounters a timeout while trying to compact a table, it fails to record an entry in the completed_compaction_queue, leading to an IllegalStateException indicating that no record with CQ_ID=0 was found in COMPACTION_QUEUE. This behavior occurs even when the compaction process is marked as failed due to a timeout.",
            "StackTrace": [
                "2016-04-29 18:49:31,594 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(141)) - Caught exception while trying to determine if we should compact id:0,dbname:default,tableName:service_logs_v2,par\\ntName:ds=2016-04-21,state:^@,type:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking clean to avoid repeated failures, MetaException(message:Timeout when executing method: getTable)",
                "at org.apache.hadoop.hive.metastore.Deadline.newMetaException(Deadline.java:187)",
                "at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:177)",
                "at org.apache.hadoop.hive.metastore.Deadline.checkTimeout(Deadline.java:160)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1839)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2255)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.access$300(ObjectStore.java:165)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2051)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2043)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2400)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal(ObjectStore.java:2043)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNames(ObjectStore.java:2037)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)",
                "at com.sun.proxy.$Proxy0.getPartitionsByNames(Unknown Source)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolvePartition(CompactorThread.java:111)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:129)",
                "Caused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable",
                "at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)",
                "... 16 more",
                "2016-04-29 18:49:31,595 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(154)) - Initiator loop caught unexpected exception this time through the loop: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)"
            ],
            "StepsToReproduce": [
                "1. Start a compaction process on a table with a known timeout configuration.",
                "2. Ensure that the compaction process encounters a timeout while executing the getTable method.",
                "3. Observe the logs for errors related to compaction and check the completed_compaction_queue."
            ],
            "ExpectedBehavior": "The compactor should record an entry in the completed_compaction_queue even if the compaction fails due to a timeout.",
            "ObservedBehavior": "The compactor fails to record an entry in the completed_compaction_queue, resulting in an IllegalStateException indicating that no record with CQ_ID=0 was found in COMPACTION_QUEUE.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-17758.json",
        "creation_time": "2017-10-10T12:33:52.000+0000",
        "bug_report": {
            "BugID": "HIVE-17758",
            "Title": "Negative Timeout Value in Notification Sequence Lock Retry Logic",
            "Description": "The introduction of retry logic in HIVE-16886 has led to a situation where the default value for the timeout in the notification sequence lock can be set to -1. This occurs because the default value is not properly loaded into the relevant field, causing the client code to use this invalid value, which results in an IllegalArgumentException when attempting to sleep for a negative duration.",
            "StackTrace": [
                "2017-10-10 11:22:37,638 ERROR [load-dynamic-partitions-12]: metastore.ObjectStore (ObjectStore.java:addNotificationEvent(7444)) - could not get lock for update",
                "java.lang.IllegalArgumentException: timeout value is negative",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:7407)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:7361)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7424)",
                "at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)"
            ],
            "StepsToReproduce": [
                "1. Configure the Hive metastore with the default settings.",
                "2. Trigger the addition of a notification event in the metastore.",
                "3. Observe the logs for any errors related to acquiring the notification sequence lock."
            ],
            "ExpectedBehavior": "The notification sequence lock should be acquired successfully without any exceptions, and the timeout value should be a valid positive integer.",
            "ObservedBehavior": "An IllegalArgumentException is thrown with the message 'timeout value is negative', indicating that the timeout value was incorrectly set to -1.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-14898.json",
        "creation_time": "2016-10-06T00:02:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-14898",
            "Title": "HS2 Logs Callstack for Empty Auth Header Instead of Handling Gracefully",
            "Description": "When the authentication header is not sent by the client, HiveServer2 (HS2) logs an error with a full call stack, which is unnecessary and clutters the logs. This occurs specifically when the client (Knox) does not send the auth header until it receives a 401 response. The expected behavior is for HS2 to handle this situation gracefully without logging the call stack.",
            "StackTrace": [
                "2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(169)) - Failed to authenticate with hive/_HOST kerberos principal",
                "2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(104)) - Error: ",
                "org.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:170)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:83)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)",
                "at org.eclipse.jetty.server.ScopedHandler.handle(ScopedHandler.java:117)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)",
                "at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:952)",
                "at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:167)",
                "... 23 more",
                "Caused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(ThriftHttpServlet.java:311)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.access$100(ThriftHttpServlet.java:59)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:212)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "... 24 more"
            ],
            "StepsToReproduce": [
                "1. Configure Knox to connect to HiveServer2 (HS2).",
                "2. Send a request to HS2 without the authentication header.",
                "3. Observe the logs generated by HS2."
            ],
            "ExpectedBehavior": "HS2 should handle the absence of the authentication header gracefully without logging the full call stack.",
            "ObservedBehavior": "HS2 logs an error message along with a complete stack trace when the authentication header is missing, which is unnecessary and clutters the logs.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-5546.json",
        "creation_time": "2013-10-15T15:06:59.000+0000",
        "bug_report": {
            "BugID": "HIVE-5546",
            "Title": "OutOfMemoryError in OrcInputFormat when includedColumnIds is empty",
            "Description": "When processing ORC files, an OutOfMemoryError occurs due to incorrect handling of the includedColumnIds parameter in the OrcInputFormat class. The current implementation assumes that if includedStr is an empty string, all columns should be read, which leads to excessive memory usage and ultimately crashes the application.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with Hive.",
                "2. Create an ORC file with a significant number of columns.",
                "3. Execute a Hive query that uses the ORC file and specifies an empty includedColumnIds parameter.",
                "4. Monitor the application for memory usage."
            ],
            "ExpectedBehavior": "The application should process the ORC file without running out of memory, and it should skip reading columns when includedColumnIds is empty.",
            "ObservedBehavior": "The application throws an OutOfMemoryError, causing it to crash when processing ORC files with an empty includedColumnIds parameter.",
            "Resolution": "The issue has been addressed in a subsequent release. Ensure that the OrcInputFormat correctly handles cases where includedColumnIds is empty to prevent reading all columns unnecessarily."
        }
    },
    {
        "filename": "HIVE-7557.json",
        "creation_time": "2014-07-30T19:25:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-7557",
            "Title": "ClassCastException in Tez when processing vectorized reduce with dynpart_sort_opt_vectorization.q",
            "Description": "When the reduce operation is vectorized in Tez, the query 'dynpart_sort_opt_vectorization.q' fails due to a ClassCastException. This occurs when the system attempts to cast a DoubleColumnVector to a LongColumnVector, leading to a runtime error.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:394)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)"
            ],
            "StepsToReproduce": [
                "1. Disable the dynpart_sort_opt_vectorization.q optimization.",
                "2. Execute the query 'dynpart_sort_opt_vectorization.q' under Tez with vectorization enabled.",
                "3. Observe the runtime error in the logs."
            ],
            "ExpectedBehavior": "The query should execute successfully without any runtime exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that a DoubleColumnVector cannot be cast to a LongColumnVector.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-1712.json",
        "creation_time": "2010-10-14T17:17:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-1712",
            "Title": "NullPointerException during Metadata Migration from Derby to MySQL",
            "Description": "When migrating metadata from Derby to MySQL, a NullPointerException is thrown while executing a Hive query that worked successfully in Derby. The issue arises during the retrieval of table metadata, specifically when the HiveMetaStore attempts to access the schema of a table that may not exist or is improperly defined in the MySQL database.",
            "StackTrace": [
                "2010-10-16 08:57:30,140 ERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException",
                "at java.util.Hashtable.put(Hashtable.java:394)",
                "at java.util.Hashtable.putAll(Hashtable.java:466)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)",
                "at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)"
            ],
            "StepsToReproduce": [
                "1. Export data from Derby to CSV format.",
                "2. Load the exported data into MySQL.",
                "3. Execute a Hive query that retrieves metadata for a table that was migrated from Derby to MySQL."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully and return the expected metadata for the specified table without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the Hive query, indicating that the metadata retrieval process encountered an issue.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-12608.json",
        "creation_time": "2015-12-07T21:26:01.000+0000",
        "bug_report": {
            "BugID": "HIVE-12608",
            "Title": "Exception on Dropping Column from array<struct<>> in Parquet Schema Evolution",
            "Description": "When attempting to drop a column from an array of structs in a Parquet table, an exception is thrown indicating that a field cannot be found. This issue arises during schema evolution operations, specifically when using the ALTER TABLE command to replace columns in an array of structs.",
            "StackTrace": [
                "2015-12-07 11:47:28,503 ERROR [main]: CliDriver (SessionState.java:printError(921)) - Failed with exception java.io.IOException:java.lang.RuntimeException: cannot find field c2 in [c1]",
                "java.io.IOException: java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "at org.apache.hadoop.hive.cli.QTestUtil.executeClientInternal(QTestUtil.java:1029)",
                "at org.apache.hadoop.hive.cli.QTestUtil.executeClient(QTestUtil.java:1003)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:139)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_type_promotion(TestCliDriver.java:123)"
            ],
            "StepsToReproduce": [
                "1. Create a table with an array of structs using the following SQL command: CREATE TABLE arrays_of_struct_to_map (locations1 array<struct<c1:int,c2:int>>, locations2 array<struct<f1:int,f2:int,f3:int>>) STORED AS PARQUET;",
                "2. Insert data into the table: INSERT INTO TABLE arrays_of_struct_to_map select array(named_struct('c1',1,'c2',2)), array(named_struct('f1',77,'f2',88,'f3',99)) FROM parquet_type_promotion LIMIT 1;",
                "3. Execute a SELECT statement to verify the data: SELECT * FROM arrays_of_struct_to_map;",
                "4. Attempt to drop a column from the array of structs using the following command: ALTER TABLE arrays_of_struct_to_map REPLACE COLUMNS (locations1 array<struct<c1:int>>, locations2 array<struct<f2:int>>);",
                "5. Execute another SELECT statement: SELECT * FROM arrays_of_struct_to_map;"
            ],
            "ExpectedBehavior": "The command to drop the column should execute successfully, and the resulting table should reflect the updated schema without throwing any exceptions.",
            "ObservedBehavior": "An exception is thrown indicating that the field 'c2' cannot be found in the updated struct definition, leading to a failure in executing the SELECT statement after the ALTER TABLE command.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-17774.json",
        "creation_time": "2017-10-11T20:02:01.000+0000",
        "bug_report": {
            "BugID": "HIVE-17774",
            "Title": "Compaction Job Fails When No Splits Are Available",
            "Description": "The compaction job fails with a FileNotFoundException when it attempts to run without any available splits. This occurs due to the job being submitted with zero map and reduce tasks, leading to an invalid state where the expected temporary files do not exist.",
            "StackTrace": [
                "java.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit a compaction job with no available splits.",
                "2. Monitor the job execution.",
                "3. Observe the failure due to missing temporary files."
            ],
            "ExpectedBehavior": "The compaction job should not be submitted if there are no splits available, preventing any FileNotFoundException.",
            "ObservedBehavior": "The compaction job is submitted with zero splits, resulting in a FileNotFoundException when it attempts to access non-existent temporary files.",
            "Resolution": "The logic for submitting compaction jobs should be updated to check for available splits before attempting to run the job."
        }
    },
    {
        "filename": "HIVE-14564.json",
        "creation_time": "2016-08-18T00:11:34.000+0000",
        "bug_report": {
            "BugID": "HIVE-14564",
            "Title": "ArrayIndexOutOfBoundsException due to Column Pruning in SelectOperator",
            "Description": "The Column Pruning feature in the SelectOperator is causing an ArrayIndexOutOfBoundsException during the processing of rows in a MapReduce job. This issue arises when the serialization and deserialization of columns do not match, leading to corrupted data being processed.",
            "StackTrace": [
                "2016-07-26 21:49:24,390 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)",
                "... 9 more",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at java.lang.System.arraycopy(Native Method)",
                "at org.apache.hadoop.io.Text.set(Text.java:225)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(LazyBinaryString.java:48)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)",
                "at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:94)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(ReduceSinkOperator.java:550)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:377)",
                "... 13 more"
            ],
            "StepsToReproduce": [
                "1. Execute a MapReduce job that utilizes the Column Pruning feature in the SelectOperator.",
                "2. Ensure that the job processes data serialized by a previous MapReduce job with a different column order.",
                "3. Observe the logs for any ArrayIndexOutOfBoundsException errors."
            ],
            "ExpectedBehavior": "The MapReduce job should process rows without throwing an ArrayIndexOutOfBoundsException, regardless of the column order used in previous jobs.",
            "ObservedBehavior": "The job fails with an ArrayIndexOutOfBoundsException due to mismatched serialization and deserialization of columns caused by Column Pruning.",
            "Resolution": "A fix for this issue has been implemented and tested in version 3.0.0."
        }
    },
    {
        "filename": "HIVE-3651.json",
        "creation_time": "2012-11-01T23:31:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-3651",
            "Title": "MapJoin Task Fails Due to Missing Hashtable File in Hive with Hadoop 0.23",
            "Description": "The Hive job fails during execution of a MapJoin task, resulting in a runtime exception due to a missing hashtable file. This issue occurs when running the bucketmapjoin?.q test case with Hadoop version 0.23.",
            "StackTrace": [
                "2012-11-01 15:51:20,253 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(479)) - job_local_0001",
                "java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)",
                "Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:679)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with Hadoop version 0.23.",
                "2. Execute the bucketmapjoin?.q test case.",
                "3. Observe the job execution logs for errors."
            ],
            "ExpectedBehavior": "The MapJoin task should execute successfully without any missing file errors, and the test case should pass.",
            "ObservedBehavior": "The MapJoin task fails with a runtime exception indicating that the hashtable file is missing, causing the job to terminate.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-5199.json",
        "creation_time": "2013-09-03T20:40:29.000+0000",
        "bug_report": {
            "BugID": "HIVE-5199",
            "Title": "ClassCastException when using Custom SerDe with Non-Settable Complex Data Types in Hive 0.11",
            "Description": "A ClassCastException occurs when using a custom SerDe that contains a non-settable complex data type row object inspector. This issue arises due to changes introduced in HIVE-3833, which do not properly handle the conversion of nested complex data types that extend nonSettableObjectInspector to a settableObjectInspector type. The problem manifests in both FetchOperator and MapOperator, leading to failures when attempting to fetch rows from partitioned tables with different SerDe configurations.",
            "StackTrace": [
                "2013-08-28 17:57:25,307 ERROR CliDriver (SessionState.java:printError(432)) - Failed with exception java.io.IOException:java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "java.io.IOException: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1412)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "Caused by: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:144)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:307)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:138)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:406)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)"
            ],
            "StepsToReproduce": [
                "1. Create a partitioned table in Hive with a custom SerDe that has a non-settable complex data type.",
                "2. Attempt to fetch data from the table using FetchOperator.",
                "3. Observe the ClassCastException in the logs."
            ],
            "ExpectedBehavior": "The data should be fetched successfully without any ClassCastException, regardless of the SerDe configuration.",
            "ObservedBehavior": "A ClassCastException is thrown when attempting to fetch data, indicating that the ProtoMapObjectInspector cannot be cast to SettableMapObjectInspector.",
            "Resolution": "A patch has been created to address this issue, ensuring proper conversion of nested complex data types. See attached patch HIVE-5199.2.patch.txt for details."
        }
    },
    {
        "filename": "HIVE-17368.json",
        "creation_time": "2017-08-22T01:27:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-17368",
            "Title": "DBTokenStore Fails to Connect in Kerberos-Enabled Remote HMS Environment",
            "Description": "In setups where the Hive Metastore (HMS) is running as a remote process secured using Kerberos, and when DBTokenStore is configured as the token store, the HS2 Thrift API call GetDelegationToken fails with an exception. This occurs because HS2 cannot invoke HMS APIs needed to manage tokens due to the user issuing the GetDelegationToken not being Kerberos enabled. For example, when Oozie submits a job on behalf of a user (e.g., 'Joe'), it uses Oozie's principal to create a proxy UGI with Hive. The transport fails to establish a connection using the HMSToken string available in the session configuration, leading to a SASL negotiation failure.",
            "StackTrace": [
                "2017-08-21T18:07:19,644 ERROR [HiveServer2-Handler-Pool: Thread-61] transport.TSaslTransport: SASL negotiation failure",
                "javax.security.sasl.SaslException: GSS initiate failed",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_121]",
                "at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:488) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:255) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_121]",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_121]",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3595) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3647) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3627) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]",
                "at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnTokenStore(DBTokenStore.java:157) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.DBTokenStore.addToken(DBTokenStore.java:74) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:142) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:56) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.security.token.Token.<init>(Token.java:59) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getDelegationToken(DelegationTokenSecretManager.java:109) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:123) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationToken(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationTokenWithService(HiveDelegationTokenManager.java:130) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.auth.HiveAuthFactory.getDelegationToken(HiveAuthFactory.java:261) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken(HiveSessionImplwithUGI.java:174) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at com.sun.proxy.$Proxy36.getDelegationToken(Unknown Source) [?:?]",
                "at org.apache.hive.service.cli.CLIService.getDelegationToken(CLIService.java:589) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.GetDelegationToken(ThriftCLIService.java:254) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService$Processor$GetDelegationToken.java:1737) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService$Processor$GetDelegationToken.java:1722) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:621) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [libthrift-0.9.3.jar:0.9.3]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive Metastore (HMS) as a remote process secured with Kerberos.",
                "2. Configure DBTokenStore as the token store.",
                "3. Submit a job using Oozie on behalf of a user who is not Kerberos enabled.",
                "4. Attempt to call the GetDelegationToken API."
            ],
            "ExpectedBehavior": "The GetDelegationToken API should successfully return a delegation token for the specified user.",
            "ObservedBehavior": "The GetDelegationToken API fails with a SASL negotiation failure due to the user not being Kerberos enabled.",
            "Resolution": "[Provide additional details about the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-4233.json",
        "creation_time": "2013-03-26T13:02:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-4233",
            "Title": "Kerberos Authentication Failure After 7 Days of HiveServer2 Operation",
            "Description": "When HiveServer2 has been running for more than 7 days, attempts to connect using the Beeline shell result in all operations failing due to a Kerberos authentication failure. The logs indicate that the failure is caused by an invalid ticket.",
            "StackTrace": [
                "2013-03-26 11:55:20,932 ERROR hive.ql.metadata.Hive: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1084)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:51)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:61)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2140)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2151)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(Hive.java:2275)",
                "at org.apache.hive.service.cli.CLIService.getDelegationTokenFromMetaStore(CLIService.java:358)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:127)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1073)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1058)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:565)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedConstructorAccessor52.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at sun.reflect.Constructor.newInstance(Constructor.java:513)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1082)",
                "... 16 more",
                "Caused by: java.lang.IllegalStateException: This ticket is no longer valid",
                "at javax.security.auth.kerberos.KerberosTicket.toString(KerberosTicket.java:601)",
                "at java.lang.String.valueOf(String.java:2826)",
                "at java.lang.StringBuilder.append(StringBuilder.java:115)",
                "at sun.security.jgss.krb5.SubjectComber.findAux(SubjectComber.java:120)",
                "at sun.security.jgss.krb5.SubjectComber.find(SubjectComber.java:41)",
                "at sun.security.jgss.krb5.Krb5Util.getTicket(Krb5Util.java:130)",
                "at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:328)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:277)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:163)",
                "... 20 more"
            ],
            "StepsToReproduce": [
                "Start HiveServer2 and let it run for more than 7 days.",
                "Use the Beeline shell to connect to the HiveServer2 instance.",
                "Attempt to perform any operation (e.g., querying a table)."
            ],
            "ExpectedBehavior": "The Beeline shell should successfully connect to HiveServer2 and allow operations to be performed without authentication errors.",
            "ObservedBehavior": "All operations fail with a Kerberos authentication error indicating that the ticket is no longer valid.",
            "Resolution": "The issue is suspected to be caused by the lack of a timer to renew the TGT (Ticket Granting Ticket) in the HiveAuthFactory.loginFromKeytab method. Implementing a mechanism to renew the TGT periodically may resolve the issue."
        }
    },
    {
        "filename": "HIVE-14303.json",
        "creation_time": "2016-07-21T03:16:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-14303",
            "Title": "NullPointerException in CommonJoinOperator.checkAndGenObject when ExecReducer.close is called multiple times",
            "Description": "The method CommonJoinOperator.checkAndGenObject should return directly after CommonJoinOperator.closeOp is called to prevent a NullPointerException (NPE) when ExecReducer.close is invoked multiple times. The NPE is triggered because the first call to reducer.close() clears the storage, leading to a null reference on subsequent calls.",
            "StackTrace": [
                "Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: null",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)",
                "at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)",
                "... 8 more"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves a join operation using CommonJoinOperator.",
                "2. Ensure that the ExecReducer.close() method is called multiple times during the job execution.",
                "3. Observe the logs for any NullPointerException or related runtime errors."
            ],
            "ExpectedBehavior": "The method CommonJoinOperator.checkAndGenObject should handle multiple calls to ExecReducer.close gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when ExecReducer.close is called multiple times, leading to a runtime error and masking the original exception.",
            "Resolution": "A fix has been implemented to ensure that CommonJoinOperator.checkAndGenObject returns directly after closeOp is called, preventing the NPE."
        }
    },
    {
        "filename": "HIVE-19248.json",
        "creation_time": "2018-04-19T17:45:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-19248",
            "Title": "File Copy Failure in REPL LOAD Due to HDFS Block Size Mismatch",
            "Description": "The Hive replication process fails to copy files from the source to the target warehouse when there is a mismatch in HDFS block sizes between the two clusters. This results in an IOException without proper error handling, leading to potential data integrity issues.",
            "StackTrace": [
                "2018-04-09 14:32:06,690 ERROR [main] org.apache.hadoop.tools.mapred.CopyMapper: Failure in copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "java.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:299)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:266)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)",
                "Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:296)",
                "... 10 more",
                "Caused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:212)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)",
                "at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "1. Set up two HDFS clusters with different block sizes.",
                "2. Attempt to replicate a Hive table using the REPL LOAD command.",
                "3. Monitor the logs for any file copy errors."
            ],
            "ExpectedBehavior": "The REPL LOAD command should successfully copy files from the source to the target warehouse without errors, regardless of block size differences.",
            "ObservedBehavior": "The REPL LOAD command fails to copy files due to a checksum mismatch caused by differing HDFS block sizes, resulting in an IOException without proper error reporting.",
            "Resolution": "A fix has been implemented to ensure that the REPL LOAD command handles block size mismatches correctly and throws an error if file copy fails after maximum attempts."
        }
    },
    {
        "filename": "HIVE-7167.json",
        "creation_time": "2014-06-02T18:13:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-7167",
            "Title": "Hive Metastore Connection Refused Error When Starting HiveServer2 and HiveServer Simultaneously",
            "Description": "When both HiveServer2 and HiveServer are started at the same time, a connection error occurs due to the metastore being unable to connect. This results in a failure to instantiate the HiveMetaStoreClient, leading to multiple failures in Hive-related system tests.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:347)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1413)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2444)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2456)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:341)",
                "... 7 more",
                "Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused: connect",
                "at org.apache.thrift.transport.TSocket.open(TSocket.java:185)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:336)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:214)",
                "... 17 more",
                "Caused by: java.net.ConnectException: Connection refused: connect",
                "at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method)",
                "at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)",
                "at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)",
                "at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)",
                "at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:157)",
                "at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)",
                "at java.net.Socket.connect(Socket.java:579)",
                "at org.apache.thrift.transport.TSocket.open(TSocket.java:180)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "1. Start HiveServer2 with an embedded metastore.",
                "2. Start HiveServer with a remote metastore simultaneously.",
                "3. Attempt to launch the Hive CLI."
            ],
            "ExpectedBehavior": "The Hive CLI should start successfully without any connection errors to the metastore.",
            "ObservedBehavior": "The Hive CLI fails to start, displaying a connection refused error related to the metastore.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-12360.json",
        "creation_time": "2015-11-06T18:04:00.000+0000",
        "bug_report": {
            "BugID": "HIVE-12360",
            "Title": "IOException: Seek in index to 4613 is outside of the data when reading uncompressed ORC with predicate pushdown",
            "Description": "When attempting to read from an uncompressed ORC file in HDP-2.3.2 with predicate pushdown enabled, an IOException occurs indicating that the seek operation is outside the data bounds. This issue appears to be related to the handling of row groups and index filtering in the ORC file format.",
            "StackTrace": [
                "java.io.IOException: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "Caused by: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.seek(InStream.java:139)",
                "at org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.read(InStream.java:87)",
                "at java.io.InputStream.read(InputStream.java:102)",
                "at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)",
                "at com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7429)",
                "at org.apache.hadoop.hive.ql.io.orc.MetadataReader.readRowIndex(MetadataReader.java:88)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1166)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)"
            ],
            "StepsToReproduce": [
                "1. Set up an environment with Oracle Linux 6.4 and HDP 2.3.2.0-2950.",
                "2. Create an uncompressed ORC file with data.",
                "3. Enable predicate pushdown in Hive settings.",
                "4. Execute a query that reads from the ORC file."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any exceptions.",
            "ObservedBehavior": "An IOException is thrown indicating that the seek operation is outside the data bounds, specifically: 'Seek in index to 4613 is outside of the data'.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-13160.json",
        "creation_time": "2016-02-26T00:02:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-13160",
            "Title": "HiveServer2 Fails to Load User-Defined Functions on Startup When MetaStore is Unavailable",
            "Description": "When HiveServer2 (HS2) starts up and the Hive MetaStore (HMS) is not ready, HS2 fails to load User-Defined Functions (UDFs). This results in a state where HS2 is operational but lacks the necessary function list, leading to a situation where no functions are available for use. The expected behavior is for HS2 to either wait for the HMS to be ready or to handle the absence of UDFs gracefully.",
            "StackTrace": [
                "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)"
            ],
            "StepsToReproduce": [
                "1. Start the HiveServer2 service.",
                "2. Ensure that the Hive MetaStore service is not running or is unreachable.",
                "3. Observe the logs for connection attempts to the MetaStore and subsequent warnings/errors."
            ],
            "ExpectedBehavior": "HiveServer2 should either wait for the Hive MetaStore to become available before starting or handle the absence of UDFs gracefully, allowing for a functional state.",
            "ObservedBehavior": "HiveServer2 starts but fails to load UDFs, resulting in an operational state without any available functions. The logs indicate repeated connection attempts to the MetaStore followed by warnings about the failure to connect.",
            "Resolution": "A fix has been implemented to ensure that HiveServer2 does not enter a servicing state if the function list is not ready. The initialization of the function list will be deferred until a Hive session is created, with caching mechanisms in place for performance."
        }
    },
    {
        "filename": "HIVE-12008.json",
        "creation_time": "2015-10-01T19:26:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-12008",
            "Title": "Runtime Exception on count(*) with get_json_object() in Hive View",
            "Description": "Executing a Hive query that uses count(*) on a column in a view results in a RuntimeException. The issue appears to be related to the handling of prunelists when selecting all columns from a table, leading to an IndexOutOfBoundsException.",
            "StackTrace": [
                "2015-10-27 17:51:33,742 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 9 more",
                "Caused by: java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)",
                "... 14 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 17 more",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:147)",
                "... 22 more",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive view that includes a column with a get_json_object() UDF.",
                "2. Execute a query using count(*) on the view.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The query should return the count of rows in the view without any exceptions.",
            "ObservedBehavior": "The query fails with a RuntimeException indicating an error in configuring the object, specifically an IndexOutOfBoundsException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-6205.json",
        "creation_time": "2014-01-15T07:34:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-6205",
            "Title": "NullPointerException during ALTER TABLE operation in Hive authorization",
            "Description": "When executing an ALTER TABLE command to change the partition column type, a NullPointerException is thrown during the authorization phase, indicating that the operation for TOK_ALTERTABLE_ALTERPARTS is not defined.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)"
            ],
            "StepsToReproduce": [
                "1. Start the Hive CLI.",
                "2. Execute the following command: `ALTER TABLE <table_name> ALTER COLUMN <column_name> TYPE <new_type>;`",
                "3. Observe the output for any errors."
            ],
            "ExpectedBehavior": "The ALTER TABLE command should execute successfully, updating the column type without any errors.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the command, indicating an issue with the authorization process.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-15309.json",
        "creation_time": "2016-11-29T21:56:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-15309",
            "Title": "FileNotFoundException in OrcAcidUtils.getLastFlushLength() due to missing file check",
            "Description": "The method OrcAcidUtils.getLastFlushLength() does not check for the existence of a file before attempting to access it. This results in a FileNotFoundException being thrown, which leads to unnecessary and confusing logging messages. The logging indicates that a file does not exist, but does not provide a clear indication of the underlying issue.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/rslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1831)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1744)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:373)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1496)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1396)",
                "at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1211)",
                "at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)",
                "at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)",
                "at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)",
                "at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1536)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:330)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:326)",
                "at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:782)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:460)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1525)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:631)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "1. Attempt to access the last flush length of a delta file that does not exist.",
                "2. Observe the logging output generated by the system."
            ],
            "ExpectedBehavior": "The system should check for the existence of the file before attempting to access it, preventing a FileNotFoundException and reducing unnecessary logging.",
            "ObservedBehavior": "A FileNotFoundException is thrown, leading to confusing log messages indicating that the file does not exist.",
            "Resolution": "A fix has been implemented to check for file existence in OrcAcidUtils.getLastFlushLength() before attempting to access it."
        }
    },
    {
        "filename": "HIVE-10808.json",
        "creation_time": "2015-05-23T02:24:16.000+0000",
        "bug_report": {
            "BugID": "HIVE-10808",
            "Title": "ClassCastException during Inner Join due to Null Struct Serialization",
            "Description": "A ClassCastException occurs when executing a Hive query that involves an inner join on a table with null values. The error arises from the attempt to cast a NullStructSerDeObjectInspector to a PrimitiveObjectInspector, which is not valid. This issue impacts the ability to run queries that involve inner joins on tables with nullable fields.",
            "StackTrace": [
                "java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 9 more",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:157)",
                "... 22 more",
                "Caused by: org.apache.hadoop.hive.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:334)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:352)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)",
                "... 22 more"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table 'tab1' with nullable columns.",
                "2. Insert data into 'tab1' including rows with null values.",
                "3. Execute the following query: ",
                "   SELECT a.col1, a.col2, a.col3, a.col4 FROM tab1 a INNER JOIN (SELECT max(x) as x FROM tab1 WHERE x < 20130327) r ON a.x = r.x WHERE a.col1 = 'F' AND a.col3 IN ('A', 'S', 'G');"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected result set without any exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that a NullStructSerDeObjectInspector cannot be cast to a PrimitiveObjectInspector.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-18429.json",
        "creation_time": "2018-01-10T20:45:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-18429",
            "Title": "Compaction Fails When No Output is Produced During Delta Processing",
            "Description": "When the compaction process runs with empty delta files (delta_8_8 and delta_9_9), it fails to produce the expected output (delta_8_9). This results in a failure of the job commit process due to the absence of the temporary location ({{CompactorMR.TMP_LOCATION}}), leading to a FileNotFoundException. This issue prevents further compaction unless new delta files with data are created, and if the number of empty deltas exceeds the configured maximum, compaction cannot proceed at all.",
            "StackTrace": [
                "2017-12-27 17:19:28,850 ERROR CommitterEvent Processor #1 org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job",
                "java.io.FileNotFoundException: File hdfs://OTCHaaS/apps/hive/warehouse/momi.db/sensor_data/babyid=5911806ebf69640100004257/_tmp_b4c5a3f3-44e5-4d45-86af-5b773bf0fc96 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:923)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:114)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:985)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:981)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:992)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:785)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start with empty delta files: delta_8_8 and delta_9_9.",
                "2. Trigger the compaction process.",
                "3. Observe the logs for errors related to job commitment."
            ],
            "ExpectedBehavior": "The compaction process should produce a new delta file (delta_8_9) even if it is empty, allowing for further compaction to proceed.",
            "ObservedBehavior": "The compaction process fails to produce the expected delta file, resulting in a FileNotFoundException during the job commit process.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the compaction process can handle cases where no output is produced."
        }
    },
    {
        "filename": "HIVE-10776.json",
        "creation_time": "2015-05-21T00:56:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-10776",
            "Title": "NullPointerException on Insert with Select * in Bucketed Tables",
            "Description": "When executing insert queries with 'select *' on bucketed tables in Hive, a NullPointerException is thrown, causing the operation to fail. This issue occurs specifically during the planning phase of the query execution.",
            "StackTrace": [
                "2015-05-15 19:29:01,278 ERROR [main]: ql.Driver (SessionState.java:printError(957)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:7257)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6100)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6271)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8972)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8863)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9708)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9601)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10037)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:323)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Set the following Hive configurations:",
                "set hive.support.concurrency=true;",
                "set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;",
                "set hive.enforce.bucketing=true;",
                "Drop the table if it exists: drop table if exists studenttab10k;",
                "Create the table: create table studenttab10k (age int, name varchar(50), gpa decimal(3,2));",
                "Insert values into the table: insert into studenttab10k values(1,'foo', 1.1), (2,'bar', 2.3), (3,'baz', 3.1);",
                "Drop the bucketed table if it exists: drop table if exists student_acid;",
                "Create the bucketed table: create table student_acid (age int, name varchar(50), gpa decimal(3,2), grade int) clustered by (age) into 2 buckets stored as orc tblproperties ('transactional'='true');",
                "Attempt to insert data using select *: insert into student_acid(name, age, gpa) select * from studenttab10k;"
            ],
            "ExpectedBehavior": "The insert operation should successfully copy data from 'studenttab10k' to 'student_acid' without throwing any exceptions.",
            "ObservedBehavior": "The operation fails with a NullPointerException during the planning phase of the query execution.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-6301.json",
        "creation_time": "2014-01-24T01:42:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-6301",
            "Title": "UDFJson.evaluate() throws IllegalStateException due to unmatched regex group",
            "Description": "The method `UDFJson.evaluate()` is throwing a `java.lang.IllegalStateException` when attempting to access a regex group that has not been matched. This occurs because the method `_mKey.matches()` is not called before `_mKey.group(1)`, leading to an exception when there is no match found. This issue persists in the latest version of the code.",
            "StackTrace": [
                "2014-01-23 11:08:19,869 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String) on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2",
                "Caused by: java.lang.IllegalStateException: No match found",
                "at java.util.regex.Matcher.group(Matcher.java:468)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)",
                "... 24 more"
            ],
            "StepsToReproduce": [
                "1. Prepare a JSON string that does not match the expected regex pattern.",
                "2. Call the `UDFJson.evaluate()` method with the JSON string and a path string that leads to the unmatched group.",
                "3. Observe the exception thrown."
            ],
            "ExpectedBehavior": "The method `UDFJson.evaluate()` should handle unmatched regex groups gracefully, returning null or an appropriate error message without throwing an exception.",
            "ObservedBehavior": "The method `UDFJson.evaluate()` throws a `java.lang.IllegalStateException` when attempting to access a regex group that has not been matched.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-8295.json",
        "creation_time": "2014-09-29T21:16:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-8295",
            "Title": "Direct SQL Fails with Large Partition IDs Due to Oracle Limitations",
            "Description": "When fetching partition objects in the MetastoreDirectSql class, if the number of partition IDs that match the filter exceeds 1000, the direct SQL query fails with a SQLSyntaxErrorException. This is due to Oracle's limitation on the maximum number of expressions in a list. The current implementation constructs partition objects by fetching partition IDs first, which leads to this issue. A proposed solution is to retrieve partition objects in batches to avoid exceeding this limit, which would also improve performance and reduce memory usage.",
            "StackTrace": [
                "2014-09-29 19:30:02,942 DEBUG [pool-1-thread-1] metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(604)) - Direct SQL query in 122.085893ms + 13.048901ms, the query is [select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\"  inner join \"TBLS\" on \"PARTITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\"     and \"TBLS\".\"TBL_NAME\" = ?   inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\"      and \"DBS\".\"NAME\" = ? inner join \"PARTITION_KEY_VALS\" \"FILTER2\" on \"FILTER2\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER2\".\"INTEGER_IDX\" = 2 where (\"FILTER2\".\"PART_KEY_VAL\" = ?)]",
                "2014-09-29 19:30:02,949 ERROR [pool-1-thread-1] metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2248)) - Direct SQL failed, falling back to ORM",
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\", \"SDS\".\"SD_ID\", \"SDS\".\"CD_ID\", \"SERDES\".\"SERDE_ID\", \"PARTITIONS\".\"CREATE_TIME\", \"PARTITIONS\".\"LAST_ACCESS_TIME\", \"SDS\".\"INPUT_FORMAT\", \"SDS\".\"IS_COMPRESSED\", \"SDS\".\"IS_STOREDASSUBDIRECTORIES\", \"SDS\".\"LOCATION\", \"SDS\".\"NUM_BUCKETS\", \"SDS\".\"OUTPUT_FORMAT\", \"SERDES\".\"NAME\", \"SERDES\".\"SLIB\" from \"PARTITIONS\"  left outer join \"SDS\" on \"PARTITIONS\".\"SD_ID\" = \"SDS\".\"SD_ID\"   left outer join \"SERDES\" on \"SDS\".\"SERDE_ID\" = \"SERDES\".\"SERDE_ID\" where \"PART_ID\" in (136,140,143,147,152,156,160,163,167,171,174,180,185,191,196,198,203,208,212,217...) order by \"PART_NAME\" asc\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:422)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:331)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1920)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1914)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2213)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1914)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1887)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)",
                "at com.sun.proxy.$Proxy8.getPartitionsByExpr(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3800)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9366)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9350)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:206)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive metastore with a table that has more than 1000 partitions.",
                "2. Execute a query that attempts to fetch all partitions using direct SQL.",
                "3. Observe the error in the logs indicating a SQLSyntaxErrorException."
            ],
            "ExpectedBehavior": "The system should retrieve partition objects without exceeding the Oracle limit of 1000 expressions in a list, either by batching the requests or handling the error gracefully.",
            "ObservedBehavior": "The system fails with a SQLSyntaxErrorException when attempting to execute a direct SQL query that includes more than 1000 partition IDs.",
            "Resolution": "Implement batch retrieval of partition objects in the MetastoreDirectSql class to avoid exceeding the Oracle limit and improve performance."
        }
    },
    {
        "filename": "HIVE-8915.json",
        "creation_time": "2014-11-19T19:40:17.000+0000",
        "bug_report": {
            "BugID": "HIVE-8915",
            "Title": "Endless Log File Growth Due to Missing COMPACTION_QUEUE Table",
            "Description": "When starting the Hive metastore without the required COMPACTION_QUEUE table, an endless loop of errors occurs, causing the log file to grow excessively. This results in significant resource consumption and potential system instability.",
            "StackTrace": [
                "2014-11-19 01:44:57,654 ERROR compactor.Cleaner (Cleaner.java:run(143)) - Caught an exception in the main loop of compactor cleaner, MetaException(message:Unable to connect to transaction database",
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hive.COMPACTION_QUEUE' doesn't exist",
                "at sun.reflect.GeneratedConstructorAccessor20.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:386)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2569)",
                "at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1524)",
                "at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:266)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)"
            ],
            "StepsToReproduce": [
                "1. Set up a fresh Hive environment without creating the necessary database tables as specified in hive-txn-schema-0.14.0.mysql.sql.",
                "2. Start the Hive metastore.",
                "3. Monitor the log file for error messages."
            ],
            "ExpectedBehavior": "The Hive metastore should start without errors, or provide a clear error message indicating the missing COMPACTION_QUEUE table without causing excessive log file growth.",
            "ObservedBehavior": "An endless loop of errors is logged, causing the log file to grow to 1.7GB within 5 minutes, with repeated instances of the same error message.",
            "Resolution": "[Provide additional details about the resolution, if applicable]"
        }
    },
    {
        "filename": "HIVE-7249.json",
        "creation_time": "2014-06-18T00:09:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-7249",
            "Title": "NoSuchLockException Thrown When Closing Transaction Manager After Commit",
            "Description": "When executing a transaction that involves acquiring locks and then committing, calling `closeTxnManager()` results in a `NoSuchLockException`. This indicates that the transaction manager does not recognize that the locks were released upon committing the transaction.",
            "StackTrace": [
                "2014-06-17 15:54:40,804 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - NoSuchLockException(message:No such lock: 1)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(TxnHandler.java:1407)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(TxnHandler.java:477)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.unlock(HiveMetaStore.java:4817)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy14.unlock(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.unlock(HiveMetaStoreClient.java:1598)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(DbLockManager.java:110)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close(DbLockManager.java:162)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct(DbTxnManager.java:300)",
                "at org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager(HiveTxnManagerImpl.java:39)",
                "at org.apache.hive.hcatalog.mapreduce.TransactionContext.cleanup(TransactionContext.java:327)",
                "at org.apache.hive.hcatalog.mapreduce.TransactionContext.onCommitJob(TransactionContext.java:142)",
                "at org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.commitJob(OutputCommitterContainer.java:61)",
                "at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(FileOutputCommitterContainer.java:251)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:537)"
            ],
            "StepsToReproduce": [
                "1. Open a transaction using `openTxn()`.",
                "2. Acquire locks using `acquireLocks()` for a query like 'INSERT INTO T PARTITION(p) SELECT * FROM T'.",
                "3. Commit the transaction using `commitTxn()`.",
                "4. Call `closeTxnManager()`."
            ],
            "ExpectedBehavior": "The transaction manager should successfully close without throwing any exceptions, indicating that all locks have been released properly.",
            "ObservedBehavior": "A `NoSuchLockException` is thrown, indicating that the transaction manager does not recognize the lock associated with the transaction.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11540.json",
        "creation_time": "2015-08-12T23:12:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-11540",
            "Title": "OutOfMemoryError during Compaction due to Excessive Delta Files",
            "Description": "When streaming weblogs to Kafka and then to Flume 1.6 using a Hive sink, the compaction process runs out of memory while attempting to clean up delta files. This issue occurs regardless of the compactor's scheduling configuration, leading to performance degradation and failure to compact delta files effectively.",
            "StackTrace": [
                "2015-08-12 15:05:01,197 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Direct buffer memory",
                "2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12. Marking clean to avoid repeated failures, java.io.IOException: Job failed!",
                "at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)"
            ],
            "StepsToReproduce": [
                "1. Stream weblogs to Kafka and then to Flume 1.6 using a Hive sink.",
                "2. Configure the compactor with 5 threads and set various compaction intervals (30m/5m/5s).",
                "3. Monitor the compaction process while a high volume of records (20 million/day) is being processed."
            ],
            "ExpectedBehavior": "The compactor should efficiently clean up delta files without running out of memory, allowing for smooth operation and timely compaction.",
            "ObservedBehavior": "The compactor runs out of memory, resulting in an OutOfMemoryError and failure to compact delta files, leading to performance issues and delays.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-15755.json",
        "creation_time": "2017-01-30T20:48:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-15755",
            "Title": "NullPointerException on Invalid Table Name in ON Clause of Merge Statement",
            "Description": "A NullPointerException occurs when an invalid table name is specified in the ON clause of a MERGE statement. This issue arises during the parsing and semantic analysis of the query, specifically when the system attempts to validate the target table and its columns.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)",
                "    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "    at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)",
                "    at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)",
                "    at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)",
                "    at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)",
                "    at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)",
                "    at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)",
                "    at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:298)",
                "    at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:506)",
                "    at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317)",
                "    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "    at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Create a source table with the command: `create table src (col1 int,col2 int);`",
                "2. Create a target table with the command: `create table trgt (tcol1 int,tcol2 int);`",
                "3. Insert a row into the source table: `insert into src values (1,232);`",
                "4. Execute a MERGE statement with an invalid table name in the ON clause: `merge into trgt using (select * from src) sub on sub.col1 = *invalidtablename.tcol1* when not matched then insert values (sub.col1,sub.col2);`"
            ],
            "ExpectedBehavior": "The system should return an error message indicating that the specified table name is invalid, without throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException when an invalid table name is specified in the ON clause of the MERGE statement.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-9390.json",
        "creation_time": "2015-01-15T18:50:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-9390",
            "Title": "Timeout Exception When Retrieving Open Transactions from Hive Metastore",
            "Description": "A timeout exception occurs when attempting to retrieve open transactions from the Hive Metastore, indicating an inability to obtain a JDBC connection from the connection pool. This issue can lead to failures in transaction management and may impact the overall stability of the Hive service.",
            "StackTrace": [
                "2015-01-13 16:09:21,148 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(141)) - org.apache.thrift.TException: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5324)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)",
                "at com.sun.proxy.$Proxy11.get_open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidTxns(HiveMetaStoreClient.java:1696)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidTxns(DbTxnManager.java:289)",
                "at org.apache.hadoop.hive.ql.Driver.recordValidTxns(Driver.java:882)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:399)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)",
                "at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)",
                "at com.sun.proxy.$Proxy21.executeStatementAsync(Unknown Source)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:401)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.thrift.server.TServlet.doPost(TServlet.java:83)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:101)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)",
                "at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857)",
                "at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:850)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:196)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5322)",
                "... 66 more"
            ],
            "StepsToReproduce": [
                "1. Start the Hive Metastore service.",
                "2. Execute a query that requires retrieving open transactions.",
                "3. Observe the logs for timeout errors related to JDBC connections."
            ],
            "ExpectedBehavior": "The system should successfully retrieve the list of open transactions without any timeout errors.",
            "ObservedBehavior": "The system throws a timeout exception indicating an inability to obtain a JDBC connection from the connection pool.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-7623.json",
        "creation_time": "2014-08-05T23:58:27.000+0000",
        "bug_report": {
            "BugID": "HIVE-7623",
            "Title": "InvalidOperationException when renaming Hive partition across different filesystems",
            "Description": "When attempting to rename a Hive partition, an InvalidOperationException is thrown if the source and destination locations are on different filesystems. This issue is similar to HIVE-3815 and can be temporarily resolved by disabling filesystem caching.",
            "StackTrace": [
                "2014-08-05 21:46:14,522 ERROR [pool-3-thread-1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - InvalidOperationException(message:table new location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=123 is on a different file system than the old location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=456. This operation is not supported)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(HiveAlterHandler.java:361)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2629)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2602)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:622)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy5.rename_partition(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9057)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9041)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with two different filesystems.",
                "2. Create a Hive table with partitions located on one filesystem.",
                "3. Attempt to rename a partition to a new location on a different filesystem.",
                "4. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The partition should be renamed successfully if the source and destination locations are valid.",
            "ObservedBehavior": "An InvalidOperationException is thrown indicating that the new location is on a different filesystem than the old location.",
            "Resolution": "To work around this issue, set 'fs.hdfs.impl.disable.cache=false' and 'fs.file.impl.disable.cache=false'. A fix is needed to properly handle filesystem comparisons in the HiveAlterHandler."
        }
    },
    {
        "filename": "HIVE-15997.json",
        "creation_time": "2017-02-21T16:49:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-15997",
            "Title": "Resource Leaks Occur When Query is Cancelled",
            "Description": "When a query is cancelled in Hive, there are potential resource leaks related to scratch directories and locks. The logs indicate that there are issues with removing scratch directories and releasing ZooKeeper locks, which can lead to resource exhaustion and degraded performance.",
            "StackTrace": [
                "2017-02-02 06:23:25,410 WARN hive.ql.Context: [HiveServer2-Background-Pool: Thread-61]: Error Removing Scratch: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: \"ychencdh511t-1.vpc.cloudera.com/172.26.11.50\"; destination host is: \"ychencdh511t-1.vpc.cloudera.com\":8020;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1476)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1409)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)",
                "at com.sun.proxy.$Proxy25.delete(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:535)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy26.delete(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:2059)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:675)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:671)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:671)",
                "at org.apache.hadoop.hive.ql.Context.removeScratchDir(Context.java:405)",
                "at org.apache.hadoop.hive.ql.Context.clear(Context.java:541)",
                "at org.apache.hadoop.hive.ql.Driver.releaseContext(Driver.java:2109)",
                "at org.apache.hadoop.hive.ql.Driver.closeInProcess(Driver.java:2150)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1472)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a Hive query that is expected to run for a significant amount of time.",
                "2. Cancel the query before it completes.",
                "3. Check the logs for any warnings or errors related to resource cleanup."
            ],
            "ExpectedBehavior": "Upon cancelling a query, all associated resources, including scratch directories and locks, should be released without any errors or warnings in the logs.",
            "ObservedBehavior": "When a query is cancelled, warnings are logged indicating that scratch directories could not be removed and that ZooKeeper locks could not be released, suggesting potential resource leaks.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-7009.json",
        "creation_time": "2014-05-02T20:50:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-7009",
            "Title": "HIVE_USER_INSTALL_DIR Enforced to HDFS, Preventing Non-HDFS Filesystem Usage",
            "Description": "The current implementation in `DagUtils.java` enforces that the user path obtained from `HIVE_USER_INSTALL_DIR` must be an HDFS path. This restriction prevents users from running Hive+Tez jobs on non-HDFS filesystems, such as WASB. The relevant code checks if the filesystem is an instance of `DistributedFileSystem`, throwing an `IOException` if it is not. This behavior leads to exceptions when attempting to execute jobs with `defaultFs` configured to WASB.",
            "StackTrace": [
                "2014-05-01 00:21:39,847 ERROR exec.Task (TezTask.java:execute(192)) - Failed to execute tez graph.",
                "java.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with Tez.",
                "2. Configure the `defaultFs` to a non-HDFS filesystem, such as WASB.",
                "3. Attempt to run a Hive+Tez job that utilizes the `HIVE_USER_INSTALL_DIR` variable.",
                "4. Observe the resulting exception indicating that the URI is not an HDFS URI."
            ],
            "ExpectedBehavior": "The Hive+Tez job should execute successfully regardless of whether the `HIVE_USER_INSTALL_DIR` is set to an HDFS or non-HDFS filesystem.",
            "ObservedBehavior": "An `IOException` is thrown indicating that the specified URI is not an HDFS URI, preventing the job from executing.",
            "Resolution": "A fix has been implemented to allow `HIVE_USER_INSTALL_DIR` to accept non-HDFS filesystems, enabling successful execution of Hive+Tez jobs on such filesystems."
        }
    },
    {
        "filename": "HIVE-2031.json",
        "creation_time": "2011-03-08T11:38:53.000+0000",
        "bug_report": {
            "BugID": "HIVE-2031",
            "Title": "SemanticException: Partition not found when loading data into partitioned table",
            "Description": "When attempting to load data into a partitioned table with two partitions by specifying only one partition in the load statement, a SemanticException is thrown indicating that the specified partition is not found. This issue affects the usability of the Hive system when dealing with partitioned tables.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)",
                "at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)",
                "at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Create a partitioned table with at least two partitions.",
                "2. Attempt to load data into the table by specifying only one of the partitions in the load statement.",
                "3. Observe the exception thrown during the operation."
            ],
            "ExpectedBehavior": "The data should be loaded into the specified partition without any errors, or an appropriate message should indicate that the partition does not exist.",
            "ObservedBehavior": "A SemanticException is thrown indicating that the specified partition is not found, preventing the load operation from completing successfully.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-4018.json",
        "creation_time": "2013-02-13T09:02:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-4018",
            "Title": "MapJoin Fails with Distributed Cache Error During Star Join Query Execution",
            "Description": "When executing a star join query after the implementation of HIVE-3784, the process fails with a Distributed Cache error, resulting in a HiveException. The error occurs specifically in the MapJoinOperator's loadHashTable method, indicating issues with loading the hash table from the distributed cache.",
            "StackTrace": [
                "2013-02-13 08:36:04,584 ERROR org.apache.hadoop.hive.ql.exec.MapJoinOperator: Load Distributed Cache Error",
                "2013-02-13 08:36:04,585 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:189)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:203)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1421)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:614)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:266)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:260)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with the necessary configurations.",
                "2. Execute a star join query that involves multiple tables.",
                "3. Ensure that the query is run after the implementation of HIVE-3784.",
                "4. Observe the logs for any errors related to Distributed Cache."
            ],
            "ExpectedBehavior": "The star join query should execute successfully without any errors related to the Distributed Cache.",
            "ObservedBehavior": "The execution fails with a Distributed Cache error, leading to a HiveException indicating an EOFException.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-11255.json",
        "creation_time": "2015-07-14T15:39:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-11255",
            "Title": "get_table_objects_by_name() in HiveMetaStore.java fails with SQLSyntaxErrorException due to exceeding Oracle limit",
            "Description": "The method get_table_objects_by_name() in HiveMetaStore.java currently attempts to retrieve all tables of a database in a single call to ObjectStore. This results in a SQLSyntaxErrorException in Oracle databases when the number of tables exceeds 1000, as Oracle has a limit on the maximum number of expressions in a list. The method should be modified to break the table list into multiple sublists to avoid this issue.",
            "StackTrace": [
                "2015-06-29 13:36:00,093 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: Retrying HMSHandler after 1000 ms (attempt 1 of 1) with error: javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:945)",
                "at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy0.getTableObjectsByName(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1618)",
                "at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:106)",
                "at com.sun.proxy.$Proxy5.get_table_objects_by_name(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8172)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8156)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "StepsToReproduce": [
                "1. Set up an Oracle database with more than 1000 tables in a single schema.",
                "2. Call the get_table_objects_by_name() method with the database name and a list of all table names.",
                "3. Observe the error thrown due to exceeding the maximum number of expressions in the SQL query."
            ],
            "ExpectedBehavior": "The method should retrieve table objects without throwing an SQLSyntaxErrorException, even when the number of tables exceeds 1000.",
            "ObservedBehavior": "The method throws a java.sql.SQLSyntaxErrorException indicating that the maximum number of expressions in a list is 1000.",
            "Resolution": "The issue has been fixed by modifying the get_table_objects_by_name() method to split the list of table names into multiple sublists, each containing fewer than 1000 entries."
        }
    },
    {
        "filename": "HIVE-10151.json",
        "creation_time": "2015-03-31T00:17:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-10151",
            "Title": "Insert from One Bucketed Acid Table to Another Fails with ORC Error",
            "Description": "The `BucketingSortingReduceSinkOptimizer` encounters an issue when performing an insert operation from one bucketed Acid table to another. The operation fails due to the `BucketizedHiveInputFormat` bypassing the ORC merge logic, leading to an error when the ORC input format attempts to read bucket files instead of the table directory. This issue arises specifically when both the source and destination Acid tables are bucketed in the same manner.",
            "StackTrace": [
                "2015-04-29 13:57:35,807 ERROR [main]: exec.Task (SessionState.java:printError(956)) - Job Submission failed with exception 'java.lang.RuntimeException(serious problem)'",
                "java.lang.RuntimeException: serious problem",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)",
                "at org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(BucketizedHiveInputFormat.java:141)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:624)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:492)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:430)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)",
                "at org.apache.hadoop.hive.ql.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:225)",
                "at org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn2(TestTxnCommands2.java:148)"
            ],
            "StepsToReproduce": [
                "Create two Acid tables with the same bucket configuration.",
                "Insert data into the first Acid table.",
                "Attempt to insert data from the first Acid table into the second Acid table using an insert statement."
            ],
            "ExpectedBehavior": "The insert operation should complete successfully, transferring data from the source Acid table to the destination Acid table without errors.",
            "ObservedBehavior": "The insert operation fails with a RuntimeException indicating a serious problem, specifically stating that 'delta_0000001_0000001 does not start with base_'.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-13546.json",
        "creation_time": "2016-04-19T07:43:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-13546",
            "Title": "IndexOutOfBoundsException in ExecReducer during dynamic partitioning",
            "Description": "An IndexOutOfBoundsException occurs in the ExecReducer class when processing rows with dynamic partitioning enabled. This issue arises when the dynamic partition columns are not correctly populated, leading to an attempt to access an index that does not exist in the list of values.",
            "StackTrace": [
                "2016-04-19 15:15:35,252 FATAL [main] ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":null},\"value\":{\"_col0\":null,\"_col1\":5588,\"_col2\":170300,\"_col3\":null,\"_col4\":756,\"_col5\":91384,\"_col6\":16,\"_col7\":null,\"_col8\":855582,\"_col9\":28,\"_col10\":null,\"_col11\":48.83,\"_col12\":null,\"_col13\":0.0,\"_col14\":null,\"_col15\":899.64,\"_col16\":null,\"_col17\":6.14,\"_col18\":0.0,\"_col19\":null,\"_col20\":null,\"_col21\":null,\"_col22\":null}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:180)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:174)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:653)",
                "at java.util.ArrayList.get(ArrayList.java:429)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:151)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:131)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:1003)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:919)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:713)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)",
                "... 7 more"
            ],
            "StepsToReproduce": [
                "1. Set the following Hive configuration parameters: hive.map.aggr=true, mapreduce.reduce.speculative=false, hive.auto.convert.join=true, hive.optimize.reducededuplication=false, hive.optimize.reducededuplication.min.reducer=1, hive.optimize.mapjoin.mapreduce=true, hive.stats.autogather=true.",
                "2. Use the following SQL command to insert data into a partitioned table: INSERT OVERWRITE TABLE store_sales PARTITION (ss_sold_date_sk) SELECT ... FROM tpcds_text_4.store_sales ss;",
                "3. Ensure that the data being processed includes rows with null values in the dynamic partition columns."
            ],
            "ExpectedBehavior": "The SQL command should execute successfully, inserting data into the specified partition without any runtime errors.",
            "ObservedBehavior": "The SQL command fails with an IndexOutOfBoundsException, indicating that the dynamic partition columns are not populated correctly, leading to an attempt to access an index that does not exist.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-7049.json",
        "creation_time": "2014-05-12T21:46:48.000+0000",
        "bug_report": {
            "BugID": "HIVE-7049",
            "Title": "Deserialization Failure for AVRO Data with Mismatched Nullable Schemas",
            "Description": "The deserialization process fails when the file schema and record schema are different, particularly when the record schema is nullable but the file schema is not. This issue arises in the `AvroDeserialize` class, specifically in the `deserializeNullableUnion` method, where the file schema is not validated against the nullable type of the record schema.",
            "StackTrace": [
                "org.apache.avro.AvroRuntimeException: Not a union: \"string\"",
                "at org.apache.avro.Schema.getTypes(Schema.java:272)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)"
            ],
            "StepsToReproduce": [
                "1. Create a record schema that includes a nullable type, e.g., [\"null\", \"string\"].",
                "2. Create a file schema that does not include a nullable type, e.g., \"string\".",
                "3. Attempt to deserialize the AVRO data using the `AvroDeserializer` class."
            ],
            "ExpectedBehavior": "The deserialization process should correctly handle nullable types and not throw an exception when the file schema and record schema are different.",
            "ObservedBehavior": "An `AvroRuntimeException` is thrown indicating that the file schema is not a union type, which prevents successful deserialization.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-9755.json",
        "creation_time": "2015-02-23T20:50:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-9755",
            "Title": "Hive UDAF 'ngram' Fails with Null Values in Input",
            "Description": "When executing the Hive built-in 'ngram' UDAF on a dataset where any result has a value equal to null, a fatal error occurs. The error is related to a mismatch in the expected value for 'n', which is typically caused by a non-constant expression.",
            "StackTrace": [
                "2015-01-08 09:15:00,262 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":[\"0\",\"0\",\"0\",\"0\"]},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:258)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:506)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:447)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: GenericUDAFnGramEvaluator: mismatch in value for 'n', which usually is caused by a non-constant expression. Found '0' and '1'.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams$GenericUDAFnGramEvaluator.merge(GenericUDAFnGrams.java:242)",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:142)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:658)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:911)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:753)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:249)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table named 'ngramtest' with columns 'col1' (int) and 'col3' (string).",
                "2. Insert data into 'ngramtest' where 'col3' contains null values.",
                "3. Execute the following Hive query: SELECT explode(ngrams(sentences(lower(t.col3)), 3, 10)) as x FROM (SELECT col3 FROM ngramtest WHERE col1=0) t;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected n-grams without errors, even if some input values are null.",
            "ObservedBehavior": "The query fails with a fatal error indicating a mismatch in the value for 'n', specifically when processing rows with null values.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-19130.json",
        "creation_time": "2018-04-09T10:18:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-19130",
            "Title": "NullPointerException Thrown During REPL LOAD with Drop Partition Event",
            "Description": "A NullPointerException (NPE) occurs when performing an incremental replication with a specific sequence of events involving dropping and creating partitions. The issue arises when the REPL LOAD command is applied to the second batch of events, leading to a failure in locating the required table.",
            "StackTrace": [
                "2018-04-05 16:20:36,531 ERROR [HiveServer2-Background-Pool: Thread-107044]: metadata.Hive (Hive.java:getTable(1219)) - Table catalog_sales_new not found: new5_tpcds_real_bin_partitioned_orc_1000.catalog_sales_new table not found",
                "2018-04-05 16:20:36,538 ERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4016)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3983)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:341)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:162)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1765)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1506)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1303)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)",
                "at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)"
            ],
            "StepsToReproduce": [
                "1. Create a table named 't1'.",
                "2. Execute the following commands in the first batch: CREATE_TABLE(t1), ADD_PARTITION(t1.p1), DROP_PARTITION(t1.p1).",
                "3. In the second batch, execute: DROP_TABLE(t1), CREATE_TABLE(t1), ADD_PARTITION(t1.p1), DROP_PARTITION(t1.p1).",
                "4. Perform a REPL LOAD operation on the second batch."
            ],
            "ExpectedBehavior": "The REPL LOAD operation should successfully process the second batch of events without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown during the REPL LOAD operation, indicating that the required table cannot be found.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-13090.json",
        "creation_time": "2016-02-18T21:58:48.000+0000",
        "bug_report": {
            "BugID": "HIVE-13090",
            "Title": "NullPointerException in ZooKeeperTokenStore during Hive Metastore Shutdown",
            "Description": "The Hive Metastore crashes with a NullPointerException (NPE) when attempting to remove expired delegation tokens from ZooKeeperTokenStore. This occurs when no delegation token is found, leading to an unexpected exception in the ExpiredTokenRemover thread.",
            "StackTrace": [
                "ERROR [Thread[Thread-6,5,main]]: thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(331)) - ExpiredTokenRemover thread received unexpected exception. org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.NullPointerException",
                "at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)",
                "at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)"
            ],
            "StepsToReproduce": [
                "1. Start the Hive Metastore service.",
                "2. Ensure that there are no valid delegation tokens present in ZooKeeper.",
                "3. Trigger the shutdown of the Hive Metastore service.",
                "4. Observe the logs for any exceptions thrown during the shutdown process."
            ],
            "ExpectedBehavior": "The Hive Metastore should shut down gracefully without throwing any exceptions, even when no delegation tokens are present.",
            "ObservedBehavior": "The Hive Metastore crashes with a NullPointerException during the shutdown process when attempting to remove expired tokens from ZooKeeper.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-5664.json",
        "creation_time": "2013-10-28T03:50:29.000+0000",
        "bug_report": {
            "BugID": "HIVE-5664",
            "Title": "Database Drop Cascade Fails When Indexes Exist on Tables",
            "Description": "When attempting to drop a database that contains tables with indexes, the operation fails with an error indicating that the database does not exist. This issue arises due to the failure to properly handle the dependencies of indexes during the drop operation.",
            "StackTrace": [
                "2013-10-27 20:46:16,629 ERROR exec.DDLTask (DDLTask.java:execute(434)) - org.apache.hadoop.hive.ql.metadata.HiveException: Database does not exist: db2",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3473)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1441)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1219)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1047)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:915)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)",
                "Caused by: NoSuchObjectException(message:db2.tab1_indx table not found)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1376)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:890)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:660)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:546)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:284)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3470)",
                "... 18 more"
            ],
            "StepsToReproduce": [
                "1. Create a new database named 'db2'.",
                "2. Use the database 'db2'.",
                "3. Create a table named 'tab1' with an index on the 'id' column.",
                "4. Attempt to drop the database 'db2' using the command: DROP DATABASE db2 CASCADE."
            ],
            "ExpectedBehavior": "The database 'db2' should be dropped successfully, including all tables and indexes associated with it.",
            "ObservedBehavior": "The operation fails with an error indicating that the database does not exist, despite it being created earlier. The logs show a NoSuchObjectException related to the index on the table.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-15778.json",
        "creation_time": "2017-02-01T04:20:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-15778",
            "Title": "NullPointerException when Dropping Non-Existent Index with DbNotificationListener",
            "Description": "Executing a DROP INDEX operation on a non-existent index results in a NullPointerException (NPE) due to improper handling of the exception in the HiveMetaStore. The issue arises when the control flow attempts to notify event listeners after the failure of the drop operation.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)",
                "at org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropIndexMessage(JSONMessageFactory.java:159)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name_core(HiveMetaStore.java:4469)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4396)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)"
            ],
            "StepsToReproduce": [
                "1. Connect to the Hive metastore using a JDBC client.",
                "2. Execute the command: DROP INDEX IF EXISTS vamsee1 ON sample_07;",
                "3. Observe the logs for any errors."
            ],
            "ExpectedBehavior": "The system should handle the attempt to drop a non-existent index gracefully, without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to drop an index that does not exist, leading to a failure in the operation.",
            "Resolution": "A fix has been implemented to ensure that the exception handling in HiveMetaStore properly checks for null values before attempting to create a JSONDropIndexMessage."
        }
    },
    {
        "filename": "HIVE-8386.json",
        "creation_time": "2014-10-07T22:30:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-8386",
            "Title": "HCatalog API Call Fails Due to Case Sensitivity in Struct Column Fields",
            "Description": "When using the HCatalog API to verify the target table schema, an error occurs indicating that a field cannot be found due to case sensitivity. The error message states that the field 'givenName' (in lowercase form: 'givenname') is not found in the list of available fields, which includes 'givenName', 'surname', 'middleName', etc. This suggests that the API is not handling field names in a case-insensitive manner as expected.",
            "StackTrace": [
                "java.lang.RuntimeException: cannot find field givenName(lowercase form: givenname) in [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase]",
                "at org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(StructTypeInfo.java:109)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(HCatSchemaUtils.java:154)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(HCatSchemaUtils.java:165)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:127)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:115)",
                "at org.apache.hive.hcatalog.api.HCatTable.<init>(HCatTable.java:59)",
                "at org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(HCatClientHMSImpl.java:157)",
                "at org.apache.falcon.catalog.HiveCatalogService.tableExists(HiveCatalogService.java:143)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateStorageExists(FeedEntityParser.java:367)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateFeedStorage(FeedEntityParser.java:309)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:79)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:54)",
                "at org.apache.falcon.resource.AbstractEntityManager.validate(AbstractEntityManager.java:364)",
                "at org.apache.falcon.resource.AbstractEntityManager.submitInternal(AbstractEntityManager.java:331)",
                "at org.apache.falcon.resource.AbstractEntityManager.submit(AbstractEntityManager.java:153)",
                "at org.apache.falcon.resource.ConfigSyncService.submit(ConfigSyncService.java:44)"
            ],
            "StepsToReproduce": [
                "1. Use the HCatalog API to verify the schema of a target table that includes a field named 'givenName'.",
                "2. Ensure that the field is referenced in lowercase as 'givenname'.",
                "3. Observe the error message indicating that the field cannot be found."
            ],
            "ExpectedBehavior": "The HCatalog API should handle field names in a case-insensitive manner, allowing 'givenname' to be recognized as equivalent to 'givenName'.",
            "ObservedBehavior": "The API throws a RuntimeException indicating that it cannot find the field 'givenName' when it is referenced in lowercase as 'givenname'.",
            "Resolution": "[Provide additional details about the resolution, if applicable]"
        }
    },
    {
        "filename": "HIVE-14714.json",
        "creation_time": "2016-09-07T15:46:07.000+0000",
        "bug_report": {
            "BugID": "HIVE-14714",
            "Title": "IOException: Stream closed when shutting down HiveServer2 with Spark",
            "Description": "When executing Hive commands using Spark and subsequently finishing the Beeline session or switching the execution engine, an IOException occurs. This issue manifests when using Ctrl-D to finish the session, as well as when using commands like '!quit' or 'set hive.execution.engine=mr;'. The HiveServer2 logs indicate a timeout while shutting down the remote driver, leading to the error.",
            "StackTrace": [
                "java.io.IOException: Stream closed",
                "at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)",
                "at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)",
                "at java.io.BufferedInputStream.read(BufferedInputStream.java:334)",
                "at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)",
                "at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)",
                "at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)",
                "at java.io.InputStreamReader.read(InputStreamReader.java:184)",
                "at java.io.BufferedReader.fill(BufferedReader.java:154)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:317)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:382)",
                "at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a Beeline session connected to HiveServer2 with Spark as the execution engine.",
                "2. Execute a Hive command.",
                "3. Attempt to finish the session using Ctrl-D.",
                "4. Alternatively, try using '!quit' or 'set hive.execution.engine=mr;' to switch the execution engine."
            ],
            "ExpectedBehavior": "The Beeline session should terminate gracefully without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown with the message 'Stream closed' when attempting to finish the session or switch the execution engine.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-5428.json",
        "creation_time": "2013-10-02T20:46:10.000+0000",
        "bug_report": {
            "BugID": "HIVE-5428",
            "Title": "Direct SQL Check Fails During Tests Due to Missing Database Table",
            "Description": "When running the command `ant test -Dtestcase=TestCliDriver -Dqfile=udf_case.q -Dtest.silent=false`, an exception is thrown indicating that the database table 'DBS' does not exist. This issue appears to be related to the initialization order of the ObjectStore and the MetaStoreDirectSql components.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"DB_ID\" from \"DBS\"\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:230)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:108)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:249)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:418)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:405)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:444)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:329)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:289)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4084)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1211)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2404)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2415)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:871)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:853)",
                "at org.apache.hadoop.hive.ql.QTestUtil.cleanUp(QTestUtil.java:534)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.<clinit>(TestCliDriver.java:44)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:374)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)"
            ],
            "StepsToReproduce": [
                "1. Open a terminal.",
                "2. Navigate to the project directory.",
                "3. Run the command: `ant test -Dtestcase=TestCliDriver -Dqfile=udf_case.q -Dtest.silent=false`.",
                "4. Observe the logs for the exception related to the 'DBS' table."
            ],
            "ExpectedBehavior": "The test should run successfully without any SQL exceptions, and the database should be initialized correctly.",
            "ObservedBehavior": "An exception is thrown indicating that the table 'DBS' does not exist, leading to a failure in the test execution.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-12567.json",
        "creation_time": "2015-12-02T16:38:52.000+0000",
        "bug_report": {
            "BugID": "HIVE-12567",
            "Title": "Lock Acquisition Failure Due to ORA-08176 Error in TxnHandler",
            "Description": "The system encounters a lock acquisition failure when attempting to communicate with the metastore, resulting in an ORA-08176 error. This issue arises during the execution of transactions in Hive, specifically when the transaction database cannot be updated due to rollback data not being available.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:132)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)",
                "at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Hive server.",
                "2. Execute a transaction that requires acquiring locks on the metastore.",
                "3. Observe the logs for any lock acquisition errors."
            ],
            "ExpectedBehavior": "The transaction should acquire the necessary locks without errors and proceed with execution.",
            "ObservedBehavior": "The transaction fails with a LockException indicating an error in acquiring locks due to communication issues with the metastore, specifically an ORA-08176 error.",
            "Resolution": "[Provide additional details on the resolution or workaround for this issue]"
        }
    },
    {
        "filename": "HIVE-6984.json",
        "creation_time": "2014-04-28T23:08:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-6984",
            "Title": "NullPointerException during partition analysis with NULL values in Hive",
            "Description": "When analyzing a partitioned table in Hive that contains NULL values for the partition column, a NullPointerException (NPE) occurs, leading to a failure in processing the data. This issue arises specifically when the mapper attempts to gather statistics on rows with NULL values.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table named 'test2' with the following data:",
                "   | name                | age  |",
                "   |---------------------|------|",
                "   | 6666666666666666666 | NULL |",
                "   | 5555555555555555555 | NULL |",
                "   | tom                 | 15   |",
                "   | john                | NULL |",
                "   | mayr                | 40   |",
                "   | NULL                | 30   |",
                "2. Create a partitioned table 'test3' with the command:",
                "   CREATE TABLE test3(name STRING) PARTITIONED BY (age INT);",
                "3. Run the following command to insert data from 'test2' into 'test3':",
                "   FROM test2 INSERT OVERWRITE TABLE test3 PARTITION(age) SELECT test2.name, test2.age;",
                "4. Execute the command to analyze the partitioned table:",
                "   ANALYZE TABLE test3 PARTITION(age) COMPUTE STATISTICS;"
            ],
            "ExpectedBehavior": "The statistics for the partitioned table 'test3' should be computed successfully without any errors, even when NULL values are present in the partition column.",
            "ObservedBehavior": "A NullPointerException occurs during the analysis of the partitioned table, specifically when processing rows with NULL values in the partition column, leading to a failure in the task.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-10736.json",
        "creation_time": "2015-05-18T03:25:45.000+0000",
        "bug_report": {
            "BugID": "HIVE-10736",
            "Title": "ConcurrentModificationException during HiveServer2 Shutdown Process",
            "Description": "The shutdown process of HiveServer2 fails to clean up cached Tez application masters, resulting in a ConcurrentModificationException. This issue occurs when multiple threads attempt to modify the session pool concurrently during the shutdown sequence.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)",
                "at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)"
            ],
            "StepsToReproduce": [
                "1. Start the HiveServer2 service with active Tez sessions.",
                "2. Initiate a shutdown of the HiveServer2 service while multiple Tez sessions are still running.",
                "3. Observe the logs for any errors during the shutdown process."
            ],
            "ExpectedBehavior": "The HiveServer2 service should shut down cleanly, terminating all active Tez sessions without throwing any exceptions.",
            "ObservedBehavior": "During the shutdown process, a ConcurrentModificationException is thrown, indicating that the Tez session pool manager encountered concurrent modifications, leading to an incomplete shutdown of cached Tez application masters.",
            "Resolution": "A fix for this issue has been implemented and tested, ensuring that the shutdown process handles concurrent modifications correctly."
        }
    },
    {
        "filename": "HIVE-7710.json",
        "creation_time": "2014-08-13T10:46:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-7710",
            "Title": "Rename Table Operation Fails Due to Duplicate Key Constraint",
            "Description": "When attempting to rename a table from one database to another using the command `ALTER TABLE d1.t1 RENAME TO d2.t2`, the operation fails if the target table name already exists in the destination database. This results in a SQLIntegrityConstraintViolationException due to a duplicate key value in the unique constraint defined on the TBLS table.",
            "StackTrace": [
                "2014-08-13 03:32:40,512 ERROR Datastore.Persist (Log4JLogger.java:error(115)) - Update of object \"org.apache.hadoop.hive.metastore.model.MTable@729c5167\" using statement \"UPDATE TBLS SET TBL_NAME=? WHERE TBL_ID=?\" failed : java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:399)",
                "at org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:439)",
                "at org.datanucleus.store.rdbms.request.UpdateRequest.execute(UpdateRequest.java:374)",
                "at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateTable(RDBMSPersistenceHandler.java:417)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2572)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:205)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:293)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(SessionHiveMetaStoreClient.java:201)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3542)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:318)"
            ],
            "StepsToReproduce": [
                "1. Ensure that there is a table named 't2' in database 'd2'.",
                "2. Execute the command: `ALTER TABLE d1.t1 RENAME TO d2.t2`.",
                "3. Observe the error message indicating a SQLIntegrityConstraintViolationException."
            ],
            "ExpectedBehavior": "The table 't1' should be renamed to 't2' in database 'd2' if 't2' does not already exist.",
            "ObservedBehavior": "The rename operation fails with a SQLIntegrityConstraintViolationException due to a duplicate key value in the unique constraint on the TBLS table.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-8735.json",
        "creation_time": "2014-11-04T22:20:02.000+0000",
        "bug_report": {
            "BugID": "HIVE-8735",
            "Title": "SQLDataException: Truncation Error During Statistics Publishing Due to Long File Paths",
            "Description": "When attempting to publish statistics for a file with a long path, a truncation error occurs, preventing the statistics from being successfully published. This issue arises specifically when the length of the VARCHAR exceeds the database's limit of 255 characters.",
            "StackTrace": [
                "2014-11-04 01:34:38,610 ERROR jdbc.JDBCStatsPublisher (JDBCStatsPublisher.java:publishStat(198)) - Error during publishing statistics.",
                "java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:147)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:144)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.executeWithRetry(Utilities.java:2910)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.publishStat(JDBCStatsPublisher.java:160)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats(FileSinkOperator.java:1153)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:992)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:205)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with a database that has a VARCHAR column limited to 255 characters.",
                "2. Create a file with a path longer than 255 characters.",
                "3. Attempt to publish statistics for the file using the Hive statistics publishing feature.",
                "4. Observe the error in the logs indicating a truncation error."
            ],
            "ExpectedBehavior": "The statistics should be published successfully without any truncation errors, regardless of the length of the file path.",
            "ObservedBehavior": "A SQLDataException is thrown indicating a truncation error when attempting to publish statistics for a file with a path longer than 255 characters.",
            "Resolution": "[Provide additional details on the resolution, if applicable]"
        }
    },
    {
        "filename": "HIVE-13209.json",
        "creation_time": "2016-03-04T21:39:50.000+0000",
        "bug_report": {
            "BugID": "HIVE-13209",
            "Title": "Metastore get_delegation_token Fails Due to Null IP Address",
            "Description": "After changes made in HIVE-13169, the metastore's get_delegation_token method fails with a null IP address, resulting in an unauthorized connection error. This issue is critical as it prevents users from obtaining delegation tokens necessary for secure operations.",
            "StackTrace": [
                "2016-03-03 07:45:31,055 ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/<hostname@realm> from IP null)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Ensure the environment is set up with the latest changes from HIVE-13169.",
                "2. Attempt to call the get_delegation_token method on the metastore.",
                "3. Observe the logs for any errors related to unauthorized connections."
            ],
            "ExpectedBehavior": "The get_delegation_token method should successfully return a delegation token without any errors, regardless of the IP address.",
            "ObservedBehavior": "The get_delegation_token method fails with a MetaException indicating an unauthorized connection due to a null IP address.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-13065.json",
        "creation_time": "2016-02-16T21:11:31.000+0000",
        "bug_report": {
            "BugID": "HIVE-13065",
            "Title": "NullPointerException when inserting NULL values in map type column of HBase backed Hive table",
            "Description": "When attempting to write data to a HBase backed Hive table that contains a map type column with NULL values, a NullPointerException (NPE) is thrown during the MapReduce job execution. This issue occurs specifically under the following conditions:\n\n1. The table has a map type column.\n2. The map type column contains NULL values in its entries.\n\nThis bug impacts the ability to insert data into HBase backed tables, leading to job failures.",
            "StackTrace": [
                "2016-02-15 02:26:33,225 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)",
                "... 14 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)",
                "at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)",
                "... 15 more"
            ],
            "StepsToReproduce": [
                "1. Create a HBase backed Hive table with a map type column.",
                "   SQL: CREATE TABLE hbase_test (id BIGINT, data MAP<STRING, STRING>) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ('hbase.columns.mapping' = ':key,cf:map_col') TBLPROPERTIES ('hbase.table.name' = 'hive_test');",
                "2. Insert data into the created table with a NULL value in the map column.",
                "   SQL: INSERT OVERWRITE TABLE hbase_test SELECT 1 AS id, MAP('abcd', NULL) AS data FROM src LIMIT 1;"
            ],
            "ExpectedBehavior": "The data should be inserted into the HBase backed Hive table without any errors, even if the map type column contains NULL values.",
            "ObservedBehavior": "The MapReduce job fails with a NullPointerException when attempting to insert data with NULL values in the map type column.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11470.json",
        "creation_time": "2015-08-05T18:45:26.000+0000",
        "bug_report": {
            "BugID": "HIVE-11470",
            "Title": "NullPointerException in DynamicPartitionFileRecordWriterContainer when using null partition keys",
            "Description": "When partitioning data using HCatStorer, a NullPointerException (NPE) occurs if the dynamic partition key is null. This issue arises from the assumption made in the DynamicPartitionFileRecordWriterContainer when fetching a local file-writer instance without checking for null values.",
            "StackTrace": [
                "2015-07-30 23:59:59,627 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.lang.NullPointerException",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)",
                "at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)",
                "at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)",
                "at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)",
                "at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)",
                "at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with HCatalog.",
                "2. Attempt to partition data using HCatStorer with a dynamic partition key that is null.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The system should handle null partition keys gracefully, either by substituting a default value or by providing a meaningful error message.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the job to fail.",
            "Resolution": "The issue has been fixed by adding a null check in the getLocalFileWriter method of DynamicPartitionFileRecordWriterContainer. If the dynamic partition key is null, it should be substituted with '__HIVE_DEFAULT_PARTITION__'."
        }
    },
    {
        "filename": "HIVE-12476.json",
        "creation_time": "2015-11-20T03:30:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-12476",
            "Title": "NullPointerException in Metastore on Oracle with Direct SQL Mode",
            "Description": "A NullPointerException occurs in the Hive Metastore when using Direct SQL mode on Oracle databases. This issue appears to be related to the handling of Partition and StorageDescriptorSerDe parameters, similar to the previously reported issue HIVE-8485.",
            "StackTrace": [
                "2015-11-19 18:08:33,841 ERROR [pool-5-thread-2]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.",
                "java.lang.NullPointerException",
                "    at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)",
                "    at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)",
                "    at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)",
                "    at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)",
                "    at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)",
                "    at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)",
                "    at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)",
                "    at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)",
                "    at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)",
                "    at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)",
                "    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)",
                "    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)",
                "    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)",
                "    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)",
                "    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)",
                "    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Configure Hive Metastore to use Direct SQL mode with an Oracle database.",
                "2. Attempt to retrieve partitions from a table that has been defined with specific SerDe parameters.",
                "3. Observe the logs for any NullPointerException errors."
            ],
            "ExpectedBehavior": "The Hive Metastore should successfully retrieve partitions without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the processing of messages in the Hive Metastore, leading to failure in retrieving partitions.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-10559.json",
        "creation_time": "2015-04-30T21:12:40.000+0000",
        "bug_report": {
            "BugID": "HIVE-10559",
            "Title": "IndexOutOfBoundsException in RemoveDynamicPruningBySize during query optimization",
            "Description": "An IndexOutOfBoundsException occurs when executing a query that involves dynamic pruning optimizations. The error is triggered in the RemoveDynamicPruningBySize class when attempting to access an element from an empty list.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process(RemoveDynamicPruningBySize.java:61)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:77)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:281)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:123)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10092)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9932)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1026)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1000)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:139)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_q85(TestMiniTezCliDriver.java:123)"
            ],
            "StepsToReproduce": [
                "1. Prepare a Hive query that utilizes dynamic pruning optimizations.",
                "2. Execute the query using the Hive CLI or a compatible interface.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing an IndexOutOfBoundsException.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown, indicating an attempt to access an element from an empty list.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-9721.json",
        "creation_time": "2015-02-19T06:56:17.000+0000",
        "bug_report": {
            "BugID": "HIVE-9721",
            "Title": "UnsupportedOperationException in Hadoop23Shims.setFullFileStatus due to ACLs configuration",
            "Description": "An UnsupportedOperationException is thrown when attempting to access ACLs on a RawLocalFileSystem, which does not support ACLs. This occurs when the configuration for ACLs is enabled, leading to failures in file operations within Hive.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus",
                "at org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)",
                "at org.apache.hadoop.fs.FilterFileSystem.getAclStatus(FilterFileSystem.java:562)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:645)",
                "at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:524)",
                "at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)",
                "at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set the configuration property 'dfs.namenode.acls.enabled' to true.",
                "2. Attempt to create a staging directory using Hive on a RawLocalFileSystem.",
                "3. Execute a Hive query that requires file operations."
            ],
            "ExpectedBehavior": "The staging directory should be created successfully without any exceptions.",
            "ObservedBehavior": "An UnsupportedOperationException is thrown indicating that the RawLocalFileSystem does not support ACLs.",
            "Resolution": "[Provide additional details on the resolution or workaround]"
        }
    },
    {
        "filename": "HIVE-4216.json",
        "creation_time": "2013-03-21T20:53:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-4216",
            "Title": "Infinite Loop in TestHBaseMinimrCliDriver with NPE in Hadoop 23 and HBase 0.94.5",
            "Description": "After upgrading to Hadoop 23 and HBase 0.94.5, the TestHBaseMinimrCliDriver fails during execution. The test hangs indefinitely after encountering a NullPointerException (NPE) in the reducer phase, specifically when processing certain rows. This issue arises despite setting the necessary properties for the total order partitioner.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:268)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:448)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:399)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:157)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:152)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:237)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:477)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:525)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)",
                "... 7 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.TaskID$CharTaskTypeMaps.getRepresentingCharacter(TaskID.java:265)",
                "at org.apache.hadoop.mapreduce.TaskID.appendTo(TaskID.java:153)",
                "at org.apache.hadoop.mapreduce.TaskAttemptID.appendTo(TaskAttemptID.java:119)",
                "at org.apache.hadoop.mapreduce.TaskAttemptID.toString(TaskAttemptID.java:151)",
                "at java.lang.String.valueOf(String.java:282)",
                "at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209)",
                "at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:69)",
                "at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.getRecordWriter(HFileOutputFormat.java:90)",
                "at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getFileWriter(HiveHFileOutputFormat.java:67)",
                "at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getHiveRecordWriter(HiveHFileOutputFormat.java:104)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:246)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:234)",
                "... 14 more"
            ],
            "StepsToReproduce": [
                "Upgrade to Hadoop 23 and HBase 0.94.5.",
                "Update 'hbase_bulk.m' with the following properties:",
                "set mapreduce.totalorderpartitioner.naturalorder=false;",
                "set mapreduce.totalorderpartitioner.path=/tmp/hbpartition.lst;",
                "Run the TestHBaseMinimrCliDriver."
            ],
            "ExpectedBehavior": "The TestHBaseMinimrCliDriver should complete successfully without errors or infinite loops.",
            "ObservedBehavior": "The test fails during the reducer phase with a NullPointerException, causing the MiniMRCluster to spin up new reducers indefinitely, leading to an infinite loop.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-13836.json",
        "creation_time": "2016-05-24T22:37:59.000+0000",
        "bug_report": {
            "BugID": "HIVE-13836",
            "Title": "Concurrent DDL Operations Cause Transaction State Error in Hive Metastore",
            "Description": "When executing multiple DDL queries concurrently using the pyhs2 Python client, an error occurs indicating that a transaction has already started. This issue arises specifically when multiple threads attempt to create tables or partitions simultaneously, leading to a failure in the transaction management within the Hive Metastore.",
            "StackTrace": [
                "2016-05-04 17:49:26,226 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-4-thread-194]: HMSHandler Fatal error: Invalid state. Transaction has already started",
                "org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started",
                "at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)",
                "at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)",
                "at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)",
                "at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:463)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7522)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)",
                "at com.sun.proxy.$Proxy10.addNotificationEvent(Unknown Source)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.enqueue(DbNotificationListener.java:261)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onCreateTable(DbNotificationListener.java:123)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1483)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1502)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive Metastore and ensure it is accessible via the pyhs2 Python client.",
                "2. Create a multithreaded script that establishes 8 concurrent connections to the Hive Metastore.",
                "3. Execute DDL queries (e.g., creating tables or partitions) simultaneously across these connections.",
                "4. Observe the logs for any errors related to transaction states."
            ],
            "ExpectedBehavior": "The Hive Metastore should handle concurrent DDL operations without throwing transaction state errors, allowing all operations to complete successfully.",
            "ObservedBehavior": "An error is thrown indicating 'Invalid state. Transaction has already started', preventing the successful execution of concurrent DDL operations.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-9873.json",
        "creation_time": "2015-03-05T17:35:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-9873",
            "Title": "IOException in DeprecatedParquetHiveInput due to Incorrect Metadata Handling",
            "Description": "An IOException is thrown when attempting to read column information after modifying it via `projectionPusher.pushProjectionsAndFilters`. The error message indicates a size mismatch between the expected and actual object sizes, leading to null records during joins.",
            "StackTrace": [
                "2015-02-26 15:56:40,275 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(HiveContextAwareRecordReader.java:105)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(HiveContextAwareRecordReader.java:41)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:224)",
                "... 11 more",
                "Caused by: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:199)",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:52)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)",
                "... 15 more"
            ],
            "StepsToReproduce": [
                "1. Modify the column information in the Hive table.",
                "2. Call `projectionPusher.pushProjectionsAndFilters` to push the changes.",
                "3. Attempt to read the modified column information."
            ],
            "ExpectedBehavior": "The modified column information should be read without any exceptions, and the records should be joined correctly.",
            "ObservedBehavior": "An IOException is thrown indicating a size mismatch in the object being read, resulting in null records during joins.",
            "Resolution": "A fix for this issue has been implemented and tested in the codebase."
        }
    },
    {
        "filename": "HIVE-13174.json",
        "creation_time": "2016-02-26T23:34:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-13174",
            "Title": "Excessive Logging of Vectorization Failures for Binary Columns",
            "Description": "When executing queries on tables with binary columns, the Hive server logs excessive stack traces related to vectorization failures. This behavior clutters the logs and makes it difficult to identify other important log messages. The stack traces should either be logged at a debug level or replaced with a simpler log message indicating the failure without the stack trace.",
            "StackTrace": [
                "2015-10-12 12:34:23,922 INFO  [main]: physical.Vectorizer (Vectorizer.java:validateExprNodeDesc(1249)) - Failed to vectorize",
                "org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with a binary column.",
                "2. Execute a query that involves the binary column.",
                "3. Check the Hive server logs for stack traces related to vectorization."
            ],
            "ExpectedBehavior": "The Hive server should log a simple message indicating that vectorization failed for the binary column without excessive stack traces.",
            "ObservedBehavior": "The Hive server logs multiple stack traces indicating vectorization failures for binary columns, cluttering the logs.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-5431.json",
        "creation_time": "2013-10-03T03:35:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-5431",
            "Title": "IllegalArgumentException on Read Operations Due to Unset Job Properties in PassthroughOutputFormat",
            "Description": "The recent changes introduced by HIVE-4331 have led to an issue where the new key 'hive.passthrough.storagehandler.of' is not set during read operations. This results in an IllegalArgumentException being thrown when attempting to read from a Hive table without a preceding write operation. The exception occurs because the job properties are not correctly configured for read operations, leading to a failure in the FetchOperator.",
            "StackTrace": [
                "2013-09-30 16:20:01,989 ERROR CliDriver (SessionState.java:printError(419)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Property value must not be null",
                "java.io.IOException: java.lang.IllegalArgumentException: Property value must not be null",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1826)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:380)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)",
                "... 17 more"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with a storage handler that utilizes passthrough output format.",
                "2. Insert data into the table using a Hive query.",
                "3. Execute a separate Hive query that only reads data from the same table without any preceding write operation.",
                "4. Observe the IllegalArgumentException thrown during the read operation."
            ],
            "ExpectedBehavior": "The read operation should complete successfully without throwing any exceptions, returning the expected results from the Hive table.",
            "ObservedBehavior": "An IllegalArgumentException is thrown with the message 'Property value must not be null', indicating that the job properties were not set correctly for the read operation.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-13115.json",
        "creation_time": "2016-02-22T21:43:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-13115",
            "Title": "MetaStore Direct SQL getPartitions call fails when column schemas for a partition are null",
            "Description": "The MetaStore logs indicate that a Direct SQL query fails during the execution of the `getPartitions` method when the column descriptor ID (CD_ID) is null. This results in a `MetaException` being thrown, which causes the system to fall back to ORM. The issue arises specifically when adding a new partition without setting column-level schemas using the MetaStoreClient API, while the CLI does not trigger this exception.",
            "StackTrace": [
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)",
                "at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Use the MetaStoreClient API to add a new partition without setting column-level schemas.",
                "2. Call the `getPartitions` method on the affected table.",
                "3. Observe the logs for the `MetaException` indicating a null column descriptor ID."
            ],
            "ExpectedBehavior": "The `getPartitions` method should return the list of partitions without throwing an exception, regardless of whether column-level schemas are set.",
            "ObservedBehavior": "The `getPartitions` method fails with a `MetaException` due to a null column descriptor ID, causing a fallback to ORM.",
            "Resolution": "Consider making the Direct SQL code path and the ORM code path more consistent, allowing for null column descriptor IDs without failure. Alternatively, enforce stricter checks in the MetaStoreClient API to prevent the creation of partitions without column-level schemas."
        }
    },
    {
        "filename": "HIVE-4723.json",
        "creation_time": "2013-06-12T20:37:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-4723",
            "Title": "NullPointerException in DDLSemanticAnalyzer when archiving non-partitioned table",
            "Description": "A NullPointerException (NPE) occurs in the DDLSemanticAnalyzer when attempting to archive a partition on a non-partitioned table. The error message generated is misleading, as it does not provide sufficient information about the underlying issue. The stack trace indicates that the exception is thrown during the execution of the `addTablePartsOutputs` method.",
            "StackTrace": [
                "2013-06-09 16:36:12,628 ERROR parse.DDLSemanticAnalyzer (DDLSemanticAnalyzer.java:addTablePartsOutputs(2899)) - Got HiveException during obtaining list of partitions",
                "2013-06-09 16:36:12,628 ERROR ql.Driver (SessionState.java:printError(383)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2912)",
                "\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2877)",
                "\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(DDLSemanticAnalyzer.java:2730)",
                "\tat org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:316)",
                "\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:277)",
                "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)",
                "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)",
                "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "\tat java.lang.reflect.Method.invoke(Method.java:597)",
                "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "1. Create a non-partitioned table in Hive.",
                "2. Attempt to archive the table using the command: `ALTER TABLE <table_name> ARCHIVE`.",
                "3. Observe the error message and stack trace generated."
            ],
            "ExpectedBehavior": "The system should provide a clear error message indicating that archiving is not applicable to non-partitioned tables, without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, leading to a misleading error message that does not clarify the issue.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-15686.json",
        "creation_time": "2017-01-20T22:29:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-15686",
            "Title": "Error in Encryption Zone Checks for Partitions with Remote HDFS Paths",
            "Description": "This bug relates to HIVE-13243, which aimed to fix encryption-zone checks for external tables. However, the implementation fails for partitions that have remote HDFS paths, leading to an IllegalArgumentException when attempting to access the encryption zone.",
            "StackTrace": [
                "2015-12-09 19:26:14,997 ERROR [pool-4-thread-1476] server.TThreadPoolServer (TThreadPoolServer.java:run_aroundBody0(305)) - Error occurred during processing of message.",
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://remote-cluster-nn1.myth.net:8020/dbs/mythdb/myth_table/dt=20170120, expected: hdfs://local-cluster-n1.myth.net:8020",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1985)",
                "at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1290)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.checkTrashPurgeCombination(HiveMetaStore.java:1746)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2974)",
                "at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy5.drop_partitions_req(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:10005)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:9989)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:767)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:763)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:763)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:285)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with partitions that have remote HDFS paths.",
                "2. Attempt to drop a partition using the Hive metastore.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The system should correctly handle encryption zone checks for partitions with remote HDFS paths without throwing an IllegalArgumentException.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a mismatch in the expected and actual file system paths.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-4975.json",
        "creation_time": "2013-08-01T16:21:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-4975",
            "Title": "ArrayIndexOutOfBoundsException when reading ORC file after adding new column",
            "Description": "An ArrayIndexOutOfBoundsException occurs when attempting to read an ORC file after adding a new column to the table. This issue arises specifically when executing a SELECT query on the newly modified table structure.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "Create a table with three columns: a (string), b (string), c (string).",
                "Add a new column d (string) to the table using the command: ALTER TABLE table ADD COLUMNS (d string).",
                "Execute the HiveQL command: SELECT d FROM table."
            ],
            "ExpectedBehavior": "The query should return the new column 'd' without any exceptions.",
            "ObservedBehavior": "The query throws an ArrayIndexOutOfBoundsException, indicating an issue with accessing the data structure of the ORC file.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-10538.json",
        "creation_time": "2015-04-29T20:06:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-10538",
            "Title": "Null Pointer Exception in FileSinkOperator with Bucketed Tables and MultiFileSpray Enabled",
            "Description": "A Null Pointer Exception occurs in the FileSinkOperator when using bucketed tables with the 'distribute by' clause and multiFileSpray enabled. This issue arises during the execution of a join operation between two bucketed tables, leading to a failure in processing the output.",
            "StackTrace": [
                "2015-04-29 12:54:12,841 FATAL [pool-110-thread-1]: ExecReducer (ExecReducer.java:reduce(255)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"113\",\"_col1\":\"val_113\"}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)",
                "... 8 more"
            ],
            "StepsToReproduce": [
                "Set the following Hive configurations:",
                "1. set hive.enforce.bucketing = true;",
                "2. set hive.exec.reducers.max = 20;",
                "Create the following bucketed tables:",
                "3. create table bucket_a(key int, value_a string) clustered by (key) into 256 buckets;",
                "4. create table bucket_b(key int, value_b string) clustered by (key) into 256 buckets;",
                "5. create table bucket_ab(key int, value_a string, value_b string) clustered by (key) into 256 buckets;",
                "Insert data into bucket_a and bucket_b.",
                "6. Execute the following query:",
                "insert overwrite table bucket_ab select a.key, a.value_a, b.value_b from bucket_a a join bucket_b b on (a.key = b.key) distribute by key;"
            ],
            "ExpectedBehavior": "The query should successfully execute and insert the joined results into the bucket_ab table without any exceptions.",
            "ObservedBehavior": "A Null Pointer Exception is thrown during the execution of the query, causing the job to fail.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11902.json",
        "creation_time": "2015-09-21T16:12:37.000+0000",
        "bug_report": {
            "BugID": "HIVE-11902",
            "Title": "SQL Syntax Error in Transaction Cleanup Due to Empty Transaction List",
            "Description": "When the DeadTxnReaper attempts to clean up timed-out transactions, it encounters a SQL syntax error due to an empty transaction ID list being passed to the abortTxns method. This results in an invalid SQL query being generated, causing the cleanup process to fail.",
            "StackTrace": [
                "2015-09-21 05:23:38,148 WARN  [DeadTxnReaper-0]: txn.TxnHandler (TxnHandler.java:performTimeOuts(1876)) - Aborting timedout transactions failed due to You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1(SQLState=42000,ErrorCode=1064)",
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:360)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)",
                "at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)",
                "at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Hive service with ACID transactions enabled.",
                "2. Create a transaction and let it timeout.",
                "3. Observe the DeadTxnReaper thread attempting to clean up timed-out transactions."
            ],
            "ExpectedBehavior": "The DeadTxnReaper should successfully abort timed-out transactions without encountering SQL syntax errors.",
            "ObservedBehavior": "The DeadTxnReaper throws a MySQLSyntaxErrorException due to an invalid SQL query generated when the transaction ID list is empty.",
            "Resolution": "Ensure that the abortTxns method checks if the txnids list is empty before constructing the SQL query. If it is empty, skip the execution of the query to prevent syntax errors."
        }
    },
    {
        "filename": "HIVE-18918.json",
        "creation_time": "2018-03-09T00:47:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-18918",
            "Title": "Inadequate Error Messaging in CompactorMR.launchCompactionJob() During Major Compaction",
            "Description": "The error message generated during a major compaction in the CompactorMR class is insufficient, leading to confusion when a compaction job fails. The current implementation throws a generic IOException without providing specific details about the failure, making it difficult for users to diagnose issues effectively.",
            "StackTrace": [
                "2018-02-28 00:59:16,416 ERROR [gdpr1-61]: compactor.Worker (Worker.java:run(191)) - Caught exception while trying to compact id:38602, dbname:audit, tableName:COMP_ENTRY_AF_A, partName:partition_dt=2017-04-11, state:^@, type:MAJOR, properties:null, runAs:null, tooManyAborts:false, highestTxnId:0. Marking failed to avoid repeated failures, java.io.IOException: Major",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob(CompactorMR.java:314)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:269)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:172)"
            ],
            "StepsToReproduce": [
                "1. Initiate a major compaction on a Hive table with the command: [Provide command].",
                "2. Monitor the logs for the compaction process.",
                "3. Observe the error message generated when the compaction fails."
            ],
            "ExpectedBehavior": "When a major compaction fails, the system should provide a detailed error message indicating the reason for the failure, including relevant job details and context.",
            "ObservedBehavior": "The system throws a generic IOException with the message 'Major compactor job failed for [jobName]! Hadoop JobId: [jobId]', which lacks specific information about the failure.",
            "Resolution": "A fix has been implemented to enhance the error messaging in the CompactorMR.launchCompactionJob() method to include more detailed information about the failure context."
        }
    },
    {
        "filename": "HIVE-8107.json",
        "creation_time": "2014-09-15T19:49:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-8107",
            "Title": "Improper Error Message for Non-Existent Table in Update/Delete Operations",
            "Description": "When executing an update or delete operation on a non-existent table, the system produces a confusing error message that buries the relevant 'Table not found' information within a stack trace. This makes it difficult for users to quickly identify the root cause of the error.",
            "StackTrace": [
                "2014-09-12 19:45:00,138 ERROR [main]: ql.Driver (SessionState.java:printError(824)) - FAILED: SemanticException [Error 10290]: Encountered parse error while parsing rewritten update or delete query",
                "org.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:97)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:66)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:217)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:406)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:302)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1051)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1121)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:988)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:978)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:441)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:457)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found no_such_table",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:978)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:128)"
            ],
            "StepsToReproduce": [
                "1. Execute the command: `UPDATE no_such_table SET x = 3;`",
                "2. Observe the error message returned by the system."
            ],
            "ExpectedBehavior": "The system should return a clear and concise error message indicating that the specified table does not exist, without burying it in a stack trace.",
            "ObservedBehavior": "The system returns a complex error message that includes a stack trace, making it difficult to identify the root cause of the error. The relevant 'Table not found' message is not prominently displayed.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-1326.json",
        "creation_time": "2010-04-25T20:50:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-1326",
            "Title": "RowContainer Class Uses Hard-Coded '/tmp/' Path for Temporary Files, Causing Disk Space Issues",
            "Description": "In a production Hadoop environment, the RowContainer class is creating temporary files in the hard-coded '/tmp/' directory instead of utilizing the configured Hadoop temporary path. This behavior leads to disk space exhaustion, resulting in failures during query execution. A patch has been provided to address this issue.",
            "StackTrace": [
                "2010-04-25 12:05:28,155 FATAL ExecReducer: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)",
                "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)",
                "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)",
                "\tat java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:346)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)",
                "\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1013)",
                "\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)",
                "\tat org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat$1.write(HiveSequenceFileOutputFormat.java:70)",
                "\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)",
                "\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)",
                "\tat org.apache.hadoop.hive.ql.exec.JoinOperator.processOp(JoinOperator.java:118)",
                "\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:456)",
                "\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:244)",
                "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)",
                "\tat org.apache.hadoop.mapred.Child.main(Child.main:158)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with limited space in the '/tmp/' directory.",
                "2. Execute a query that utilizes the RowContainer class.",
                "3. Monitor the disk space usage in the '/tmp/' directory during the execution of the query."
            ],
            "ExpectedBehavior": "The RowContainer class should create temporary files in the configured Hadoop temporary path, preventing disk space exhaustion.",
            "ObservedBehavior": "The RowContainer class creates temporary files in the hard-coded '/tmp/' directory, leading to 'No space left on device' errors during query execution.",
            "Resolution": "A patch has been created to modify the RowContainer class to use the configured Hadoop temporary path instead of the hard-coded '/tmp/' path."
        }
    },
    {
        "filename": "HIVE-11369.json",
        "creation_time": "2015-07-24T16:28:47.000+0000",
        "bug_report": {
            "BugID": "HIVE-11369",
            "Title": "Map Joins Fail in HiveServer2 with JMX Remote Configuration",
            "Description": "When the JMX remote options are enabled in HiveServer2, map joins fail to execute properly, resulting in an execution error. This issue does not occur when the JMX options are removed, indicating a potential conflict between JMX settings and HiveServer2's execution environment.",
            "StackTrace": [
                "org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)",
                "org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:146)",
                "org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:173)",
                "org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:379)",
                "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "java.lang.reflect.Method.invoke(Method.java:606)",
                "org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)",
                "org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)",
                "org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)",
                "java.security.AccessController.doPrivileged(Native Method)",
                "javax.security.auth.Subject.doAs(Subject.java:415)",
                "org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)",
                "org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:258)",
                "org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)",
                "org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set the property 'hive.auto.convert.join' to true in Hive configuration.",
                "2. Start HiveServer2 with the following JMX options in hive-env.sh:",
                "   -Dcom.sun.management.jmxremote",
                "   -Dcom.sun.management.jmxremote.authenticate=false",
                "   -Dcom.sun.management.jmxremote.ssl=false",
                "   -Dcom.sun.management.jmxremote.port=8009",
                "3. Execute a map join query in HiveServer2.",
                "4. Observe the execution error in the logs."
            ],
            "ExpectedBehavior": "The map join query should execute successfully without any errors when JMX remote options are enabled.",
            "ObservedBehavior": "The map join query fails with an execution error, indicating a return code of 1 from the MapredLocalTask.",
            "Resolution": "A fix for this issue has been implemented and is included in Hive version 2.0.0."
        }
    },
    {
        "filename": "HIVE-9055.json",
        "creation_time": "2014-12-09T19:51:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-9055",
            "Title": "IndexOutOfBoundsException when executing UNION ALL with GROUP BY in Hive using Tez",
            "Description": "Executing a Hive query that combines UNION ALL with GROUP BY results in an IndexOutOfBoundsException when using the Tez execution engine. The error does not occur when using the MapReduce execution engine.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: -1, Size: 1",
                "at java.util.LinkedList.checkElementIndex(LinkedList.java:553)",
                "at java.util.LinkedList.get(LinkedList.java:474)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:354)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:420)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1108)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1035)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:151)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:362)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:297)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "StepsToReproduce": [
                "Set the Hive execution engine to Tez: `set hive.execution.engine=tez;`",
                "Execute the following query: `SELECT key FROM (SELECT key FROM src UNION ALL SELECT key FROM src) tab GROUP BY key UNION ALL SELECT key FROM src;`"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors.",
            "ObservedBehavior": "The query fails with an IndexOutOfBoundsException, indicating an issue with the processing of the query in the Tez execution engine.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-13856.json",
        "creation_time": "2016-05-25T21:50:49.000+0000",
        "bug_report": {
            "BugID": "HIVE-13856",
            "Title": "SQL Syntax Error When Fetching Transaction Batches in Hive Metastore with Oracle DB",
            "Description": "When attempting to fetch transaction batches during ACID streaming against the Hive Metastore using an Oracle database, a SQL syntax error occurs. The error message indicates that the SQL command is not properly ended, which is likely due to the way multiple rows are being inserted into the TXNS table.",
            "StackTrace": [
                "2016-05-25 00:43:49,682 INFO  [pool-4-thread-5]: txn.TxnHandler (TxnHandler.java:checkRetryable(1585)) - Non-retryable error: ORA-00933: SQL command not properly ended (SQLState=42000, ErrorCode=933)",
                "2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)",
                "at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)",
                "at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)",
                "at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)",
                "at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)",
                "at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)",
                "at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)",
                "at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)",
                "at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)",
                "at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)",
                "at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)",
                "at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy15.open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up Hive Metastore with Oracle DB.",
                "2. Attempt to fetch transaction batches using the ACID streaming feature.",
                "3. Observe the error in the logs indicating a SQL syntax error."
            ],
            "ExpectedBehavior": "The transaction batches should be fetched successfully without any SQL syntax errors.",
            "ObservedBehavior": "An SQL syntax error occurs, specifically 'ORA-00933: SQL command not properly ended', preventing the fetching of transaction batches.",
            "Resolution": "The issue arises from the way multiple rows are inserted into the TXNS table. Oracle does not support the syntax used for inserting multiple rows in a single statement. The code should be modified to either insert each row individually or use the 'INSERT ALL' syntax."
        }
    },
    {
        "filename": "HIVE-7374.json",
        "creation_time": "2014-07-09T16:02:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-7374",
            "Title": "Thrift Protocol Exception on SHOW COMPACTIONS with Remote Metastore and No Compactions",
            "Description": "When executing the 'SHOW COMPACTIONS' command in the Hive CLI with a remote metastore and no existing compactions, an error occurs due to a missing required field in the Thrift response. This issue prevents users from retrieving compaction information when none exists, leading to confusion and potential mismanagement of compaction tasks.",
            "StackTrace": [
                "org.apache.thrift.protocol.TProtocolException: Required field 'compacts' is unset! Struct:ShowCompactResponse(compacts:null)",
                "at org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate(ShowCompactResponse.java:310)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer$WorkerProcess.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Ensure a remote metastore is configured and accessible.",
                "2. Ensure there are no existing compactions in the metastore.",
                "3. Open the Hive CLI.",
                "4. Execute the command: SHOW COMPACTIONS;"
            ],
            "ExpectedBehavior": "The command 'SHOW COMPACTIONS;' should return an empty result or a message indicating that there are no compactions available.",
            "ObservedBehavior": "The command fails with an error: 'FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.thrift.transport.TTransportException'. Additionally, the metastore logs show a Thrift protocol exception indicating that the required field 'compacts' is unset.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-12206.json",
        "creation_time": "2015-10-17T00:30:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-12206",
            "Title": "ClassNotFoundException for UDF during query compilation with Tez and Union query",
            "Description": "A ClassNotFoundException occurs when attempting to execute a Hive query that utilizes a User Defined Function (UDF) within a UNION operation. The error indicates that the class 'com.aginity.amp.hive.udf.UniqueNumberGenerator' cannot be found during the deserialization process.",
            "StackTrace": [
                "2015-10-16 17:00:55,557 ERROR ql.Driver (SessionState.java:printError(963)) - FAILED: KryoException Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "Serialization trace:",
                "genericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)",
                "colExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "parentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)",
                "org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "Caused by: java.lang.ClassNotFoundException: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:358)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:270)",
                "at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136)"
            ],
            "StepsToReproduce": [
                "1. Add the UDF JAR file to the Hive session using the command: add jar /tmp/udf-2.2.0-snapshot.jar;",
                "2. Create a temporary function using the command: create temporary function myudf as 'com.aginity.amp.hive.udf.UniqueNumberGenerator';",
                "3. Execute the following query: explain select myudf() from (select key from src limit 1) a union all select myudf() from (select key from src limit 1) a;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors related to class loading.",
            "ObservedBehavior": "The query fails with a ClassNotFoundException indicating that the UDF class 'com.aginity.amp.hive.udf.UniqueNumberGenerator' cannot be found.",
            "Resolution": "[Provide additional details about the resolution, if applicable]"
        }
    },
    {
        "filename": "HIVE-10098.json",
        "creation_time": "2015-03-26T17:12:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-10098",
            "Title": "MapJoin Fails with UndeclaredThrowableException in KMS Encrypted Hive Cluster",
            "Description": "When executing a Hive query that performs a MapJoin in a Kerberos-secured cluster with KMS enabled, the operation fails with a java.lang.reflect.UndeclaredThrowableException. This issue arises specifically when the KMSClientProvider attempts to add delegation tokens, leading to an AuthenticationException due to missing Kerberos credentials.",
            "StackTrace": [
                "java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:634)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:363)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:337)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:303)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:735)",
                "Caused by: java.io.IOException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:826)",
                "Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:306)"
            ],
            "StepsToReproduce": [
                "1. Ensure KMS is enabled and the cluster is secured with Kerberos.",
                "2. Create two tables in Hive: a small table (jsmall) and a large table (jbig1).",
                "3. Load data into both tables from local paths.",
                "4. Execute the following Hive query: `SELECT COUNT(*) FROM jsmall small JOIN jbig1 big ON (small.code = big.code);`",
                "5. Observe the error during execution."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully and return the count of joined records without any exceptions.",
            "ObservedBehavior": "The Hive query fails with a java.lang.reflect.UndeclaredThrowableException, indicating an issue with KMS delegation token retrieval due to missing Kerberos credentials.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-7745.json",
        "creation_time": "2014-08-16T01:20:06.000+0000",
        "bug_report": {
            "BugID": "HIVE-7745",
            "Title": "NullPointerException when hive.optimize.union.remove, hive.merge.mapfiles, and hive.merge.mapredfiles are enabled",
            "Description": "When the configuration options `hive.optimize.union.remove`, `hive.merge.mapfiles`, and `hive.merge.mapredfiles` are enabled, executing specific queries results in a NullPointerException. This issue occurs during the processing of the query plan in the Hive on Spark environment.",
            "StackTrace": [
                "2014-08-16 01:32:26,849 ERROR [main]: ql.Driver (SessionState.java:printError(681)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(GenMapRedUtils.java:1738)",
                "at org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.processFileSink(GenSparkUtils.java:281)",
                "at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:187)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:199)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9508)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:414)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1005)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1070)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "StepsToReproduce": [
                "1. Set the following Hive configuration options: `hive.optimize.union.remove=true`, `hive.merge.mapfiles=true`, `hive.merge.mapredfiles=true`.",
                "2. Execute the following Hive queries:",
                "   ```sql",
                "   create table inputTbl1(key string, val string) stored as textfile;",
                "   create table outputTbl1(key string, values bigint) stored as rcfile;",
                "   load data local inpath '../../data/files/T1.txt' into table inputTbl1;",
                "   explain insert overwrite table outputTbl1 SELECT * FROM (",
                "       select key, count(1) as values from inputTbl1 group by key ",
                "       union all",
                "       select * FROM (",
                "           SELECT key, 1 as values from inputTbl1 ",
                "           UNION ALL",
                "           SELECT key, 2 as values from inputTbl1",
                "       ) a",
                "   ) b;",
                "   ```",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing a NullPointerException, and the output table should be populated with the expected results.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the query when the specified configuration options are enabled.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11762.json",
        "creation_time": "2015-09-08T20:10:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-11762",
            "Title": "NoSuchMethodError in TestHCatLoaderEncryption with Hadoop 2.7",
            "Description": "When running the TestHCatLoaderEncryption test with the Hadoop version set to 2.7.0, a NoSuchMethodError is encountered during the setup phase. This error indicates that the method signature for setting the key provider in the DFSClient class has changed between Hadoop versions 2.6 and 2.7.",
            "StackTrace": [
                "java.lang.NoSuchMethodError: org.apache.hadoop.hdfs.DFSClient.setKeyProvider(Lorg/apache/hadoop/crypto/key/KeyProviderCryptoExtension;)V",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Hadoop23Shims.java:534)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.initEncryptionShim(TestHCatLoaderEncryption.java:252)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.setup(TestHCatLoaderEncryption.java:200)"
            ],
            "StepsToReproduce": [
                "1. Set up the environment with Hadoop version 2.7.0.",
                "2. Run the TestHCatLoaderEncryption test using the command: `mvn test -Dtest=TestHCatLoaderEncryption -Dhadoop23.version=2.7.0`.",
                "3. Observe the output during the setup phase."
            ],
            "ExpectedBehavior": "The TestHCatLoaderEncryption test should complete successfully without any errors during the setup phase.",
            "ObservedBehavior": "The test fails with a NoSuchMethodError indicating that the method setKeyProvider with the specified argument type does not exist in the DFSClient class.",
            "Resolution": "A fix for this issue has been checked into the tree and tested. The method signature in the Hadoop codebase has been updated to match the new expected argument type."
        }
    },
    {
        "filename": "HIVE-6990.json",
        "creation_time": "2014-04-30T04:24:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-6990",
            "Title": "Direct SQL Execution Fails with Custom Schema Configuration in Hive",
            "Description": "When executing Hive queries with a custom schema set in `hive-site.xml`, a JDODataStoreException occurs during direct SQL execution, causing the system to fall back to ORM. This issue arises specifically when the schema is set to a value different from the default.",
            "StackTrace": [
                "2014-04-23 17:30:23,331 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(1756)) - Direct SQL failed, falling back to ORM",
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select PARTITIONS.PART_ID from PARTITIONS inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:98)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1806)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)",
                "at java.lang.reflect.Method.invoke(Method.java:619)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)",
                "at com.sun.proxy.$Proxy11.getPartitionsByFilter(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:3310)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)"
            ],
            "StepsToReproduce": [
                "1. Set the following properties in `hive-site.xml`:",
                "   <property>",
                "     <name>javax.jdo.mapping.Schema</name>",
                "     <value>HIVE</value>",
                "   </property>",
                "   <property>",
                "     <name>javax.jdo.option.ConnectionUserName</name>",
                "     <value>user1</value>",
                "   </property>",
                "2. Execute the following Hive queries:",
                "   hive> create table mytbl ( key int, value string);",
                "   hive> load data local inpath 'examples/files/kv1.txt' overwrite into table mytbl;",
                "   hive> select * from mytbl;",
                "   hive> create view myview partitioned on (value) as select key, value from mytbl where key=98;",
                "   hive> alter view myview add partition (value='val_98') partition (value='val_xyz');",
                "   hive> alter view myview drop partition (value='val_xyz');"
            ],
            "ExpectedBehavior": "The Hive queries should execute successfully without any errors, and the partitions should be managed correctly without falling back to ORM.",
            "ObservedBehavior": "An error occurs during the execution of the SQL query, resulting in a fallback to ORM and the following exception being logged: `javax.jdo.JDODataStoreException`.",
            "Resolution": "A fix for this issue has been implemented and tested, resolving the direct SQL execution failure when using a custom schema."
        }
    },
    {
        "filename": "HIVE-7114.json",
        "creation_time": "2014-05-22T14:58:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-7114",
            "Title": "Unexpected Extra Tez Session Launched During HiveServer2 Startup",
            "Description": "When starting HiveServer2, an additional Tez Application Master (AM) is unexpectedly launched, which may lead to resource inefficiencies and potential conflicts in session management.",
            "StackTrace": [
                "java.lang.Exception: Opening session",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)",
                "at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)",
                "at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)",
                "at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)"
            ],
            "StepsToReproduce": [
                "1. Start the HiveServer2 service.",
                "2. Monitor the logs for Tez session initialization.",
                "3. Observe the number of Tez sessions launched."
            ],
            "ExpectedBehavior": "Only one Tez session should be launched during the startup of HiveServer2.",
            "ObservedBehavior": "An extra Tez session is launched, leading to potential resource conflicts and inefficiencies.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-11991.json",
        "creation_time": "2015-09-29T21:46:06.000+0000",
        "bug_report": {
            "BugID": "HIVE-11991",
            "Title": "ClassCastException when executing groupby11.q on branch-1.0",
            "Description": "Executing the query 'groupby11.q' on the branch-1.0 branch results in a ClassCastException. The error indicates that a String cannot be cast to org.apache.hadoop.io.Text, which suggests a type mismatch in the data processing pipeline.",
            "StackTrace": [
                "2015-09-29 14:27:51,676 ERROR CliDriver (SessionState.java:printError(833)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1621)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:221)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:153)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:364)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:299)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:832)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11(TestCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:85)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:572)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:564)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "... 27 more",
                "Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:225)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:492)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:445)",
                "at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:429)",
                "at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:50)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:71)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:40)",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)",
                "... 34 more"
            ],
            "StepsToReproduce": [
                "1. Checkout the branch-1.0 branch of the Hive repository.",
                "2. Execute the query 'groupby11.q'.",
                "3. Observe the error message in the console output."
            ],
            "ExpectedBehavior": "The query 'groupby11.q' should execute successfully and return the expected results without any exceptions.",
            "ObservedBehavior": "The execution of 'groupby11.q' fails with a ClassCastException indicating that a String cannot be cast to org.apache.hadoop.io.Text.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-17900.json",
        "creation_time": "2017-10-25T17:12:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-17900",
            "Title": "Malformed SQL Generated During Compaction for Tables with Multiple Partition Columns",
            "Description": "When attempting to analyze statistics on columns triggered by the Compactor, a malformed SQL statement is generated, leading to a ParseException. This issue occurs specifically for tables that have more than one partition column, causing the compaction process to fail.",
            "StackTrace": [
                "2017-10-16 09:01:51,255 ERROR [haddl0007.mycenterpointenergy.com-51]: ql.Driver (SessionState.java:printError(993)) - FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement",
                "org.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1158)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:294)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with multiple partition columns.",
                "2. Insert data into the table.",
                "3. Trigger the compaction process for the table.",
                "4. Observe the logs for any errors related to SQL parsing."
            ],
            "ExpectedBehavior": "The compaction process should successfully analyze the statistics for the table without generating any SQL errors.",
            "ObservedBehavior": "The compaction process fails with a ParseException indicating a malformed SQL statement due to mismatched input near the partition column 'dates'.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-10816.json",
        "creation_time": "2015-05-25T08:08:37.000+0000",
        "bug_report": {
            "BugID": "HIVE-10816",
            "Title": "NullPointerException in ExecDriver::handleSampling when hive.exec.submitviachild is true",
            "Description": "When the configuration property `hive.exec.submitviachild` is set to true, executing a parallel order by operation results in a NullPointerException (NPE) in the `handleSampling` method of the `ExecDriver` class. This causes the job to fall back to single-reducer mode, which may lead to performance degradation.",
            "StackTrace": [
                "2015-05-25 08:41:04,446 ERROR [main]: mr.ExecDriver (ExecDriver.java:execute(386)) - Sampling error",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:497)",
                "    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Set the configuration property `hive.exec.submitviachild` to true.",
                "Execute a Hive query that involves a parallel order by operation.",
                "Observe the logs for any errors related to sampling."
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing a NullPointerException, and the job should utilize multiple reducers as intended.",
            "ObservedBehavior": "The query fails with a NullPointerException in the `handleSampling` method, causing the job to revert to single-reducer mode.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-13017.json",
        "creation_time": "2016-02-05T23:40:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-13017",
            "Title": "Execution Error in Hive when using Temporary Tables with Azure Filesystem",
            "Description": "When executing a Hive query that involves temporary tables while using Azure Filesystem as the default file system, an execution error occurs. The error is specifically related to the child process of HiveServer2 failing to obtain a delegation token from the non-default filesystem, leading to a failure in executing the query.",
            "StackTrace": [
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Set Azure Filesystem as the default file system.",
                "Create a temporary table using a Hive query.",
                "Execute a join query involving the temporary table and other tables.",
                "Observe the execution error."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors.",
            "ObservedBehavior": "The query fails with an execution error, specifically a return code 2 from the MapredLocalTask, indicating that the child process could not obtain the necessary delegation token.",
            "Resolution": "[Provide additional details about the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-11303.json",
        "creation_time": "2015-07-18T01:31:02.000+0000",
        "bug_report": {
            "BugID": "HIVE-11303",
            "Title": "LimitExceededException Thrown When Executing Large DAG in Tez Framework",
            "Description": "When executing a Directed Acyclic Graph (DAG) with a large number of counters in the Tez framework, a LimitExceededException is thrown due to exceeding the maximum allowed counters. This issue occurs during the execution of a Hive query that generates more counters than the configured limit.",
            "StackTrace": [
                "org.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200",
                "at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)",
                "at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)",
                "at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)",
                "at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that generates a large number of counters.",
                "2. Monitor the execution of the query in the Tez framework.",
                "3. Observe the logs for the LimitExceededException."
            ],
            "ExpectedBehavior": "The query should execute successfully without exceeding the maximum counter limit.",
            "ObservedBehavior": "The query fails with a LimitExceededException indicating that the number of counters has exceeded the maximum allowed limit of 1200.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-5899.json",
        "creation_time": "2013-11-27T02:48:39.000+0000",
        "bug_report": {
            "BugID": "HIVE-5899",
            "Title": "NullPointerException during Explain Extended with Char/Varchar Columns",
            "Description": "When executing the 'EXPLAIN EXTENDED' command on a Hive table that contains char/varchar columns, a NullPointerException is thrown. This occurs during the process of annotating the operator tree with statistics, specifically when Hive attempts to retrieve column statistics for the specified columns.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:367)",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats(ColumnStatisticsData.java:444)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:414)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStatsForColumn(StatsUtils.java:369)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:465)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:109)",
                "at org.apache.hadoop.hive.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:102)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)",
                "at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)",
                "at org.apache.hadoop.hive.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:76)",
                "at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:136)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8913)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:341)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:994)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:905)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with char/varchar columns.",
                "2. Run the command 'ANALYZE TABLE <table_name> COMPUTE STATISTICS FOR COLUMNS'.",
                "3. Execute the command 'EXPLAIN EXTENDED <query>' on the table."
            ],
            "ExpectedBehavior": "The command should return the execution plan without any errors.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the command to fail.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11102.json",
        "creation_time": "2015-06-24T22:54:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-11102",
            "Title": "IndexOutOfBoundsException in OrcReader when processing ACID data files",
            "Description": "The ORC reader implementation fails to correctly estimate the size of ACID data files, leading to an IndexOutOfBoundsException. This occurs when the method getColumnIndicesFromNames is called with an empty list of column names, resulting in an attempt to access an index in an empty list.",
            "StackTrace": [
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:938)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:847)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:713)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Create an ORC file with ACID data.",
                "2. Attempt to read the file using the ORC reader without specifying any column names.",
                "3. Observe the exception thrown during the reading process."
            ],
            "ExpectedBehavior": "The ORC reader should correctly handle the case where no column names are provided and should not throw an IndexOutOfBoundsException.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when the method getColumnIndicesFromNames is called with an empty list of column names.",
            "Resolution": "A fix has been implemented and tested, ensuring that the ORC reader can handle empty column name lists without throwing exceptions."
        }
    },
    {
        "filename": "HIVE-9195.json",
        "creation_time": "2014-12-23T01:08:45.000+0000",
        "bug_report": {
            "BugID": "HIVE-9195",
            "Title": "CBO Incorrectly Converts Constant Expression to Column Expression in Hive",
            "Description": "During the execution of a query using the Cost-Based Optimizer (CBO) in Hive, a constant expression is incorrectly transformed into a column expression, leading to an argument type exception. This issue arises specifically when using the `percentile_approx` function in a test case, where the second argument is expected to be a constant but is instead treated as a variable type.",
            "StackTrace": [
                "2014-12-22 17:03:31,433 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:analyzeInternal(10102)) - CBO failed, skipping CBO.",
                "org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:146)",
                "at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:1160)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:3794)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:4467)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5536)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8884)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9745)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9638)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10086)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:877)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23(TestCliDriver.java:120)"
            ],
            "StepsToReproduce": [
                "1. Create a table with a double key and a string value.",
                "2. Load data into the table from local files.",
                "3. Execute the following query: `select percentile_approx(case when key < 100 then cast('NaN' as double) else key end, 0.5) from bucket;`",
                "4. Observe the error message indicating that the second argument must be a constant."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected percentile value without any errors.",
            "ObservedBehavior": "The query fails with an `UDFArgumentTypeException`, indicating that the second argument must be a constant, but a double was passed instead.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-11285.json",
        "creation_time": "2015-07-16T20:46:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-11285",
            "Title": "ClassCastException in SMBJoin due to ObjectInspector for partition columns",
            "Description": "A ClassCastException occurs during the execution of a Hive query involving a Sort-Merge Bucket Join (SMBJoin) when processing partition columns. The error arises from an attempt to cast an IntWritable object to a Java Integer, which leads to a runtime failure.",
            "StackTrace": [
                "2015-07-15 13:39:04,333 WARN main org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)",
                "... 8 more",
                "Caused by: java.lang.RuntimeException: Map local work failed",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:569)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchNextGroup(SMBMapJoinOperator.java:429)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:260)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 9 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Integer",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(JavaIntObjectInspector.java:35)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:305)",
                "at org.apache.hadoop.hive.exec.JoinUtil.computeValues(JoinUtil.java:193)",
                "at org.apache.hadoop.hive.exec.CommonJoinOperator.getFilteredValue(CommonJoinOperator.java:408)",
                "at org.apache.hadoop.hive.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:270)",
                "... 17 more"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with partitioning.",
                "2. Load data into the table using the provided SQL commands.",
                "3. Execute the following query: SELECT s1.key, s2.p1 FROM smb_table s1 INNER JOIN smb_table_part s2 ON s1.key = s2.key ORDER BY s1.key;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any runtime exceptions.",
            "ObservedBehavior": "A ClassCastException occurs, preventing the query from executing and resulting in a runtime error.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-10288.json",
        "creation_time": "2015-04-09T23:24:16.000+0000",
        "bug_report": {
            "BugID": "HIVE-10288",
            "Title": "NullPointerException when calling permanent UDFs after exiting Hive CLI",
            "Description": "After creating a permanent User Defined Function (UDF) in Hive and exiting the CLI, attempting to call the UDF results in a NullPointerException. This issue does not occur if the UDF is called immediately after registration, nor does it occur in the Apache Hive 1.0.0 release.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:232)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1048)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1265)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:205)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:149)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10383)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10338)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3815)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9663)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)"
            ],
            "StepsToReproduce": [
                "1. Build the Hive binary from the trunk.",
                "2. Start the Hive CLI.",
                "3. Create a permanent UDF using the appropriate command.",
                "4. Exit the Hive CLI.",
                "5. Reopen the Hive CLI.",
                "6. Attempt to call the previously created permanent UDF."
            ],
            "ExpectedBehavior": "The UDF should execute successfully and return the expected result.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to call the UDF after reopening the Hive CLI.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-8771.json",
        "creation_time": "2014-11-07T00:30:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-8771",
            "Title": "AbstractFileMergeOperator Fails to Move Incompatible Files Due to Incorrect Destination Path Handling",
            "Description": "The AbstractFileMergeOperator is designed to move incompatible files to a final destination. However, it fails when the destination path is a file instead of a directory, leading to an IOException. This issue occurs specifically when executing the orc_merge_incompat2.q test case under CentOS.",
            "StackTrace": [
                "2014-11-05 02:38:56,588 DEBUG fs.FileSystem (RawLocalFileSystem.java:rename(337)) - Falling through to a copy of file:/home/prasanth/hive/itests/qtest/target/warehouse/orc_merge5a/st=80.0/000000_0 to file:/home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0/000000_0",
                "2014-11-05 02:38:56,589 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.",
                "2014-11-05 02:38:56,590 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local1144733438_0036",
                "java.lang.Exception: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)",
                "Caused by: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:100)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:679)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:233)",
                "at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.closeOp(OrcFileMergeOperator.java:220)",
                "at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:98)",
                "... 10 more",
                "Caused by: java.io.FileNotFoundException: Destination exists and is not a directory: /home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0",
                "at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:423)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:267)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:257)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:339)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:507)",
                "at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)",
                "at org.apache.hadoop.fs.ProxyFileSystem.rename(ProxyFileSystem.java:177)",
                "at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.renameOrMoveFiles(Utilities.java:1589)",
                "at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:218)",
                "... 12 more"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment on CentOS.",
                "2. Create a test case that includes incompatible files for merging.",
                "3. Execute the orc_merge_incompat2.q test case.",
                "4. Observe the failure due to IOException."
            ],
            "ExpectedBehavior": "The AbstractFileMergeOperator should successfully move incompatible files to the specified final destination directory.",
            "ObservedBehavior": "The operation fails with an IOException indicating that the destination exists and is not a directory.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-8008.json",
        "creation_time": "2014-09-05T23:00:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-8008",
            "Title": "NullPointerException when fetching decimal values from Hive table",
            "Description": "When querying a Hive table containing a decimal value that exceeds the defined precision, a NullPointerException (NPE) is thrown during the fetch operation. This issue occurs specifically when the table has a row with a decimal value of 9999999999.5.",
            "StackTrace": [
                "2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)",
                "... 12 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "Create a Hive table with a decimal column, e.g., `CREATE TABLE dec_test (dec DECIMAL(10,0));`",
                "Insert a row with a decimal value of 9999999999.5 into the table, e.g., `INSERT INTO dec_test VALUES (9999999999.5);`",
                "Execute a query to select all rows from the table, e.g., `SELECT * FROM dec_test;`"
            ],
            "ExpectedBehavior": "The query should return the row with the decimal value without throwing any exceptions.",
            "ObservedBehavior": "The query throws a NullPointerException, causing the Hive CLI to crash.",
            "Resolution": "A fix for this issue has been implemented and tested in version 0.14.0."
        }
    },
    {
        "filename": "HIVE-6915.json",
        "creation_time": "2014-04-15T20:20:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-6915",
            "Title": "Hive HBase Queries Fail Due to SASL Authentication Issues in Secure Tez Cluster",
            "Description": "Hive queries that read and write to HBase are failing in a secure Tez cluster due to SASL authentication errors. The error message indicates that there are missing or invalid credentials, suggesting that the Kerberos ticket granting ticket (TGT) is not available.",
            "StackTrace": [
                "2014-04-14 13:47:05,644 FATAL [InputInitializer [Map 1] #0] org.apache.hadoop.ipc.RpcClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.",
                "javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)",
                "at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:918)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:915)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:915)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1065)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1032)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1474)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1684)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)",
                "at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:415)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)",
                "at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Set up a secure Tez cluster with Kerberos authentication.",
                "2. Execute Hive queries that read from and write to HBase.",
                "3. Observe the logs for SASL authentication errors."
            ],
            "ExpectedBehavior": "Hive queries should execute successfully without authentication errors in a secure Tez cluster.",
            "ObservedBehavior": "Hive queries fail with SASL authentication errors indicating missing or invalid credentials.",
            "Resolution": "The issue was resolved by ensuring that valid Kerberos credentials are provided before executing Hive queries."
        }
    },
    {
        "filename": "HIVE-12364.json",
        "creation_time": "2015-11-07T02:04:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-12364",
            "Title": "DistCp Job Fails with IOException When Executed Under Tez Framework",
            "Description": "When executing a DistCp job under the Tez framework, the job fails due to an IOException indicating that the input format class is incompatible with the map compatibility mode. This issue arises during the move task when attempting to move files from a temporary directory to the final destination.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir",
                "at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)",
                "at org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(MoveTask.java:105)",
                "at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:222)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "Caused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatibility mode.",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1156)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2647)"
            ],
            "StepsToReproduce": [
                "Set the configuration parameter 'hive.exec.copyfile.maxsize' to 40000.",
                "Execute the following Hive query: 'insert overwrite into '/tmp/testinser' select * from customer;'",
                "Monitor the execution logs for the move task."
            ],
            "ExpectedBehavior": "The DistCp job should successfully move the files from the temporary directory to the specified destination without any errors.",
            "ObservedBehavior": "The DistCp job fails with an IOException indicating that the input format class is incompatible with the map compatibility mode, preventing the move task from completing successfully.",
            "Resolution": "[Provide additional details about the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-8766.json",
        "creation_time": "2014-11-06T22:08:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-8766",
            "Title": "HMS Retrying Mechanism Fails on NucleusException During Metastore Operations",
            "Description": "When executing Metastore operations on a heavily loaded Metastore Database (SQL Server), NucleusExceptions may occur due to connection timeouts. The current HMS retrying mechanism does not handle these exceptions properly, leading to failed Hive queries. The proposed solution is to enhance the retry mechanism to allow retries when encountering a NucleusException.",
            "StackTrace": [
                "2014-11-04 06:40:03,208 ERROR bonecp.ConnectionHandle (ConnectionHandle.java:markPossiblyBroken(388)) - Database access problem. Killing off this connection and all remaining connections in the connection pool. SQL State = 08S01",
                "2014-11-04 06:40:03,213 ERROR DataNucleus.Transaction (Log4JLogger.java:error(115)) - Operation rollback failed on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16, error code UNKNOWN and transaction: [DataNucleus Transaction, ID=Xid=   \ufffd, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16]]",
                "2014-11-04 06:40:03,217 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(139)) - MetaException(message:org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5183)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1738)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1699)",
                "at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:101)",
                "at com.sun.proxy.$Proxy11.get_table(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:112)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)",
                "at com.sun.proxy.$Proxy12.getTable(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1060)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1015)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerde(DDLSemanticAnalyzer.java:1356)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:299)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:396)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:666)",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.size(ElementContainerStore.java:429)",
                "at org.datanucleus.store.types.backed.List.size(List.java:581)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToSkewedValues(ObjectStore.java:1190)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1168)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1178)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToTable(ObjectStore.java:1035)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:893)",
                "at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1727)",
                "... 41 more",
                "Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: SSL peer shut down incorrectly",
                "at com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1352)",
                "at com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1339)",
                "at com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:1694)",
                "at com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:3734)",
                "at com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:506)",
                "at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:388)",
                "at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:340)",
                "at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615)",
                "at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1400)",
                "at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:179)",
                "at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:154)",
                "at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:283)",
                "at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)",
                "at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:638)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with a SQL Server as the Metastore Database.",
                "2. Configure the SQL Server to timeout connections after a specified duration.",
                "3. Execute a Hive query that involves Metastore operations while the SQL Server is under heavy load.",
                "4. Observe the logs for NucleusExceptions and failed queries."
            ],
            "ExpectedBehavior": "The Hive Metastore should retry the operation when a NucleusException occurs due to a timeout, allowing the query to succeed after a few retries.",
            "ObservedBehavior": "The Hive query fails immediately with a NucleusException without any retries, leading to a poor user experience.",
            "Resolution": "[Provide additional details on the proposed fix or workaround]"
        }
    },
    {
        "filename": "HIVE-5857.json",
        "creation_time": "2013-11-20T01:12:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-5857",
            "Title": "NullPointerException in ExecReducer.configure when running reduce tasks in uber mode on YARN",
            "Description": "A Hive query fails when it attempts to run a reduce task in uber mode on YARN, resulting in a NullPointerException. The issue arises because the plan file (reduce.xml) for the reduce task is not found, leading to a FileNotFoundException in the Utilities.getBaseWork method. This occurs due to the LocalContainerLauncher changing the configuration to local mode, which is not compatible with the expected behavior for reduce tasks.",
            "StackTrace": [
                "2013-11-20 00:50:56,862 INFO [uber-SubtaskRunner] org.apache.hadoop.hive.ql.exec.Utilities: No plan file found: hdfs://namenode.c.lon.spotify.net:54310/var/tmp/kawaa/hive_2013-11-20_00-50-43_888_3938384086824086680-2/-mr-10003/e3caacf6-15d6-4987-b186-d2906791b5b0/reduce.xml",
                "2013-11-20 00:50:56,862 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 7 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)",
                "... 12 more"
            ],
            "StepsToReproduce": [
                "1. Submit a Hive query that includes a reduce task.",
                "2. Ensure that the query is set to run in uber mode on YARN.",
                "3. Monitor the execution of the query."
            ],
            "ExpectedBehavior": "The reduce task should execute successfully without throwing a NullPointerException, and the plan file (reduce.xml) should be found and processed correctly.",
            "ObservedBehavior": "The reduce task fails with a NullPointerException due to the plan file not being found, resulting in a FileNotFoundException in the Utilities.getBaseWork method.",
            "Resolution": "A potential fix is to add a conditional check in the code to handle the case when running a reduce task in uber mode, ensuring that the plan file is searched in the correct location."
        }
    },
    {
        "filename": "HIVE-1547.json",
        "creation_time": "2010-08-17T02:09:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-1547",
            "Title": "Null Pointer Exception during Partition Unarchiving in Hive",
            "Description": "When attempting to unarchive a partition in Hive, a Null Pointer Exception (NPE) is thrown, causing the operation to fail. This issue appears to be specific to the Distributed File System (DFS) as local file system tests do not replicate the error.",
            "StackTrace": [
                "2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)",
                "    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)",
                "    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)",
                "    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)",
                "    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)",
                "    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)",
                "    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "    at java.lang.reflect.Method.invoke(Method.java:597)",
                "    at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with a partitioned table.",
                "2. Attempt to unarchive a partition using the DDL command.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The partition should be successfully unarchived without any exceptions.",
            "ObservedBehavior": "A Null Pointer Exception is thrown, preventing the unarchiving operation from completing.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-6113.json",
        "creation_time": "2013-12-27T07:07:00.000+0000",
        "bug_report": {
            "BugID": "HIVE-6113",
            "Title": "Error Instantiating HiveMetaStoreClient Due to Duplicate Database Entry",
            "Description": "When executing the SQL command 'use fdm; desc formatted fdm.tableName;' in Python, an error is thrown indicating that the HiveMetaStoreClient cannot be instantiated due to a duplicate entry for the database key 'UNIQUE_DATABASE'. This issue occurs intermittently, as the command may succeed upon retrying.",
            "StackTrace": [
                "2013-12-25 03:01:32,290 ERROR exec.DDLTask (DDLTask.java:execute(435)) - org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1143)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:260)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:217)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:507)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:875)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:769)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1217)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2372)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2383)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1139)",
                "... 20 more",
                "Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore",
                "NestedThrowables:",
                "java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:165)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:358)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:404)",
                "... 46 more",
                "Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with a database named 'fdm'.",
                "2. Execute the SQL command: 'use fdm; desc formatted fdm.tableName;' in a Python script.",
                "3. Observe the error thrown during the execution."
            ],
            "ExpectedBehavior": "The command should execute successfully and return the formatted description of the specified table without any errors.",
            "ObservedBehavior": "The command fails with an error indicating that the HiveMetaStoreClient cannot be instantiated due to a duplicate entry for the database key 'UNIQUE_DATABASE'. Retrying the command may lead to successful execution.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-9570.json",
        "creation_time": "2015-02-03T23:30:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-9570",
            "Title": "NullPointerException in SparkCompiler during UNION ALL query execution",
            "Description": "The execution of the query 'union_view.q' fails with a NullPointerException in the SparkCompiler class. This issue occurs when the input format is being set for a Spark task, indicating a potential problem with the task or its associated work.",
            "StackTrace": [
                "2015-02-03 15:27:05,723 ERROR [main]: ql.Driver (SessionState.java:printError(861)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:274)",
                "  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:253)",
                "  at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:222)",
                "  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10231)",
                "  at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "  at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "  at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1160)",
                "  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1039)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "  at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1019)",
                "  at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:993)",
                "  at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:136)",
                "  at org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_view(TestSparkCliDriver.java:120)"
            ],
            "StepsToReproduce": [
                "1. Execute the query 'union_view.q' using the Hive CLI.",
                "2. Observe the output for any errors."
            ],
            "ExpectedBehavior": "The query 'union_view.q' should execute successfully without any exceptions.",
            "ObservedBehavior": "The execution fails with a NullPointerException, indicating an issue in the SparkCompiler's setInputFormat method.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-1678.json",
        "creation_time": "2010-10-01T05:41:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-1678",
            "Title": "NullPointerException in MapJoinOperator during query execution with multiple map joins",
            "Description": "A NullPointerException (NPE) occurs in the MapJoinOperator when executing a query that involves two map joins and a group by operation. This issue arises during the processing of rows, leading to a failure in the query execution.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)"
            ],
            "StepsToReproduce": [
                "1. Prepare a Hive query that includes two map joins and a group by clause.",
                "2. Execute the query in the Hive environment.",
                "3. Observe the execution failure due to a NullPointerException."
            ],
            "ExpectedBehavior": "The query should execute successfully, returning the expected results without any exceptions.",
            "ObservedBehavior": "The query fails with a NullPointerException, preventing successful execution and returning no results.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-11820.json",
        "creation_time": "2015-09-14T22:20:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-11820",
            "Title": "IllegalArgumentException when exporting tables larger than 32MB due to Skip CRC validation",
            "Description": "When attempting to export tables larger than 32MB, an IllegalArgumentException is thrown with the message 'Skip CRC is valid only with update options'. This issue arises from the order of method calls in the DistCpOptions configuration.",
            "StackTrace": [
                "2015-09-14 21:44:16,817 ERROR [main]: exec.Task (SessionState.java:printError(960)) - Failed with exception Skip CRC is valid only with update options",
                "java.lang.IllegalArgumentException: Skip CRC is valid only with update options",
                "at org.apache.hadoop.tools.DistCpOptions.validate(DistCpOptions.java:556)",
                "at org.apache.hadoop.tools.DistCpOptions.setSkipCRC(DistCpOptions.java:311)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1147)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)"
            ],
            "StepsToReproduce": [
                "1. Prepare a Hive table with data size greater than 32MB.",
                "2. Execute the export command using DistCp with the Skip CRC option set to true.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The export operation should complete successfully without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that 'Skip CRC is valid only with update options'.",
            "Resolution": "To resolve this issue, reverse the order of the following two lines in the DistCpOptions configuration:\n\n```java\noptions.setSkipCRC(true);\noptions.setSyncFolder(true);\n```"
        }
    },
    {
        "filename": "HIVE-17274.json",
        "creation_time": "2017-08-08T22:20:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-17274",
            "Title": "IllegalArgumentException when spilling RowContainer with timestamp column",
            "Description": "When attempting to spill a RowContainer that contains a timestamp column, an IllegalArgumentException is thrown due to an invalid path name containing a colon. This occurs because the join key's toString() method is used as part of the filename, which is not valid in Hadoop's file system.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:205)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:171)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:93)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:94)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:404)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926)",
                "at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1137)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:530)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(Utilities.java:1643)",
                "at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.getHiveRecordWriter(HiveSequenceFileOutputFormat.java:64)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:243)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter(RowContainer.java:538)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:299)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.copyToDFSDirecory(RowContainer.java:407)",
                "at org.apache.hadoop.hive.ql.exec.SkewJoinHandler.endGroup(SkewJoinHandler.java:185)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:249)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:195)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with a timestamp column.",
                "2. Insert data into the table that includes timestamp values.",
                "3. Execute a query that triggers a spill operation on the RowContainer.",
                "4. Observe the exception thrown in the logs."
            ],
            "ExpectedBehavior": "The RowContainer should spill data to the specified output path without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown due to an invalid path name containing a colon, causing the spill operation to fail.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-12522.json",
        "creation_time": "2015-11-25T19:48:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-12522",
            "Title": "IllegalArgumentException: Wrong FS error during Tez merge files when warehouse and scratchdir are on different filesystems",
            "Description": "When the configuration 'hive.merge.tezfiles' is set to true, and the warehouse directory and scratch directory are located on different filesystems, an IllegalArgumentException is thrown indicating a wrong filesystem. This issue occurs during the execution of a Tez task, leading to a failure in processing the job.",
            "StackTrace": [
                "2015-11-13 10:22:10,617 ERROR exec.Task (TezTask.java:execute(184)) - Failed to execute tez graph.",
                "java.lang.IllegalArgumentException: Wrong FS: wasb://chaoyiteztest@chaoyiteztest.blob.core.windows.net/hive/scratch/chaoyitest/c888f405-3c98-46b1-bf39-e57f067dfe4c/hive_2015-11-13_10-16-10_216_8161037519951665173-1/_tmp.-ext-10000, expected: hdfs://headnodehost:9000",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1136)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1423)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:579)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1083)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:156)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1606)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1367)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1179)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:733)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Set the configuration 'hive.merge.tezfiles' to true.",
                "Ensure that the warehouse directory and scratch directory are on different filesystems.",
                "Execute a Tez job that requires merging files."
            ],
            "ExpectedBehavior": "The Tez job should execute successfully without throwing an IllegalArgumentException related to filesystem paths.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a wrong filesystem when the scratch directory is on a different filesystem than the warehouse directory.",
            "Resolution": "The issue has been fixed by ensuring that both the warehouse and scratch directories are on the same filesystem."
        }
    },
    {
        "filename": "HIVE-16845.json",
        "creation_time": "2017-06-07T17:07:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-16845",
            "Title": "NullPointerException during INSERT OVERWRITE with Dynamic Partitions on S3",
            "Description": "When executing an INSERT OVERWRITE statement on a partitioned table stored in S3, a NullPointerException (NPE) occurs, preventing the operation from completing successfully. This issue arises specifically when dynamic partitions are involved.",
            "StackTrace": [
                "2017-05-08 21:32:50,607 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-184028]: Error running hive query:",
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)",
                "at org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "Create a partitioned table on S3 using the following command:",
                "CREATE EXTERNAL TABLE s3table(user_id string COMMENT '', event_name string COMMENT '') PARTITIONED BY (reported_date string, product_id int) LOCATION 's3a://<bucket name>';",
                "Create a temporary table with the following command:",
                "create table tmp_table (id string, name string, date string, pid int) row format delimited fields terminated by '\\t' lines terminated by '\\n' stored as textfile;",
                "Load the following rows into the temporary table:",
                "u1\tvalue1\t2017-04-10\t10000",
                "u2\tvalue2\t2017-04-10\t10000",
                "u3\tvalue3\t2017-04-10\t10001",
                "Set the following parameters:",
                "-- hive.exec.dynamic.partition.mode=nonstrict",
                "-- mapreduce.input.fileinputformat.split.maxsize=10",
                "-- hive.blobstore.optimizations.enabled=true",
                "-- hive.blobstore.use.blobstore.as.scratchdir=false",
                "-- hive.merge.mapfiles=true",
                "Execute the INSERT OVERWRITE command:",
                "INSERT OVERWRITE TABLE s3table PARTITION (reported_date, product_id) SELECT t.id as user_id, t.name as event_name, t.date as reported_date, t.pid as product_id FROM tmp_table t;"
            ],
            "ExpectedBehavior": "The INSERT OVERWRITE command should successfully insert the data into the partitioned table without any errors.",
            "ObservedBehavior": "A NullPointerException occurs during the execution of the INSERT OVERWRITE command, resulting in a failure to complete the operation.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-9655.json",
        "creation_time": "2015-02-11T20:58:13.000+0000",
        "bug_report": {
            "BugID": "HIVE-9655",
            "Title": "Error during dynamic partition insertion due to missing column reference",
            "Description": "When attempting to insert data from table `t1` into table `t2` with dynamic partitioning, an error occurs indicating that a field cannot be found. This issue arises specifically when the `distribute by` clause is used with the same column structure in both tables.",
            "StackTrace": [
                "2015-02-11 12:50:52,756 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(178)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"c1\":1,\"c2\":\"one\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 10 more",
                "Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)",
                "at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)",
                "... 16 more"
            ],
            "StepsToReproduce": [
                "Create table `t1` with the following schema: `create table t1 (c1 bigint, c2 string);`",
                "Create table `t2` with the following schema: `CREATE TABLE t2 (c1 int, c2 string) PARTITIONED BY (p1 string);`",
                "Load data into `t1` using: `load data local inpath 'data' into table t1;`",
                "Set the dynamic partition mode: `SET hive.exec.dynamic.partition.mode=nonstrict;`",
                "Execute the insert statement: `insert overwrite table t2 partition(p1) select *, c1 as p1 from t1 distribute by p1;`"
            ],
            "ExpectedBehavior": "The data from table `t1` should be successfully inserted into table `t2` with the appropriate partitions created based on the `p1` column.",
            "ObservedBehavior": "The query fails with a HiveException indicating that it cannot find the field `_col2`, which suggests a mismatch in the expected column structure during the processing of the insert operation.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11441.json",
        "creation_time": "2015-08-03T17:42:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-11441",
            "Title": "SemanticException on Alter Table Location with Invalid HDFS Path",
            "Description": "When attempting to alter the location of a Hive table to an invalid HDFS path, Hive throws a SemanticException instead of providing a more informative error message or allowing the user to correct the mistake. This behavior occurs specifically when the StorageBasedAuthorizationProvider is enabled.",
            "StackTrace": [
                "2015-07-30 12:19:43,573 DEBUG [main]: transport.TSaslTransport (TSaslTransport.java:readFrame(429)) - CLIENT: reading data length: 293",
                "2015-07-30 12:19:43,720 ERROR [main]: ql.Driver (SessionState.java:printError(833)) - FAILED: SemanticException Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "org.apache.hadoop.hive.ql.parse.SemanticException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1323)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(DDLSemanticAnalyzer.java:1452)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:295)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1069)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1131)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1072)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)",
                "... 23 more"
            ],
            "StepsToReproduce": [
                "1. Create a table with an incorrect HDFS location.",
                "   Example: `create table testwrongloc(id int);`",
                "2. Attempt to alter the table location to an invalid HDFS path.",
                "   Example: `alter table testwrongloc set location 'hdfs://a-valid-hostname/tmp/testwrongloc';`",
                "3. Observe the error message thrown by Hive."
            ],
            "ExpectedBehavior": "Hive should throw an error indicating that the specified HDFS path is invalid and provide guidance on the correct format for HDFS paths.",
            "ObservedBehavior": "Hive throws a SemanticException indicating that the table cannot be fetched due to a connection issue, without clear guidance on the invalid path.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-10801.json",
        "creation_time": "2015-05-22T19:43:23.000+0000",
        "bug_report": {
            "BugID": "HIVE-10801",
            "Title": "NullPointerException when dropping a view in Hive",
            "Description": "When attempting to drop a view in Hive, a NullPointerException is thrown due to a missing encryption key provider URI. The error occurs in the HiveMetaStore when the method `isPathEncrypted` is called with a null path, leading to a failure in the drop operation.",
            "StackTrace": [
                "2015-05-21 11:53:06,126 ERROR [HiveServer2-Background-Pool: Thread-197]: hdfs.KeyProviderCache (KeyProviderCache.java:createKeyProviderURI(87)) - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!",
                "2015-05-21 11:53:06,134 ERROR [HiveServer2-Background-Pool: Thread-197]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(155)) - MetaException(message:java.lang.NullPointerException)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy8.dropTable(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)",
                "... 40 more"
            ],
            "StepsToReproduce": [
                "1. Connect to Hive using HiveServer2.",
                "2. Create a view using a valid SQL statement.",
                "3. Attempt to drop the view using the command: `DROP VIEW <view_name>`.",
                "4. Observe the error in the Hive logs."
            ],
            "ExpectedBehavior": "The view should be dropped successfully without any errors.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the path is null when checking if it is encrypted.",
            "Resolution": "A fix has been implemented to ensure that the path is not null before calling the `isPathEncrypted` method."
        }
    },
    {
        "filename": "HIVE-9141.json",
        "creation_time": "2014-12-17T07:23:05.000+0000",
        "bug_report": {
            "BugID": "HIVE-9141",
            "Title": "ClassCastException in HiveOnTez when using UNION ALL with GROUP BY",
            "Description": "A ClassCastException occurs when executing a Hive query that combines UNION ALL and GROUP BY operations while using the Tez execution engine. The error indicates that a MapWork cannot be cast to a ReduceWork, which suggests a problem in the query plan generation.",
            "StackTrace": [
                "2014-12-16 23:19:13,593 ERROR ql.Driver (SessionState.java:printError(834)) - FAILED: ClassCastException org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "java.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:361)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "StepsToReproduce": [
                "1. Set the Hive execution engine to Tez: `set hive.execution.engine=tez;`",
                "2. Execute the following query:",
                "   SELECT key, value FROM (",
                "       SELECT key, value FROM src",
                "       UNION ALL",
                "       SELECT key, key AS value FROM (",
                "           SELECT DISTINCT key FROM (",
                "               SELECT key, value FROM src",
                "               UNION ALL",
                "               SELECT key, value FROM src",
                "           ) t1 GROUP BY key, value",
                "       ) t2",
                "   ) t3 GROUP BY key, value;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that a MapWork cannot be cast to a ReduceWork.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-10010.json",
        "creation_time": "2015-03-18T17:48:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-10010",
            "Title": "NullPointerException during ALTER TABLE operation in Hive Metastore",
            "Description": "When performing an ALTER TABLE operation in Hive, a NullPointerException is thrown, indicating a failure in the metastore's handling of storage descriptors. This issue occurs specifically in the hbase-metastore branch.",
            "StackTrace": [
                "2015-03-18 10:45:54,189 ERROR [main]: exec.DDLTask (DDLTask.java:failed(512)) - java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)",
                "at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)",
                "at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)",
                "at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:164)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)",
                "at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with the hbase-metastore branch.",
                "2. Create a table in the metastore.",
                "3. Execute an ALTER TABLE command on the created table.",
                "4. Observe the logs for any errors."
            ],
            "ExpectedBehavior": "The ALTER TABLE operation should complete successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the ALTER TABLE operation, leading to a failure in the process.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7763.json",
        "creation_time": "2014-08-18T09:35:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-7763",
            "Title": "RuntimeException: Map operator initialization failed due to inconsistent configuration and input path",
            "Description": "When attempting to query a TABLESAMPLE on an empty bucket table in the Spark branch of Hive, a RuntimeException occurs indicating that the map operator initialization has failed. The underlying cause is a HiveException stating that the configuration and input path are inconsistent.",
            "StackTrace": [
                "2014-08-18 16:23:15,213 ERROR [Executor task launch worker-0]: executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)",
                "java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:127)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:52)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:30)",
                "at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)",
                "at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)",
                "at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "at org.apache.spark.scheduler.Task.run(Task.scala:54)",
                "at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:404)",
                "at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:93)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with Spark integration.",
                "2. Create an empty bucket table in Hive.",
                "3. Execute a query using TABLESAMPLE on the empty bucket table.",
                "4. Observe the error in the logs."
            ],
            "ExpectedBehavior": "The query should execute without errors, returning an empty result set for the TABLESAMPLE operation on an empty bucket table.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the map operator initialization failed due to inconsistent configuration and input path.",
            "Resolution": "[Provide additional details on the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-12083.json",
        "creation_time": "2015-10-09T22:45:47.000+0000",
        "bug_report": {
            "BugID": "HIVE-12083",
            "Title": "Thrift Protocol Error When AggrStats is Returned with Empty partNames or colNames",
            "Description": "In the fix for HIVE-10965, a short-circuit path was introduced in the method `aggrColStatsForPartitions` that returns an empty `AggrStats` object if either `partNames` or `colNames` is empty. This leads to a Thrift protocol error because the `AggrStats` struct requires the `colStats` field to be set. The error occurs when the Thrift server attempts to validate the `AggrStats` object, resulting in a `TProtocolException` due to the unset required field.",
            "StackTrace": [
                "2015-10-08 00:00:25,413 ERROR server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Call the method `aggrColStatsForPartitions` with an empty list for `partNames` or `colNames`.",
                "2. Observe the Thrift server logs for errors."
            ],
            "ExpectedBehavior": "The method should handle empty `partNames` or `colNames` gracefully without returning an empty `AggrStats` object that violates Thrift protocol requirements.",
            "ObservedBehavior": "A Thrift protocol error occurs indicating that the required field 'colStats' is unset when an empty `AggrStats` object is returned.",
            "Resolution": "A fix should be implemented to ensure that the method does not return an empty `AggrStats` object when `partNames` or `colNames` are empty, or to provide default values for the required fields."
        }
    },
    {
        "filename": "HIVE-14784.json",
        "creation_time": "2016-09-17T02:00:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-14784",
            "Title": "Operation Logging Fails When Parent Directory is Deleted",
            "Description": "When the parent directory for operation logs is deleted (e.g., due to OS purging), Hive fails to create the operation log file, leading to subsequent errors when attempting to fetch operation results. This issue is particularly problematic when using HUE, as it does not close Hive sessions and may attempt to retrieve logs long after they were created.",
            "StackTrace": [
                "2016-09-15 15:09:16,723 WARN org.apache.hive.service.cli.operation.Operation: Unable to create operation log file: /tmp/hive/operation_logs/b8809985-6b38-47ec-a49b-6158a67cd9fc/d35414f7-2418-426c-8489-c6f643ca4599",
                "java.io.IOException: No such file or directory",
                "\tat java.io.UnixFileSystem.createFileExclusively(Native Method)",
                "\tat java.io.File.createNewFile(File.java:1012)",
                "\tat org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)",
                "\tat org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)",
                "\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)",
                "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:385)",
                "\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)",
                "\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set the operation log directory to a location that can be purged by the OS (e.g., /tmp).",
                "2. Start a Hive session and run a query.",
                "3. Wait for the OS to purge the log directory.",
                "4. Attempt to run another query from the same session."
            ],
            "ExpectedBehavior": "The operation log file should be created successfully, and subsequent queries should execute without errors.",
            "ObservedBehavior": "An IOException occurs stating 'No such file or directory', preventing the creation of the operation log file and leading to errors when fetching results.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    }
]