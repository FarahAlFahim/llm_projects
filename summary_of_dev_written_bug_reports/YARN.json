[
    {
        "filename": "YARN-5918.json",
        "creation_time": "2016-11-20T14:19:00.000+0000",
        "bug_report": {
            "BugID": "YARN-5918",
            "Title": "Handle Opportunistic scheduling allocate request failure when NM is lost",
            "Description": "Allocate request failure during Opportunistic container allocation when nodemanager is lost.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully allocate containers even when the node manager is lost.",
            "ObservedBehavior": "A NullPointerException occurs during the allocation process, preventing successful container allocation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8629.json",
        "creation_time": "2018-08-07T00:14:14.000+0000",
        "bug_report": {
            "BugID": "YARN-8629",
            "Title": "Container cleanup fails while trying to delete Cgroups",
            "Description": "When an application failed to launch a container successfully, the cleanup of the container also failed with a warning about a missing cgroup tasks file.",
            "StackTrace": [
                "java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:93)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The container cleanup process should successfully delete the cgroup associated with the container.",
            "ObservedBehavior": "The cleanup process fails with a FileNotFoundException indicating that the cgroup tasks file does not exist.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4431.json",
        "creation_time": "2015-12-07T18:31:36.000+0000",
        "bug_report": {
            "BugID": "YARN-4431",
            "Title": "Not necessary to do unRegisterNM() if NM get stop due to failed to connect to RM",
            "Description": "The NodeManager (NM) attempts to unregister itself from the ResourceManager (RM) when it shuts down due to connection issues, which is unnecessary and leads to repeated retries.",
            "StackTrace": [
                "java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused",
                "at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager.",
                "Ensure the ResourceManager is down or unreachable.",
                "Observe the NodeManager's behavior during shutdown."
            ],
            "ExpectedBehavior": "NodeManager should not attempt to unregister itself if it is shutting down due to connection issues.",
            "ObservedBehavior": "NodeManager retries to unregister itself from ResourceManager multiple times, leading to unnecessary operations.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2273.json",
        "creation_time": "2014-07-10T18:38:53.000+0000",
        "bug_report": {
            "BugID": "YARN-2273",
            "Title": "NPE in ContinuousScheduling thread when we lose a node",
            "Description": "A node experienced memory errors, leading to a cycle of rebooting and rejoining the cluster. This caused a NullPointerException in the ContinuousScheduling thread of the FairScheduler.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)",
                "at java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)",
                "at java.util.TimSort.sort(TimSort.java:203)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "Introduce memory errors on a node.",
                "Allow the node to reboot and rejoin the cluster multiple times."
            ],
            "ExpectedBehavior": "The FairScheduler should handle node failures gracefully without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException occurs, leading to YARN being crippled, where containers are not assigned and no progress is made.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2834.json",
        "creation_time": "2014-11-09T06:07:01.000+0000",
        "bug_report": {
            "BugID": "YARN-2834",
            "Title": "Resource manager crashed with Null Pointer Exception",
            "Description": "The resource manager fails after a restart due to a Null Pointer Exception.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The resource manager should start without crashing.",
            "ObservedBehavior": "The resource manager crashes with a Null Pointer Exception upon restart.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-370.json",
        "creation_time": "2013-02-01T04:02:58.000+0000",
        "bug_report": {
            "BugID": "YARN-370",
            "Title": "CapacityScheduler app submission fails when min alloc size not multiple of AM size",
            "Description": "The application submission fails due to an unauthorized request to start a container, as the expected resource allocation does not match the actual allocation.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unauthorized request to start container. Expected resource <memory:2048, vCores:1> but found <memory:1536, vCores:1>",
                "at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:383)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:400)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:68)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:111)"
            ],
            "StepsToReproduce": [
                "Run 2.0.3-SNAPSHOT with the capacity scheduler configured with minimum allocation size 1G.",
                "Set the AM size to 1.5G without specifying a resource calculator."
            ],
            "ExpectedBehavior": "The application should launch successfully with the specified resource allocations.",
            "ObservedBehavior": "The application fails to launch due to a mismatch in expected and actual resource allocations.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3675.json",
        "creation_time": "2015-05-18T22:38:39.000+0000",
        "bug_report": {
            "BugID": "YARN-3675",
            "Title": "FairScheduler: RM quits when node removal races with continuous scheduling on the same node",
            "Description": "With continuous scheduling, scheduling can be done on a node that's just removed causing errors.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserve(FSAppAttempt.java:469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:815)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:763)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "ResourceManager should handle node removal without crashing.",
            "ObservedBehavior": "ResourceManager crashes with a NullPointerException when handling APP_ATTEMPT_REMOVED event.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4763.json",
        "creation_time": "2016-03-04T10:03:56.000+0000",
        "bug_report": {
            "BugID": "YARN-4763",
            "Title": "RMApps Page crashes with NPE",
            "Description": "The RMApps page crashes due to a NullPointerException when handling a specific URI.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)",
                "at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The RMApps page should render without crashing.",
            "ObservedBehavior": "The RMApps page crashes with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8202.json",
        "creation_time": "2018-04-24T15:52:00.000+0000",
        "bug_report": {
            "BugID": "YARN-8202",
            "Title": "DefaultAMSProcessor should properly check units of requested custom resource types against minimum/maximum allocation",
            "Description": "When executing a pi job with specific resource arguments, an InvalidResourceRequestException occurs, causing the job to hang.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation.",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)",
                "at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)"
            ],
            "StepsToReproduce": [
                "Execute a pi job with arguments: -Dmapreduce.map.resource.memory-mb=200 -Dmapreduce.map.resource.resource1=500M 1 1000",
                "Observe the job hanging and the exception being thrown."
            ],
            "ExpectedBehavior": "The job should run successfully without hanging.",
            "ObservedBehavior": "The job hangs and throws an InvalidResourceRequestException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7118.json",
        "creation_time": "2017-08-29T12:04:01.000+0000",
        "bug_report": {
            "BugID": "YARN-7118",
            "Title": "AHS REST API can return NullPointerException",
            "Description": "The ApplicationHistoryService REST API returns a NullPointerException when accessed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)"
            ],
            "StepsToReproduce": [
                "Send a GET request to the AHS REST API endpoint: http://<ATS IP>:8188/ws/v1/applicationhistory/apps?queue=test"
            ],
            "ExpectedBehavior": "The API should return a list of applications without errors.",
            "ObservedBehavior": "The API returns a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4743.json",
        "creation_time": "2016-02-27T09:12:28.000+0000",
        "bug_report": {
            "BugID": "YARN-4743",
            "Title": "FairSharePolicy breaks TimSort assumption",
            "Description": "The FairSharePolicy implementation violates the transitive property of the comparison method, leading to an IllegalArgumentException during scheduling.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:868)",
                "at java.util.TimSort.mergeAt(TimSort.java:485)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:410)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The FairSharePolicy should correctly handle resource scheduling without violating comparison contracts.",
            "ObservedBehavior": "An IllegalArgumentException is thrown during the scheduling process due to a violation of the comparison method's contract.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2414.json",
        "creation_time": "2014-08-12T23:48:48.000+0000",
        "bug_report": {
            "BugID": "YARN-2414",
            "Title": "RM web UI: app page will crash if app is failed before any attempt has been created",
            "Description": "The application page in the RM web UI crashes when an application fails before any attempts are created.",
            "StackTrace": [
                "2014-08-12 16:45:13,573 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/app/application_1407887030038_0001",
                "java.lang.reflect.InvocationTargetException",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application page should display the application details without crashing.",
            "ObservedBehavior": "The application page crashes with a NullPointerException when the application fails before any attempts are created.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3878.json",
        "creation_time": "2015-07-02T00:20:59.000+0000",
        "bug_report": {
            "BugID": "YARN-3878",
            "Title": "AsyncDispatcher can hang while stopping if it is configured for draining events on stop",
            "Description": "The AsyncDispatcher hangs during the stop process when it is configured to drain events, leading to an InterruptedException and indefinite waiting for the event queue to drain.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)"
            ],
            "StepsToReproduce": [
                "Stop the ResourceManager while putting an RMStateStore Event to RMStateStore's AsyncDispatcher."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should stop without hanging and drain all events properly.",
            "ObservedBehavior": "The AsyncDispatcher hangs indefinitely while waiting for the event queue to drain.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6683.json",
        "creation_time": "2017-06-02T00:29:13.000+0000",
        "bug_report": {
            "BugID": "YARN-6683",
            "Title": "Invalid event: COLLECTOR_UPDATE at KILLED",
            "Description": "An error occurs when trying to handle a COLLECTOR_UPDATE event while the application is in the KILLED state, leading to an InvalidStateTransitionException.",
            "StackTrace": [
                "2017-06-01 20:01:22,686 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(905)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle the COLLECTOR_UPDATE event without throwing an exception, regardless of the application state.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when attempting to handle a COLLECTOR_UPDATE event while in the KILLED state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2910.json",
        "creation_time": "2014-11-27T06:19:00.000+0000",
        "bug_report": {
            "BugID": "YARN-2910",
            "Title": "FSLeafQueue can throw ConcurrentModificationException",
            "Description": "The list that maintains the runnable and non-runnable apps is a standard ArrayList, which can lead to ConcurrentModificationException due to unsynchronized access by multiple threads.",
            "StackTrace": [
                "2014-11-12 02:29:01,169 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.util.ConcurrentModificationException: java.util.ConcurrentModificationException",
                "at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)",
                "at java.util.ArrayList$Itr.next(ArrayList.java:831)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application should manage runnable and non-runnable apps without throwing exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown when accessing the list of apps.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-192.json",
        "creation_time": "2012-11-01T05:00:41.000+0000",
        "bug_report": {
            "BugID": "YARN-192",
            "Title": "Node update causes NPE in the fair scheduler",
            "Description": "The exception occurs when unreserve is called on an FSSchedulerApp with a NodeId that it does not know about. The RM seems to have a different idea about what apps are reserved for which node than the scheduler.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.unreserve(FSSchedulerApp.java:356)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.unreserve(AppSchedulable.java:214)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:266)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:330)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueSchedulable.assignContainer(FSQueueSchedulable.java:161)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:759)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:836)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:329)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The scheduler should correctly handle node updates without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when unreserve is called on an FSSchedulerApp with an unknown NodeId.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4581.json",
        "creation_time": "2016-01-12T03:37:40.000+0000",
        "bug_report": {
            "BugID": "YARN-4581",
            "Title": "AHS writer thread leak makes RM crash while RM is recovering",
            "Description": "Enabling ApplicationHistoryWriter leads to thousands of errors and ultimately crashes the ResourceManager (RM) due to thread leaks.",
            "StackTrace": [
                "java.io.IOException: Output file not at zero offset.",
                "at org.apache.hadoop.io.file.tfile.BCFile$Writer.<init>(BCFile.java:288)",
                "at org.apache.hadoop.io.file.tfile.TFile$Writer.<init>(TFile.java:288)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:728)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:714)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.start(DFSOutputStream.java:2033)"
            ],
            "StepsToReproduce": [
                "Enable ApplicationHistoryWriter",
                "Monitor the application history service for errors"
            ],
            "ExpectedBehavior": "The ResourceManager should handle application history without crashing.",
            "ObservedBehavior": "The ResourceManager crashes due to thread leaks and OutOfMemoryError.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7786.json",
        "creation_time": "2018-01-22T14:29:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7786",
            "Title": "NullPointerException while launching ApplicationMaster",
            "Description": "A NullPointerException occurs when sending a kill command to the job before launching the ApplicationMaster.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Send a kill command to the job before launching the ApplicationMaster."
            ],
            "ExpectedBehavior": "The ApplicationMaster should launch without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the launch process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8035.json",
        "creation_time": "2018-03-16T12:02:04.000+0000",
        "bug_report": {
            "BugID": "YARN-8035",
            "Title": "Uncaught exception in ContainersMonitorImpl during relaunch due to the process ID changing",
            "Description": "The ContainersMonitorImpl encounters a MetricsException when a container is relaunched and the original PID is still associated with the container metrics.",
            "StackTrace": [
                "2018-03-16 11:59:02,563 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Uncaught exception in ContainersMonitorImpl while monitoring resource of container_1521201379995_0001_01_000002",
                "org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should update the existing ContainerPid tag with the new PID instead of throwing an exception.",
            "ObservedBehavior": "The system throws a MetricsException indicating that the ContainerPid tag already exists.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4152.json",
        "creation_time": "2015-09-12T15:02:22.000+0000",
        "bug_report": {
            "BugID": "YARN-4152",
            "Title": "NM crash with NPE when LogAggregationService#stopContainer called for absent container",
            "Description": "The NodeManager crashes due to a NullPointerException when the LogAggregationService attempts to stop a container that is absent.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Run a Pi job with 500 containers.",
                "Kill the application in between."
            ],
            "ExpectedBehavior": "The NodeManager should handle the absence of a container gracefully without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when trying to stop an absent container.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3697.json",
        "creation_time": "2015-05-21T18:05:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3697",
            "Title": "FairScheduler: ContinuousSchedulingThread can fail to shutdown",
            "Description": "The ContinuousSchedulingThread in FairScheduler sometimes fails to shut down due to an InterruptedException being blocked in continuousSchedulingAttempt.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ContinuousSchedulingThread should shut down without issues when requested.",
            "ObservedBehavior": "The ContinuousSchedulingThread fails to shut down due to a blocked InterruptedException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2340.json",
        "creation_time": "2014-07-23T15:18:38.000+0000",
        "bug_report": {
            "BugID": "YARN-2340",
            "Title": "NPE thrown when RM restart after queue is STOPPED",
            "Description": "While job is in progress, making the Queue state as STOPPED and then restarting the Resource Manager causes the standby RM to fail to come up as active, throwing a NullPointerException.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:568)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:916)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:602)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Start a job in the Resource Manager.",
                "Change the Queue state to STOPPED.",
                "Restart the Resource Manager."
            ],
            "ExpectedBehavior": "The Resource Manager should recover and come up as active without errors.",
            "ObservedBehavior": "The standby Resource Manager fails to start and throws a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8022.json",
        "creation_time": "2018-03-10T19:29:27.000+0000",
        "bug_report": {
            "BugID": "YARN-8022",
            "Title": "ResourceManager UI cluster/app/<app-id> page fails to render",
            "Description": "The page displays the message 'Failed to read the attempts of the application'.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.webapp.AppBlock: Failed to read the attempts of the application application_1520597233415_0002.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:283)",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:280)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                "at org.apache.hadoop.yarn.server.webapp.AppBlock.render(AppBlock.java:279)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock.render(RMAppBlock.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:54)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager UI should render the application attempts correctly.",
            "ObservedBehavior": "The UI fails to render and displays an error message.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3793.json",
        "creation_time": "2015-06-10T20:52:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3793",
            "Title": "Several NPEs when deleting local files on NM recovery",
            "Description": "When NM work-preserving restart is enabled, several NullPointerExceptions (NPEs) occur during recovery, likely due to sub-directories that need to be deleted. This issue is to investigate and fix the potential resource tracking problems.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "at org.apache.hadoop.fs.FileContext.delete(FileContext.java:755)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)",
                "at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should correctly delete local files without throwing NullPointerExceptions.",
            "ObservedBehavior": "NullPointerExceptions are thrown during the deletion of local files.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6102.json",
        "creation_time": "2017-01-17T09:36:29.000+0000",
        "bug_report": {
            "BugID": "YARN-6102",
            "Title": "RMActiveService context to be updated with new RMContext on failover",
            "Description": "An error occurs in the dispatcher thread due to a failure in handling events during a ResourceManager failover, leading to a fatal exception.",
            "StackTrace": [
                "2017-01-17 16:42:17,911 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(200)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Send Node heartbeat to ResourceTrackerService",
                "Call RM failover before passing to dispatcher",
                "Observe the dispatcher reset and event registration order"
            ],
            "ExpectedBehavior": "The dispatcher should handle events correctly without throwing exceptions during failover.",
            "ObservedBehavior": "The dispatcher throws a fatal exception due to an unregistered handler for RMNodeEventType.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8409.json",
        "creation_time": "2018-06-08T20:36:32.000+0000",
        "bug_report": {
            "BugID": "YARN-8409",
            "Title": "ActiveStandbyElectorBasedElectorService is failing with NPE",
            "Description": "In RM-HA environment, killing the ZK leader and performing RM failover sometimes causes the active RM to throw a NullPointerException and fail to start successfully.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)"
            ],
            "StepsToReproduce": [
                "Kill the ZK leader in the RM-HA environment.",
                "Perform RM failover."
            ],
            "ExpectedBehavior": "The active ResourceManager should come up successfully after failover.",
            "ObservedBehavior": "The active ResourceManager throws a NullPointerException and fails to start.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8223.json",
        "creation_time": "2018-04-27T11:49:02.000+0000",
        "bug_report": {
            "BugID": "YARN-8223",
            "Title": "ClassNotFoundException when auxiliary service is loaded from HDFS",
            "Description": "Loading an auxiliary jar from HDFS fails with a ClassNotFoundException, while loading from a local location works as expected.",
            "StackTrace": [
                "java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:381)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:424)",
                "at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:357)",
                "at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)",
                "at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:348)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)"
            ],
            "StepsToReproduce": [
                "Load an auxiliary jar from a local location on a node manager.",
                "Load the same jar from a location on HDFS."
            ],
            "ExpectedBehavior": "The auxiliary service should load successfully from HDFS without any exceptions.",
            "ObservedBehavior": "A ClassNotFoundException occurs when loading the auxiliary service from HDFS.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8331.json",
        "creation_time": "2018-05-21T05:19:35.000+0000",
        "bug_report": {
            "BugID": "YARN-8331",
            "Title": "Race condition in NM container launched after done",
            "Description": "A race condition occurs when a container is launched, transitioning through various states, and an event is sent that leads to an invalid state transition.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The container should transition to the appropriate state without encountering an invalid state transition.",
            "ObservedBehavior": "The container transitions to DONE state and cannot handle the CONTAINER_LAUNCHED event, resulting in an InvalidStateTransitionException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2931.json",
        "creation_time": "2014-12-08T21:09:13.000+0000",
        "bug_report": {
            "BugID": "YARN-2931",
            "Title": "PublicLocalizer may fail until directory is initialized by LocalizeRunner",
            "Description": "When the data directory is cleaned up and NM is started with existing recovery state, it will not recreate the local dirs, causing a PublicLocalizer to fail until getInitializedLocalDirs is called.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)",
                "at org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The local directories should be recreated successfully when the NodeManager is started.",
            "ObservedBehavior": "The PublicLocalizer fails due to the absence of the local directory.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6837.json",
        "creation_time": "2017-07-18T11:17:55.000+0000",
        "bug_report": {
            "BugID": "YARN-6837",
            "Title": "Null LocalResource visibility or resource type can crash the nodemanager",
            "Description": "The issue occurs when a LocalResource is created without setting its visibility, leading to a NullPointerException in the NodeManager's log and causing it to shut down.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.addResources(ResourceSet.java:84)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:868)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:819)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1684)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1418)"
            ],
            "StepsToReproduce": [
                "Create a LocalResource without setting its visibility.",
                "Submit the job to the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should handle the LocalResource correctly without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when the LocalResource visibility is not set.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4762.json",
        "creation_time": "2016-03-04T02:24:47.000+0000",
        "bug_report": {
            "BugID": "YARN-4762",
            "Title": "NMs failing on DelegatingLinuxContainerRuntime init with LCE on",
            "Description": "Node Managers are crashing due to an exception related to the initialization of the Linux Container Executor.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)",
                "Caused by: java.io.IOException: Failed to initialize linux container runtime(s)!",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)"
            ],
            "StepsToReproduce": [
                "Enable LinuxContainerExecutor.",
                "Start the NodeManager."
            ],
            "ExpectedBehavior": "NodeManager should initialize without crashing.",
            "ObservedBehavior": "NodeManager fails to start and crashes with an initialization error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2823.json",
        "creation_time": "2014-11-06T21:38:47.000+0000",
        "bug_report": {
            "BugID": "YARN-2823",
            "Title": "NullPointerException in RM HA enabled 3-node cluster",
            "Description": "A NullPointerException occurs in the ResourceManager when handling application attempts in a 3-node cluster with ResourceManager High Availability (HA) enabled.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should handle application attempts without throwing a NullPointerException.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException and fails to recover application attempts.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5098.json",
        "creation_time": "2016-05-17T00:43:08.000+0000",
        "bug_report": {
            "BugID": "YARN-5098",
            "Title": "Yarn Application log Aggregation fails due to NM cannot get correct HDFS delegation token",
            "Description": "Yarn application logs for a long-running application could not be gathered because the NodeManager failed to communicate with HDFS due to an invalid token error.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:583)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:398)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:752)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1597)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1439)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The NodeManager should successfully retrieve the HDFS delegation token and aggregate application logs.",
            "ObservedBehavior": "The NodeManager fails to retrieve the HDFS delegation token, resulting in an inability to gather application logs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3971.json",
        "creation_time": "2015-07-24T10:17:05.000+0000",
        "bug_report": {
            "BugID": "YARN-3971",
            "Title": "Skip RMNodeLabelsManager#checkRemoveFromClusterNodeLabelsOfQueue on nodelabel recovery",
            "Description": "An exception occurs during the recovery of node labels, causing both Resource Managers to become standby.",
            "StackTrace": [
                "java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue(RMNodeLabelsManager.java:104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels(RMNodeLabelsManager.java:118)",
                "at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.recover(FileSystemNodeLabelsStore.java:221)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:232)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:245)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)"
            ],
            "StepsToReproduce": [
                "Create label x,y",
                "Delete label x,y",
                "Create label x,y and add capacity scheduler xml for labels x and y",
                "Restart RM"
            ],
            "ExpectedBehavior": "Resource Managers should not become standby during node label recovery.",
            "ObservedBehavior": "Both Resource Managers become standby due to an IOException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6948.json",
        "creation_time": "2017-08-04T08:23:46.000+0000",
        "bug_report": {
            "BugID": "YARN-6948",
            "Title": "Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
            "Description": "An exception occurs when sending a kill command to a running job, indicating that the system cannot handle the event in its current state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Send a kill command to a running job.",
                "Check the logs for exceptions."
            ],
            "ExpectedBehavior": "The system should handle the kill command without throwing an exception.",
            "ObservedBehavior": "An exception is thrown indicating an invalid event due to the current state of the application.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1409.json",
        "creation_time": "2013-11-13T11:25:56.000+0000",
        "bug_report": {
            "BugID": "YARN-1409",
            "Title": "NonAggregatingLogHandler can throw RejectedExecutionException",
            "Description": "This problem is caused by handling APPLICATION_FINISHED events after calling sched.shotdown() in NonAggregatingLongHandler#serviceStop().",
            "StackTrace": [
                "2013-11-13 10:53:06,970 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(166)) - Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle APPLICATION_FINISHED events without throwing exceptions.",
            "ObservedBehavior": "The system throws a RejectedExecutionException when handling APPLICATION_FINISHED events.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5545.json",
        "creation_time": "2016-08-21T12:57:35.000+0000",
        "bug_report": {
            "BugID": "YARN-5545",
            "Title": "Fix issues related to Max App in capacity scheduler",
            "Description": "Issues as part of Max apps in Capacity scheduler, including application submission failures when the default partition capacity is set to zero.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)",
                "at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)"
            ],
            "StepsToReproduce": [
                "Configure capacity scheduler with yarn.scheduler.capacity.root.default.capacity=0",
                "Set yarn.scheduler.capacity.root.queue1.accessible-node-labels.labelx.capacity=50",
                "Set yarn.scheduler.capacity.root.default.accessible-node-labels.labelx.capacity=50",
                "Submit application using the command: ./yarn jar ../share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-alpha2-SNAPSHOT-tests.jar sleep -Dmapreduce.job.node-label-expression=labelx -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 10000000 -rt 1"
            ],
            "ExpectedBehavior": "The application should be submitted successfully to the queue.",
            "ObservedBehavior": "The application submission fails with an AccessControlException due to the queue having zero applications.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-301.json",
        "creation_time": "2013-01-01T05:40:18.000+0000",
        "bug_report": {
            "BugID": "YARN-301",
            "Title": "Fair scheduler throws ConcurrentModificationException when iterating over app's priorities",
            "Description": "In my test cluster, fairscheduler appears to throw a ConcurrentModificationException, causing the ResourceManager to crash.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)",
                "at java.util.TreeMap$KeyIterator.nextEntry(TreeMap.java:1154)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:181)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:780)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:842)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:340)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The fair scheduler should handle node updates without throwing exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, leading to a crash of the ResourceManager.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7942.json",
        "creation_time": "2018-02-16T19:09:39.000+0000",
        "bug_report": {
            "BugID": "YARN-7942",
            "Title": "Yarn ServiceClient does not not delete znode from secure ZooKeeper",
            "Description": "The ResourceManager fails to remove a znode from ZooKeeper despite having the correct permissions set, resulting in a NoPathPermissionsException.",
            "StackTrace": [
                "org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:412)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:722)",
                "at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.delete(RegistryOperationsService.java:162)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionDestroy(ServiceClient.java:462)",
                "Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should successfully delete the znode from ZooKeeper when the correct permissions are set.",
            "ObservedBehavior": "The ResourceManager fails to delete the znode and throws a NoPathPermissionsException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7692.json",
        "creation_time": "2017-12-29T06:00:34.000+0000",
        "bug_report": {
            "BugID": "YARN-7692",
            "Title": "Skip validating priority acls while recovering applications",
            "Description": "When a job is submitted by a user not included in the ACLs after enabling them, the job is rejected as expected. However, the Resource Manager crashes while trying to recover previous applications.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2348)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:358)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:567)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1390)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1143)"
            ],
            "StepsToReproduce": [
                "1. Create a cluster without ACLs.",
                "2. Submit jobs with an existing user 'user_a'.",
                "3. Enable ACLs and create a priority ACL entry without including 'user_a'.",
                "4. Submit a job with 'user_a'."
            ],
            "ExpectedBehavior": "The job should be rejected as 'user_a' does not have permission to run the job.",
            "ObservedBehavior": "The job is rejected, but the Resource Manager crashes while trying to recover previous applications.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3917.json",
        "creation_time": "2015-07-11T00:41:28.000+0000",
        "bug_report": {
            "BugID": "YARN-3917",
            "Title": "getResourceCalculatorPlugin for the default should intercept all exceptions",
            "Description": "The default resource calculator instantiation fails due to an UnsupportedOperationException when the OS cannot be determined.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: Could not determine OS",
                "at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The resource calculator plugin should handle exceptions without failing the service initialization.",
            "ObservedBehavior": "The service fails to initialize due to an UnsupportedOperationException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3537.json",
        "creation_time": "2015-04-23T11:34:23.000+0000",
        "bug_report": {
            "BugID": "YARN-3537",
            "Title": "NPE when NodeManager.serviceInit fails and stopRecoveryStore invoked",
            "Description": "The NodeManager service fails with a NullPointerException when the serviceInit method is called and stopRecoveryStore is invoked.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore(NodeManager.java:181)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:326)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:106)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The NodeManager should handle service initialization without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the service initialization process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7962.json",
        "creation_time": "2018-02-22T22:32:20.000+0000",
        "bug_report": {
            "BugID": "YARN-7962",
            "Title": "Race Condition When Stopping DelegationTokenRenewer causes RM crash during failover",
            "Description": "A race condition occurs when stopping the DelegationTokenRenewer, leading to a crash of the ResourceManager during failover.",
            "StackTrace": [
                "java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager.",
                "Trigger a failover while the DelegationTokenRenewer is active.",
                "Stop the DelegationTokenRenewer."
            ],
            "ExpectedBehavior": "The ResourceManager should stop the DelegationTokenRenewer without crashing.",
            "ObservedBehavior": "The ResourceManager crashes due to a rejected execution exception when trying to process a token renewal event after the service has stopped.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8357.json",
        "creation_time": "2018-05-24T16:46:57.000+0000",
        "bug_report": {
            "BugID": "YARN-8357",
            "Title": "Yarn Service: NPE when service is saved first and then started.",
            "Description": "A NullPointerException occurs when attempting to start a service that was saved without an application ID.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)"
            ],
            "StepsToReproduce": [
                "Save a service without an application ID.",
                "Attempt to start the saved service."
            ],
            "ExpectedBehavior": "The service should start without errors.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the service from starting.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6534.json",
        "creation_time": "2017-04-26T21:43:52.000+0000",
        "bug_report": {
            "BugID": "YARN-6534",
            "Title": "ResourceManager failed due to TimelineClient try to init SSLFactory even https is not enabled",
            "Description": "In a non-secured cluster, RM fails consistently because TimelineServiceV1Publisher attempts to initialize TimelineClient with SSLFactory without checking if HTTPS is used.",
            "StackTrace": [
                "2017-04-26 21:09:10,683 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(1457)) - Error starting ResourceManager",
                "org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)",
                "Caused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "ResourceManager should start successfully without requiring SSL when HTTPS is not enabled.",
            "ObservedBehavior": "ResourceManager fails to start due to missing SSL configuration file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4227.json",
        "creation_time": "2015-10-06T04:59:10.000+0000",
        "bug_report": {
            "BugID": "YARN-4227",
            "Title": "Ignore expired containers from removed nodes in FairScheduler",
            "Description": "Under some circumstances, the node is removed before an expired container event is processed, causing the ResourceManager to exit.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:849)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1273)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:122)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:585)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should handle expired containers without exiting.",
            "ObservedBehavior": "The ResourceManager exits with a NullPointerException when handling expired containers from removed nodes.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2649.json",
        "creation_time": "2014-10-06T22:57:46.000+0000",
        "bug_report": {
            "BugID": "YARN-2649",
            "Title": "Flaky test TestAMRMRPCNodeUpdates",
            "Description": "The test fails intermittently due to an incorrect AppAttempt state, where the expected state is ALLOCATED but the observed state is SCHEDULED.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>",
                "at junit.framework.Assert.fail(Assert.java:50)",
                "at junit.framework.Assert.failNotEquals(Assert.java:287)",
                "at junit.framework.Assert.assertEquals(Assert.java:67)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)",
                "at org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)"
            ],
            "StepsToReproduce": [
                "Run the test TestAMRMRPCNodeUpdates.",
                "Use a custom AsyncDispatcher with CountDownLatch."
            ],
            "ExpectedBehavior": "The AppAttempt state should transition to ALLOCATED after being submitted.",
            "ObservedBehavior": "The AppAttempt state transitions to SCHEDULED instead of ALLOCATED.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4288.json",
        "creation_time": "2015-10-22T12:30:16.000+0000",
        "bug_report": {
            "BugID": "YARN-4288",
            "Title": "NodeManager restart should keep retrying to register to RM while connection exception happens during RM failed over.",
            "Description": "When the NodeManager (NM) is restarted, it attempts to register with the ResourceManager (RM). If the RM is also restarting, this can lead to connection exceptions, causing the NM to fail to restart properly.",
            "StackTrace": [
                "java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1473)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)",
                "Caused by: java.io.IOException: Connection reset by peer"
            ],
            "StepsToReproduce": [
                "Restart the NodeManager while the ResourceManager is also restarting."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager even if there are connection issues during the RM's restart.",
            "ObservedBehavior": "The NodeManager fails to restart due to connection reset exceptions when trying to register with the ResourceManager.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1032.json",
        "creation_time": "2013-08-05T21:10:46.000+0000",
        "bug_report": {
            "BugID": "YARN-1032",
            "Title": "NPE in RackResolve",
            "Description": "A NullPointerException occurred in the RackResolver due to a problem with resolving host addresses, which was caught in RMContainerAllocator.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)",
                "at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$400(RMContainerAllocator.java:681)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The RackResolver should successfully resolve the rack without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the resolution of the rack.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5837.json",
        "creation_time": "2016-11-04T16:06:59.000+0000",
        "bug_report": {
            "BugID": "YARN-5837",
            "Title": "NPE when getting node status of a decommissioned node after an RM restart",
            "Description": "A NullPointerException occurs when attempting to retrieve the status of a decommissioned node after restarting the ResourceManager.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)"
            ],
            "StepsToReproduce": [
                "Decommission a node using the yarn command.",
                "Restart the ResourceManager.",
                "Attempt to get the status of the decommissioned node using the yarn command."
            ],
            "ExpectedBehavior": "The system should return the status of the decommissioned node without errors.",
            "ObservedBehavior": "The system throws a NullPointerException when trying to get the status of the decommissioned node.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6827.json",
        "creation_time": "2017-07-15T05:14:25.000+0000",
        "bug_report": {
            "BugID": "YARN-6827",
            "Title": "[ATS1/1.5] NPE exception while publishing recovering applications into ATS during RM restart.",
            "Description": "A NullPointerException (NPE) occurs when recovering applications during ResourceManager (RM) restart, due to the timing of service starts in non-HA and HA cases.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:178)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.putEntity(TimelineServiceV1Publisher.java:368)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Applications should be published into ATS without throwing exceptions during RM restart.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to publish applications into ATS before the ATS services are started.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3832.json",
        "creation_time": "2015-06-19T13:31:18.000+0000",
        "bug_report": {
            "BugID": "YARN-3832",
            "Title": "Resource Localization fails on a cluster due to existing cache directories",
            "Description": "Resource localization fails on a cluster with an error indicating that a rename operation cannot overwrite a non-empty destination directory.",
            "StackTrace": [
                "java.io.IOException: Rename cannot overwrite non empty destination directory /opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39",
                "at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:735)",
                "at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:244)",
                "at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:678)",
                "at org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The resource localization should succeed without errors.",
            "ObservedBehavior": "The application fails with an error indicating that it cannot overwrite a non-empty directory.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2409.json",
        "creation_time": "2014-08-12T10:53:06.000+0000",
        "bug_report": {
            "BugID": "YARN-2409",
            "Title": "Active to StandBy transition does not stop rmDispatcher that causes 1 AsyncDispatcher thread leak.",
            "Description": "The transition from Active to StandBy does not properly stop the rmDispatcher, leading to a thread leak in the AsyncDispatcher.",
            "StackTrace": [
                "at java.lang.Thread.run(Thread.java:662)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The rmDispatcher should stop properly during the Active to StandBy transition.",
            "ObservedBehavior": "The rmDispatcher continues running, causing a thread leak.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8116.json",
        "creation_time": "2018-04-04T15:30:52.000+0000",
        "bug_report": {
            "BugID": "YARN-8116",
            "Title": "Nodemanager fails with NumberFormatException: For input string: \"\"",
            "Description": "Nodemanager fails to start after updating the debug delay config and launching a distributed shell application multiple times.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)",
                "at java.lang.Long.parseLong(Long.java:601)",
                "at java.lang.Long.parseLong(Long.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)"
            ],
            "StepsToReproduce": [
                "Update nodemanager debug delay config with yarn.nodemanager.delete.debug-delay-sec set to 350.",
                "Launch distributed shell application multiple times using the specified yarn command.",
                "Restart NodeManager."
            ],
            "ExpectedBehavior": "NodeManager should start successfully without errors.",
            "ObservedBehavior": "NodeManager fails to start with a NumberFormatException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8403.json",
        "creation_time": "2018-06-06T22:34:42.000+0000",
        "bug_report": {
            "BugID": "YARN-8403",
            "Title": "Nodemanager logs failed to download file with INFO level",
            "Description": "Some of the container execution related stack traces are printing in INFO or WARN level instead of ERROR level, which is not ideal for exception logging.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed",
                "Caused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)",
                "Caused by: java.io.InterruptedIOException: java.lang.InterruptedException"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Exceptions should be logged at ERROR level.",
            "ObservedBehavior": "Exceptions are logged at INFO or WARN level.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1458.json",
        "creation_time": "2013-11-29T03:31:39.000+0000",
        "bug_report": {
            "BugID": "YARN-1458",
            "Title": "FairScheduler: Zero weight can lead to livelock",
            "Description": "The ResourceManager$SchedulerEventDispatcher$EventProcessor is blocked when clients submit many jobs, making it difficult to reproduce the issue. The problem was observed during long-running tests.",
            "StackTrace": [
                "java.lang.Thread.State: BLOCKED (on object monitor)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplication(FairScheduler.java:671)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1023)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:440)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should handle job submissions without blocking.",
            "ObservedBehavior": "The ResourceManager becomes blocked, preventing it from processing job submissions.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8209.json",
        "creation_time": "2018-04-26T00:22:23.000+0000",
        "bug_report": {
            "BugID": "YARN-8209",
            "Title": "NPE in DeletionService",
            "Description": "A NullPointerException occurs in the DeletionService when attempting to delete a Docker container.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile(DockerClient.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand(DockerCommandExecutor.java:85)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeStatusCommand(DockerCommandExecutor.java:192)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.getContainerStatus(DockerCommandExecutor.java:128)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer(LinuxContainerExecutor.java:935)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask.run(DockerContainerDeletionTask.java:61)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully delete Docker containers without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the deletion process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3804.json",
        "creation_time": "2015-06-15T08:54:42.000+0000",
        "bug_report": {
            "BugID": "YARN-3804",
            "Title": "Both RM are on standBy state when kerberos user not in yarn.admin.acl",
            "Description": "When configuring a cluster in secure mode, both Resource Managers (RM) remain in standby indefinitely due to permission issues with the kerberos user not being included in the yarn.admin.acl.",
            "StackTrace": [
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "Caused by: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:182)"
            ],
            "StepsToReproduce": [
                "1. Configure cluster in secure mode",
                "2. On RM, configure yarn.admin.acl=dsperf",
                "3. Set yarn.resourcemanager.principal=yarn",
                "4. Start both RMs"
            ],
            "ExpectedBehavior": "The RM should receive a shutdown event after a few retries or even at the first attempt if the user does not have the necessary permissions.",
            "ObservedBehavior": "Both RMs remain in standby state indefinitely due to permission issues.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1839.json",
        "creation_time": "2014-03-14T23:52:29.000+0000",
        "bug_report": {
            "BugID": "YARN-1839",
            "Title": "Capacity scheduler preempts an AM out. AM attempt 2 fails to launch task container with SecretManager$InvalidToken: No NMToken sent",
            "Description": "When using a single-node cluster with capacity scheduler preemption, the second attempt of the application master fails to launch a task container due to an invalid token error.",
            "StackTrace": [
                "org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "Use single-node cluster.",
                "Turn on capacity scheduler preemption.",
                "Run MR sleep job as app 1.",
                "Take entire cluster.",
                "Run MR sleep job as app 2.",
                "Preempt app 1 out.",
                "Wait till app 2 finishes.",
                "Observe the AM attempt 2 for app 1."
            ],
            "ExpectedBehavior": "The application master should successfully launch task containers after preemption.",
            "ObservedBehavior": "The application master fails to launch task containers due to an InvalidToken exception.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6714.json",
        "creation_time": "2017-06-15T09:56:15.000+0000",
        "bug_report": {
            "BugID": "YARN-6714",
            "Title": "IllegalStateException while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler",
            "Description": "In async-scheduling mode of CapacityScheduler, after AM failover and unreserving all reserved containers, an outdated reserve proposal can be committed, leading to an IllegalStateException and crashing the ResourceManager.",
            "StackTrace": [
                "java.lang.IllegalStateException: Trying to unreserve for application appattempt_1495188831758_0121_000002 when currently reserved for application application_1495188831758_0121 on node host: node1:45454",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should not allow committing outdated reserve proposals after AM failover.",
            "ObservedBehavior": "The ResourceManager crashes due to an IllegalStateException when trying to unreserve resources for a failed application attempt.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3351.json",
        "creation_time": "2015-03-16T14:19:59.000+0000",
        "bug_report": {
            "BugID": "YARN-3351",
            "Title": "AppMaster tracking URL is broken in HA",
            "Description": "After YARN-2713, the AppMaster link is broken in HA when the first RM is not active.",
            "StackTrace": [
                "java.net.BindException: Cannot assign requested address",
                "at java.net.PlainSocketImpl.socketBind(Native Method)",
                "at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)",
                "at java.net.Socket.bind(Socket.java:631)",
                "at java.net.Socket.<init>(Socket.java:423)",
                "at java.net.Socket.<init>(Socket.java:280)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)",
                "at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)"
            ],
            "StepsToReproduce": [
                "Setup RM HA and ensure the first RM is not active.",
                "Run a long sleep job and view the tracking URL on the RM applications page."
            ],
            "ExpectedBehavior": "The AppMaster tracking URL should be accessible and functional.",
            "ObservedBehavior": "The AppMaster tracking URL is broken, resulting in a BindException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2813.json",
        "creation_time": "2014-11-05T22:29:46.000+0000",
        "bug_report": {
            "BugID": "YARN-2813",
            "Title": "NPE from MemoryTimelineStore.getDomains",
            "Description": "A NullPointerException occurs in the MemoryTimelineStore.getDomains method, leading to an INTERNAL_SERVER_ERROR.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)",
                "at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should return the domains without throwing an exception.",
            "ObservedBehavior": "The system throws a NullPointerException, resulting in an INTERNAL_SERVER_ERROR.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1550.json",
        "creation_time": "2013-12-30T03:58:32.000+0000",
        "bug_report": {
            "BugID": "YARN-1550",
            "Title": "NPE in FairSchedulerAppsBlock#render",
            "Description": "A NullPointerException occurs in the FairSchedulerAppsBlock#render method when trying to access the scheduler page after submitting an application.",
            "StackTrace": [
                "java.lang.reflect.InvocationTargetException",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock.render(FairSchedulerAppsBlock.java:96)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:66)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:76)"
            ],
            "StepsToReproduce": [
                "1. Debug at RMAppManager#submitApplication after the code block that checks for duplicate applications.",
                "2. Submit an application using the command: hadoop jar ~/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.0.0-ydh2.2.0-tests.jar sleep -Dhadoop.job.ugi=test2,#111111 -Dmapreduce.job.queuename=p1 -m 1 -mt 1 -r 1",
                "3. Go to the page: http://ip:50030/cluster/scheduler and observe the 500 ERROR."
            ],
            "ExpectedBehavior": "The scheduler page should display the applications without errors.",
            "ObservedBehavior": "A 500 ERROR is displayed on the scheduler page due to a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5006.json",
        "creation_time": "2016-04-28T08:26:38.000+0000",
        "bug_report": {
            "BugID": "YARN-5006",
            "Title": "ResourceManager quit due to ApplicationStateData exceed the limit size of znode in zk",
            "Description": "When a job adds 10,000 files into DistributedCache, the ResourceManager attempts to store ApplicationStateData into Zookeeper, exceeding the znode size limit, causing the ResourceManager to exit.",
            "StackTrace": [
                "org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:936)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1075)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:138)"
            ],
            "StepsToReproduce": [
                "Submit a job that adds 10,000 files into DistributedCache.",
                "Monitor the ResourceManager logs for errors related to znode size."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully store ApplicationStateData without exceeding znode size limits.",
            "ObservedBehavior": "The ResourceManager exits with a ConnectionLossException due to exceeding the znode size limit in Zookeeper.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5728.json",
        "creation_time": "2016-10-13T05:16:28.000+0000",
        "bug_report": {
            "BugID": "YARN-5728",
            "Title": "TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization timeout",
            "Description": "The test TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization is failing due to a timeout after 60 seconds.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 60000 milliseconds",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.processWaitTimeAndRetryInfo(RetryInvocationHandler.java:130)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:107)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy85.nodeHeartbeat(Unknown Source)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:113)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The test should complete successfully within the allotted time.",
            "ObservedBehavior": "The test fails due to a timeout after 60 seconds.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2805.json",
        "creation_time": "2014-11-04T20:37:09.000+0000",
        "bug_report": {
            "BugID": "YARN-2805",
            "Title": "RM2 in HA setup tries to login using the RM1's kerberos principal",
            "Description": "ResourceManager fails to start due to a login failure when using Kerberos authentication in a High Availability setup.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)",
                "Caused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "ResourceManager should successfully start and authenticate using the correct Kerberos principal.",
            "ObservedBehavior": "ResourceManager fails to start due to a login failure with the error message indicating a problem with the Kerberos principal.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4744.json",
        "creation_time": "2016-02-29T10:08:57.000+0000",
        "bug_report": {
            "BugID": "YARN-4744",
            "Title": "Too many signal to container failure in case of LCE",
            "Description": "The issue occurs when submitting a MapReduce application (terasort/teragen) with the dsperf user, leading to multiple signal failures for the container.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=9:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer(DefaultLinuxContainerRuntime.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.signalContainer(DelegatingLinuxContainerRuntime.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.signalContainer(LinuxContainerExecutor.java:513)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:520)"
            ],
            "StepsToReproduce": [
                "Install HA cluster in secure mode",
                "Enable LCE with cgroups",
                "Start server with dsperf user",
                "Submit mapreduce application terasort/teragen with user yarn/dsperf"
            ],
            "ExpectedBehavior": "The container should execute the MapReduce application without signal failures.",
            "ObservedBehavior": "The application fails with multiple signal errors, indicating issues with container execution.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1752.json",
        "creation_time": "2014-02-22T05:51:42.000+0000",
        "bug_report": {
            "BugID": "YARN-1752",
            "Title": "Unexpected Unregistered event at Attempt Launched state",
            "Description": "An error occurs when an unregistered event is received while the application attempt is in the launched state.",
            "StackTrace": [
                "2014-02-21 14:56:03,453 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle events appropriately based on the current state of the application attempt.",
            "ObservedBehavior": "The system throws an InvalidStateTransitonException when an unregistered event is received in the launched state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6629.json",
        "creation_time": "2017-05-22T08:31:16.000+0000",
        "bug_report": {
            "BugID": "YARN-6629",
            "Title": "NPE occurred when container allocation proposal is applied but its resource requests are removed before",
            "Description": "A NullPointerException (NPE) occurs during the handling of an event type NODE_UPDATE in the Event Dispatcher when a container allocation proposal is applied after its resource requests have been removed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:446)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply(FiCaSchedulerApp.java:516)",
                "at org.apache.hadoop.yarn.client.TestNegativePendingResource$1.answer(TestNegativePendingResource.java:225)",
                "at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:31)",
                "at org.mockito.internal.MockHandler.handle(MockHandler.java:97)",
                "at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp$$EnhancerByMockitoWithCGLIB$$29eb8afc.apply(<generated>)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit(CapacityScheduler.java:2396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.submitResourceCommitRequest(CapacityScheduler.java:2281)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1247)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1236)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1325)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:987)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1367)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. AM started and requested 1 container with schedulerRequestKey#1.",
                "2. Scheduler allocated 1 container for this request and accepted the proposal.",
                "3. AM removed this request.",
                "4. Scheduler applied this proposal, leading to NPE."
            ],
            "ExpectedBehavior": "The system should handle the container allocation proposal without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown when the resource requests are removed before the allocation proposal is applied.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3493.json",
        "creation_time": "2015-04-15T22:03:19.000+0000",
        "bug_report": {
            "BugID": "YARN-3493",
            "Title": "RM fails to come up with error \"Failed to load/recover state\" when mem settings are changed",
            "Description": "ResourceManager (RM) fails to start after changing memory settings in yarn-site.xml and restoring them before a job completes.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)"
            ],
            "StepsToReproduce": [
                "Change yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb to 4000 in yarn-site.xml",
                "Start a randomtextwriter job with mapreduce.map.memory.mb=4000 in background and wait for the job to reach running state",
                "Restore yarn-site.xml to have yarn.scheduler.maximum-allocation-mb to 2048 before the above job completes",
                "Restart RM"
            ],
            "ExpectedBehavior": "ResourceManager should start successfully without errors.",
            "ObservedBehavior": "ResourceManager fails to start with an error related to invalid memory requests.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7645.json",
        "creation_time": "2017-12-12T21:19:53.000+0000",
        "bug_report": {
            "BugID": "YARN-7645",
            "Title": "TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers is flakey with FairScheduler",
            "Description": "The test case for resource usage after an Application Master restart is flaky when using FairScheduler, leading to assertion errors.",
            "StackTrace": [
                "java.lang.AssertionError: Attempt state is not correct (timeout). expected:<ALLOCATED> but was:<SCHEDULED>",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.amRestartTests(TestContainerResourceUsage.java:275)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.testUsageAfterAMRestartWithMultipleContainers(TestContainerResourceUsage.java:254)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application master should transition to the ALLOCATED state after a restart.",
            "ObservedBehavior": "The application master remains in the SCHEDULED state instead of transitioning to ALLOCATED.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6054.json",
        "creation_time": "2017-01-04T20:58:59.000+0000",
        "bug_report": {
            "BugID": "YARN-6054",
            "Title": "TimelineServer fails to start when some LevelDb state files are missing.",
            "Description": "The TimelineServer fails to start due to missing state files, leading to a service state exception.",
            "StackTrace": [
                "org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)",
                "Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The TimelineServer should start successfully without missing state files.",
            "ObservedBehavior": "The TimelineServer fails to start due to missing state files, resulting in a service state exception.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-196.json",
        "creation_time": "2012-01-16T09:52:45.000+0000",
        "bug_report": {
            "BugID": "YARN-196",
            "Title": "Nodemanager should be more robust in handling connection failure to ResourceManager when a cluster is started",
            "Description": "If NodeManager (NM) is started before ResourceManager (RM), NM shuts down with an error related to connection failure.",
            "StackTrace": [
                "ERROR org.apache.hadoop.yarn.service.CompositeService: Error starting services org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)",
                "Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused",
                "Caused by: java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)"
            ],
            "StepsToReproduce": [
                "Start NodeManager before starting ResourceManager."
            ],
            "ExpectedBehavior": "NodeManager should handle connection failures gracefully without shutting down.",
            "ObservedBehavior": "NodeManager shuts down with an error when it cannot connect to ResourceManager.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8508.json",
        "creation_time": "2018-07-09T23:37:49.000+0000",
        "bug_report": {
            "BugID": "YARN-8508",
            "Title": "On NodeManager container gets cleaned up before its pid file is created",
            "Description": "GPU failed to release even though the container using it is being killed. New container requesting for GPU fails to launch due to insufficient available GPUs.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.internalAssignGpus(GpuResourceAllocator.java:225)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.assignGpus(GpuResourceAllocator.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl.preStart(GpuResourceHandlerImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.preStart(ResourceHandlerChain.java:75)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:509)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The GPU should be released properly when the container is killed, and new containers should launch successfully if resources are available.",
            "ObservedBehavior": "The GPU fails to release, and new container requests for GPUs fail due to insufficient available GPUs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2308.json",
        "creation_time": "2014-07-17T10:01:57.000+0000",
        "bug_report": {
            "BugID": "YARN-2308",
            "Title": "NPE happened when RM restart after CapacityScheduler queue configuration changed",
            "Description": "A NullPointerException (NPE) occurs when the ResourceManager (RM) is restarted after changes to the CapacityScheduler queue configuration, leading to RM failure.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "Change the queue configuration by removing some queues and adding new ones.",
                "Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully and recover all applications.",
            "ObservedBehavior": "The ResourceManager fails to restart due to a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-933.json",
        "creation_time": "2013-07-17T12:29:28.000+0000",
        "bug_report": {
            "BugID": "YARN-933",
            "Title": "Potential InvalidStateTransitonException: Invalid event: LAUNCHED at FINAL_SAVING",
            "Description": "The application fails to transition states correctly after a connection loss, leading to an InvalidStateTransitonException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: LAUNCH_FAILED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:630)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:495)"
            ],
            "StepsToReproduce": [
                "Step 1: Install cluster with NM on 2 Machines",
                "Step 2: Make Ping using IP from RM machine to NM1 machine as successful, but using Hostname should fail",
                "Step 3: Execute a job",
                "Step 4: After AM [AppAttempt_1] allocation to NM1 machine is done, connection loss happened."
            ],
            "ExpectedBehavior": "The application should handle connection loss gracefully and not attempt to relaunch a failed AppAttempt.",
            "ObservedBehavior": "The application fails with InvalidStateTransitonException when trying to relaunch AppAttempt_1 after it has already failed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1374.json",
        "creation_time": "2013-10-30T11:49:49.000+0000",
        "bug_report": {
            "BugID": "YARN-1374",
            "Title": "Resource Manager fails to start due to ConcurrentModificationException",
            "Description": "Resource Manager is failing to start with a ConcurrentModificationException.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)",
                "at java.util.AbstractList$Itr.next(AbstractList.java:343)",
                "at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Resource Manager should start without errors.",
            "ObservedBehavior": "The Resource Manager fails to start and throws a ConcurrentModificationException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-174.json",
        "creation_time": "2012-10-19T17:25:40.000+0000",
        "bug_report": {
            "BugID": "YARN-174",
            "Title": "TestNodeStatusUpdater is failing in trunk",
            "Description": "Error starting NodeManager due to invalid log directory path.",
            "StackTrace": [
                "org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "NodeManager should start without errors.",
            "ObservedBehavior": "NodeManager fails to start due to invalid log directory path.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6448.json",
        "creation_time": "2017-04-05T18:39:49.000+0000",
        "bug_report": {
            "BugID": "YARN-6448",
            "Title": "Continuous scheduling thread crashes while sorting nodes",
            "Description": "The continuous scheduling thread in YARN crashes due to a violation of the comparison method's general contract while sorting nodes.",
            "StackTrace": [
                "2017-04-04 23:42:26,123 FATAL org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler: Critical thread FairSchedulerContinuousScheduling crashed!",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:899)",
                "at java.util.TimSort.mergeAt(TimSort.java:516)",
                "at java.util.TimSort.mergeForceCollapse(TimSort.java:457)",
                "at java.util.TimSort.sort(TimSort.java:254)",
                "at java.util.Arrays.sort(Arrays.java:1512)",
                "at java.util.ArrayList.sort(ArrayList.java:1454)",
                "at java.util.Collections.sort(Collections.java:175)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The continuous scheduling thread should sort nodes without crashing.",
            "ObservedBehavior": "The continuous scheduling thread crashes with an IllegalArgumentException during sorting.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4530.json",
        "creation_time": "2015-12-30T15:19:19.000+0000",
        "bug_report": {
            "BugID": "YARN-4530",
            "Title": "LocalizedResource trigger a NPE Cause the NodeManager exit",
            "Description": "In our cluster, I found that LocalizedResource download failed trigger a NPE causing the NodeManager to shut down.",
            "StackTrace": [
                "java.lang.NullPointerException at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The NodeManager should handle resource download failures without crashing.",
            "ObservedBehavior": "The NodeManager shuts down due to a NullPointerException when a resource download fails.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7737.json",
        "creation_time": "2018-01-11T19:35:01.000+0000",
        "bug_report": {
            "BugID": "YARN-7737",
            "Title": "prelaunch.err file not found exception on container failure",
            "Description": "An exception occurs when a container fails due to the absence of the prelaunch error log file.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The prelaunch error log file should be created and accessible when a container fails.",
            "ObservedBehavior": "The system throws a FileNotFoundException indicating that the prelaunch error log file does not exist.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5136.json",
        "creation_time": "2016-05-24T15:34:28.000+0000",
        "bug_report": {
            "BugID": "YARN-5136",
            "Title": "Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
            "Description": "The application removal process in the ResourceManager fails with an IllegalStateException when attempting to remove an application that does not exist in the queue.",
            "StackTrace": [
                "java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should successfully handle the removal of application attempts without throwing an exception.",
            "ObservedBehavior": "An IllegalStateException is thrown when trying to remove an application that does not exist in the queue.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8211.json",
        "creation_time": "2018-04-26T02:13:22.000+0000",
        "bug_report": {
            "BugID": "YARN-8211",
            "Title": "Yarn registry dns log finds BufferUnderflowException on port ping",
            "Description": "Yarn registry dns server is constantly getting BufferUnderflowException.",
            "StackTrace": [
                "java.nio.BufferUnderflowException",
                "at java.nio.Buffer.nextGetIndex(Buffer.java:500)",
                "at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The DNS server should handle requests without throwing exceptions.",
            "ObservedBehavior": "The DNS server throws a BufferUnderflowException during operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2124.json",
        "creation_time": "2014-06-05T07:44:27.000+0000",
        "bug_report": {
            "BugID": "YARN-2124",
            "Title": "ProportionalCapacityPreemptionPolicy cannot work because it's initialized before scheduler initialized",
            "Description": "When using the scheduler with preemption, a NullPointerException (NPE) is raised when the ResourceManager starts due to the ProportionalCapacityPreemptionPolicy being initialized before the CapacityScheduler.",
            "StackTrace": [
                "2014-06-05 11:01:33,201 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[SchedulingMonitor (ProportionalCapacityPreemptionPolicy),5,main] threw an Exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ProportionalCapacityPreemptionPolicy should function correctly without raising a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is raised when the ResourceManager starts, preventing the ProportionalCapacityPreemptionPolicy from working.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7849.json",
        "creation_time": "2018-01-29T23:49:33.000+0000",
        "bug_report": {
            "BugID": "YARN-7849",
            "Title": "TestMiniYarnClusterNodeUtilization#testUpdateNodeUtilization fails due to heartbeat sync error",
            "Description": "The testUpdateNodeUtilization method in TestMiniYarnClusterNodeUtilization is failing due to an assertion error related to container utilization not being propagated correctly.",
            "StackTrace": [
                "java.lang.AssertionError: Containers Utilization not propagated to RMNode expected:<<pmem:1024, vmem:2048, vCores:11.0>> but was:<null>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.verifySimulatedUtilization(TestMiniYarnClusterNodeUtilization.java:227)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:116)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The container utilization should be correctly propagated to RMNode with expected values.",
            "ObservedBehavior": "The container utilization was null instead of the expected values.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8591.json",
        "creation_time": "2018-07-27T05:56:26.000+0000",
        "bug_report": {
            "BugID": "YARN-8591",
            "Title": "[ATSv2] NPE while checking for entity acl in non-secure cluster",
            "Description": "A NullPointerException occurs when checking for entity ACL in a non-secure cluster environment.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccess(TimelineReaderWebServices.java:3536)"
            ],
            "StepsToReproduce": [
                "Send a GET request to the timeline service endpoint for YARN_CONTAINER entities."
            ],
            "ExpectedBehavior": "The system should return the requested entity details without throwing an exception.",
            "ObservedBehavior": "An INTERNAL_SERVER_ERROR is returned due to a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6649.json",
        "creation_time": "2017-05-25T20:36:08.000+0000",
        "bug_report": {
            "BugID": "YARN-6649",
            "Title": "RollingLevelDBTimelineServer throws RuntimeException if object decoding ever fails runtime exception",
            "Description": "When using the Tez UI, some REST API calls to the timeline service return a 500 internal server error due to object decoding issues. This bug aims to handle the decoding to prevent internal server errors and instead return a partial message.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)",
                "Caused by: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:240)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle object decoding gracefully and return a partial message instead of a 500 internal server error.",
            "ObservedBehavior": "The system throws a RuntimeException leading to a 500 internal server error when object decoding fails.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3742.json",
        "creation_time": "2015-05-29T06:00:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3742",
            "Title": "YARN RM will shut down if ZKClient creation times out",
            "Description": "The ResourceManager (RM) goes down showing a stack trace if the ZK client connection fails to be created. The expected behavior is to transition to StandBy instead of shutting down.",
            "StackTrace": [
                "2015-04-19 01:22:20,513  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED. Cause:",
                "java.io.IOException: Wait for ZKClient creation timed out",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should transition to StandBy and allow another RM to take over.",
            "ObservedBehavior": "The ResourceManager shuts down instead of transitioning to StandBy.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4984.json",
        "creation_time": "2016-04-21T19:16:03.000+0000",
        "bug_report": {
            "BugID": "YARN-4984",
            "Title": "LogAggregationService shouldn't swallow exception in handling createAppDir() which cause thread leak.",
            "Description": "Due to YARN-4325, many stale applications exist in NM state store and get recovered after NM restart. The app initiation fails due to an invalid token, but the exception is swallowed, leading to the creation of an aggregator thread for the invalid app.",
            "StackTrace": [
                "2016-04-19 23:38:33,039 ERROR logaggregation.LogAggregationService (LogAggregationService.java:run(300)) - Failed to setup application log directory for application_1448060878692_11842",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be found in cache",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1427)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1358)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)",
                "at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The LogAggregationService should properly handle exceptions and not create aggregator threads for invalid applications.",
            "ObservedBehavior": "An exception is swallowed, and an aggregator thread is still created for an invalid application.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4584.json",
        "creation_time": "2016-01-12T09:08:31.000+0000",
        "bug_report": {
            "BugID": "YARN-4584",
            "Title": "RM startup failure when AM attempts greater than max-attempts",
            "Description": "ResourceManager fails to restart after Application Master (AM) is preempted multiple times due to resource limits in the default queue.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover(RMAppAttemptImpl.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:953)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1232)"
            ],
            "StepsToReproduce": [
                "Configure 3 queues in cluster with 8 GB total resources.",
                "Submit applications to all 3 queues with container size as 1024MB (sleep job with 50 containers on all queues).",
                "Observe that the AM assigned to the default queue gets preempted multiple times."
            ],
            "ExpectedBehavior": "ResourceManager should successfully restart after AM preemption.",
            "ObservedBehavior": "ResourceManager fails to restart due to a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2846.json",
        "creation_time": "2014-11-11T15:30:08.000+0000",
        "bug_report": {
            "BugID": "YARN-2846",
            "Title": "Incorrect persist exit code for running containers in reacquireContainer() that interrupted by NodeManager restart.",
            "Description": "The NM restart work preserving feature could make running AM container get LOST and killed during stop NM daemon. The issue arises when the process is interrupted, leading to an incorrect exit code being recorded.",
            "StackTrace": [
                "java.io.IOException: Interrupted while waiting for process 20001 to exit",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The exit code of running containers should not be recorded if the process is interrupted during NodeManager shutdown.",
            "ObservedBehavior": "The exit code is incorrectly recorded as LOST (154) for the AM container after a NodeManager restart.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7890.json",
        "creation_time": "2018-02-03T21:10:43.000+0000",
        "bug_report": {
            "BugID": "YARN-7890",
            "Title": "NPE during container relaunch",
            "Description": "A NullPointerException occurs while relaunching a container in Hadoop YARN.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)",
                "at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)",
                "at java.util.Collections.unmodifiableList(Collections.java:1287)",
                "at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The container should relaunch successfully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the relaunch of the container.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-139.json",
        "creation_time": "2012-10-01T19:51:20.000+0000",
        "bug_report": {
            "BugID": "YARN-139",
            "Title": "Interrupted Exception within AsyncDispatcher leads to user confusion",
            "Description": "Successful applications tend to get InterruptedExceptions during shutdown. The exception is harmless but it leads to lots of user confusion and therefore could be cleaned up.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "at java.lang.Thread.join(Thread.java:1196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:105)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:437)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:402)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "No InterruptedExceptions should be logged during normal application shutdown.",
            "ObservedBehavior": "InterruptedExceptions are logged, causing user confusion.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-42.json",
        "creation_time": "2012-05-14T11:38:55.000+0000",
        "bug_report": {
            "BugID": "YARN-42",
            "Title": "Node Manager throws NPE on startup",
            "Description": "Node Manager throws a NullPointerException (NPE) on startup if it doesn't have permissions on the local directories.",
            "StackTrace": [
                "org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)",
                "Caused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Node Manager should start without throwing an exception.",
            "ObservedBehavior": "Node Manager fails to start and throws a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7453.json",
        "creation_time": "2017-11-07T09:46:28.000+0000",
        "bug_report": {
            "BugID": "YARN-7453",
            "Title": "Fix issue where RM fails to switch to active after first successful start",
            "Description": "ResourceManager fails to switch to ACTIVE after the first successful start, resulting in a continuous loop of state transitions.",
            "StackTrace": [
                "org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1006)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:910)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:159)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:122)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction.commit(ZKCuratorManager.java:403)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.getAndIncrementEpoch(ZKRMStateStore.java:493)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1198)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "ResourceManager should successfully transition to ACTIVE state after the first successful start.",
            "ObservedBehavior": "ResourceManager fails to transition to ACTIVE and remains in a loop of state transitions.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3369.json",
        "creation_time": "2015-03-18T23:29:06.000+0000",
        "bug_report": {
            "BugID": "YARN-3369",
            "Title": "Missing NullPointer check in AppSchedulingInfo causes RM to die",
            "Description": "The method checkForDeactivation() in AppSchedulingInfo.java does not check for null after calling getResourceRequest, leading to a NullPointerException that crashes the ResourceManager.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should handle resource requests without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when a null ResourceRequest is dereferenced.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-945.json",
        "creation_time": "2013-07-19T22:59:06.000+0000",
        "bug_report": {
            "BugID": "YARN-945",
            "Title": "AM register failing after AMRMToken",
            "Description": "The application master (AM) fails to register due to a missing AMRM token, resulting in an AccessControlException.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]",
                "at org.apache.hadoop.ipc.Server$Connection.initializeAuthContext(Server.java:1531)",
                "at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1482)",
                "at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:788)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:587)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:562)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application master should successfully register with the resource manager using the AMRM token.",
            "ObservedBehavior": "The application master fails to register, throwing an AccessControlException due to authentication issues.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6072.json",
        "creation_time": "2017-01-08T09:21:12.000+0000",
        "bug_report": {
            "BugID": "YARN-6072",
            "Title": "RM unable to start in secure mode",
            "Description": "Resource manager is unable to start in secure mode due to a NullPointerException during the refresh of service ACLs.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:552)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Error on refreshAll during transition to Active"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager in secure mode."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully in secure mode.",
            "ObservedBehavior": "The ResourceManager fails to start due to a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7663.json",
        "creation_time": "2017-12-15T01:52:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7663",
            "Title": "RMAppImpl:Invalid event: START at KILLED",
            "Description": "An InvalidStateTransitionException occurs when sending a kill command to an application, indicating an invalid event transition.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: START at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:805)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:885)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Send a kill command to an application.",
                "Observe the ResourceManager logs."
            ],
            "ExpectedBehavior": "The application should transition to a killed state without errors.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating an invalid event transition.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5873.json",
        "creation_time": "2016-11-12T09:54:20.000+0000",
        "bug_report": {
            "BugID": "YARN-5873",
            "Title": "RM crashes with NPE if generic application history is enabled",
            "Description": "The ResourceManager crashes with a NullPointerException when generic application history is enabled.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should handle application history without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3227.json",
        "creation_time": "2015-02-19T16:58:01.000+0000",
        "bug_report": {
            "BugID": "YARN-3227",
            "Title": "Timeline renew delegation token fails when RM user's TGT is expired",
            "Description": "When the RM user's kerberos TGT is expired, the RM renew delegation token operation fails as part of job submission. Expected behavior is that RM will relogin to get a new TGT.",
            "StackTrace": [
                "java.io.IOException: Failed to renew token: Kind: TIMELINE_DELEGATION_TOKEN, Service: timelineserver.example.com:4080, Ident: (owner=user, renewer=rmuser, realUser=oozie, issueDate=1423248845528, maxDate=1423853645528, sequenceNumber=9716, masterKeyId=9)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:443)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:808)",
                "Caused by: java.io.IOException: HTTP status [401], message [Unauthorized]",
                "at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "RM will relogin to get a new TGT.",
            "ObservedBehavior": "The RM renew delegation token operation fails.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4235.json",
        "creation_time": "2015-10-07T19:26:24.000+0000",
        "bug_report": {
            "BugID": "YARN-4235",
            "Title": "FairScheduler PrimaryGroup does not handle empty groups returned for a user",
            "Description": "The system encounters a NullPointerException (NPE) when empty groups are returned for a user, leading to a crash of the ResourceManager.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The FairScheduler should handle cases where a user has no associated groups without causing a crash.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when empty groups are returned for a user.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4833.json",
        "creation_time": "2016-03-17T13:22:23.000+0000",
        "bug_report": {
            "BugID": "YARN-4833",
            "Title": "For Queue AccessControlException client retries multiple times on both RM",
            "Description": "Submitting an application to a queue with ACL enabled fails due to lack of permissions, causing the client to retry multiple times.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): User hdfs does not have permission to submit application_1458273884145_0001 to queue default"
            ],
            "StepsToReproduce": [
                "Submit application to a queue where ACL is enabled.",
                "Ensure the submitting user does not have access.",
                "Observe the client retries until the maximum attempts are reached."
            ],
            "ExpectedBehavior": "The client should handle AccessControlException gracefully and not retry unnecessarily.",
            "ObservedBehavior": "The client retries multiple times even after receiving AccessControlException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1689.json",
        "creation_time": "2014-02-05T19:16:00.000+0000",
        "bug_report": {
            "BugID": "YARN-1689",
            "Title": "RMAppAttempt is not killed when RMApp is at ACCEPTED",
            "Description": "When running some Hive on Tez jobs, the Resource Manager (RM) becomes unusable, preventing any jobs from running. The logs indicate a NullPointerException and an InvalidStateTransitionException.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Resource Manager should handle application attempts correctly and transition states as expected.",
            "ObservedBehavior": "The Resource Manager enters an unusable state, and application attempts are not killed when the application is in the ACCEPTED state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5594.json",
        "creation_time": "2016-08-30T15:14:19.000+0000",
        "bug_report": {
            "BugID": "YARN-5594",
            "Title": "Handle old RMDelegationToken format when recovering RM",
            "Description": "After upgrading the cluster from version 2.5.1 to 2.7.0, an error occurs during the ResourceManager's state recovery due to incompatible file formats.",
            "StackTrace": [
                "2016-08-25 17:20:33,293 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state",
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).",
                "at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should successfully load and recover its state without errors.",
            "ObservedBehavior": "The ResourceManager fails to recover its state due to an InvalidProtocolBufferException caused by incompatible file formats.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7511.json",
        "creation_time": "2017-11-16T11:41:43.000+0000",
        "bug_report": {
            "BugID": "YARN-7511",
            "Title": "NPE in ContainerLocalizer when localization failed for running container",
            "Description": "A NullPointerException occurs in the ContainerLocalizer when resource localization fails for a running container.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)",
                "at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": [
                "1. Container was running and ContainerManagerImpl#localize was called for this container",
                "2. Localization failed in ResourceLocalizationService$LocalizerRunner#run and sent out ContainerResourceFailedEvent with null LocalResourceRequest.",
                "3. NPE when ResourceLocalizationFailedWhileRunningTransition#transition --> container.resourceSet.resourceLocalizationFailed(null)"
            ],
            "ExpectedBehavior": "The system should handle resource localization failures without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when resource localization fails for a running container.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3790.json",
        "creation_time": "2015-06-10T04:53:40.000+0000",
        "bug_report": {
            "BugID": "YARN-3790",
            "Title": "usedResource from rootQueue metrics may get stale data for FS scheduler after recovering the container",
            "Description": "The test 'testSchedulerRecovery' fails due to an assertion error where the expected resource value does not match the actual value after recovery.",
            "StackTrace": [
                "java.lang.AssertionError: expected:<6144> but was:<8192>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.junit.Assert.assertEquals(Assert.java:555)",
                "at org.junit.Assert.assertEquals(Assert.java:542)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.assertMetrics(TestWorkPreservingRMRestart.java:853)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.checkFSQueue(TestWorkPreservingRMRestart.java:342)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.testSchedulerRecovery(TestWorkPreservingRMRestart.java:241)"
            ],
            "StepsToReproduce": [
                "Run the test suite for org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart",
                "Observe the failure in testSchedulerRecovery"
            ],
            "ExpectedBehavior": "The usedResource metric should accurately reflect the expected value after recovering the container.",
            "ObservedBehavior": "The usedResource metric shows a stale value, leading to a test failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6068.json",
        "creation_time": "2017-01-07T03:16:07.000+0000",
        "bug_report": {
            "BugID": "YARN-6068",
            "Title": "Log aggregation get failed when NM restart even with recovery",
            "Description": "The log aggregation fails after a NodeManager (NM) restart, even when recovery is expected.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Log aggregation should complete successfully after a NodeManager restart.",
            "ObservedBehavior": "Log aggregation fails with an InvalidStateTransitonException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-903.json",
        "creation_time": "2013-07-07T08:35:30.000+0000",
        "bug_report": {
            "BugID": "YARN-903",
            "Title": "DistributedShell throwing Errors in logs after successful completion",
            "Description": "The DistributedShell application logs errors even after successful execution, indicating potential issues with container management.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000002 is not handled by this NodeManager",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)"
            ],
            "StepsToReproduce": [
                "Run the DistributedShell application.",
                "Monitor the logs for errors after successful completion."
            ],
            "ExpectedBehavior": "The application should complete without logging errors.",
            "ObservedBehavior": "Errors are logged indicating that containers are not handled by the NodeManager despite successful execution.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8236.json",
        "creation_time": "2018-04-29T16:28:11.000+0000",
        "bug_report": {
            "BugID": "YARN-8236",
            "Title": "Invalid kerberos principal file name cause NPE in native service",
            "Description": "A NullPointerException occurs in the ServiceClient when an invalid Kerberos principal file name is provided.",
            "StackTrace": [
                "2018-04-29 16:22:54,266 WARN webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle invalid Kerberos principal file names gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing an internal server error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2857.json",
        "creation_time": "2014-10-24T20:47:51.000+0000",
        "bug_report": {
            "BugID": "YARN-2857",
            "Title": "ConcurrentModificationException in ContainerLogAppender",
            "Description": "A ConcurrentModificationException occurs in the ContainerLogAppender when a job is submitted by Oozie to launch a Pig script.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)",
                "at org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)",
                "at org.apache.log4j.Category.removeAllAppenders(Category.java:891)",
                "at org.apache.log4j.Hierarchy.shutdown(Hierarchy.java:471)",
                "at org.apache.log4j.LogManager.shutdown(LogManager.java:267)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogsShutdown(TaskLog.java:286)",
                "at org.apache.hadoop.mapred.TaskLog$2.run(TaskLog.java:339)"
            ],
            "StepsToReproduce": [
                "Submit a job using Oozie to launch a Pig script."
            ],
            "ExpectedBehavior": "The job should run without throwing a ConcurrentModificationException.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, causing the job to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2416.json",
        "creation_time": "2014-08-13T22:36:31.000+0000",
        "bug_report": {
            "BugID": "YARN-2416",
            "Title": "InvalidStateTransitonException in ResourceManager if AMLauncher does not receive response for startContainers() call in time",
            "Description": "AMLauncher calls startContainers(allRequests) to launch a container for application master. In some cases, the RPC call returns late while the AM container has already started, causing the RMAppAttempt to remain in ALLOCATED state and throw InvalidStateTransitonException when receiving the REGISTERED event.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: REGISTERED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The RMAppAttempt should transition from ALLOCATED to LAUNCHED state without errors.",
            "ObservedBehavior": "The RMAppAttempt remains in ALLOCATED state and throws InvalidStateTransitonException for subsequent events.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-345.json",
        "creation_time": "2013-01-17T12:57:46.000+0000",
        "bug_report": {
            "BugID": "YARN-345",
            "Title": "Many InvalidStateTransitonException errors for ApplicationImpl in Node Manager",
            "Description": "The Node Manager is encountering multiple InvalidStateTransitonException errors related to application state transitions.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application should transition states without throwing InvalidStateTransitonException errors.",
            "ObservedBehavior": "The application throws InvalidStateTransitonException errors when attempting to transition states.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3894.json",
        "creation_time": "2015-07-08T07:00:51.000+0000",
        "bug_report": {
            "BugID": "YARN-3894",
            "Title": "RM startup should fail for wrong CS xml NodeLabel capacity configuration",
            "Description": "The ResourceManager (RM) fails to shut down when there is a capacity configuration mismatch for NodeLabels in the Capacity Scheduler.",
            "StackTrace": [
                "java.io.IOException: Failed to re-init queues",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:383)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:376)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:605)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)",
                "Caused by: java.lang.IllegalArgumentException: Illegal capacity of 0.5 for children of queue root for label=node2",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues(ParentQueue.java:159)"
            ],
            "StepsToReproduce": [
                "Configure RM with capacity scheduler",
                "Add one or two node labels from rmadmin",
                "Configure capacity XML with node label but issue with capacity configuration for already added label",
                "Restart both RM",
                "Check on service init of capacity scheduler node label list is populated"
            ],
            "ExpectedBehavior": "RM should not start.",
            "ObservedBehavior": "RM starts despite the capacity configuration mismatch, leading to exceptions during initialization.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1903.json",
        "creation_time": "2014-04-04T20:51:24.000+0000",
        "bug_report": {
            "BugID": "YARN-1903",
            "Title": "Killing Container on NEW and LOCALIZING will result in exitCode and diagnostics not set",
            "Description": "The container status after stopping the container is not as expected, leading to an assertion error in the test.",
            "StackTrace": [
                "java.lang.AssertionError: 4:",
                "at org.junit.Assert.fail(Assert.java:93)",
                "at org.junit.Assert.assertTrue(Assert.java:43)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testGetContainerStatus(TestNMClient.java:382)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testContainerManagement(TestNMClient.java:346)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient(TestNMClient.java:226)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The container should have the correct exitCode and diagnostics set after being killed.",
            "ObservedBehavior": "The exitCode and diagnostics are not set, leading to a test failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4347.json",
        "creation_time": "2015-11-11T22:32:59.000+0000",
        "bug_report": {
            "BugID": "YARN-4347",
            "Title": "Resource manager fails with Null pointer exception",
            "Description": "Resource manager fails with NPE while trying to load or recover a finished application.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:719)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The resource manager should successfully load or recover the state of finished applications.",
            "ObservedBehavior": "The resource manager fails with a NullPointerException during the recovery process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1692.json",
        "creation_time": "2014-02-07T02:01:17.000+0000",
        "bug_report": {
            "BugID": "YARN-1692",
            "Title": "ConcurrentModificationException in fair scheduler AppSchedulable",
            "Description": "A ConcurrentModificationException is thrown in the fair scheduler when iterating over application priorities without proper synchronization.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:954)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The fair scheduler should update application demands without throwing exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown during the update process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7697.json",
        "creation_time": "2018-01-03T19:28:50.000+0000",
        "bug_report": {
            "BugID": "YARN-7697",
            "Title": "NM goes down with OOM due to leak in log-aggregation",
            "Description": "The Node Manager (NM) crashes due to an OutOfMemoryError caused by a leak in the log aggregation process.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:823)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:840)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling(LogAggregationIndexedFileController.java:293)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.access$600(LogAggregationIndexedFileController.java:98)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:216)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:205)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:312)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:284)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Node Manager should handle log aggregation without running out of memory.",
            "ObservedBehavior": "The Node Manager crashes with an OutOfMemoryError.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7382.json",
        "creation_time": "2017-10-23T23:36:59.000+0000",
        "bug_report": {
            "BugID": "YARN-7382",
            "Title": "NoSuchElementException in FairScheduler after failover causes RM crash",
            "Description": "While running an MR job and an RM failover occurs, the active RM crashes due to a NoSuchElementException.",
            "StackTrace": [
                "java.util.NoSuchElementException",
                "at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)",
                "at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)"
            ],
            "StepsToReproduce": [
                "Run an MR job (e.g. sleep)",
                "Trigger an RM failover",
                "Observe the active RM crash after maps reach 100%"
            ],
            "ExpectedBehavior": "The Resource Manager should handle failover without crashing.",
            "ObservedBehavior": "The active Resource Manager crashes due to a NoSuchElementException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1094.json",
        "creation_time": "2013-08-23T19:06:17.000+0000",
        "bug_report": {
            "BugID": "YARN-1094",
            "Title": "RM restart throws Null pointer Exception in Secure Env",
            "Description": "Resource Manager fails to start with a NullPointerException when the rmrestart feature is enabled and a job is running.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.setTimerForTokenRenewal(DelegationTokenRenewer.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplication(DelegationTokenRenewer.java:307)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:819)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:613)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:832)"
            ],
            "StepsToReproduce": [
                "Enable rmrestart feature",
                "Restart Resource Manager while a job is running"
            ],
            "ExpectedBehavior": "Resource Manager should start successfully without errors.",
            "ObservedBehavior": "Resource Manager fails to start and throws a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7269.json",
        "creation_time": "2017-09-28T23:56:42.000+0000",
        "bug_report": {
            "BugID": "YARN-7269",
            "Title": "Tracking URL in the app state does not get redirected to ApplicationMaster for Running applications",
            "Description": "The application fails to redirect the tracking URL to the ApplicationMaster for running applications, resulting in a ServletException.",
            "StackTrace": [
                "javax.servlet.ServletException: Could not determine the proxy server for redirection",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:199)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:141)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1426)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The tracking URL should redirect to the ApplicationMaster for running applications.",
            "ObservedBehavior": "The application throws a ServletException and does not redirect the tracking URL.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7249.json",
        "creation_time": "2017-09-25T16:49:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7249",
            "Title": "Fix CapacityScheduler NPE issue when a container preempted while the node is being removed",
            "Description": "This issue occurs when a node is being removed from the scheduler, a container running on that node is preempted, and a race condition leads to a null node being passed to the leaf queue.",
            "StackTrace": [
                "2017-08-31 02:51:24,748 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(714)) - Error in handling event type KILL_RESERVED_CONTAINER to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705)"
            ],
            "StepsToReproduce": [
                "Remove a node from the scheduler.",
                "Preempt a container running on that node.",
                "Trigger the race condition that leads to a null node being passed to the leaf queue."
            ],
            "ExpectedBehavior": "The scheduler should handle the removal of nodes and preemption of containers without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when handling the event type KILL_RESERVED_CONTAINER.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4598.json",
        "creation_time": "2016-01-15T06:48:48.000+0000",
        "bug_report": {
            "BugID": "YARN-4598",
            "Title": "Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
            "Description": "In our cluster, I found that the container has some problems in state transition.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:83)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1071)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The container should handle the RESOURCE_FAILED event correctly.",
            "ObservedBehavior": "The container fails to handle the RESOURCE_FAILED event when in the CONTAINER_CLEANEDUP_AFTER_KILL state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1149.json",
        "creation_time": "2013-09-04T21:46:58.000+0000",
        "bug_report": {
            "BugID": "YARN-1149",
            "Title": "NM throws InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
            "Description": "When nodemanager receives a kill signal after an application has finished execution but log aggregation has not started, an InvalidStateTransitonException is thrown.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:689)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Submit an application to the nodemanager.",
                "Send a kill signal to the nodemanager after the application has finished execution but before log aggregation has started."
            ],
            "ExpectedBehavior": "The nodemanager should handle the kill signal without throwing an exception.",
            "ObservedBehavior": "An InvalidStateTransitonException is thrown, indicating that the application cannot handle the event in its current state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7818.json",
        "creation_time": "2018-01-25T18:42:55.000+0000",
        "bug_report": {
            "BugID": "YARN-7818",
            "Title": "Remove privileged operation warnings during container launch for the ContainerRuntimes",
            "Description": "The issue occurs when running the Dshell application, leading to a failure in launching containers after restarting the Node Manager (NM).",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.launchContainer(DefaultLinuxContainerRuntime.java:124)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:285)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "Run Dshell Application using the specified yarn command.",
                "Find out the host where the Application Master (AM) is running.",
                "Identify the containers launched by the application.",
                "Restart the Node Manager where the AM is running.",
                "Validate that a new attempt is not started and that the previously launched containers are in RUNNING state."
            ],
            "ExpectedBehavior": "Containers should continue running after the Node Manager is restarted.",
            "ObservedBehavior": "Containers fail to launch with error code 143 after the Node Manager restart.",
            "Resolution": "Fixed"
        }
    }
]