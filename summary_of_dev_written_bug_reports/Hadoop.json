[
    {
        "filename": "HADOOP-6989.json",
        "creation_time": "2010-10-04T23:55:16.000+0000",
        "bug_report": {
            "BugID": "HADOOP-6989",
            "Title": "TestSetFile is failing on trunk",
            "Description": "The TestSetFile test case is failing due to an IllegalArgumentException indicating that the key class or comparator option must be set.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: key class or comparator option must be set",
                "at org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)",
                "at org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)",
                "at org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)",
                "at org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The TestSetFile should execute without errors and complete successfully.",
            "ObservedBehavior": "The test fails with an IllegalArgumentException indicating that the key class or comparator option must be set.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10823.json",
        "creation_time": "2014-07-15T07:20:00.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10823",
            "Title": "TestReloadingX509TrustManager is flaky",
            "Description": "The unit test for ReloadingX509TrustManager fails intermittently due to an assertion error.",
            "StackTrace": [
                "junit.framework.Assert.fail(Assert.java:50)",
                "junit.framework.Assert.failNotEquals(Assert.java:287)",
                "junit.framework.Assert.assertEquals(Assert.java:67)",
                "junit.framework.Assert.assertEquals(Assert.java:199)",
                "junit.framework.Assert.assertEquals(Assert.java:205)",
                "org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)",
                "java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)",
                "java.security.KeyStore.load(KeyStore.java:1185)",
                "org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)",
                "org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)",
                "java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The test should pass with the expected value of 2.",
            "ObservedBehavior": "The test fails with an assertion error indicating expected:<2> but was:<1>.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-9125.json",
        "creation_time": "2012-12-10T02:07:52.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9125",
            "Title": "LdapGroupsMapping threw CommunicationException after some idle time",
            "Description": "The LdapGroupsMapping component throws a CommunicationException after a period of inactivity, indicating a connection issue with the LDAP server.",
            "StackTrace": [
                "javax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'",
                "at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)",
                "Caused by: java.io.IOException: connection closed",
                "at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should maintain a stable connection to the LDAP server and not throw exceptions after idle periods.",
            "ObservedBehavior": "The system throws a CommunicationException when attempting to retrieve groups for a user after a period of inactivity.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10252.json",
        "creation_time": "2014-01-22T16:43:27.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10252",
            "Title": "HttpServer can't start if hostname is not specified",
            "Description": "The HttpServer fails to start when the hostname is not specified, leading to a fatal error.",
            "StackTrace": [
                "2014-01-22 08:43:05,969 FATAL [M:0;localhost:48573] master.HMaster(2187): Unhandled exception. Starting shutdown.",
                "java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:958)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:940)",
                "at org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)",
                "at org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)",
                "at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The HttpServer should start successfully when a valid hostname is provided.",
            "ObservedBehavior": "The HttpServer fails to start and throws an IllegalArgumentException due to a null property value.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-12239.json",
        "creation_time": "2015-07-15T18:06:14.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12239",
            "Title": "StorageException complaining \" no lease ID\" when updating FolderLastModifiedTime in WASB",
            "Description": "This issue is related to a failure in log splitting due to a StorageException indicating that there is currently a lease on the blob and no lease ID was specified in the request.",
            "StackTrace": [
                "java.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374, will retry",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully update the last modified time of the folder without encountering a lease issue.",
            "ObservedBehavior": "The system throws a StorageException indicating a lease issue when attempting to update the folder's last modified time.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11878.json",
        "creation_time": "2015-04-27T08:06:22.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11878",
            "Title": "FileContext.java # fixRelativePart should check for not null for a more informative exception",
            "Description": "A NullPointerException occurs during the deletion of log files when a job fails.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)",
                "at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The deletion service should handle null paths gracefully without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to delete a log file with a null path.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-14949.json",
        "creation_time": "2017-10-13T23:44:09.000+0000",
        "bug_report": {
            "BugID": "HADOOP-14949",
            "Title": "TestKMS#testACLs fails intermittently",
            "Description": "Intermittent failures observed in the test case, specifically an AssertionError indicating that re-encryption of an encrypted key should not have been possible.",
            "StackTrace": [
                "java.lang.AssertionError: Should not have been able to reencryptEncryptedKey",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should not allow re-encryption of an already encrypted key.",
            "ObservedBehavior": "The test fails with an AssertionError indicating that re-encryption was incorrectly allowed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10540.json",
        "creation_time": "2014-04-11T05:14:07.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10540",
            "Title": "Datanode upgrade in Windows fails with hardlink error.",
            "Description": "DataNode fails to start after upgrading from Hadoop 1.x to 2.4 due to a hard link exception.",
            "StackTrace": [
                "java.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.",
                "at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)",
                "at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)"
            ],
            "StepsToReproduce": [
                "Installed Hadoop 1.x",
                "hadoop dfsadmin -safemode enter",
                "hadoop dfsadmin -saveNamespace",
                "hadoop namenode -finalize",
                "Stop all services",
                "Uninstall Hadoop 1.x",
                "Install Hadoop 2.4",
                "Start namenode with -upgrade option",
                "Try to start datanode"
            ],
            "ExpectedBehavior": "DataNode should start successfully after the upgrade.",
            "ObservedBehavior": "DataNode fails to start with a hard link exception.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-7629.json",
        "creation_time": "2011-09-09T17:45:14.000+0000",
        "bug_report": {
            "BugID": "HADOOP-7629",
            "Title": "Regression with MAPREDUCE-2289 - setPermission passed immutable FsPermission (rpc failure)",
            "Description": "The introduction of a change in MAPREDUCE-2289 caused an issue where an immutable FsPermission is passed in RPC calls, leading to a runtime exception.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)",
                "at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)",
                "at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)",
                "at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)",
                "at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)",
                "at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)",
                "at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)",
                "at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)",
                "Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "at java.lang.Class.getConstructor0(Class.java:2706)",
                "at java.lang.Class.getDeclaredConstructor(Class.java:1985)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle RPC calls with FsPermission without throwing exceptions.",
            "ObservedBehavior": "The system throws a RuntimeException due to the inability to find the constructor for FsPermission.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15060.json",
        "creation_time": "2017-11-22T00:18:55.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15060",
            "Title": "TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime flaky",
            "Description": "The test fails due to an assertion error where the expected log message about command timeout is not present.",
            "StackTrace": [
                "java.lang.AssertionError:",
                "Expected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser",
                "PartialGroupNameException The user name 'foobarnonexistinguser' is not found.",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)",
                "at org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The logs should contain a message about command timeout.",
            "ObservedBehavior": "The logs do not contain the expected message, leading to a test failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10937.json",
        "creation_time": "2014-08-04T21:22:55.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10937",
            "Title": "Need to set version name correctly before decrypting EEK",
            "Description": "Touchz-ing a file results in a Null Pointer Exception when attempting to decrypt an encrypted key.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)"
            ],
            "StepsToReproduce": [
                "Run the command: hdfs dfs -touchz /enc3/touchFIle"
            ],
            "ExpectedBehavior": "The file should be created without any errors.",
            "ObservedBehavior": "A Null Pointer Exception is thrown, preventing the file from being created.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-9103.json",
        "creation_time": "2012-04-20T01:07:25.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9103",
            "Title": "UTF8 class does not properly decode Unicode characters outside the basic multilingual plane",
            "Description": "An IOException occurs in the SecondaryNameNode due to a lease found for a non-existent file, indicating a potential bug in the HDFS code related to UTF8 encoding.",
            "StackTrace": [
                "java.io.IOException: Found lease for non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle file leases correctly without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating a lease for a non-existent file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11151.json",
        "creation_time": "2014-09-29T08:18:04.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11151",
            "Title": "Automatically refresh auth token and retry on auth failure",
            "Description": "After enabling CFS and KMS service in the cluster, the system initially allows file operations in the encryption zone. However, after some time, it fails with a 403 Forbidden error due to authentication issues.",
            "StackTrace": [
                "org.apache.hadoop.security.authentication.client.AuthenticationException: Anonymous requests are disallowed",
                "at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)",
                "at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)"
            ],
            "StepsToReproduce": [
                "Enable CFS and KMS service in the cluster.",
                "Attempt to put/copy a file into the encryption zone."
            ],
            "ExpectedBehavior": "The system should allow file operations in the encryption zone without authentication errors.",
            "ObservedBehavior": "The system returns a 403 Forbidden error after some time, indicating authentication issues.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-8031.json",
        "creation_time": "2012-02-07T20:22:24.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8031",
            "Title": "Configuration class fails to find embedded .jar resources; should use URL.openStream()",
            "Description": "The Hadoop configuration class fails to locate embedded .jar resources due to incorrect URL handling, leading to a RuntimeException when attempting to load core-site.xml.",
            "StackTrace": [
                "org.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].",
                "Caused by: java.lang.RuntimeException: core-site.xml not found",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:438)"
            ],
            "StepsToReproduce": [
                "Run a Hadoop client within RHQ using its classloader.",
                "Attempt to start the NameNode component."
            ],
            "ExpectedBehavior": "The configuration class should successfully locate and load core-site.xml and other resources.",
            "ObservedBehavior": "The system throws a RuntimeException indicating that core-site.xml cannot be found.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15411.json",
        "creation_time": "2018-04-24T23:18:39.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15411",
            "Title": "AuthenticationFilter should use Configuration.getPropsWithPrefix instead of iterator",
            "Description": "Node manager start up fails with a YarnRuntimeException due to NMWebapps failing to start.",
            "StackTrace": [
                "2018-04-19 13:08:30,638 ERROR nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(921)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.",
                "at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)",
                "Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)",
                "at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)",
                "Caused by: java.io.IOException: java.util.ConcurrentModificationException",
                "at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)",
                "at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)",
                "at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)",
                "Caused by: java.util.ConcurrentModificationException",
                "at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)",
                "at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)",
                "at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)",
                "at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)",
                "at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)",
                "at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "NodeManager should start without errors.",
            "ObservedBehavior": "NodeManager fails to start due to a ConcurrentModificationException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15850.json",
        "creation_time": "2018-10-13T14:24:29.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15850",
            "Title": "CopyCommitter#concatFileChunks should check that the blocks per chunk is not 0",
            "Description": "The test failure of TestIncrementalBackupWithBulkLoad in HBase against Hadoop 3.1.1 is caused by the CopyCommitter#concatFileChunks method throwing an IOException due to inconsistent sequence files when two bulk loaded hfiles are processed.",
            "StackTrace": [
                "java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)"
            ],
            "StepsToReproduce": [
                "Run the TestIncrementalBackupWithBulkLoad test case against Hadoop 3.1.1",
                "Ensure two bulk loaded hfiles are included in the input listing"
            ],
            "ExpectedBehavior": "The CopyCommitter should correctly handle multiple bulk loaded hfiles without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown due to inconsistent sequence files when processing the two bulk loaded hfiles.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11693.json",
        "creation_time": "2015-03-05T23:19:13.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11693",
            "Title": "Azure Storage FileSystem rename operations are throttled too aggressively to complete HBase WAL archiving.",
            "Description": "HBase clusters are periodically throttled by Azure storage during WAL archiving, causing region servers to abort and leading to the hbase:meta table going offline.",
            "StackTrace": [
                "2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error: ABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)",
                "at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "HBase should successfully archive old WALs without being throttled by Azure storage.",
            "ObservedBehavior": "HBase region servers abort due to throttling by Azure storage, causing the hbase:meta table to go offline.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-12441.json",
        "creation_time": "2015-09-25T23:26:43.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12441",
            "Title": "Fix kill command behavior under some Linux distributions.",
            "Description": "After HADOOP-12317, the kill command's execution fails under Ubuntu 12. The NodeManager cannot determine if a process is alive via the PID of containers, leading to incorrect process termination.",
            "StackTrace": [
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:461)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The kill command should successfully terminate the specified process.",
            "ObservedBehavior": "The kill command fails with an error indicating a garbage process ID.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11685.json",
        "creation_time": "2015-03-06T22:00:14.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11685",
            "Title": "StorageException complaining 'no lease ID' during HBase distributed log splitting",
            "Description": "During HBase distributed log splitting, multiple threads access the same folder 'recovered.edits', leading to an IOException due to lease issues with Azure storage.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle concurrent access to the 'recovered.edits' folder without throwing exceptions.",
            "ObservedBehavior": "An IOException is thrown due to lease issues when multiple threads attempt to access the same folder.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-8589.json",
        "creation_time": "2012-07-11T23:27:00.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8589",
            "Title": "ViewFs tests fail when tests and home dirs are nested",
            "Description": "TestFSMainOperationsLocalFileSystem fails when the test root directory is under the user's home directory, which is deeper than 2 levels from /. This occurs with the default 1-node installation of Jenkins.",
            "StackTrace": [
                "org.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here",
                "at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)",
                "at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)",
                "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2128)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)",
                "at org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)"
            ],
            "StepsToReproduce": [
                "Set up a Jenkins instance with a default 1-node installation.",
                "Create a test root directory under the user's home directory that is deeper than 2 levels from /."
            ],
            "ExpectedBehavior": "The test should pass without any exceptions related to file system paths.",
            "ObservedBehavior": "The test fails with a FileAlreadyExistsException when trying to create a link for a directory that already exists.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11754.json",
        "creation_time": "2015-03-26T05:22:54.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11754",
            "Title": "RM fails to start in non-secure mode due to authentication filter failure",
            "Description": "Resource Manager fails to start in non-secure mode due to an exception related to the authentication filter.",
            "StackTrace": [
                "javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)",
                "at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)",
                "Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Resource Manager should start successfully in non-secure mode.",
            "ObservedBehavior": "Resource Manager fails to start with a ServletException indicating it could not read the signature secret file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-8225.json",
        "creation_time": "2012-03-13T20:33:53.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8225",
            "Title": "DistCp fails when invoked by Oozie",
            "Description": "When DistCp is invoked through a proxy-user (e.g. through Oozie), the delegation-token-store isn't picked up by DistCp correctly, leading to failures.",
            "StackTrace": [
                "ERROR [main] org.apache.hadoop.tools.DistCp: Couldn't complete DistCp operation:",
                "java.lang.SecurityException: Intercepted System.exit(-999)",
                "at org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)",
                "at java.lang.Runtime.exit(Runtime.java:88)",
                "at java.lang.System.exit(System.java:904)",
                "at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "DistCp should complete successfully when invoked through a proxy-user.",
            "ObservedBehavior": "DistCp fails with a SecurityException and does not complete the operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10866.json",
        "creation_time": "2014-07-18T22:00:15.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10866",
            "Title": "RawLocalFileSystem fails to read symlink targets via the stat command when the format of stat command uses non-curly quotes",
            "Description": "Symlink tests failure happened from time to time, specifically in the TestSymlinkLocalFSFileContext tests.",
            "StackTrace": [
                "java.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link",
                "at org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)",
                "at org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)",
                "at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)",
                "at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)",
                "at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)",
                "at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should correctly read symlink targets without errors.",
            "ObservedBehavior": "The system throws an IOException indicating that the specified path is not a symbolic link.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-12089.json",
        "creation_time": "2015-06-15T17:34:46.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12089",
            "Title": "StorageException complaining \" no lease ID\" when updating FolderLastModifiedTime in WASB",
            "Description": "This issue occurs when HBase is deleting old WALs and trying to update the /hbase/oldWALs folder, similar to HADOOP-11523.",
            "StackTrace": [
                "org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)",
                "at org.apache.hadoop.hbase.Chore.run(Chore.java:80)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully delete old WALs without encountering a lease issue.",
            "ObservedBehavior": "The system throws a StorageException indicating a lease on the blob and no lease ID was specified.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11934.json",
        "creation_time": "2015-05-07T00:38:53.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11934",
            "Title": "Use of JavaKeyStoreProvider in LdapGroupsMapping causes infinite loop",
            "Description": "The LdapGroupsMapping code and the JavaKeyStoreProvider interact in a way that leads to a fatal issue, causing an infinite loop that results in a stack overflow.",
            "StackTrace": [
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)",
                "at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)",
                "at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)",
                "at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)",
                "at org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:70)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:66)",
                "at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)",
                "at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)",
                "at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)",
                "at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)",
                "at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)",
                "at org.apache.hadoop.fs.FileSystem.Cache.get(FileSystem.java:2611)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should not enter an infinite loop when using LdapGroupsMapping with JavaKeyStoreProvider.",
            "ObservedBehavior": "The system enters an infinite loop that leads to a stack overflow, causing the application to crash.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11722.json",
        "creation_time": "2015-03-16T21:39:07.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11722",
            "Title": "Some Instances of Services using ZKDelegationTokenSecretManager go down when old token cannot be deleted",
            "Description": "When multiple instances of a Service using ZKDelegationTokenSecretManager attempt to delete a node simultaneously, only one succeeds, causing the others to throw an exception and potentially bring down the node.",
            "StackTrace": [
                "java.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28",
                "at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28"
            ],
            "StepsToReproduce": [
                "Start multiple instances of a Service using ZKDelegationTokenSecretManager.",
                "Trigger the deletion of a node simultaneously from all instances."
            ],
            "ExpectedBehavior": "Only one instance should successfully delete the node without causing others to fail.",
            "ObservedBehavior": "Multiple instances attempt to delete the same node, leading to exceptions and potential service downtime.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15331.json",
        "creation_time": "2018-03-21T01:15:56.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15331",
            "Title": "Fix a race condition causing parsing error of java.io.BufferedInputStream in class org.apache.hadoop.conf.Configuration",
            "Description": "There is a race condition in the way Hadoop handles the Configuration class, leading to a stream closed exception when multiple threads interact with the same Configuration instance.",
            "StackTrace": [
                "2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346",
                "com.ctc.wstx.exc.WstxIOException: Stream closed",
                "at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)",
                "at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)",
                "at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)",
                "at org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)",
                "at org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)"
            ],
            "StepsToReproduce": [
                "Create a Configuration instance.",
                "In one thread, add resources to the Configuration.",
                "In another thread, clone the Configuration and attempt to access its properties."
            ],
            "ExpectedBehavior": "The Configuration should handle concurrent access without throwing exceptions.",
            "ObservedBehavior": "A stream closed exception occurs when one thread tries to access a resource while another thread is modifying the Configuration.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-14062.json",
        "creation_time": "2016-12-20T00:35:48.000+0000",
        "bug_report": {
            "BugID": "HADOOP-14062",
            "Title": "ApplicationMasterProtocolPBClientImpl.allocate fails with EOFException when RPC privacy is enabled",
            "Description": "When privacy is enabled for RPC (hadoop.rpc.protection = privacy), ApplicationMasterProtocolPBClientImpl.allocate sometimes fails with an EOFException, blocking YARN users from encrypting RPC in their Hadoop clusters.",
            "StackTrace": [
                "java.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException;",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:422)",
                "at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1428)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1338)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy80.allocate(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy81.allocate(Unknown Source)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:392)",
                "at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)",
                "at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)",
                "at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)"
            ],
            "StepsToReproduce": [
                "Set hadoop.rpc.protection equal to privacy",
                "Write data to HDFS using Spark",
                "Attempt to distcp that data to another location in HDFS"
            ],
            "ExpectedBehavior": "The allocate method should successfully allocate resources without throwing an EOFException.",
            "ObservedBehavior": "The allocate method fails with an EOFException when RPC privacy is enabled.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11149.json",
        "creation_time": "2014-09-27T16:37:06.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11149",
            "Title": "Increase the timeout of TestZKFailoverController",
            "Description": "The test 'testGracefulFailover' in 'TestZKFailoverController' fails due to a timeout after 25000 milliseconds.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 25000 milliseconds",
                "at java.lang.Object.wait(Native Method)",
                "at org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)",
                "at org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)",
                "at org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)",
                "at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)",
                "at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)",
                "at org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)",
                "at org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)",
                "at org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)"
            ],
            "StepsToReproduce": [
                "Run the test suite for org.apache.hadoop.ha.TestZKFailoverController"
            ],
            "ExpectedBehavior": "The test should complete successfully without timing out.",
            "ObservedBehavior": "The test fails with a timeout exception after 25000 milliseconds.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15059.json",
        "creation_time": "2017-11-21T20:54:52.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15059",
            "Title": "3.0 deployment cannot work with old version MR tar ball which breaks rolling upgrade",
            "Description": "Deploying a 3.0 cluster with a 2.9 MR tarball fails due to token incompatibility, preventing rolling upgrades.",
            "StackTrace": [
                "2017-11-21 12:42:51,118 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster",
                "java.lang.RuntimeException: Unable to determine current user",
                "at org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)",
                "Caused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens",
                "Caused by: java.io.IOException: Unknown version 1 in token storage."
            ],
            "StepsToReproduce": [
                "Deploy a 3.0 cluster using a 2.9 MR tarball.",
                "Run a MapReduce job."
            ],
            "ExpectedBehavior": "The MR job should run successfully without errors.",
            "ObservedBehavior": "The MR job fails with a runtime exception related to user determination and token incompatibility.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15307.json",
        "creation_time": "2018-03-12T14:56:59.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15307",
            "Title": "NFS: flavor AUTH_SYS should use VerifierNone",
            "Description": "When NFS gateway starts and if the portmapper request is denied by rpcbind, NFS gateway fails with an UnsupportedOperationException due to the verifier not handling AUTH_SYS.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: Unsupported verifier flavor AUTH_SYS",
                "at org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)",
                "at org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)",
                "at org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)"
            ],
            "StepsToReproduce": [
                "Start the NFS gateway.",
                "Ensure that the portmapper request is denied by rpcbind (e.g., by modifying /etc/hosts.allow)."
            ],
            "ExpectedBehavior": "The NFS gateway should start successfully without throwing an exception.",
            "ObservedBehavior": "The NFS gateway fails to start and throws an UnsupportedOperationException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11446.json",
        "creation_time": "2014-12-23T22:15:23.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11446",
            "Title": "S3AOutputStream should use shared thread pool to avoid OutOfMemoryError",
            "Description": "An OutOfMemoryError occurs when exporting HBase snapshots to S3A due to each TransferManager creating its own thread pool.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:713)",
                "at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)",
                "at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)",
                "at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)",
                "at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)",
                "at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)"
            ],
            "StepsToReproduce": [
                "Increase nofile ulimit to 102400.",
                "Use s3a for HBase snapshot export."
            ],
            "ExpectedBehavior": "The export process should complete without running out of memory.",
            "ObservedBehavior": "An OutOfMemoryError occurs, preventing the export from completing.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-12689.json",
        "creation_time": "2016-01-04T23:30:49.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12689",
            "Title": "S3 filesystem operations stopped working correctly",
            "Description": "After the resolution of HADOOP-10542, several S3 filesystem operations fail due to changes in exception handling, where IOException is thrown instead of returning null.",
            "StackTrace": [
                "2015-12-11 10:04:34,030 FATAL [IPC Server handler 6 on 44861] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1449826461866_0005_m_000006_0 - exited : java.io.IOException: /test doesn't exist",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)",
                "at org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "StepsToReproduce": [
                "Run command: hadoop distcp hdfs://localhost:9000/test s3://xxx:yyy@com.bar.foo/"
            ],
            "ExpectedBehavior": "S3 filesystem operations should succeed without throwing IOException.",
            "ObservedBehavior": "S3 filesystem operations fail with IOException instead of returning null or false.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-13132.json",
        "creation_time": "2016-05-11T11:42:30.000+0000",
        "bug_report": {
            "BugID": "HADOOP-13132",
            "Title": "Handle ClassCastException on AuthenticationException in LoadBalancingKMSClientProvider",
            "Description": "An Oozie job with a single shell action fails due to a ClassCastException when attempting to cast an AuthenticationException to a GeneralSecurityException.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException",
                "at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)",
                "at org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)",
                "at org.apache.hadoop.fs.FileContext.create(FileContext.java:679)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Oozie job should complete successfully without throwing exceptions.",
            "ObservedBehavior": "The Oozie job fails with a ClassCastException, preventing the job from completing and causing YARN logs not to be reported.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15121.json",
        "creation_time": "2017-12-15T07:41:39.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15121",
            "Title": "Encounter NullPointerException when using DecayRpcScheduler",
            "Description": "A NullPointerException occurs in the namenode when setting ipc.8020.scheduler.impl to org.apache.hadoop.ipc.DecayRpcScheduler.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)",
                "at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)",
                "at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)"
            ],
            "StepsToReproduce": [
                "Set ipc.8020.scheduler.impl to org.apache.hadoop.ipc.DecayRpcScheduler",
                "Start the namenode"
            ],
            "ExpectedBehavior": "The namenode should start without exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the namenode from starting.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-8110.json",
        "creation_time": "2012-02-24T18:49:27.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8110",
            "Title": "TestViewFsTrash occasionally fails",
            "Description": "The TestViewFsTrash test fails intermittently, indicating an issue with the trash functionality in the Hadoop file system.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>",
                "at junit.framework.Assert.fail(Assert.java:47)",
                "at junit.framework.Assert.failNotEquals(Assert.java:283)",
                "at junit.framework.Assert.assertEquals(Assert.java:64)",
                "at junit.framework.Assert.assertEquals(Assert.java:195)",
                "at org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)",
                "at org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The trash functionality should correctly handle file deletions without errors.",
            "ObservedBehavior": "The test fails with an assertion error indicating an unexpected value.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11400.json",
        "creation_time": "2014-12-12T10:05:52.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11400",
            "Title": "GraphiteSink does not reconnect to Graphite after 'broken pipe'",
            "Description": "After a network error, GraphiteSink fails to reconnect to the Graphite server, resulting in metrics not being sent.",
            "StackTrace": [
                "org.apache.hadoop.metrics2.MetricsException: Error flushing metrics",
                "at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)",
                "at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)",
                "Caused by: java.net.SocketException: Broken pipe",
                "at java.net.SocketOutputStream.socketWrite0(Native Method)",
                "at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)",
                "at java.net.SocketOutputStream.write(SocketOutputStream.java:159)",
                "at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)",
                "at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)",
                "at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)",
                "at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)",
                "at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)",
                "at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "GraphiteSink should automatically reconnect to the Graphite server after a network error.",
            "ObservedBehavior": "GraphiteSink does not reconnect, and metrics are not sent after a network error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-9865.json",
        "creation_time": "2013-08-12T23:22:58.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9865",
            "Title": "FileContext.globStatus() has a regression with respect to relative path",
            "Description": "The issue occurs when running the unit test TestMRJobClient on Windows, where a job fails due to a relative path being passed to FileContext.globStatus(), causing the test to fail.",
            "StackTrace": [
                "org.apache.hadoop.HadoopIllegalArgumentException: Path is relative",
                "at org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)",
                "at org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)",
                "at org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)",
                "at org.apache.hadoop.fs.Globber.glob(Globber.java:128)",
                "at org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)",
                "at org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the unit test TestMRJobClient on Windows."
            ],
            "ExpectedBehavior": "The job should launch successfully and the unit test should pass.",
            "ObservedBehavior": "The job fails due to a relative path issue, causing the unit test to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-9977.json",
        "creation_time": "2013-09-17T22:11:54.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9977",
            "Title": "Hadoop services won't start with different keypass and keystorepass when https is enabled",
            "Description": "When enabling SSL in the configuration with different keypass and keystore password, Hadoop services such as Namenode, ResourceManager, Datanode, Nodemanager, and SecondaryNamenode fail to start.",
            "StackTrace": [
                "2013-09-17 21:39:00,794 FATAL namenode.NameNode (NameNode.java:main(1325)) - Exception in namenode join",
                "java.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)",
                "Caused by: java.security.UnrecoverableKeyException: Cannot recover key",
                "at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)",
                "at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)",
                "at java.security.KeyStore.getKey(KeyStore.java:792)"
            ],
            "StepsToReproduce": [
                "Enable SSL in the configuration.",
                "Create a keystore with different keypass and keystore password.",
                "Set the properties in ssl-server.xml for keypassword and password.",
                "Attempt to start Hadoop services."
            ],
            "ExpectedBehavior": "Hadoop services should start successfully with the provided SSL configuration.",
            "ObservedBehavior": "Hadoop services fail to start with an UnrecoverableKeyException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-12611.json",
        "creation_time": "2015-12-01T17:14:38.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12611",
            "Title": "TestZKSignerSecretProvider#testMultipleInit occasionally fail",
            "Description": "The test 'testMultipleInit' in 'TestZKSignerSecretProvider' fails intermittently, indicating a potential issue with the ZKSignerSecretProvider.",
            "StackTrace": [
                "java.lang.AssertionError: expected null, but was:<[B@142bad79>",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.junit.Assert.failNotNull(Assert.java:664)",
                "\tat org.junit.Assert.assertNull(Assert.java:646)",
                "\tat org.junit.Assert.assertNull(Assert.java:656)",
                "\tat org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)",
                "2015-11-29 00:24:33,325 ERROR ZKSignerSecretProvider - An unexpected exception occurred while pulling data from ZooKeeper",
                "java.lang.IllegalStateException: instance must be started before calling this method",
                "\tat com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "\tat org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)",
                "\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)",
                "\tat org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The test should pass without any assertion errors.",
            "ObservedBehavior": "The test fails with an assertion error indicating that a value was expected to be null but was not.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10142.json",
        "creation_time": "2013-12-03T06:33:32.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10142",
            "Title": "Avoid groups lookup for unprivileged users such as \"dr.who\"",
            "Description": "Reduce the logs generated by ShellBasedUnixGroupsMapping when using WebHdfs from Windows, which logs warnings for unprivileged users.",
            "StackTrace": [
                "org.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:417)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)",
                "at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)"
            ],
            "StepsToReproduce": [
                "Use WebHdfs from Windows with an unprivileged user such as 'dr.who'."
            ],
            "ExpectedBehavior": "No warnings should be logged for unprivileged users.",
            "ObservedBehavior": "Warnings are logged for each request indicating 'No such user'.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-14727.json",
        "creation_time": "2017-08-02T21:56:38.000+0000",
        "bug_report": {
            "BugID": "HADOOP-14727",
            "Title": "Socket not closed properly when reading Configurations with BlockReaderRemote",
            "Description": "Hosts ran out of file descriptors due to numerous sockets in CLOSE_WAIT state when accessing the Yarn JobHistoryServer web UI. Investigation revealed that the issue is related to the BlockReaderRemote implementation.",
            "StackTrace": [
                "java.lang.Exception: test",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)",
                "at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)",
                "at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)",
                "at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)",
                "at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)"
            ],
            "StepsToReproduce": [
                "Visit the Yarn JobHistoryServer web UI",
                "Click through a job and its logs"
            ],
            "ExpectedBehavior": "Sockets should be closed properly after reading configurations.",
            "ObservedBehavior": "Numerous sockets remain in CLOSE_WAIT state, leading to exhaustion of file descriptors.",
            "Resolution": "Fixed"
        }
    }
]