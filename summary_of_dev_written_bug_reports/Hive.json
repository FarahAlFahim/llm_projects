[
    {
        "filename": "HIVE-10992.json",
        "creation_time": "2015-06-12T19:52:35.000+0000",
        "bug_report": {
            "BugID": "HIVE-10992",
            "Title": "WebHCat should not create delegation tokens when Kerberos is not enabled",
            "Description": "The TempletonControllerJob.run() method incorrectly creates delegation tokens even when Kerberos is not enabled, leading to token cancellation for long-running jobs.",
            "StackTrace": [
                "java.lang.RuntimeException: Exception occurred while finding child jobs",
                "at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:204)",
                "at org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(WebHCatJTShim23.java:158)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(LaunchMapper.java:156)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(LaunchMapper.java:124)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(LaunchMapper.java:261)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "Caused by: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache"
            ],
            "StepsToReproduce": [
                "Submit a long-running job via WebHCat without Kerberos enabled.",
                "Wait for the job to run for more than 24 hours."
            ],
            "ExpectedBehavior": "The system should not create delegation tokens when Kerberos is not enabled.",
            "ObservedBehavior": "The system creates delegation tokens, which are then prematurely cancelled, leading to errors in job execution.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-16450.json",
        "creation_time": "2017-04-14T13:59:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-16450",
            "Title": "Some metastore operations are not retried even with desired underlining exceptions",
            "Description": "In the RetryingHMSHandler class, operations are expected to retry when the cause of MetaException is JDOException or NucleusException. However, in ObjectStore, many instances throw new MetaException without the cause, leading to missed retries.",
            "StackTrace": [
                "2017-04-04 17:28:21,602 ERROR metastore.ObjectStore (ObjectStore.java:getMTableColumnStatistics(6555)) - Error retrieving statistics via jdo",
                "javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(ObjectStore.java:6546)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(ObjectStore.java:6594)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:787)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Operations should retry when the cause of MetaException is JDOException or NucleusException.",
            "ObservedBehavior": "Retries are missed for certain JDOExceptions due to the lack of cause in thrown MetaExceptions.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6389.json",
        "creation_time": "2014-02-07T01:33:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-6389",
            "Title": "LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.",
            "Description": "RCFile tables using LazyBinaryColumnarSerDe fail to handle look-ups into map-columns when the column value is null, resulting in a runtime error.",
            "StackTrace": [
                "2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":null,\"mymap\":null,\"isnull\":null}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)"
            ],
            "StepsToReproduce": [
                "Create an RCFile table with LazyBinaryColumnarSerDe.",
                "Query the table with a map column that contains null values using the syntax: select mymap['1024'] from mytable."
            ],
            "ExpectedBehavior": "The query should return null when the map or the lookup key is null.",
            "ObservedBehavior": "A runtime error occurs, indicating a ClassCastException due to null values in the map.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-2372.json",
        "creation_time": "2011-08-12T09:07:34.000+0000",
        "bug_report": {
            "BugID": "HIVE-2372",
            "Title": "java.io.IOException: error=7, Argument list too long",
            "Description": "A huge query with a perl reducer fails due to an IOException caused by exceeding the argument list size limit in Linux.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)",
                "org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)",
                "org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)",
                "org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "java.io.IOException: Cannot run program \"/usr/bin/perl\": java.io.IOException: error=7, Argument list too long"
            ],
            "StepsToReproduce": [
                "Execute a large query on a table with multiple 2-level partitions.",
                "Include a perl reducer in the query."
            ],
            "ExpectedBehavior": "The reducer should execute successfully without errors.",
            "ObservedBehavior": "The reducer fails with an IOException indicating the argument list is too long.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-2958.json",
        "creation_time": "2012-04-17T15:02:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-2958",
            "Title": "GROUP BY causing ClassCastException [LazyDioInteger cannot be cast LazyInteger]",
            "Description": "A ClassCastException occurs when executing a GROUP BY query on an HBase table in Hive.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)"
            ],
            "StepsToReproduce": [
                "Create an external table in HBase with the specified schema.",
                "Run the query: SELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id."
            ],
            "ExpectedBehavior": "The query should return the count of records grouped by data_resource_id without errors.",
            "ObservedBehavior": "A ClassCastException is thrown during the execution of the GROUP BY query.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13392.json",
        "creation_time": "2016-03-30T22:32:50.000+0000",
        "bug_report": {
            "BugID": "HIVE-13392",
            "Title": "disable speculative execution for ACID Compactor",
            "Description": "CompactorMR is currently not set up to handle speculative execution, leading to file creation issues.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2451)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)"
            ],
            "StepsToReproduce": [
                "Enable speculative execution for mappers and reducers.",
                "Run a job that triggers CompactorMR."
            ],
            "ExpectedBehavior": "The job should complete without file creation errors.",
            "ObservedBehavior": "The job fails with AlreadyBeingCreatedException due to file lease conflicts.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11301.json",
        "creation_time": "2015-07-18T00:41:40.000+0000",
        "bug_report": {
            "BugID": "HIVE-11301",
            "Title": "thrift metastore issue when getting stats results in disconnect",
            "Description": "A Thrift error occurs during the processing of messages in the metastore, leading to a disconnect and retries on the client side.",
            "StackTrace": [
                "2015-07-17 20:32:27,795 ERROR [pool-3-thread-150]: server.TThreadPoolServer (TThreadPoolServer.java:run(294)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "2015-07-17 20:32:27,796 WARN  [main()]: metastore.RetryingMetaStoreClient (RetryingMetaStoreClient.java:invoke(187)) - MetaStoreClient lost connection. Attempting to reconnect.",
                "org.apache.thrift.transport.TTransportException",
                "at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The metastore should return aggregate statistics without errors.",
            "ObservedBehavior": "The client hangs while retrying due to a Thrift error and lost connection.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11028.json",
        "creation_time": "2015-06-16T23:03:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-11028",
            "Title": "Tez: table self join and join with another table fails with IndexOutOfBoundsException",
            "Description": "The issue occurs when performing a self join on a table and joining it with another table, resulting in an IndexOutOfBoundsException.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)"
            ],
            "StepsToReproduce": [
                "Create table tez_self_join1 with columns id1, id2, id3 and insert values.",
                "Create table tez_self_join2 with column id1 and insert values.",
                "Run the explain query that performs a self join on tez_self_join1 and join with tez_self_join2."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results.",
            "ObservedBehavior": "The query fails with an IndexOutOfBoundsException error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14380.json",
        "creation_time": "2016-07-29T00:14:58.000+0000",
        "bug_report": {
            "BugID": "HIVE-14380",
            "Title": "Queries on tables with remote HDFS paths fail in 'encryption' checks.",
            "Description": "When querying tables with remote HDFS paths, an IAException occurs due to an inability to determine if the path is encrypted.",
            "StackTrace": [
                "2016-07-26 01:16:27,471 ERROR parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1867)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to determine if hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table, expected: hdfs://bar.ygrid.yahoo.com:8020",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2204)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStrongestEncryptedTablePath(SemanticAnalyzer.java:2274)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should correctly determine if the HDFS path is encrypted.",
            "ObservedBehavior": "The system throws an exception indicating it cannot determine if the path is encrypted due to a mismatch in the expected filesystem.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7799.json",
        "creation_time": "2014-08-20T09:45:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-7799",
            "Title": "TRANSFORM failed in transform_ppr1.q[Spark Branch]",
            "Description": "A NullPointerException occurs during the execution of a task in Hive on Spark, specifically when using the HiveKVResultCache.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.next(HiveKVResultCache.java:113)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:124)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:82)",
                "at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)",
                "at scala.collection.Iterator$class.foreach(Iterator.scala:727)",
                "at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)",
                "at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.java:65)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "at org.apache.spark.scheduler.Task.run(Task.scala:54)",
                "at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The transform operation should execute without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the transform operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6537.json",
        "creation_time": "2014-03-03T18:57:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-6537",
            "Title": "NullPointerException when loading hashtable for MapJoin directly",
            "Description": "A NullPointerException occurs when attempting to load a hashtable in the MapJoin operation, likely due to uninitialized tables.",
            "StackTrace": [
                "2014-02-20 23:33:15,743 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.NullPointerException",
                "at java.util.Arrays.fill(Arrays.java:2685)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The hashtable should load without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown due to null tables when loading the hashtable.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13691.json",
        "creation_time": "2016-05-04T23:40:03.000+0000",
        "bug_report": {
            "BugID": "HIVE-13691",
            "Title": "No record with CQ_ID=0 found in COMPACTION_QUEUE",
            "Description": "The compactor fails to record an entry in the completed_compaction_queue when there is no corresponding entry in the compaction_queue, leading to an IllegalStateException.",
            "StackTrace": [
                "2016-04-29 18:49:31,594 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(141)) - Caught exception while trying to determine if we should compact id:0,dbname:default,tableName:service_logs_v2,...",
                "Caused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable",
                "at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should record an entry in completed_compaction_queue for failed compactions, even if no entry exists in compaction_queue.",
            "ObservedBehavior": "The system throws an IllegalStateException indicating no record with CQ_ID=0 found in COMPACTION_QUEUE.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-17758.json",
        "creation_time": "2017-10-10T12:33:52.000+0000",
        "bug_report": {
            "BugID": "HIVE-17758",
            "Title": "NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL.defaultLongVal is -1",
            "Description": "The introduction of retry logic in HIVE-16886 caused a negative timeout value to be used, leading to an IllegalArgumentException.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: timeout value is negative",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:7407)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:7361)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7424)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle timeout values correctly without causing exceptions.",
            "ObservedBehavior": "An IllegalArgumentException is thrown due to a negative timeout value.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14898.json",
        "creation_time": "2016-10-06T00:02:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-14898",
            "Title": "HS2 shouldn't log callstack for an empty auth header error",
            "Description": "When the auth header is not sent by the client, HS2 logs an error with a callstack, which is unnecessary since a 401 is returned to the client.",
            "StackTrace": [
                "org.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:170)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:83)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)",
                "Caused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(ThriftHttpServlet.java:311)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "HS2 should not log the callstack for an empty auth header error.",
            "ObservedBehavior": "HS2 logs an error with a callstack when the auth header is empty.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5546.json",
        "creation_time": "2013-10-15T15:06:59.000+0000",
        "bug_report": {
            "BugID": "HIVE-5546",
            "Title": "A change in ORCInputFormat made by HIVE-4113 was reverted by HIVE-5391",
            "Description": "The issue arises from an incorrect assumption in OrcInputFormat.findIncludedColumns when includedStr is an empty string, leading to unnecessary column reads.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "If includedColumnIds is an empty list, no columns should be read.",
            "ObservedBehavior": "The system attempts to read all columns when includedStr is an empty string, leading to an OutOfMemoryError.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7557.json",
        "creation_time": "2014-07-30T19:25:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-7557",
            "Title": "When reduce is vectorized, dynpart_sort_opt_vectorization.q under Tez fails",
            "Description": "The issue occurs when the reduce operation is vectorized, leading to a failure in the dynpart_sort_opt_vectorization.q query under Tez.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The query should execute successfully without runtime errors.",
            "ObservedBehavior": "The query fails with a ClassCastException when reduce is vectorized.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-1712.json",
        "creation_time": "2010-10-14T17:17:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-1712",
            "Title": "Migrating metadata from derby to mysql thrown NullPointerException",
            "Description": "Exported derby data to csv, loaded data into mysql and ran hive query which worked in derby and got a NullPointerException.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.Hashtable.put(Hashtable.java:394)",
                "at java.util.Hashtable.putAll(Hashtable.java:466)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)",
                "at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully migrate metadata from derby to mysql without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the migration process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12608.json",
        "creation_time": "2015-12-07T21:26:01.000+0000",
        "bug_report": {
            "BugID": "HIVE-12608",
            "Title": "Parquet Schema Evolution doesn't work when a column is dropped from array<struct<>>",
            "Description": "An exception occurs when attempting to drop a column from an array of structs in a Parquet table.",
            "StackTrace": [
                "2015-12-07 11:47:28,503 ERROR [main]: CliDriver (SessionState.java:printError(921)) - Failed with exception java.io.IOException:java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)",
                "Caused by: java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getStructFieldTypeInfo(HiveStructConverter.java:130)"
            ],
            "StepsToReproduce": [
                "Create a table with array of structs.",
                "Insert data into the table.",
                "Alter the table to drop a column from the array of structs.",
                "Select data from the table."
            ],
            "ExpectedBehavior": "The system should successfully drop the specified column without errors.",
            "ObservedBehavior": "An IOException is thrown indicating that the field 'c2' cannot be found.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-17774.json",
        "creation_time": "2017-10-11T20:02:01.000+0000",
        "bug_report": {
            "BugID": "HIVE-17774",
            "Title": "Compaction may start with 0 splits and fail",
            "Description": "The system attempts to start a compaction job with no splits, leading to a failure due to a missing file.",
            "StackTrace": [
                "java.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The compaction job should not be attempted if there are no splits.",
            "ObservedBehavior": "The job fails with a FileNotFoundException due to the absence of required files.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14564.json",
        "creation_time": "2016-08-18T00:11:34.000+0000",
        "bug_report": {
            "BugID": "HIVE-14564",
            "Title": "Column Pruning generates out of order columns in SelectOperator which cause ArrayIndexOutOfBoundsException.",
            "Description": "The issue arises from a mismatch in serialization and deserialization of columns due to Column Pruning, leading to an ArrayIndexOutOfBoundsException.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at java.lang.System.arraycopy(Native Method)",
                "at org.apache.hadoop.io.Text.set(Text.java:225)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(LazyBinaryString.java:48)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)",
                "at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:94)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(ReduceSinkOperator.java:550)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:377)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should correctly process rows without throwing an ArrayIndexOutOfBoundsException.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown during processing due to out of order columns.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-3651.json",
        "creation_time": "2012-11-01T23:31:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-3651",
            "Title": "bucketmapjoin?.q tests fail with hadoop 0.23",
            "Description": "The hive.log shows an error in the MapReduce job indicating that a required file is missing, leading to task failure.",
            "StackTrace": [
                "java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)",
                "Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:679)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The MapReduce job should complete successfully without missing file errors.",
            "ObservedBehavior": "The job fails with a missing file error, causing the task to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5199.json",
        "creation_time": "2013-09-03T20:40:29.000+0000",
        "bug_report": {
            "BugID": "HIVE-5199",
            "Title": "Custom SerDe containing a nonSettable complex data type row object inspector throws cast exception with HIVE 0.11",
            "Description": "The issue occurs due to changes in HIVE-3833, where a ClassCastException is thrown when trying to convert nested complex data types with incompatible object inspectors.",
            "StackTrace": [
                "2013-08-28 17:57:25,307 ERROR CliDriver (SessionState.java:printError(432)) - Failed with exception java.io.IOException:java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1412)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "Caused by: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:144)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:307)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:138)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:406)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should correctly convert nested complex data types without throwing a ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown when attempting to convert a non-settable complex data type to a settable type.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-17368.json",
        "creation_time": "2017-08-22T01:27:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-17368",
            "Title": "DBTokenStore fails to connect in Kerberos enabled remote HMS environment",
            "Description": "The HS2 Thrift API call GetDelegationToken fails due to Kerberos authentication issues when the DBTokenStore is configured, preventing HS2 from invoking necessary HMS APIs.",
            "StackTrace": [
                "javax.security.sasl.SaslException: GSS initiate failed",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211)",
                "at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)",
                "Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)"
            ],
            "StepsToReproduce": [
                "Configure HMS as a remote process secured with Kerberos.",
                "Set DBTokenStore as the token store.",
                "Submit a job via Oozie that issues a GetDelegationToken request."
            ],
            "ExpectedBehavior": "HS2 should successfully invoke HMS APIs to manage delegation tokens.",
            "ObservedBehavior": "HS2 fails to connect to HMS due to Kerberos authentication issues, resulting in a SASL negotiation failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-4233.json",
        "creation_time": "2013-03-26T13:02:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-4233",
            "Title": "The TGT gotten from class 'CLIService' should be renewed on time",
            "Description": "When the HiveServer2 has been running for more than 7 days, connecting via beeline shell fails due to Kerberos authentication issues.",
            "StackTrace": [
                "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1084)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:51)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:61)",
                "Caused by: java.lang.IllegalStateException: This ticket is no longer valid",
                "at javax.security.auth.kerberos.KerberosTicket.toString(KerberosTicket.java:601)"
            ],
            "StepsToReproduce": [
                "Start HiveServer2 and let it run for more than 7 days.",
                "Use beeline shell to connect to HiveServer2."
            ],
            "ExpectedBehavior": "The connection to HiveServer2 should succeed and operations should be performed without errors.",
            "ObservedBehavior": "All operations fail due to Kerberos authentication failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14303.json",
        "creation_time": "2016-07-21T03:16:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-14303",
            "Title": "CommonJoinOperator.checkAndGenObject should return directly to avoid NPE if ExecReducer.close is called twice.",
            "Description": "The method CommonJoinOperator.checkAndGenObject should return directly after CommonJoinOperator.closeOp is called to prevent a NullPointerException (NPE) when ExecReducer.close is invoked multiple times.",
            "StackTrace": [
                "Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: null",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)",
                "at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)"
            ],
            "StepsToReproduce": [
                "Call ExecReducer.close() multiple times."
            ],
            "ExpectedBehavior": "The system should handle multiple calls to ExecReducer.close() without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when ExecReducer.close() is called for the second time due to the first call clearing necessary data.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-19248.json",
        "creation_time": "2018-04-19T17:45:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-19248",
            "Title": "REPL LOAD couldn't copy file from source CM path and also doesn't throw error if file copy fails.",
            "Description": "Hive replication fails to copy files from the primary to the replica warehouse when HDFS block sizes differ, leading to file copy failures without proper error reporting.",
            "StackTrace": [
                "2018-04-09 14:32:06,690 ERROR [main] org.apache.hadoop.tools.mapred.CopyMapper: Failure in copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "java.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "Caused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0."
            ],
            "StepsToReproduce": [
                "Attempt to replicate Hive data using REPL LOAD with differing HDFS block sizes between source and target clusters."
            ],
            "ExpectedBehavior": "The file should be copied successfully from the source to the target without errors.",
            "ObservedBehavior": "File copy fails due to block size differences, and no error is thrown despite the failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7167.json",
        "creation_time": "2014-06-02T18:13:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-7167",
            "Title": "Hive Metastore fails to start with SQLServerException",
            "Description": "The Hive Metastore fails to start when hiveserver2 uses an embedded metastore and hiveserver uses a remote metastore, leading to connection errors.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:347)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused: connect",
                "at org.apache.thrift.transport.TSocket.open(TSocket.java:185)"
            ],
            "StepsToReproduce": [
                "Start hiveserver2 with embedded metastore.",
                "Start hiveserver with remote metastore.",
                "Launch hive CLI."
            ],
            "ExpectedBehavior": "The Hive Metastore should start successfully and allow connections.",
            "ObservedBehavior": "The Hive Metastore fails to start, resulting in connection errors.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12360.json",
        "creation_time": "2015-11-06T18:04:00.000+0000",
        "bug_report": {
            "BugID": "HIVE-12360",
            "Title": "Bad seek in uncompressed ORC with predicate pushdown",
            "Description": "Reading from an ORC file fails in HDP-2.3.2 when pushing down a predicate, resulting in an IOException due to an illegal seek operation.",
            "StackTrace": [
                "java.io.IOException: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "Caused by: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.seek(InStream.java:139)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully read from the ORC file without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating that the seek operation is outside of the data.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13160.json",
        "creation_time": "2016-02-26T00:02:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-13160",
            "Title": "HS2 unable to load UDFs on startup when HMS is not ready",
            "Description": "The HiveServer2 (HS2) fails to load User Defined Functions (UDFs) on startup if the Hive Metastore (HMS) is not ready, leading to a state where functions are unavailable for use.",
            "StackTrace": [
                "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)",
                "java.lang.NullPointerException",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)"
            ],
            "StepsToReproduce": [
                "Start HiveServer2 (HS2) while the Hive Metastore (HMS) is not ready.",
                "Attempt to use any UDFs."
            ],
            "ExpectedBehavior": "HS2 should not enter a servicing state if the function list is not ready.",
            "ObservedBehavior": "HS2 starts but fails to register functions, leading to a state where no functions are available for use.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12008.json",
        "creation_time": "2015-10-01T19:26:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-12008",
            "Title": "Hive queries failing when using count(*) on column in view",
            "Description": "The issue occurs when executing count(*) on a view that uses get_json_object() UDF along with lateral views and unions, resulting in a runtime exception.",
            "StackTrace": [
                "2015-10-27 17:51:33,742 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The query should execute successfully and return the correct count.",
            "ObservedBehavior": "The query fails with a runtime exception related to configuration and index out of bounds.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6205.json",
        "creation_time": "2014-01-15T07:34:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-6205",
            "Title": "alter <table> partition column throws NPE in authorization",
            "Description": "A NullPointerException occurs when attempting to alter a table's partition column.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)"
            ],
            "StepsToReproduce": [
                "Execute the command: alter table alter_coltype partition column (dt int);"
            ],
            "ExpectedBehavior": "The command should successfully alter the partition column without errors.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating an issue with authorization.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-15309.json",
        "creation_time": "2016-11-29T21:56:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-15309",
            "Title": "RemoteException(java.io.FileNotFoundException): File does not exist... _flush_length",
            "Description": "OrcAcidUtils.getLastFlushLength() should check for file existence first, currently causing unnecessary/confusing logging.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/...",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)",
                "at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should check for file existence before attempting to access it.",
            "ObservedBehavior": "The system logs a FileNotFoundException when the file does not exist, leading to confusion.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10808.json",
        "creation_time": "2015-05-23T02:24:16.000+0000",
        "bug_report": {
            "BugID": "HIVE-10808",
            "Title": "Inner join on Null throwing Cast Exception",
            "Description": "A Cast Exception occurs when performing an inner join on a null value in Hive.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:334)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:352)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:157)"
            ],
            "StepsToReproduce": [
                "Execute the following SQL query: select a.col1, a.col2, a.col3, a.col4 from tab1 a inner join (select max(x) as x from tab1 where x < 20130327) r on a.x = r.x where a.col1 = 'F' and a.col3 in ('A', 'S', 'G');"
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing any exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException during execution.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-18429.json",
        "creation_time": "2018-01-10T20:45:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-18429",
            "Title": "Compaction should handle a case when it produces no output",
            "Description": "Compaction fails to produce output when starting with empty delta files, leading to a failure in committing the job due to a missing temporary location.",
            "StackTrace": [
                "java.io.FileNotFoundException: File hdfs://OTCHaaS/apps/hive/warehouse/momi.db/sensor_data/babyid=5911806ebf69640100004257/_tmp_b4c5a3f3-44e5-4d45-86af-5b773bf0fc96 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:923)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:114)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:985)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:981)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:992)",
                "at rg.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:785)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Start with empty delta_8_8 and delta_9_9.",
                "Run the compaction process."
            ],
            "ExpectedBehavior": "The compaction process should produce a delta_8_9 even if it's empty.",
            "ObservedBehavior": "The compaction process fails to create the temporary location, leading to a FileNotFoundException during job commit.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10776.json",
        "creation_time": "2015-05-21T00:56:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-10776",
            "Title": "Schema on insert for bucketed tables throwing NullPointerException",
            "Description": "Hive schema on insert queries, with select *, are failing with a NullPointerException.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:7257)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6100)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6271)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8972)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8863)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9708)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9601)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10037)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:323)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)"
            ],
            "StepsToReproduce": [
                "set hive.support.concurrency=true;",
                "set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;",
                "set hive.enforce.bucketing=true;",
                "drop table if exists studenttab10k;",
                "create table studenttab10k (age int, name varchar(50), gpa decimal(3,2));",
                "insert into studenttab10k values(1,'foo', 1.1), (2,'bar', 2.3),(3,'baz', 3.1);",
                "drop table if exists student_acid;",
                "create table student_acid (age int, name varchar(50), gpa decimal(3,2), grade int) clustered by (age) into 2 buckets stored as orc tblproperties ('transactional'='true');",
                "insert into student_acid(name, age, gpa) select * from studenttab10k;"
            ],
            "ExpectedBehavior": "The insert operation should complete successfully without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown during the insert operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6301.json",
        "creation_time": "2014-01-24T01:42:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-6301",
            "Title": "get_json_object throw java.lang.IllegalStateException: No match found exception.",
            "Description": "The issue arises from a missing call to _mKey.matches()_ before using _mKey.group(1)_ in the UDFJson class, leading to an IllegalStateException when certain queries are executed.",
            "StackTrace": [
                "2014-01-23 11:08:19,869 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String) on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2",
                "Caused by: java.lang.IllegalStateException: No match found",
                "at java.util.regex.Matcher.group(Matcher.java:468)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)",
                "... 24 more"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The UDFJson should successfully evaluate the JSON object without throwing an exception.",
            "ObservedBehavior": "An IllegalStateException is thrown indicating 'No match found' when executing certain queries.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8295.json",
        "creation_time": "2014-09-29T21:16:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-8295",
            "Title": "Add batch retrieve partition objects for metastore direct sql",
            "Description": "Direct SQL fails when fetching partition ids exceeding 1000 due to Oracle's maximum expression limit.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\", ...\"",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:422)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:331)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)",
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "StepsToReproduce": [
                "Attempt to fetch partition ids with a filter that results in more than 1000 ids."
            ],
            "ExpectedBehavior": "The system should retrieve partition objects without exceeding SQL expression limits.",
            "ObservedBehavior": "The system fails with a SQLSyntaxErrorException when the number of partition ids exceeds 1000.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8915.json",
        "creation_time": "2014-11-19T19:40:17.000+0000",
        "bug_report": {
            "BugID": "HIVE-8915",
            "Title": "Log file explosion due to non-existence of COMPACTION_QUEUE table",
            "Description": "An endless loop of errors occurred on metastore startup due to the absence of the COMPACTION_QUEUE table, causing the log file to grow excessively.",
            "StackTrace": [
                "2014-11-19 01:44:57,654 ERROR compactor.Cleaner (Cleaner.java:run(143)) - Caught an exception in the main loop of compactor cleaner, MetaException(message:Unable to connect to transaction database",
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hive.COMPACTION_QUEUE' doesn't exist",
                "at sun.reflect.GeneratedConstructorAccessor20.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:386)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2569)",
                "at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1524)",
                "at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:266)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)"
            ],
            "StepsToReproduce": [
                "Set up Hive in a VM without creating the necessary database tables as specified by hive-txn-schema-0.14.0.mysql.sql.",
                "Start the metastore."
            ],
            "ExpectedBehavior": "The metastore should start without errors or log file growth.",
            "ObservedBehavior": "An endless loop of errors occurs, causing the log file to grow to 1.7GB in 5 minutes.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7249.json",
        "creation_time": "2014-06-18T00:09:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-7249",
            "Title": "HiveTxnManager.closeTxnManger() throws if called after commitTxn()",
            "Description": "An exception occurs when calling closeTxnManager() after commitTxn(), indicating that the transaction manager does not recognize that the locks have been released.",
            "StackTrace": [
                "ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - NoSuchLockException(message:No such lock: 1)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(TxnHandler.java:1407)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(TxnHandler.java:477)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.unlock(HiveMetaStore.java:4817)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy14.unlock(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.unlock(HiveMetaStoreClient.java:1598)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(DbLockManager.java:110)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close(DbLockManager.java:162)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct(DbTxnManager.java:300)",
                "at org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager(HiveTxnManagerImpl.java:39)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.closeTxnManager(DbTxnManager.java:43)",
                "at org.apache.hive.hcatalog.mapreduce.TransactionContext.cleanup(TransactionContext.java:327)",
                "at org.apache.hive.hcatalog.mapreduce.TransactionContext.onCommitJob(TransactionContext.java:142)",
                "at org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.commitJob(OutputCommitterContainer.java:61)",
                "at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(FileOutputCommitterContainer.java:251)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:537)"
            ],
            "StepsToReproduce": [
                "Call openTxn() and acquireLocks() for a query that looks like 'INSERT INTO T PARTITION(p) SELECT * FROM T'.",
                "Call commitTxn().",
                "Call closeTxnManager() and observe the exception."
            ],
            "ExpectedBehavior": "The transaction manager should recognize that the locks have been released after commitTxn() is called.",
            "ObservedBehavior": "An exception is thrown indicating that the lock is not found, suggesting that the transaction manager is unaware of the released locks.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11540.json",
        "creation_time": "2015-08-12T23:12:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-11540",
            "Title": "Too many delta files during Compaction - OOM",
            "Description": "The system runs out of memory while compacting delta files during Hive streaming, leading to performance issues.",
            "StackTrace": [
                "2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12. Marking clean to avoid repeated failures, java.io.IOException: Job failed!",
                "at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)"
            ],
            "StepsToReproduce": [
                "Stream weblogs to Kafka and then to Flume using a Hive sink.",
                "Run compactors at various intervals (30m/5m/5s)."
            ],
            "ExpectedBehavior": "The system should handle the load without running out of memory during compaction.",
            "ObservedBehavior": "The system runs out of memory, resulting in failed compaction jobs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-15755.json",
        "creation_time": "2017-01-30T20:48:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-15755",
            "Title": "NullPointerException on invalid table name in ON clause of Merge statement",
            "Description": "An error occurs when an invalid table name is specified in the ON clause of a Merge statement, resulting in a NullPointerException.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)"
            ],
            "StepsToReproduce": [
                "Create a table named 'src' with two columns.",
                "Create a table named 'trgt' with two columns.",
                "Insert a row into the 'src' table.",
                "Execute a merge statement using an invalid table name in the ON clause."
            ],
            "ExpectedBehavior": "The system should handle the invalid table name gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException when an invalid table name is used in the ON clause of the Merge statement.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9390.json",
        "creation_time": "2015-01-15T18:50:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-9390",
            "Title": "Enhance retry logic wrt DB access in TxnHandler",
            "Description": "The system encounters a timeout error when trying to get a JDBC connection from the pool, leading to a failure in transaction management.",
            "StackTrace": [
                "2015-01-13 16:09:21,148 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(141)) - org.apache.thrift.TException: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5324)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:850)",
                "Caused by: MetaException(message:Unable to get jdbc connection from pool, Read timed out)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully retrieve a JDBC connection and manage transactions without timing out.",
            "ObservedBehavior": "The system fails to get a JDBC connection from the pool, resulting in a read timeout error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7623.json",
        "creation_time": "2014-08-05T23:58:27.000+0000",
        "bug_report": {
            "BugID": "HIVE-7623",
            "Title": "hive partition rename fails if filesystem cache is disabled",
            "Description": "The issue occurs when attempting to rename partitions in Hive, resulting in an InvalidOperationException due to the new location being on a different file system than the old location.",
            "StackTrace": [
                "2014-08-05 21:46:14,522 ERROR [pool-3-thread-1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - InvalidOperationException(message:table new location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=123 is on a different file system than the old location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=456. This operation is not supported)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(HiveAlterHandler.java:361)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2629)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2602)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:622)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy5.rename_partition(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9057)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9041)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)"
            ],
            "StepsToReproduce": [
                "Attempt to rename a partition in Hive with filesystem cache disabled."
            ],
            "ExpectedBehavior": "The partition should be renamed successfully without any errors.",
            "ObservedBehavior": "An InvalidOperationException is thrown indicating that the new location is on a different file system than the old location.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-15997.json",
        "creation_time": "2017-02-21T16:49:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-15997",
            "Title": "Resource leaks when query is cancelled",
            "Description": "There may be some resource leaks when a query is cancelled, leading to potential file and lock leaks.",
            "StackTrace": [
                "java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1476)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:535)",
                "at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)",
                "at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:871)",
                "at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive(ZooKeeperHiveLockManager.java:488)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should properly release resources when a query is cancelled.",
            "ObservedBehavior": "Resources such as files and locks may not be released, leading to leaks.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7009.json",
        "creation_time": "2014-05-02T20:50:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-7009",
            "Title": "HIVE_USER_INSTALL_DIR could not bet set to non-HDFS filesystem",
            "Description": "The system enforces that the user path from HIVE_USER_INSTALL_DIR must be HDFS, preventing Hive+Tez jobs from running on non-HDFS filesystems like WASB.",
            "StackTrace": [
                "2014-05-01 00:21:39,847 ERROR exec.Task (TezTask.java:execute(192)) - Failed to execute tez graph.",
                "java.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Hive+Tez jobs should be able to run on non-HDFS filesystems without errors.",
            "ObservedBehavior": "An IOException is thrown indicating that the provided URI is not an HDFS URI.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-2031.json",
        "creation_time": "2011-03-08T11:38:53.000+0000",
        "bug_report": {
            "BugID": "HIVE-2031",
            "Title": "Correct the exception message for better traceability when loading into a partitioned table",
            "Description": "Loading into a partitioned table with two partitions by specifying only one partition in the load statement fails, logging a SemanticException.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)",
                "at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)",
                "at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "Load data into a partitioned table with two partitions by specifying only one partition in the load statement."
            ],
            "ExpectedBehavior": "The system should correctly load data into the specified partition without errors.",
            "ObservedBehavior": "The system throws a SemanticException indicating that the specified partition was not found.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-4018.json",
        "creation_time": "2013-02-13T09:02:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-4018",
            "Title": "MapJoin failing with Distributed Cache error",
            "Description": "When running a star join query after HIVE-3784, it fails with a Distributed Cache error.",
            "StackTrace": [
                "2013-02-13 08:36:04,584 ERROR org.apache.hadoop.hive.ql.exec.MapJoinOperator: Load Distributed Cache Error",
                "2013-02-13 08:36:04,585 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:189)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:203)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1421)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:614)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:266)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The star join query should execute successfully without errors.",
            "ObservedBehavior": "The query fails with a Distributed Cache error and an EOFException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11255.json",
        "creation_time": "2015-07-14T15:39:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-11255",
            "Title": "get_table_objects_by_name() in HiveMetaStore.java needs to retrieve table objects in multiple batches",
            "Description": "The function get_table_objects_by_name() in HiveMetaStore.java currently passes all tables of one database to ObjectStore, leading to a SQLSyntaxErrorException in Oracle due to exceeding the maximum number of expressions in a list.",
            "StackTrace": [
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:945)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1618)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should retrieve table objects without exceeding the maximum number of expressions in a list.",
            "ObservedBehavior": "The system throws a SQLSyntaxErrorException when attempting to retrieve too many table objects at once.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10151.json",
        "creation_time": "2015-03-31T00:17:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-10151",
            "Title": "insert into A select from B is broken when both A and B are Acid tables and bucketed the same way",
            "Description": "The issue occurs when using the BucketingSortingReduceSinkOptimizer with Acid tables, leading to a failure in job submission due to ORC processing errors.",
            "StackTrace": [
                "java.lang.RuntimeException: serious problem",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)",
                "at org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(BucketizedHiveInputFormat.java:141)",
                "Caused by: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_",
                "at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)"
            ],
            "StepsToReproduce": [
                "Create table acidTbl(a int, b int) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')",
                "Create table acidTblPart(a int, b int) partitioned by (p string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true')",
                "Insert into acidTblPart partition(p=1) (a,b) values(1,2)",
                "Insert into acidTbl(a,b) select a,b from acidTblPart where p = 1"
            ],
            "ExpectedBehavior": "The insert operation should complete successfully without errors.",
            "ObservedBehavior": "Job submission fails with a serious problem error related to ORC processing.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13546.json",
        "creation_time": "2016-04-19T07:43:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-13546",
            "Title": "Patch for HIVE-12893 is broken in branch-1",
            "Description": "The SQL query fails with a Hive Runtime Error while processing a row due to an IndexOutOfBoundsException.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:180)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:653)",
                "at java.util.ArrayList.get(ArrayList.java:429)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:151)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:1003)"
            ],
            "StepsToReproduce": [
                "Set various Hive configuration parameters.",
                "Use the specified SQL query to insert data into the 'store_sales' table."
            ],
            "ExpectedBehavior": "The SQL query should execute successfully and insert data into the specified partition.",
            "ObservedBehavior": "The query fails with a runtime error and does not insert any data.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7049.json",
        "creation_time": "2014-05-12T21:46:48.000+0000",
        "bug_report": {
            "BugID": "HIVE-7049",
            "Title": "Unable to deserialize AVRO data when file schema and record schema are different and nullable",
            "Description": "The issue occurs when the file schema and record schema do not match, particularly when the record schema is nullable but the file schema is not. This leads to an AvroRuntimeException during deserialization.",
            "StackTrace": [
                "org.apache.avro.AvroRuntimeException: Not a union: \"string\"",
                "at org.apache.avro.Schema.getTypes(Schema.java:272)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)"
            ],
            "StepsToReproduce": [
                "Set recordSchema to [\"null\", \"string\"]",
                "Set fileSchema to \"string\"",
                "Attempt to deserialize the data"
            ],
            "ExpectedBehavior": "The system should successfully deserialize AVRO data when the schemas are compatible.",
            "ObservedBehavior": "An AvroRuntimeException is thrown indicating that the file schema is not a union type.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9755.json",
        "creation_time": "2015-02-23T20:50:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-9755",
            "Title": "Hive built-in 'ngram' UDAF fails when a mapper has no matches.",
            "Description": "The issue occurs when executing a Hive query that uses the 'ngram' UDAF, resulting in a runtime error when any result has a value equal to null.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":[\"0\",\"0\",\"0\",\"0\"]},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:258)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:506)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:447)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: GenericUDAFnGramEvaluator: mismatch in value for 'n', which usually is caused by a non-constant expression. Found '0' and '1'.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams$GenericUDAFnGramEvaluator.merge(GenericUDAFnGrams.java:242)",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:142)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:658)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:911)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:753)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:249)"
            ],
            "StepsToReproduce": [
                "Run the command: describe ngramtest;",
                "Execute the query: SELECT explode(ngrams(sentences(lower(t.col3)), 3, 10)) as x FROM (SELECT col3 FROM ngramtest WHERE col1=0) t;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without errors.",
            "ObservedBehavior": "The query fails with a Hive Runtime Error when any result has a value equal to null.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-19130.json",
        "creation_time": "2018-04-09T10:18:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-19130",
            "Title": "NPE is thrown when REPL LOAD applied drop partition event.",
            "Description": "During incremental replication, a NullPointerException (NPE) occurs when executing a REPL LOAD on a second batch of events after certain operations on a table.",
            "StackTrace": [
                "2018-04-05 16:20:36,531 ERROR [HiveServer2-Background-Pool: Thread-107044]: metadata.Hive (Hive.java:getTable(1219)) - Table catalog_sales_new not found: new5_tpcds_real_bin_partitioned_orc_1000.catalog_sales_new table not found",
                "2018-04-05 16:20:36,538 ERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)"
            ],
            "StepsToReproduce": [
                "Batch-1: CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION(t1.p1)",
                "Batch-2: DROP_TABLE(t1) -> CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION(t1.p1)"
            ],
            "ExpectedBehavior": "The REPL LOAD should complete without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown during the REPL LOAD process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13090.json",
        "creation_time": "2016-02-18T21:58:48.000+0000",
        "bug_report": {
            "BugID": "HIVE-13090",
            "Title": "Hive metastore crashes on NPE with ZooKeeperTokenStore",
            "Description": "Observed that hive metastore shutdown with NPE from ZookeeperTokenStore.",
            "StackTrace": [
                "org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)",
                "Caused by: java.lang.NullPointerException",
                "at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)",
                "at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The hive metastore should handle token retrieval without crashing.",
            "ObservedBehavior": "The hive metastore crashes with a NullPointerException when no delegation token is found.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5664.json",
        "creation_time": "2013-10-28T03:50:29.000+0000",
        "bug_report": {
            "BugID": "HIVE-5664",
            "Title": "Drop cascade database fails when the db has any tables with indexes",
            "Description": "The last DDL command fails with an error indicating that the database does not exist, despite having just created it.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3473)",
                "org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)",
                "org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)",
                "org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1441)",
                "org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1219)",
                "org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1376)",
                "org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:546)"
            ],
            "StepsToReproduce": [
                "CREATE DATABASE db2;",
                "USE db2;",
                "CREATE TABLE tab1 (id int, name string);",
                "CREATE INDEX idx1 ON TABLE tab1(id) as 'COMPACT' with DEFERRED REBUILD IN TABLE tab1_indx;",
                "DROP DATABASE db2 CASCADE;"
            ],
            "ExpectedBehavior": "The database should be dropped successfully, even if it contains tables with indexes.",
            "ObservedBehavior": "The operation fails with an error stating that the database does not exist.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-15778.json",
        "creation_time": "2017-02-01T04:20:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-15778",
            "Title": "DROP INDEX (non-existent) throws NPE when using DbNotificationListener",
            "Description": "Executing a DROP INDEX operation on a non-existent index results in a NullPointerException (NPE).",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4403)",
                "at org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name_core(HiveMetaStore.java:4469)"
            ],
            "StepsToReproduce": [
                "Execute the command: DROP INDEX IF EXISTS vamsee1 ON sample_07;"
            ],
            "ExpectedBehavior": "The system should handle the request gracefully without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown when trying to drop a non-existent index.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8386.json",
        "creation_time": "2014-10-07T22:30:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-8386",
            "Title": "HCAT api call is case sensitive on fields in struct column",
            "Description": "Falcon using hcat api to verify the target table schema and getting an error due to case sensitivity in field names.",
            "StackTrace": [
                "java.lang.RuntimeException: cannot find field givenName(lowercase form: givenname) in [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase]",
                "at org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(StructTypeInfo.java:109)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(HCatSchemaUtils.java:154)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(HCatSchemaUtils.java:165)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:127)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:115)",
                "at org.apache.hive.hcatalog.api.HCatTable.<init>(HCatTable.java:59)",
                "at org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(HCatClientHMSImpl.java:157)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The HCAT API should handle field names in a case-insensitive manner.",
            "ObservedBehavior": "The API call fails with a RuntimeException indicating that it cannot find the field due to case sensitivity.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14714.json",
        "creation_time": "2016-09-07T15:46:07.000+0000",
        "bug_report": {
            "BugID": "HIVE-14714",
            "Title": "Avoid misleading \"java.io.IOException: Stream closed\" when shutting down HoS",
            "Description": "After executing a hive command with Spark, finishing the beeline session or switching the engine causes an IOException.",
            "StackTrace": [
                "java.io.IOException: Stream closed",
                "at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)",
                "at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)",
                "at java.io.BufferedInputStream.read(BufferedInputStream.java:334)",
                "at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)",
                "at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)",
                "at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)",
                "at java.io.InputStreamReader.read(InputStreamReader.java:184)",
                "at java.io.BufferedReader.fill(BufferedReader.java:154)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:317)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:382)",
                "at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Execute a hive command with Spark.",
                "Finish the beeline session using Ctrl-D or switch the engine."
            ],
            "ExpectedBehavior": "The session should close without errors.",
            "ObservedBehavior": "An IOException is thrown indicating that the stream is closed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5428.json",
        "creation_time": "2013-10-02T20:46:10.000+0000",
        "bug_report": {
            "BugID": "HIVE-5428",
            "Title": "Direct SQL check fails during tests",
            "Description": "An exception occurs during the execution of tests related to the initialization order of the ObjectStore in Hive, specifically when running a command with ant.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"DB_ID\" from \"DBS\"\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:230)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:108)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:249)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:418)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:405)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:444)"
            ],
            "StepsToReproduce": [
                "Run the command: ant test -Dtestcase=TestCliDriver -Dqfile=udf_case.q -Dtest.silent=false"
            ],
            "ExpectedBehavior": "The tests should execute without exceptions, and the SQL query should return valid results.",
            "ObservedBehavior": "An exception is thrown indicating that the table/view 'DBS' does not exist.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12567.json",
        "creation_time": "2015-12-02T16:38:52.000+0000",
        "bug_report": {
            "BugID": "HIVE-12567",
            "Title": "Enhance TxnHandler retry logic to handle ORA-08176",
            "Description": "Error in acquiring locks due to communication issues with the metastore, leading to a lock exception.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:132)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)",
                "Caused by: MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-08176: consistent read failure; rollback data not available)",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should acquire locks without errors when communicating with the metastore.",
            "ObservedBehavior": "The system fails to acquire locks and throws a LockException due to communication issues with the metastore.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6984.json",
        "creation_time": "2014-04-28T23:08:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-6984",
            "Title": "Analyzing partitioned table with NULL values for the partition column failed with NPE",
            "Description": "The issue occurs when analyzing a partitioned table that contains NULL values for the partition column, leading to a NullPointerException.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)"
            ],
            "StepsToReproduce": [
                "Create a table 'test2' with NULL values in the partition column.",
                "Insert data into a new partitioned table 'test3' from 'test2'.",
                "Run 'analyze table test3 partition(age) compute statistics'."
            ],
            "ExpectedBehavior": "The system should analyze the partitioned table without errors, even if NULL values are present in the partition column.",
            "ObservedBehavior": "The system throws a NullPointerException during the analysis of the partitioned table.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10736.json",
        "creation_time": "2015-05-18T03:25:45.000+0000",
        "bug_report": {
            "BugID": "HIVE-10736",
            "Title": "HiveServer2 shutdown of cached tez app-masters is not clean",
            "Description": "The shutdown process throws concurrent modification exceptions and fails to clean up the app masters per queue.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)",
                "at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The shutdown process should cleanly stop all app masters without throwing exceptions.",
            "ObservedBehavior": "The shutdown process throws concurrent modification exceptions and fails to clean up the app masters.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7710.json",
        "creation_time": "2014-08-13T10:46:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-7710",
            "Title": "Rename table across database might fail",
            "Description": "Renaming a table from one database to another fails if the target table name already exists, resulting in a SQLIntegrityConstraintViolationException.",
            "StackTrace": [
                "java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:205)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)"
            ],
            "StepsToReproduce": [
                "Create a table d1.t1.",
                "Create a table d1.t2.",
                "Execute the command: alter table d1.t1 rename to d2.t2;"
            ],
            "ExpectedBehavior": "The table should be renamed successfully if the target name does not exist.",
            "ObservedBehavior": "The rename operation fails with a SQLIntegrityConstraintViolationException due to a duplicate key.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8735.json",
        "creation_time": "2014-11-04T22:20:02.000+0000",
        "bug_report": {
            "BugID": "HIVE-8735",
            "Title": "Statistics update can fail due to long paths",
            "Description": "An error occurs during the publishing of statistics due to a truncation error when trying to shrink a VARCHAR to a length of 255.",
            "StackTrace": [
                "java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:147)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats(FileSinkOperator.java:1153)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The statistics should be published without errors regardless of the length of the path.",
            "ObservedBehavior": "The statistics update fails with a truncation error when the path exceeds the VARCHAR limit.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13209.json",
        "creation_time": "2016-03-04T21:39:50.000+0000",
        "bug_report": {
            "BugID": "HIVE-13209",
            "Title": "metastore get_delegation_token fails with null ip address",
            "Description": "After changes in HIVE-13169, metastore get_delegation_token fails with null IP address.",
            "StackTrace": [
                "2016-03-03 07:45:31,055 ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/<hostname@realm> from IP null)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The metastore should successfully retrieve a delegation token without null IP address.",
            "ObservedBehavior": "The metastore fails to retrieve a delegation token, resulting in an unauthorized connection error due to a null IP address.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13065.json",
        "creation_time": "2016-02-16T21:11:31.000+0000",
        "bug_report": {
            "BugID": "HIVE-13065",
            "Title": "Hive throws NPE when writing map type data to a HBase backed table",
            "Description": "Hive throws a NullPointerException (NPE) when writing data to a HBase backed table if a map type column contains NULL values.",
            "StackTrace": [
                "Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)"
            ],
            "StepsToReproduce": [
                "1) Create a HBase backed Hive table with a map type column.",
                "2) Insert data into the table where the map type column has NULL values."
            ],
            "ExpectedBehavior": "The data should be written to the HBase table without errors.",
            "ObservedBehavior": "The mapreduce job fails with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11470.json",
        "creation_time": "2015-08-05T18:45:26.000+0000",
        "bug_report": {
            "BugID": "HIVE-11470",
            "Title": "NPE in DynamicPartFileRecordWriterContainer on null part-keys.",
            "Description": "When partitioning data using HCatStorer, a NullPointerException occurs if the dynamic partition key is null.",
            "StackTrace": [
                "java.io.IOException: java.lang.NullPointerException",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)",
                "at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)",
                "at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)",
                "at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)",
                "at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)",
                "at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle null dynamic partition keys without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown when a null dynamic partition key is encountered.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12476.json",
        "creation_time": "2015-11-20T03:30:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-12476",
            "Title": "Metastore NPE on Oracle with Direct SQL",
            "Description": "The metastore's Direct SQL mode encounters a NullPointerException, similar to HIVE-8485, requiring additional fixes around Partition/StorageDescriptorSerDe parameters.",
            "StackTrace": [
                "2015-11-19 18:08:33,841 ERROR [pool-5-thread-2]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.",
                "java.lang.NullPointerException",
                "at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)",
                "at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)",
                "at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)",
                "at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The metastore should process messages without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException occurs during the processing of messages in the metastore's Direct SQL mode.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10559.json",
        "creation_time": "2015-04-30T21:12:40.000+0000",
        "bug_report": {
            "BugID": "HIVE-10559",
            "Title": "IndexOutOfBoundsException with RemoveDynamicPruningBySize",
            "Description": "The issue occurs when running a specific script, leading to an IndexOutOfBoundsException.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process(RemoveDynamicPruningBySize.java:61)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:77)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:281)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)"
            ],
            "StepsToReproduce": [
                "Run the attached script."
            ],
            "ExpectedBehavior": "The script should execute without throwing an IndexOutOfBoundsException.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown indicating an attempt to access an index that does not exist.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9721.json",
        "creation_time": "2015-02-19T06:56:17.000+0000",
        "bug_report": {
            "BugID": "HIVE-9721",
            "Title": "Hadoop23Shims.setFullFileStatus should check for null",
            "Description": "The method Hadoop23Shims.setFullFileStatus does not handle null values properly, leading to exceptions when ACLs are enabled on a file system that does not support them.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus",
                "at org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)",
                "at org.apache.hadoop.fs.FilterFileSystem.getAclStatus(FilterFileSystem.java:562)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:645)",
                "at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:524)",
                "at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)",
                "at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle null values gracefully without throwing exceptions when ACLs are enabled.",
            "ObservedBehavior": "The system throws UnsupportedOperationException and NullPointerException when attempting to access ACLs on a file system that does not support them.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-4216.json",
        "creation_time": "2013-03-21T20:53:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-4216",
            "Title": "TestHBaseMinimrCliDriver throws weird error with HBase 0.94.5 and Hadoop 23 and test is stuck infinitely",
            "Description": "After upgrading to Hadoop 23 and HBase 0.94.5, the TestHBaseMinimrCliDriver fails during execution, leading to an infinite loop in the MiniMRCluster.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:268)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:448)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:399)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:237)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:477)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.TaskID$CharTaskTypeMaps.getRepresentingCharacter(TaskID.java:265)"
            ],
            "StepsToReproduce": [
                "Update 'hbase_bulk.m' with properties: set mapreduce.totalorderpartitioner.naturalorder=false; set mapreduce.totalorderpartitioner.path=/tmp/hbpartition.lst;",
                "Run the TestHBaseMinimrCliDriver."
            ],
            "ExpectedBehavior": "The test should complete successfully without errors.",
            "ObservedBehavior": "The test fails with a Hive Runtime Error and the MiniMRCluster gets stuck infinitely.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13836.json",
        "creation_time": "2016-05-24T22:37:59.000+0000",
        "bug_report": {
            "BugID": "HIVE-13836",
            "Title": "DbNotifications giving an error = Invalid state. Transaction has already started",
            "Description": "The issue occurs when using the pyhs2 Python client to create tables/partitions in Hive with multiple concurrent connections, resulting in a transaction error.",
            "StackTrace": [
                "org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started",
                "at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)",
                "at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)",
                "at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)",
                "at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:463)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7522)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.enqueue(DbNotificationListener.java:261)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onCreateTable(DbNotificationListener.java:123)"
            ],
            "StepsToReproduce": [
                "Use pyhs2 Python client to create tables/partitions in Hive.",
                "Run multiple scripts concurrently (8 connections) that execute DDL queries."
            ],
            "ExpectedBehavior": "The system should allow concurrent DDL operations without transaction errors.",
            "ObservedBehavior": "An error occurs indicating that the transaction has already started.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9873.json",
        "creation_time": "2015-03-05T17:35:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-9873",
            "Title": "Hive on MR throws DeprecatedParquetHiveInput exception",
            "Description": "An IOException is thrown when changing column information on projectionPusher.pushProjectionsAndFilters, indicating a size mismatch in the object.",
            "StackTrace": [
                "java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should correctly handle column information changes without throwing exceptions.",
            "ObservedBehavior": "An IOException is thrown due to a size mismatch in the object when changing column information.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13174.json",
        "creation_time": "2016-02-26T23:34:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-13174",
            "Title": "Remove Vectorizer noise in logs",
            "Description": "Excessive stack traces are logged when a table with a bin column is processed, leading to cluttered logs. The stack traces should either be logged at a debug level or replaced with a simpler message.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Logs should not contain excessive stack traces for vectorization failures.",
            "ObservedBehavior": "Logs are filled with stack traces when processing tables with bin columns.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5431.json",
        "creation_time": "2013-10-03T03:35:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-5431",
            "Title": "PassthroughOutputFormat SH changes causes IllegalArgumentException",
            "Description": "The recent changes with HIVE-4331 introduced a new key 'hive.passthrough.storagehandler.of', which is set only on storage handler writes. This leads to failures on reads that are not preceded by writes, resulting in an IllegalArgumentException.",
            "StackTrace": [
                "java.io.IOException: java.lang.IllegalArgumentException: Property value must not be null",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "Caused by: java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1826)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:380)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)"
            ],
            "StepsToReproduce": [
                "Create a .q file that only reads data from an HBase table.",
                "Execute the .q file."
            ],
            "ExpectedBehavior": "The read operation should complete successfully without exceptions.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the property value must not be null.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13115.json",
        "creation_time": "2016-02-22T21:43:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-13115",
            "Title": "MetaStore Direct SQL getPartitions call fail when the columns schemas for a partition are null",
            "Description": "The MetaStore logs show an exception when executing a direct SQL call for getting partitions, specifically when the column schemas for a partition are not set, leading to a fallback to ORM.",
            "StackTrace": [
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)"
            ],
            "StepsToReproduce": [
                "Add a new partition without setting column level schemas using the MetaStoreClient API.",
                "Attempt to retrieve partitions using the getPartitions call."
            ],
            "ExpectedBehavior": "The getPartitions call should succeed without errors.",
            "ObservedBehavior": "The getPartitions call fails with a MetaException due to a null column descriptor ID.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-4723.json",
        "creation_time": "2013-06-12T20:37:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-4723",
            "Title": "DDLSemanticAnalyzer.addTablePartsOutputs eats several exceptions",
            "Description": "An attempt to archive a partition on a non-partitioned table resulted in a NullPointerException due to poor error handling.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2912)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2877)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(DDLSemanticAnalyzer.java:2730)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:316)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:277)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)"
            ],
            "StepsToReproduce": [
                "Attempt to archive a partition on a non-partitioned table."
            ],
            "ExpectedBehavior": "The system should provide a clear error message without throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException and does not handle the error gracefully.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-15686.json",
        "creation_time": "2017-01-20T22:29:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-15686",
            "Title": "Partitions on Remote HDFS break encryption-zone checks",
            "Description": "The issue relates to HIVE-13243, where encryption-zone checks for external tables fail for partitions with remote HDFS paths.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://remote-cluster-nn1.myth.net:8020/dbs/mythdb/myth_table/dt=20170120, expected: hdfs://local-cluster-n1.myth.net:8020",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1985)",
                "at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1290)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.checkTrashPurgeCombination(HiveMetaStore.java:1746)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2974)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should correctly check encryption zones for partitions on remote HDFS paths.",
            "ObservedBehavior": "The system throws an IllegalArgumentException indicating a wrong filesystem when checking encryption zones for remote HDFS paths.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-4975.json",
        "creation_time": "2013-08-01T16:21:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-4975",
            "Title": "Reading orc file throws exception after adding new column",
            "Description": "ORC file read failure after adding a new column to a table. The issue occurs when executing a HiveQL query after altering the table.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "Create a table with three columns (a string, b string, c string).",
                "Add a new column after c by executing 'ALTER TABLE table ADD COLUMNS (d string)'.",
                "Execute HiveQL 'select d from table'."
            ],
            "ExpectedBehavior": "The query should return the new column data without errors.",
            "ObservedBehavior": "An exception is thrown indicating a runtime error while processing the row.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10538.json",
        "creation_time": "2015-04-29T20:06:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-10538",
            "Title": "Fix NPE in FileSinkOperator from hashcode mismatch",
            "Description": "A Null Pointer Exception occurs in FileSinkOperator when using bucketed tables and distribute by with multiFileSpray enabled.",
            "StackTrace": [
                "2015-04-29 12:54:12,841 FATAL [pool-110-thread-1]: ExecReducer (ExecReducer.java:reduce(255)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"113\",\"_col1\":\"val_113\"}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)"
            ],
            "StepsToReproduce": [
                "Set hive.enforce.bucketing = true",
                "Set hive.exec.reducers.max = 20",
                "Create table bucket_a(key int, value_a string) clustered by (key) into 256 buckets",
                "Create table bucket_b(key int, value_b string) clustered by (key) into 256 buckets",
                "Create table bucket_ab(key int, value_a string, value_b string) clustered by (key) into 256 buckets",
                "Insert data into bucket_a and bucket_b",
                "Insert overwrite table bucket_ab select a.key, a.value_a, b.value_b from bucket_a a join bucket_b b on (a.key = b.key) distribute by key"
            ],
            "ExpectedBehavior": "The query should execute without throwing a Null Pointer Exception.",
            "ObservedBehavior": "A Null Pointer Exception is thrown during the execution of the query.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11902.json",
        "creation_time": "2015-09-21T16:12:37.000+0000",
        "bug_report": {
            "BugID": "HIVE-11902",
            "Title": "Abort txn cleanup thread throws SyntaxErrorException",
            "Description": "The DeadTxnReaper code throws a MySQLSyntaxErrorException when attempting to clean up timed-out transactions due to an empty transaction ID list.",
            "StackTrace": [
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:360)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)",
                "at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)",
                "at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Trigger the DeadTxnReaper process.",
                "Ensure that there are timed-out transactions with an empty transaction ID list."
            ],
            "ExpectedBehavior": "The system should handle the cleanup of timed-out transactions without throwing a syntax error.",
            "ObservedBehavior": "The system throws a MySQLSyntaxErrorException due to an invalid SQL query generated when the transaction ID list is empty.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-18918.json",
        "creation_time": "2018-03-09T00:47:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-18918",
            "Title": "Bad error message in CompactorMR.launchCompactionJob()",
            "Description": "The error message produced during major compaction does not provide useful information, making it difficult to diagnose issues.",
            "StackTrace": [
                "2018-02-28 00:59:16,416 ERROR [gdpr1-61]: compactor.Worker (Worker.java:run(191)) - Caught exception while trying to compact id:38602, dbname:audit, tableName:COMP_ENTRY_AF_A, partName:partition_dt=2017-04-11, state:^@, type:MAJOR, properties:null, runAs:null, tooManyAborts:false, highestTxnId:0. Marking failed to avoid repeated failures, java.io.IOException: Major",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob(CompactorMR.java:314)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:269)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:172)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should provide a clear and informative error message when a major compaction fails.",
            "ObservedBehavior": "The error message is vague and does not help in diagnosing the failure during major compaction.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8107.json",
        "creation_time": "2014-09-15T19:49:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-8107",
            "Title": "Bad error message for non-existent table in update and delete",
            "Description": "Executing an update on a non-existent table produces a confusing error message instead of a clear indication of the issue.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)",
                "at org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found no_such_table",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)"
            ],
            "StepsToReproduce": [
                "Run the command: update no_such_table set x = 3;"
            ],
            "ExpectedBehavior": "The system should provide a clear error message indicating that the table does not exist.",
            "ObservedBehavior": "The system produces a complex error message that buries the 'Table not found' message within an exception stack.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-1326.json",
        "creation_time": "2010-04-25T20:50:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-1326",
            "Title": "RowContainer uses hard-coded '/tmp/' path for temporary files",
            "Description": "In a production Hadoop environment, the RowContainer class fills up the '/tmp/' partition instead of using the configured Hadoop temporary path, leading to a 'No space left on device' error.",
            "StackTrace": [
                "FATAL ExecReducer: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:346)",
                "at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)"
            ],
            "StepsToReproduce": [
                "Run a query that uses the RowContainer class in a Hadoop environment with limited /tmp/ space."
            ],
            "ExpectedBehavior": "The RowContainer should use the configured Hadoop temporary path for temporary files.",
            "ObservedBehavior": "The RowContainer writes temporary files to the '/tmp/' path, leading to a full partition and subsequent errors.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11369.json",
        "creation_time": "2015-07-24T16:28:47.000+0000",
        "bug_report": {
            "BugID": "HIVE-11369",
            "Title": "Mapjoins in HiveServer2 fail when jmxremote is used",
            "Description": "The issue occurs when the hive.auto.convert.join property is set to true in HiveServer2 with JMX options enabled, causing execution failures.",
            "StackTrace": [
                "org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)",
                "org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:146)",
                "org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:173)",
                "org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:379)",
                "org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)"
            ],
            "StepsToReproduce": [
                "Set hive.auto.convert.join to true.",
                "Start HiveServer2 with the following JMX options: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=8009.",
                "Execute a MapJoin query."
            ],
            "ExpectedBehavior": "The MapJoin query should execute successfully without errors.",
            "ObservedBehavior": "The execution fails with an error indicating a return code of 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9055.json",
        "creation_time": "2014-12-09T19:51:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-9055",
            "Title": "Tez: union all followed by group by followed by another union all gives error",
            "Description": "An IndexOutOfBoundsException occurs when executing a Hive query that involves a mix of union all, distinct, and group by operations using the Tez execution engine.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: -1, Size: 1",
                "at java.util.LinkedList.checkElementIndex(LinkedList.java:553)",
                "at java.util.LinkedList.get(LinkedList.java:474)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:354)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)"
            ],
            "StepsToReproduce": [
                "Set hive.execution.engine=tez;",
                "Execute the query: select key from (select key from src union all select key from src) tab group by key union all select key from src;"
            ],
            "ExpectedBehavior": "The query should execute successfully without errors.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown, indicating an issue with the query execution.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13856.json",
        "creation_time": "2016-05-25T21:50:49.000+0000",
        "bug_report": {
            "BugID": "HIVE-13856",
            "Title": "Fetching transaction batches during ACID streaming against Hive Metastore using Oracle DB fails",
            "Description": "The system encounters a SQL syntax error when attempting to fetch transaction batches from the Hive Metastore using Oracle DB, specifically due to the way multiple rows are inserted into the TXNS table.",
            "StackTrace": [
                "java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)",
                "at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)",
                "at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)",
                "at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)",
                "at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)",
                "at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully fetch transaction batches without encountering SQL syntax errors.",
            "ObservedBehavior": "The system fails with a SQL syntax error when trying to insert multiple rows into the TXNS table.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7374.json",
        "creation_time": "2014-07-09T16:02:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-7374",
            "Title": "SHOW COMPACTIONS fail with remote metastore when there are no compactions",
            "Description": "The command 'show compactions' fails when executed against a remote metastore that has no compactions, resulting in an error.",
            "StackTrace": [
                "org.apache.thrift.protocol.TProtocolException: Required field 'compacts' is unset! Struct:ShowCompactResponse(compacts:null)",
                "at org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate(ShowCompactResponse.java:310)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "Set up a remote metastore.",
                "Ensure there are no compactions.",
                "Execute the command 'show compactions' in the CLI."
            ],
            "ExpectedBehavior": "The command should return a list of compactions or an empty response without errors.",
            "ObservedBehavior": "The command returns an error indicating a transport exception and a protocol exception due to an unset required field.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12206.json",
        "creation_time": "2015-10-17T00:30:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-12206",
            "Title": "ClassNotFound Exception during query compilation with Tez and Union query and GenericUDFs",
            "Description": "A ClassNotFoundException occurs when executing a query that uses a user-defined function (UDF) with Tez and Union queries.",
            "StackTrace": [
                "org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "Caused by: java.lang.ClassNotFoundException: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)"
            ],
            "StepsToReproduce": [
                "Run the query: explain select myudf() from (select key from src limit 1) a union all select myudf() from (select key from src limit 1) a;",
                "Ensure the UDF jar is added: add jar /tmp/udf-2.2.0-snapshot.jar;",
                "Create the temporary function: create temporary function myudf as 'com.aginity.amp.hive.udf.UniqueNumberGenerator';"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results using the UDF.",
            "ObservedBehavior": "The query fails with a ClassNotFoundException indicating that the UDF class cannot be found.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10098.json",
        "creation_time": "2015-03-26T17:12:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-10098",
            "Title": "HS2 local task for map join fails in KMS encrypted cluster",
            "Description": "Any Hive query via beeline that performs a MapJoin fails with a java.lang.reflect.UndeclaredThrowableException from KMSClientProvider.addDelegationTokens.",
            "StackTrace": [
                "java.io.IOException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:634)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:363)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:303)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1655)",
                "Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)"
            ],
            "StepsToReproduce": [
                "Create table jsmall with appropriate schema.",
                "Create table jbig1 with appropriate schema.",
                "Load data into jsmall and jbig1 from local paths.",
                "Execute a join query on jsmall and jbig1."
            ],
            "ExpectedBehavior": "The MapJoin query should execute successfully without errors.",
            "ObservedBehavior": "The query fails with an UndeclaredThrowableException due to authentication issues.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7745.json",
        "creation_time": "2014-08-16T01:20:06.000+0000",
        "bug_report": {
            "BugID": "HIVE-7745",
            "Title": "NullPointerException when turn on hive.optimize.union.remove, hive.merge.mapfiles and hive.merge.mapredfiles [Spark Branch]",
            "Description": "When the hive.optimize.union.remove, hive.merge.mapfiles and hive.merge.mapredfiles are turned on, it throws NullPointerException during specific queries. The issue does not occur when these settings are turned off.",
            "StackTrace": [
                "2014-08-16 01:32:26,849 ERROR [main]: ql.Driver (SessionState.java:printError(681)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(GenMapRedUtils.java:1738)",
                "at org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.processFileSink(GenSparkUtils.java:281)",
                "at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:187)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:199)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9508)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:414)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1005)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1070)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "StepsToReproduce": [
                "Turn on hive.optimize.union.remove, hive.merge.mapfiles, and hive.merge.mapredfiles.",
                "Execute the following queries: create table inputTbl1(key string, val string) stored as textfile; create table outputTbl1(key string, values bigint) stored as rcfile; load data local inpath '../../data/files/T1.txt' into table inputTbl1; explain insert overwrite table outputTbl1 SELECT * FROM (select key, count(1) as values from inputTbl1 group by key union all select * FROM (SELECT key, 1 as values from inputTbl1 UNION ALL SELECT key, 2 as values from inputTbl1) a) b;"
            ],
            "ExpectedBehavior": "The queries should execute without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the specified settings are enabled.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11762.json",
        "creation_time": "2015-09-08T20:10:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-11762",
            "Title": "TestHCatLoaderEncryption failures when using Hadoop 2.7",
            "Description": "When running TestHCatLoaderEncryption with -Dhadoop23.version=2.7.0, a NoSuchMethodError occurs during setup due to a method signature change in Hadoop.",
            "StackTrace": [
                "java.lang.NoSuchMethodError: org.apache.hadoop.hdfs.DFSClient.setKeyProvider(Lorg/apache/hadoop/crypto/key/KeyProviderCryptoExtension;)V",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Hadoop23Shims.java:534)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.initEncryptionShim(TestHCatLoaderEncryption.java:252)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.setup(TestHCatLoaderEncryption.java:200)"
            ],
            "StepsToReproduce": [
                "Run TestHCatLoaderEncryption with -Dhadoop23.version=2.7.0"
            ],
            "ExpectedBehavior": "The test should run successfully without errors.",
            "ObservedBehavior": "A NoSuchMethodError is thrown during the test setup.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6990.json",
        "creation_time": "2014-04-30T04:24:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-6990",
            "Title": "Direct SQL fails when the explicit schema setting is different from the default one",
            "Description": "An error occurs in hive.log indicating that a direct SQL query failed, leading to a fallback to ORM.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select PARTITIONS.PART_ID from PARTITIONS inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)"
            ],
            "StepsToReproduce": [
                "Set the following properties in hive-site.xml:",
                "<property><name>javax.jdo.mapping.Schema</name><value>HIVE</value></property>",
                "<property><name>javax.jdo.option.ConnectionUserName</name><value>user1</value></property>",
                "Execute hive queries: create table mytbl ( key int, value string);",
                "Load data local inpath 'examples/files/kv1.txt' overwrite into table mytbl;",
                "Select * from mytbl;",
                "Create view myview partitioned on (value) as select key, value from mytbl where key=98;",
                "Alter view myview add partition (value='val_98') partition (value='val_xyz');",
                "Alter view myview drop partition (value='val_xyz');"
            ],
            "ExpectedBehavior": "The SQL query should execute successfully without falling back to ORM.",
            "ObservedBehavior": "The SQL query fails, resulting in an error logged in hive.log.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7114.json",
        "creation_time": "2014-05-22T14:58:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-7114",
            "Title": "Extra Tez session is started during HiveServer2 startup",
            "Description": "When starting the HiveServer2, an extra Tez Application Master (AM) is launched, which is not expected.",
            "StackTrace": [
                "java.lang.Exception: Opening session",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)",
                "at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)",
                "at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)",
                "at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Only one Tez session should be started during HiveServer2 startup.",
            "ObservedBehavior": "An extra Tez session is launched, leading to unexpected behavior.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11991.json",
        "creation_time": "2015-09-29T21:46:06.000+0000",
        "bug_report": {
            "BugID": "HIVE-11991",
            "Title": "groupby11.q failing on branch-1.0",
            "Description": "Running groupby11.q on the branch-1.0 branch hits an error related to ClassCastException.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1621)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:221)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:153)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:364)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The query should execute successfully without any exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-17900.json",
        "creation_time": "2017-10-25T17:12:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-17900",
            "Title": "analyze stats on columns triggered by Compactor generates malformed SQL with > 1 partition column",
            "Description": "The issue occurs when analyzing stats on columns, leading to a malformed SQL statement due to a ParseException.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:296)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully analyze stats on columns without generating malformed SQL.",
            "ObservedBehavior": "The system fails with a ParseException indicating a syntax error in the SQL statement.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10816.json",
        "creation_time": "2015-05-25T08:08:37.000+0000",
        "bug_report": {
            "BugID": "HIVE-10816",
            "Title": "NPE in ExecDriver::handleSampling when submitted via child JVM",
            "Description": "When {{hive.exec.submitviachild = true}}, parallel order by fails with NPE and falls back to single-reducer mode.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle sampling without throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException and falls back to single-reducer mode.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13017.json",
        "creation_time": "2016-02-05T23:40:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-13017",
            "Title": "Child process of HiveServer2 fails to get delegation token from non default FileSystem",
            "Description": "The query execution fails when using Azure Filesystem as the default file system and HDFS for intermediate data, resulting in an execution error.",
            "StackTrace": [
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Create temporary table s10k stored as orc as select * from studenttab10k;",
                "Create temporary table v10k as select * from votertab10k;",
                "Execute the query: select registration from s10k s join v10k v on (s.name = v.name) join studentparttab30k p on (p.name = v.name) where s.age < 25 and v.age < 25 and p.age < 25;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results.",
            "ObservedBehavior": "The query fails with an execution error, returning code 2 from MapredLocalTask.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11303.json",
        "creation_time": "2015-07-18T01:31:02.000+0000",
        "bug_report": {
            "BugID": "HIVE-11303",
            "Title": "Getting Tez LimitExceededException after dag execution on large query",
            "Description": "The system throws a LimitExceededException when executing a Tez DAG that exceeds the maximum allowed counters.",
            "StackTrace": [
                "org.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200",
                "at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)",
                "at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)",
                "at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should execute the Tez DAG without exceeding the maximum counter limit.",
            "ObservedBehavior": "The system fails to execute the Tez graph due to exceeding the maximum allowed counters.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5899.json",
        "creation_time": "2013-11-27T02:48:39.000+0000",
        "bug_report": {
            "BugID": "HIVE-5899",
            "Title": "NPE during explain extended with char/varchar columns",
            "Description": "Running analyze table for columns with char/varchar columns and subsequently trying to run explain extended results in a NullPointerException when Hive attempts to annotate the operator tree with stats.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:367)",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats(ColumnStatisticsData.java:444)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:414)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStatsForColumn(StatsUtils.java:369)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:465)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:102)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)",
                "at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:76)",
                "at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:136)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8913)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:341)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:994)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:905)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "Run analyze table on columns with char/varchar types.",
                "Execute explain extended on the same table."
            ],
            "ExpectedBehavior": "The explain extended command should execute without errors and provide the expected output.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of explain extended.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11102.json",
        "creation_time": "2015-06-24T22:54:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-11102",
            "Title": "ReaderImpl: getColumnIndicesFromNames does not work for some cases",
            "Description": "The ORC reader implementation does not estimate the size of ACID data files correctly, leading to an IndexOutOfBoundsException.",
            "StackTrace": [
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:938)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:847)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:713)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ORC reader should correctly estimate the size of ACID data files without throwing exceptions.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when attempting to access an index in an empty list.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9195.json",
        "creation_time": "2014-12-23T01:08:45.000+0000",
        "bug_report": {
            "BugID": "HIVE-9195",
            "Title": "CBO changes constant to column type",
            "Description": "The cost-based optimizer (CBO) changes constant expressions to column expressions, leading to an argument type exception in TestCliDriver.",
            "StackTrace": [
                "2014-12-22 17:03:31,433 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:analyzeInternal(10102)) - CBO failed, skipping CBO.",
                "org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:146)",
                "at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:1160)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:3794)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:4467)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5536)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8884)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9745)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10086)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:120)"
            ],
            "StepsToReproduce": [
                "Create a table with clustered and sorted data.",
                "Load data into the table from local files.",
                "Execute a query using percentile_approx with a conditional expression."
            ],
            "ExpectedBehavior": "The query should execute without errors, returning the expected results.",
            "ObservedBehavior": "An argument type exception occurs, indicating that a non-constant value was passed where a constant was expected.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11285.json",
        "creation_time": "2015-07-16T20:46:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-11285",
            "Title": "ObjectInspector for partition columns in FetchOperator in SMBJoin causes exception",
            "Description": "A runtime exception occurs during the processing of rows in a Hive query involving partitioned tables.",
            "StackTrace": [
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"key\":1,\"value\":\"One\"}",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Integer",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(JavaIntObjectInspector.java:35)"
            ],
            "StepsToReproduce": [
                "Create a table 'data_table' with key and value columns.",
                "Load data from 'data.out' into 'data_table'.",
                "Create partitioned table 'smb_table_part' and insert data from 'data_table'.",
                "Execute a join query between 'smb_table' and 'smb_table_part'."
            ],
            "ExpectedBehavior": "The join query should execute successfully and return the expected results.",
            "ObservedBehavior": "A runtime exception occurs, preventing the query from completing.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10288.json",
        "creation_time": "2015-04-09T23:24:16.000+0000",
        "bug_report": {
            "BugID": "HIVE-10288",
            "Title": "Cannot call permanent UDFs",
            "Description": "After creating a permanent UDF and exiting the CLI, attempting to call the UDF fails with a NullPointerException. The call succeeds if made immediately after registering the UDF.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:232)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1048)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1265)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10338)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3815)"
            ],
            "StepsToReproduce": [
                "Create a permanent UDF.",
                "Exit the Hive CLI.",
                "Reopen the Hive CLI and attempt to call the UDF."
            ],
            "ExpectedBehavior": "The UDF should be callable after reopening the CLI.",
            "ObservedBehavior": "A NullPointerException occurs when trying to call the UDF after reopening the CLI.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8771.json",
        "creation_time": "2014-11-07T00:30:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-8771",
            "Title": "Abstract merge file operator does not move/rename incompatible files correctly",
            "Description": "The AbstractFileMergeOperator fails to move incompatible files to the final destination due to the destination path being a file instead of a directory, leading to an IOException.",
            "StackTrace": [
                "java.lang.Exception: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)",
                "Caused by: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:100)",
                "Caused by: java.io.FileNotFoundException: Destination exists and is not a directory: /home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0",
                "at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:423)"
            ],
            "StepsToReproduce": [
                "Run orc_merge_incompat2.q under CentOS with incompatible files."
            ],
            "ExpectedBehavior": "The operator should successfully move incompatible files to the final destination directory.",
            "ObservedBehavior": "The operation fails with an IOException indicating that the destination exists and is not a directory.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8008.json",
        "creation_time": "2014-09-05T23:00:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-8008",
            "Title": "NPE while reading null decimal value",
            "Description": "The system crashes with a NullPointerException (NPE) when querying a table with a decimal value of 9999999999.5.",
            "StackTrace": [
                "2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)",
                "... 12 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "Create a table with a decimal column.",
                "Insert a row with a decimal value of 9999999999.5.",
                "Execute the query 'select * from dec_test;'"
            ],
            "ExpectedBehavior": "The query should return the row without errors.",
            "ObservedBehavior": "The system crashes with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6915.json",
        "creation_time": "2014-04-15T20:20:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-6915",
            "Title": "Hive Hbase queries fail on secure Tez cluster",
            "Description": "Hive queries reading and writing to HBase are currently failing with a SASL authentication error in a secure Tez cluster.",
            "StackTrace": [
                "2014-04-14 13:47:05,644 FATAL [InputInitializer [Map 1] #0] org.apache.hadoop.ipc.RpcClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.",
                "javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)",
                "at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Hive queries should execute successfully in a secure Tez cluster.",
            "ObservedBehavior": "Hive queries fail with a SASL authentication error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12364.json",
        "creation_time": "2015-11-07T02:04:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-12364",
            "Title": "Distcp job fails when run under Tez",
            "Description": "The issue occurs when executing an insert overwrite command that invokes distcp for a moveTask, resulting in a failure when the execution engine is Tez.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir",
                "at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)",
                "at org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(MoveTask.java:105)",
                "at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:222)",
                "Caused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatibility mode."
            ],
            "StepsToReproduce": [
                "Set hive.exec.copyfile.maxsize=40000",
                "Execute the command: insert overwrite into '/tmp/testinser' select * from customer"
            ],
            "ExpectedBehavior": "The data should be successfully moved to the specified destination without errors.",
            "ObservedBehavior": "The moveTask fails with an exception indicating an inability to move the source file to the destination.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8766.json",
        "creation_time": "2014-11-06T22:08:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-8766",
            "Title": "Hive RetryHMSHandler should be retrying the metastore operation in case of NucleusException",
            "Description": "When Metastore operations are performed under heavy load or slow response from the Metastore Database, NucleusExceptions may occur. The current retry mechanism fails to handle these exceptions properly, particularly with SQL Server configured to timeout connections.",
            "StackTrace": [
                "2014-11-04 06:40:03,208 ERROR bonecp.ConnectionHandle (ConnectionHandle.java:markPossiblyBroken(388)) - Database access problem. Killing off this connection and all remaining connections in the connection pool. SQL State = 08S01",
                "2014-11-04 06:40:03,213 ERROR DataNucleus.Transaction (Log4JLogger.java:error(115)) - Operation rollback failed on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16, error code UNKNOWN and transaction: [DataNucleus Transaction, ID=Xid=   \ufffd, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16]]",
                "Caused by: org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0",
                "Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: SSL peer shut down incorrectly"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The HMS Retrying mechanism should allow retries when encountering a NucleusException, preventing failures in hive queries.",
            "ObservedBehavior": "Hive queries fail with a MetaException due to NucleusExceptions when the Metastore Database is under heavy load.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5857.json",
        "creation_time": "2013-11-20T01:12:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-5857",
            "Title": "Reduce tasks do not work in uber mode in YARN",
            "Description": "A Hive query fails when it tries to run a reduce task in uber mode in YARN due to a NullPointerException caused by a missing plan file (reduce.xml).",
            "StackTrace": [
                "java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)"
            ],
            "StepsToReproduce": [
                "Run a Hive query that requires a reduce task in uber mode in YARN."
            ],
            "ExpectedBehavior": "The reduce task should execute successfully without throwing an exception.",
            "ObservedBehavior": "The reduce task fails with a NullPointerException due to a missing plan file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-1547.json",
        "creation_time": "2010-08-17T02:09:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-1547",
            "Title": "Unarchiving operation throws NPE",
            "Description": "Unarchiving a partition throws a null pointer exception.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The unarchiving operation should complete successfully without exceptions.",
            "ObservedBehavior": "A null pointer exception is thrown during the unarchiving operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6113.json",
        "creation_time": "2013-12-27T07:07:00.000+0000",
        "bug_report": {
            "BugID": "HIVE-6113",
            "Title": "Upgrade DataNucleus [was: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient]",
            "Description": "When executing SQL 'use fdm; desc formatted fdm.tableName;' in Python, an error occurs related to the HiveMetaStoreClient instantiation. The command succeeds upon retry.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1143)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)",
                "Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore",
                "Caused by: java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'"
            ],
            "StepsToReproduce": [
                "Execute SQL command 'use fdm; desc formatted fdm.tableName;' in Python."
            ],
            "ExpectedBehavior": "The command should execute successfully without errors.",
            "ObservedBehavior": "An error occurs indicating the inability to instantiate HiveMetaStoreClient, but the command succeeds on retry.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9570.json",
        "creation_time": "2015-02-03T23:30:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-9570",
            "Title": "Investigate test failure on union_view.q [Spark Branch]",
            "Description": "The test union_view.q failed due to a NullPointerException in the SparkCompiler.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:274)",
                "at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:253)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:222)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10231)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)",
                "at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:136)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The union_view.q test should pass without exceptions.",
            "ObservedBehavior": "The test fails with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-1678.json",
        "creation_time": "2010-10-01T05:41:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-1678",
            "Title": "NPE in MapJoin",
            "Description": "The query with two map joins and a group by fails with a NullPointerException.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The query should process without throwing a NullPointerException.",
            "ObservedBehavior": "The query fails with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11820.json",
        "creation_time": "2015-09-14T22:20:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-11820",
            "Title": "Export tables with size of >32MB throws \"java.lang.IllegalArgumentException: Skip CRC is valid only with update options\"",
            "Description": "When exporting tables larger than 32MB, an IllegalArgumentException is thrown indicating that 'Skip CRC is valid only with update options'.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Skip CRC is valid only with update options",
                "at org.apache.hadoop.tools.DistCpOptions.validate(DistCpOptions.java:556)",
                "at org.apache.hadoop.tools.DistCpOptions.setSkipCRC(DistCpOptions.java:311)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1147)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The export operation should complete successfully without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown when attempting to export tables larger than 32MB.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-17274.json",
        "creation_time": "2017-08-08T22:20:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-17274",
            "Title": "RowContainer spills for timestamp column throws exception",
            "Description": "Path names cannot contain ':' which leads to an IllegalArgumentException when a join key is a timestamp column.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:205)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:171)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:93)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:94)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:404)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926)",
                "at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1137)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter(RowContainer.java:538)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:299)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle timestamp columns without throwing exceptions.",
            "ObservedBehavior": "An IllegalArgumentException is thrown when attempting to spill RowContainer with a timestamp column.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12522.json",
        "creation_time": "2015-11-25T19:48:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-12522",
            "Title": "Wrong FS error during Tez merge files when warehouse and scratchdir are on different FS",
            "Description": "When hive.merge.tezfiles=true, and the warehouse dir/scratchdir are on different filesystems, an IllegalArgumentException occurs.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Wrong FS: wasb://chaoyiteztest@chaoyiteztest.blob.core.windows.net/hive/scratch/chaoyitest/c888f405-3c98-46b1-bf39-e57f067dfe4c/hive_2015-11-13_10-16-10_216_8161037519951665173-1/_tmp.-ext-10000, expected: hdfs://headnodehost:9000",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1136)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:579)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)"
            ],
            "StepsToReproduce": [
                "Set hive.merge.tezfiles=true",
                "Ensure warehouse dir and scratchdir are on different filesystems"
            ],
            "ExpectedBehavior": "The Tez task should execute successfully without any filesystem errors.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a wrong filesystem.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-16845.json",
        "creation_time": "2017-06-07T17:07:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-16845",
            "Title": "INSERT OVERWRITE a table with dynamic partitions on S3 fails with NPE",
            "Description": "An NPE occurs when attempting to insert data into a partitioned table on S3 using dynamic partitions.",
            "StackTrace": [
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)",
                "at org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)"
            ],
            "StepsToReproduce": [
                "Create a partitioned table on S3.",
                "Create a temp table.",
                "Load rows into the temp table.",
                "Set necessary parameters.",
                "Insert rows from the temp table into the S3 table."
            ],
            "ExpectedBehavior": "The rows should be successfully inserted into the partitioned table without errors.",
            "ObservedBehavior": "An NPE occurs during the insert operation, causing the process to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9655.json",
        "creation_time": "2015-02-11T20:58:13.000+0000",
        "bug_report": {
            "BugID": "HIVE-9655",
            "Title": "Dynamic partition table insertion error",
            "Description": "An error occurs when trying to insert data into a dynamic partitioned table from another table with the same column structure.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"c1\":1,\"c2\":\"one\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]"
            ],
            "StepsToReproduce": [
                "Create table t1 with columns c1 and c2.",
                "Create table t2 with columns c1 and c2, partitioned by p1.",
                "Load data into table t1.",
                "Set dynamic partition mode to nonstrict.",
                "Insert into table t2 from t1 using the specified query."
            ],
            "ExpectedBehavior": "Data should be inserted into the dynamic partitioned table without errors.",
            "ObservedBehavior": "The query fails with a Hive Runtime Error indicating a missing field.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11441.json",
        "creation_time": "2015-08-03T17:42:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-11441",
            "Title": "No DDL allowed on table if user accidentally set table location wrong",
            "Description": "When a user sets an incorrect HDFS location for a table, Hive should either correct it or allow the user to fix it. Instead, it throws an error when trying to alter the table location.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1323)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(DDLSemanticAnalyzer.java:1452)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:295)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1131)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)"
            ],
            "StepsToReproduce": [
                "Create table testwrongloc(id int);",
                "Alter table testwrongloc set location 'hdfs://a-valid-hostname/tmp/testwrongloc';",
                "Alter table testwrongloc set location 'hdfs://correct-host:8020/tmp/testwrongloc' or drop table testwrongloc;"
            ],
            "ExpectedBehavior": "Hive should throw an error indicating that the specified HDFS path is invalid.",
            "ObservedBehavior": "Hive throws an error stating that the host 'a-valid-hostname' is not reachable.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10801.json",
        "creation_time": "2015-05-22T19:43:23.000+0000",
        "bug_report": {
            "BugID": "HIVE-10801",
            "Title": "'drop view' fails throwing java.lang.NullPointerException",
            "Description": "When trying to drop a view, a NullPointerException occurs due to a missing encryption key provider URI.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1041)"
            ],
            "StepsToReproduce": [
                "Attempt to drop a view in Hive."
            ],
            "ExpectedBehavior": "The view should be dropped without errors.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the view from being dropped.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9141.json",
        "creation_time": "2014-12-17T07:23:05.000+0000",
        "bug_report": {
            "BugID": "HIVE-9141",
            "Title": "HiveOnTez: mix of union all, distinct, group by generates error",
            "Description": "A ClassCastException occurs when executing a Hive query that combines union all, distinct, and group by operations using the Tez execution engine.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:361)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)"
            ],
            "StepsToReproduce": [
                "Set hive.execution.engine=tez;",
                "Execute the following query: SELECT key, value FROM (SELECT key, value FROM src UNION ALL SELECT key, key as value FROM (SELECT distinct key FROM (SELECT key, value FROM src UNION ALL SELECT key, value FROM src)t1 group by key, value)t2)t4 group by key, value;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results.",
            "ObservedBehavior": "A ClassCastException is thrown, preventing the query from executing.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10010.json",
        "creation_time": "2015-03-18T17:48:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-10010",
            "Title": "Alter table results in NPE [hbase-metastore branch]",
            "Description": "Performing an alter table operation results in a NullPointerException.",
            "StackTrace": [
                "2015-03-18 10:45:54,189 ERROR [main]: exec.DDLTask (DDLTask.java:failed(512)) - java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)",
                "at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)",
                "at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)",
                "at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The alter table operation should complete successfully without errors.",
            "ObservedBehavior": "The operation results in a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7763.json",
        "creation_time": "2014-08-18T09:35:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-7763",
            "Title": "Failed to query TABLESAMPLE on empty bucket table [Spark Branch]",
            "Description": "An exception occurs when attempting to query TABLESAMPLE on an empty bucket table.",
            "StackTrace": [
                "2014-08-18 16:23:15,213 ERROR [Executor task launch worker-0]: executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)",
                "java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:127)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:52)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:404)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should successfully execute the TABLESAMPLE query on a bucketed table.",
            "ObservedBehavior": "The system throws a RuntimeException indicating that the map operator initialization failed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12083.json",
        "creation_time": "2015-10-09T22:45:47.000+0000",
        "bug_report": {
            "BugID": "HIVE-12083",
            "Title": "HIVE-10965 introduces thrift error if partNames or colNames are empty",
            "Description": "A short-circuit path in the fix for HIVE-10965 returns an empty AggrStats object when partNames or colNames are empty, leading to a Thrift protocol error due to required fields being unset.",
            "StackTrace": [
                "2015-10-08 00:00:25,413 ERROR server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should not return an empty AggrStats object when partNames or colNames are empty, preventing Thrift protocol errors.",
            "ObservedBehavior": "An empty AggrStats object is returned, leading to a Thrift error due to required fields being unset.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14784.json",
        "creation_time": "2016-09-17T02:00:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-14784",
            "Title": "Operation logs are disabled automatically if the parent directory does not exist.",
            "Description": "Operation logging is disabled automatically for the query if the parent directory (named after the hive session id) is deleted, leading to errors when attempting to access operation logs.",
            "StackTrace": [
                "java.io.IOException: No such file or directory",
                "at java.io.UnixFileSystem.createFileExclusively(Native Method)",
                "at java.io.File.createNewFile(File.java:1012)",
                "at org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)",
                "at org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)"
            ],
            "StepsToReproduce": [
                "Set operation log directory to a location that can be purged by the OS (e.g., /tmp).",
                "Run a query from a session after the log directory has been purged."
            ],
            "ExpectedBehavior": "The operation logs should be created and accessible for queries executed in the session.",
            "ObservedBehavior": "An IOException occurs indicating that the operation log file cannot be created due to a missing directory, leading to subsequent errors when fetching results.",
            "Resolution": "Fixed"
        }
    }
]