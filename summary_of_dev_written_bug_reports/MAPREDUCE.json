[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6633",
            "Title": "AM should retry map attempts if the reduce task encounters compression related errors.",
            "Description": "When reduce task encounters compression related errors, the Application Master (AM) doesn't retry the corresponding map task, leading to job failures.",
            "StackTrace": [
                "2016-01-27 13:44:28,915 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)",
                "at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)",
                "at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)",
                "at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)",
                "at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Application Master should retry the map task if a compression-related error occurs during the reduce task.",
            "ObservedBehavior": "The Application Master does not retry the map task, leading to job failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6577",
            "Title": "MR AM unable to load native library without MR_AM_ADMIN_USER_ENV set",
            "Description": "If yarn.app.mapreduce.am.admin.user.env (or yarn.app.mapreduce.am.env) is not configured to set LD_LIBRARY_PATH, MR AM will fail to load the native library, causing failures in tasks that require it.",
            "StackTrace": [
                "java.lang.RuntimeException: native lz4 library not available",
                "at org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Ensure yarn.app.mapreduce.am.admin.user.env is not set.",
                "Run a MapReduce job that requires the native library."
            ],
            "ExpectedBehavior": "The MR AM should load the native library successfully and execute tasks that require it.",
            "ObservedBehavior": "The MR AM fails to load the native library, resulting in task failures.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5137",
            "Title": "AM web UI: clicking on Map Task results in 500 error",
            "Description": "When accessing the map task page of a running MapReduce application, a 500 error occurs.",
            "StackTrace": [
                "com.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
            ],
            "StepsToReproduce": [
                "Go to a running MapReduce app master web UI.",
                "Click on the job.",
                "Click on the MAP task type to bring up the list of maps.",
                "Click on a particular map task."
            ],
            "ExpectedBehavior": "The map task details should be displayed without errors.",
            "ObservedBehavior": "A 500 error is displayed when trying to access the map task details.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4008",
            "Title": "ResourceManager throws MetricsException on start up saying QueueMetrics MBean already exists",
            "Description": "The ResourceManager fails to start due to a MetricsException indicating that the QueueMetrics MBean already exists.",
            "StackTrace": [
                "org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)",
                "at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should start without any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to start and throws a MetricsException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6259",
            "Title": "IllegalArgumentException due to missing job submit time",
            "Description": "An IllegalArgumentException occurs when parsing the job history file name due to a missing job submit time, resulting in a job state of KILLED.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0",
                "at java.lang.Enum.valueOf(Enum.java:236)",
                "at org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)"
            ],
            "StepsToReproduce": [
                "Create a job that triggers an IOException during setup.",
                "Observe the job history file name generated.",
                "Check the job state and submit time in JobIndexInfo."
            ],
            "ExpectedBehavior": "The job submit time should be correctly set and the job should transition to the appropriate state.",
            "ObservedBehavior": "The job submit time remains -1, leading to an IllegalArgumentException and the job being marked as KILLED.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3333",
            "Title": "MR AM for sort-job going out of memory",
            "Description": "The usual sort job on a 350 node cluster hung due to OutOfMemory and eventually failed after an hour instead of the usual odd 20 minutes.",
            "StackTrace": [
                "2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002_01_001434 : java.lang.reflect.UndeclaredThrowableException",
                "Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "Caused by: java.lang.OutOfMemoryError: unable to create new native thread"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The sort job should complete successfully within the usual time frame.",
            "ObservedBehavior": "The sort job hangs and eventually fails due to OutOfMemory errors.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7059",
            "Title": "Downward Compatibility issue: MR job fails because of unknown setErasureCodingPolicy method from 3.x client to HDFS 2.x cluster",
            "Description": "Running teragen failed in the version of hadoop-3.1, and hdfs server is 2.8 due to the absence of the setErasureCodingPolicy method in HDFS 2.8.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol protocol.",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)"
            ],
            "StepsToReproduce": [
                "Run the command: bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0-SNAPSHOT.jar teragen 1000000 /teragen"
            ],
            "ExpectedBehavior": "The teragen job should complete successfully without errors related to erasure coding.",
            "ObservedBehavior": "The teragen job fails with a RemoteException indicating that the setErasureCodingPolicy method is unknown.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4843",
            "Title": "When using DefaultTaskController, JobLocalizer not thread safe",
            "Description": "In our cluster, sometimes jobs fail due to a DiskErrorException when initializing tasks, caused by JobLocalizer not being thread safe.",
            "StackTrace": [
                "2012-12-03 23:11:54,811 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing attempt_201212031626_1115_r_000023_0:",
                "org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)",
                "at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)",
                "at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)",
                "at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The job should initialize successfully without any DiskErrorException.",
            "ObservedBehavior": "Jobs fail with a DiskErrorException due to JobLocalizer not being thread safe.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5028",
            "Title": "Maps fail when io.sort.mb is set to high value",
            "Description": "The issue occurs when running a word count on a configuration with high io.sort.mb value, leading to a failure in the map tasks.",
            "StackTrace": [
                "java.io.IOException: Spill failed",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)",
                "at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "at org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)",
                "at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)"
            ],
            "StepsToReproduce": [
                "Set up pseudo-dist mode with 2 maps and 1 reduce.",
                "Configure mapred.child.java.opts to -Xmx2048m.",
                "Set io.sort.mb to 1280 and dfs.block.size to 2147483648.",
                "Run teragen to generate 4 GB of data.",
                "Execute wordcount on the generated data."
            ],
            "ExpectedBehavior": "The map tasks should complete successfully without errors.",
            "ObservedBehavior": "The map tasks fail with an IOException indicating a spill failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4300",
            "Title": "OOM in AM can turn it into a zombie.",
            "Description": "The application master (AM) encounters an OutOfMemoryError (OOM) causing multiple threads to die, leading to potential zombie states.",
            "StackTrace": [
                "Exception in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space",
                "\tat com.google.protobuf.CodedInputStream.<init>(CodedInputStream.java:538)",
                "\tat com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)",
                "\tat com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)",
                "\tat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)",
                "\tat org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)",
                "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)",
                "Exception in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space",
                "\tat java.util.HashMap.resize(HashMap.java:462)",
                "\tat java.util.HashMap.addEntry(HashMap.java:755)",
                "\tat java.util.HashMap.put(HashMap.java:385)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)",
                "Exception in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space",
                "Exception in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application master should handle memory efficiently without crashing.",
            "ObservedBehavior": "The application master crashes due to OutOfMemoryError, leading to zombie states.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3241",
            "Title": "(Rumen)TraceBuilder throws IllegalArgumentException",
            "Description": "When running the TraceBuilder, an IllegalArgumentException is thrown, and the output does not contain the map and reduce task information.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type",
                "at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:189)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The TraceBuilder should process the job history without throwing exceptions and include map and reduce task information in the output.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, and the output lacks map and reduce task information.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6002",
            "Title": "MR task should prevent report error to AM when process is shutting down",
            "Description": "With MAPREDUCE-5900, preempted MR task should not be treated as failed. However, a MR task can still fail and report to AM when preemption occurs before the AM receives the completed container from RM, leading to incorrect failure reporting.",
            "StackTrace": [
                "2014-07-22 01:46:19,613 FATAL [IPC Server handler 10 on 56903] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1405985051088_0018_m_000025_0 - exited : java.io.IOException: Filesystem closed",
                "at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)",
                "at java.io.DataInputStream.readByte(DataInputStream.java:265)",
                "at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)",
                "at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)",
                "at org.apache.hadoop.io.Text.readString(Text.java:464)",
                "at org.apache.hadoop.io.Text.readString(Text.java:457)",
                "at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The MR task should not report an error to the Application Master (AM) when the process is shutting down.",
            "ObservedBehavior": "The MR task reports a fatal error to the AM during shutdown, leading to incorrect failure reporting.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6452",
            "Title": "NPE when intermediate encrypt enabled for LocalRunner",
            "Description": "Enabling intermediate encryption for LocalRunner causes a NullPointerException during job execution.",
            "StackTrace": [
                "java.lang.Exception: java.lang.NullPointerException",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)",
                "at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)",
                "at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Enable the properties: mapreduce.framework.name=local and mapreduce.job.encrypted-intermediate-data=true",
                "Run a mapreduce job"
            ],
            "ExpectedBehavior": "The job should run successfully without any exceptions.",
            "ObservedBehavior": "The job fails with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6649",
            "Title": "getFailureInfo not returning any failure info",
            "Description": "The command executed does not provide any failure information when a job fails, unlike other commands that do provide detailed failure diagnostics.",
            "StackTrace": [
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.jobtracker.split.metainfo.maxsize=10 -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1"
            ],
            "ExpectedBehavior": "The job should return detailed failure information when it fails.",
            "ObservedBehavior": "The job fails without providing any failure information.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4048",
            "Title": "NullPointerException exception while accessing the Application Master UI",
            "Description": "A NullPointerException occurs when trying to access the Application Master UI, leading to an error in handling the URI.",
            "StackTrace": [
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)",
                "Caused by: java.lang.NullPointerException",
                "at com.google.common.base.Joiner.toString(Joiner.java:317)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Application Master UI should display without errors.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing access to the Application Master UI.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5198",
            "Title": "Race condition in cleanup during task tracker reinit with LinuxTaskController",
            "Description": "The issue occurs when the job tracker is restarted while jobs are running, causing the task tracker to fail during reinitialization.",
            "StackTrace": [
                "java.nio.channels.ClosedChannelException",
                "at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)",
                "at org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)",
                "at org.apache.hadoop.ipc.Server.access$2000(Server.java:98)",
                "at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)",
                "at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)",
                "org.apache.hadoop.util.Shell$ExitCodeException",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:182)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)",
                "at org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)",
                "at org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)",
                "at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)",
                "at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)",
                "at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)"
            ],
            "StepsToReproduce": [
                "Restart the job tracker while jobs are running.",
                "Observe the task tracker during reinitialization."
            ],
            "ExpectedBehavior": "The task tracker should reinitialize without errors.",
            "ObservedBehavior": "The task tracker fails with a ClosedChannelException and a Shell$ExitCodeException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7028",
            "Title": "Concurrent task progress updates causing NPE in Application Master",
            "Description": "Concurrent task progress updates can cause a NullPointerException in the Application Master during task updates.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "Run a large wordcount job with concurrent task progress updates.",
                "Increase the frequency of task updates artificially."
            ],
            "ExpectedBehavior": "The Application Master should handle task progress updates without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown in the Application Master during concurrent task progress updates.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3790",
            "Title": "Broken pipe on streaming job can lead to truncated output for a successful job",
            "Description": "If a streaming job doesn't consume all of its input, it can be marked successful even though the job's output is truncated.",
            "StackTrace": [
                "java.io.IOException: Broken pipe",
                "java.io.IOException: Bad file descriptor",
                "java.io.IOException: DFSOutputStream is closed",
                "Error: java.io.IOException: Broken pipe",
                "at java.io.FileOutputStream.writeBytes(Native Method)",
                "at java.io.FileOutputStream.write(FileOutputStream.java:282)",
                "at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)",
                "at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)",
                "at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)"
            ],
            "StepsToReproduce": [
                "$ hdfs dfs -cat in",
                "$ yarn jar ./share/hadoop/tools/lib/hadoop-streaming-0.24.0-SNAPSHOT.jar -Dmapred.map.tasks=1 -Dmapred.reduce.tasks=1 -mapper /bin/env -reducer NONE -input in -output out"
            ],
            "ExpectedBehavior": "The job should fail if it does not consume all of its input.",
            "ObservedBehavior": "The job is marked successful even though the output is truncated.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6357",
            "Title": "MultipleOutputs.write() API should document that output committing is not utilized when input path is absolute",
            "Description": "The MultipleOutputs.write() method fails to utilize output committing when the baseOutputPath is an absolute path, leading to exceptions during job retries.",
            "StackTrace": [
                "2015-04-28 23:13:10,452 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File already exists:wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The MultipleOutputs.write() method should properly handle output committing when given an absolute path.",
            "ObservedBehavior": "An IOException occurs indicating that the file already exists when retrying tasks with an absolute output path.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6091",
            "Title": "YARNRunner.getJobStatus() fails with ApplicationNotFoundException if the job rolled off the RM view",
            "Description": "Querying the job status of a job that has rolled off the ResourceManager view results in an ApplicationNotFoundException.",
            "StackTrace": [
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)",
                "at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)"
            ],
            "StepsToReproduce": [
                "Submit a job to YARN.",
                "Wait for the job to complete.",
                "Query the job status after it has rolled off the ResourceManager view."
            ],
            "ExpectedBehavior": "The system should be able to retrieve the job status from the job history server.",
            "ObservedBehavior": "An ApplicationNotFoundException is thrown indicating that the application does not exist in the ResourceManager.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4825",
            "Title": "JobImpl.finished doesn't expect ERROR as a final job state",
            "Description": "TestMRApp.testJobError is causing AsyncDispatcher to exit with System.exit due to an exception being thrown.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Illegal job state: ERROR",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The job should handle all final states correctly without causing a fatal error.",
            "ObservedBehavior": "The system exits due to an unhandled ERROR state in the job.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3319",
            "Title": "multifilewc from hadoop examples seems to be broken in 0.20.205.0",
            "Description": "The multifilewc example in Hadoop is failing due to a ClassCastException.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable",
                "at org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)",
                "at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "/usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.205.0.22.jar multifilewc examples/text examples-output/multifilewc"
            ],
            "ExpectedBehavior": "The multifilewc job should complete successfully without errors.",
            "ObservedBehavior": "The job fails with a ClassCastException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6361",
            "Title": "NPE issue in shuffle caused by concurrent issue between copySucceeded() in one thread and copyFailed() in another thread on the same host",
            "Description": "The bug is related to a NullPointerException occurring during the shuffle process in Hadoop MapReduce, specifically due to concurrent operations.",
            "StackTrace": [
                "2015-05-08 21:00:00,513 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The shuffle process should complete without errors, handling concurrent operations correctly.",
            "ObservedBehavior": "A NullPointerException occurs during the shuffle process due to concurrent modifications.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4848",
            "Title": "TaskAttemptContext cast error during AM recovery",
            "Description": "An AM failed and during recovery, a ClassCastException occurred when trying to cast TaskAttemptContextImpl to TaskAttemptContext.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext",
                "at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The recovery process should handle task attempts without throwing a ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown during the recovery process, causing the AM to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3226",
            "Title": "Few reduce tasks hanging in a gridmix-run",
            "Description": "In a gridmix run with ~1000 jobs, one job is getting stuck because of 2-3 hanging reducers. All of them are stuck after downloading all map outputs.",
            "StackTrace": [
                "\"EventFetcher for fetching Map Completion Events\" daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)",
                "\"main\" prio=10 tid=0x080ed400 nid=0x1c71 in Object.wait() [0xf73a2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "All reduce tasks should complete successfully without hanging.",
            "ObservedBehavior": "Reduce tasks are hanging and not completing, causing the job to get stuck.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3070",
            "Title": "NM not able to register with RM after NM restart",
            "Description": "After stopping NM gracefully then starting NM, NM registration fails with RM with Duplicate registration from the node! error.",
            "StackTrace": [
                "2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)",
                "at $Proxy13.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "NodeManager should register successfully with ResourceManager after restart.",
            "ObservedBehavior": "NodeManager registration fails with a Duplicate registration from the node! error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3932",
            "Title": "MR tasks failing and crashing the AM when available-resources/headRoom becomes zero",
            "Description": "A reduce task gets preempted due to zero headRoom, leading to a crash of the Application Master (AM).",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The reduce tasks should not crash the Application Master when resources are low.",
            "ObservedBehavior": "The Application Master crashes when reduce tasks are preempted due to zero headRoom.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4062",
            "Title": "AM Launcher thread can hang forever",
            "Description": "The Resource Manager (RM) stopped launching Application Masters due to a hung launcher thread caused by issues with the Node Manager (NM). This situation persisted for approximately 9 hours.",
            "StackTrace": [
                "\"pool-1-thread-1\" prio=10 tid=0x000000004343e800 nid=0x3a4c in Object.wait()",
                "[0x000000004fad2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Object.wait(Object.java:485)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1076)",
                "- locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)",
                "at $Proxy76.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Resource Manager should be able to launch Application Masters without hanging, even if some Node Managers encounter issues.",
            "ObservedBehavior": "The RM hangs and is unable to launch Application Masters due to a single launcher thread being blocked.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3066",
            "Title": "YARN NM fails to start",
            "Description": "The NodeManager fails to start due to an invalid configuration for the resource tracker address.",
            "StackTrace": [
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "Caused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "Caused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "NodeManager should start successfully without errors.",
            "ObservedBehavior": "NodeManager fails to start with a fatal error regarding the resource tracker address.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3123.json",
        "creation_time": "2011-09-29T19:03:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3123",
            "Title": "Symbolic links with special chars causing container/task.sh to fail",
            "Description": "The job throws an exception when special characters are included in the symbolic link.",
            "StackTrace": [
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh: line 26: syntax error near unexpected token `-_+='",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh: line 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir testlink!@$&*()-_+='",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:188)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:174)"
            ],
            "StepsToReproduce": [
                "Run the command: hadoop jar hadoop-streaming.jar -Dmapreduce.job.acl-view-job=* -Dmapreduce.job.queuename=queue1 -files file:///homes/user/hadoop/Streaming/data/streaming-980//InputDir#testlink!@$&*()-_+= -input Streaming/streaming-980/input.txt -mapper 'xargs cat' -reducer cat -output Streaming/streaming-980/Output -jobconf mapred.job.name=streamingTest-980 -jobconf mapreduce.job.acl-view-job=*"
            ],
            "ExpectedBehavior": "The job should execute successfully without throwing an exception.",
            "ObservedBehavior": "The job fails with a syntax error in the task.sh script due to special characters in the symbolic link.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6439",
            "Title": "AM may fail instead of retrying if RM shuts down during the allocate call",
            "Description": "The Application Master (AM) experiences a YarnRuntimeException when the Resource Manager (RM) shuts down during the allocate call, leading the AM to incorrectly assume it has exhausted its retries.",
            "StackTrace": [
                "2015-07-25 20:07:27,346 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Error communicating with RM: java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Application Master should retry the allocate call if the Resource Manager shuts down temporarily.",
            "ObservedBehavior": "The Application Master fails and assumes it has exhausted its retries when the Resource Manager shuts down during the allocate call.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4490",
            "Title": "JVM reuse is incompatible with LinuxTaskController (and therefore incompatible with Security)",
            "Description": "When using LinuxTaskController with JVM reuse, the second task in each JVM fails due to missing userlog directories, leading to immediate task failures and JVM exits.",
            "StackTrace": [
                "2012-07-24 14:29:11,914 INFO org.apache.hadoop.mapred.TaskLog: Starting logging for a new task attempt_201207241401_0013_m_000027_0 in the same JVM as that of the first task /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_000006_0",
                "2012-07-24 14:29:11,915 WARN org.apache.hadoop.mapred.Child: Error running child",
                "ENOENT: No such file or directory",
                "at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)",
                "at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)",
                "at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:229)"
            ],
            "StepsToReproduce": [
                "Set mapred.job.reuse.jvm.num.tasks > 1",
                "Submit a job with more map tasks than available map slots in the cluster using LinuxTaskController"
            ],
            "ExpectedBehavior": "The system should handle multiple map tasks in a single JVM without task failures.",
            "ObservedBehavior": "The second task in each JVM fails immediately due to missing userlog directories.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3744",
            "Title": "Unable to retrieve application logs via \"yarn logs\" or \"mapred job -logs\"",
            "Description": "Attempts to retrieve application logs using the 'yarn logs' command or 'mapred job -logs' command result in errors indicating that the log files do not exist or that the Job History Server is not configured.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)",
                "at org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)",
                "at org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)",
                "at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)",
                "at org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application logs should be retrievable without errors.",
            "ObservedBehavior": "Errors occur indicating that the log files do not exist or that the Job History Server is not configured.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6815.json",
        "creation_time": "2016-12-02T00:30:37.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6815",
            "Title": "Fix flaky TestKill.testKillTask()",
            "Description": "The test 'TestKill.testKillTask' is failing due to an assertion error indicating that the job state is not correct.",
            "StackTrace": [
                "java.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The job should complete successfully with a state of 'SUCCEEDED'.",
            "ObservedBehavior": "The job fails with a state of 'ERROR' instead of 'SUCCEEDED'.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3053",
            "Title": "YARN Protobuf RPC Failures in RM",
            "Description": "When trying to register the ApplicationMaster with YARN's ResourceManager, the registration fails due to a NullPointerException.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)",
                "at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)",
                "at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)",
                "at kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)",
                "Caused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)"
            ],
            "StepsToReproduce": [
                "Start the ApplicationMaster with the required parameters.",
                "Attempt to register the ApplicationMaster with the ResourceManager."
            ],
            "ExpectedBehavior": "The ApplicationMaster should successfully register with the ResourceManager.",
            "ObservedBehavior": "The registration fails with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5884",
            "Title": "History server uses short user name when canceling tokens",
            "Description": "When the owner of a token tries to explicitly cancel the token, an AccessControlException is thrown indicating that the user is not authorized to cancel the token due to a mismatch between the full principal name and the short name.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)",
                "at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)"
            ],
            "StepsToReproduce": [
                "Attempt to cancel a token as the owner using a short user name."
            ],
            "ExpectedBehavior": "The token should be canceled successfully if the user is the owner.",
            "ObservedBehavior": "An AccessControlException is thrown stating that the user is not authorized to cancel the token.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3706",
            "Title": "HTTP Circular redirect error on the job attempts page",
            "Description": "Submitting a job and accessing the job attempts URL results in an HTTP 500 error due to a circular redirect.",
            "StackTrace": [
                "org.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'",
                "at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)"
            ],
            "StepsToReproduce": [
                "Submit a job.",
                "Access the URL: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW"
            ],
            "ExpectedBehavior": "The job attempts page should load without errors.",
            "ObservedBehavior": "An HTTP 500 error occurs due to a circular redirect.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5451",
            "Title": "MR uses LD_LIBRARY_PATH which doesn't mean anything in Windows",
            "Description": "The MapReduce framework relies on the LD_LIBRARY_PATH environment variable for loading native libraries, which is not applicable in Windows. This leads to failures in MR jobs unless the configuration is overridden.",
            "StackTrace": [
                "2013-05-29 13:51:41,049 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)",
                "at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)",
                "at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)",
                "at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)",
                "at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)",
                "at org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)",
                "at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The MapReduce framework should correctly load native libraries on Windows without relying on LD_LIBRARY_PATH.",
            "ObservedBehavior": "MR jobs fail with an UnsatisfiedLinkError due to the incorrect handling of the LD_LIBRARY_PATH variable in Windows.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-2463",
            "Title": "Job History files are not moving to done folder when job history location is hdfs location",
            "Description": "When 'mapreduce.jobtracker.jobhistory.location' is set to an HDFS location, job history files fail to move to the DONE folder, resulting in an error.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)",
                "at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)",
                "at org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Job history files should move to the DONE folder without errors when configured with an HDFS location.",
            "ObservedBehavior": "Job history files do not move to the DONE folder and an IllegalArgumentException is thrown.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3531",
            "Title": "Sometimes java.lang.IllegalArgumentException: Invalid key to HMAC computation in NODE_UPDATE also causing RM to stop scheduling",
            "Description": "A large sleep job submitted to a 350 cluster was not running because the Resource Manager (RM) failed to allocate resources due to an IllegalArgumentException.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Invalid key to HMAC computation",
                "at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)",
                "at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)"
            ],
            "StepsToReproduce": [
                "Start a 350 cluster.",
                "Submit a large sleep job."
            ],
            "ExpectedBehavior": "The job should run successfully with resources allocated by the Resource Manager.",
            "ObservedBehavior": "The job does not run because the Resource Manager fails to allocate resources due to an exception.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3931",
            "Title": "MR tasks failing due to changing timestamps on Resources to download",
            "Description": "Tasks are randomly failing during gridmix runs due to changing timestamps on resources.",
            "StackTrace": [
                "java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)",
                "at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Tasks should complete successfully without failing due to resource timestamp changes.",
            "ObservedBehavior": "Tasks are failing with IOException due to unexpected changes in resource timestamps.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4467",
            "Title": "IndexCache failures due to missing synchronization",
            "Description": "TestMRJobs.testSleepJob fails randomly due to synchronization error in IndexCache.",
            "StackTrace": [
                "java.lang.IllegalMonitorStateException",
                "at java.lang.Object.wait(Native Method)",
                "at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)",
                "at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)",
                "at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle synchronization correctly without throwing exceptions.",
            "ObservedBehavior": "The system throws an IllegalMonitorStateException due to missing synchronization.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3463",
            "Title": "Second AM fails to recover properly when first AM is killed with java.lang.IllegalArgumentException causing lost job",
            "Description": "The second Application Master (AM) fails to recover properly after the first AM is killed, resulting in a lost job due to an IllegalArgumentException.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port",
                "at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)"
            ],
            "StepsToReproduce": [
                "Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml.",
                "Start a 4 Node YARN cluster.",
                "Run a job (e.g., Randowriter/Sort/Sort-validate) successfully.",
                "Kill the Application Master with kill -9 while the job is 50% complete."
            ],
            "ExpectedBehavior": "The second AM should recover and continue the job without errors.",
            "ObservedBehavior": "The second AM fails to recover, resulting in a lost job and an IllegalArgumentException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4164",
            "Title": "Hadoop 22 Exception thrown after task completion causes its reexecution",
            "Description": "An IOException occurs during task execution, leading to task reexecution.",
            "StackTrace": [
                "java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1062)",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)",
                "at $Proxy0.statusUpdate(Unknown Source)",
                "at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)",
                "at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)",
                "at java.io.DataOutputStream.flush(DataOutputStream.java:106)",
                "at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1040)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The task should complete without exceptions and not be reexecuted.",
            "ObservedBehavior": "An exception is thrown after task completion, causing the task to be reexecuted.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "bug_report": {
            "BugID": "12648389",
            "Title": "Job failed because of JvmManager running into inconsistent state",
            "Description": "In our cluster, jobs failed due to randomly task initialization failed because of JvmManager running into inconsistent state and TaskTracker failed to exit.",
            "StackTrace": [
                "java.lang.Throwable: Child Error",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)",
                "Caused by: java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)",
                "\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)",
                "\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The task should initialize successfully without errors.",
            "ObservedBehavior": "Tasks fail due to JvmManager being in an inconsistent state, leading to a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4774",
            "Title": "JobImpl does not handle asynchronous task events in FAILED state",
            "Description": "The test org.apache.hadoop.mapred.TestClusterMRNotification.testMR frequently fails in mapred build due to incorrect job state transitions, leading to unexpected notifications.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the test org.apache.hadoop.mapred.TestClusterMRNotification.testMR",
                "Observe the job status notifications received through HTTP Servlet"
            ],
            "ExpectedBehavior": "The servlet should receive expected notifications in the correct order for successful, killed, and failed jobs.",
            "ObservedBehavior": "The actual number and/or type of notifications differs from the expected, leading to test failures.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3005",
            "Title": "MR app hangs because of a NPE in ResourceManager",
            "Description": "The app hangs and it turns out to be a NPE in ResourceManager. This happened two of five times on a sort run on a big cluster.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The application should run without hanging and handle node updates correctly.",
            "ObservedBehavior": "The application hangs due to a NullPointerException in the ResourceManager.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6895",
            "Title": "Job end notification not send due to YarnRuntimeException",
            "Description": "The MRAppMaster fails to send job end notifications due to a YarnRuntimeException when stopping.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)",
                "Caused by: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Job end notifications should be sent even if the MRAppMaster encounters an exception during shutdown.",
            "ObservedBehavior": "Job end notifications are not sent due to a YarnRuntimeException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4451",
            "Title": "fairscheduler fail to init job with kerberos authentication configured",
            "Description": "Using FairScheduler in Hadoop 1.0.3 with kerberos authentication configured. Job initialization fails due to issues with the RPC user not having valid credentials.",
            "StackTrace": [
                "java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1097)",
                "at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)",
                "at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)",
                "at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)",
                "Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]"
            ],
            "StepsToReproduce": [
                "Configure FairScheduler with kerberos authentication.",
                "Submit a job to the FairScheduler."
            ],
            "ExpectedBehavior": "Job should initialize successfully with valid kerberos credentials.",
            "ObservedBehavior": "Job initialization fails due to lack of valid credentials for the RPC user.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4144",
            "Title": "ResourceManager NPE while handling NODE_UPDATE",
            "Description": "The ResourceManager on one of our clusters has exited twice due to a NullPointerException while processing a NODE_UPDATE event.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should handle NODE_UPDATE events without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when handling NODE_UPDATE events.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5924",
            "Title": "Windows: Sort Job failed due to 'Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING'",
            "Description": "The Sort job over 1GB data failed with an InvalidStateTransitonException indicating an invalid event at the COMMIT_PENDING state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The Sort job should complete successfully without errors.",
            "ObservedBehavior": "The Sort job fails with an error indicating an invalid state transition.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3649",
            "Title": "Job End notification gives an error on calling back.",
            "Description": "When calling job end notification for oozie, the Application Master (AM) fails with an error related to the callback URL.",
            "StackTrace": [
                "java.net.UnknownServiceException: no content-type",
                "at java.net.URLConnection.getContentHandler(URLConnection.java:1192)",
                "at java.net.URLConnection.getContent(URLConnection.java:689)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The job end notification should successfully call back without errors.",
            "ObservedBehavior": "The job end notification fails with a 'no content-type' error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7077",
            "Title": "Pipe mapreduce job fails with Permission denied for jobTokenPassword",
            "Description": "The application fails when launching the wordcount example with pipes due to a permission issue accessing the jobTokenPassword file.",
            "StackTrace": [
                "java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)",
                "at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)"
            ],
            "StepsToReproduce": [
                "Launch wordcount example with pipe",
                "/usr/hdp/current/hadoop-client/bin/hadoop pipes \"-Dhadoop.pipes.java.recordreader=true\" \"-Dhadoop.pipes.java.recordwriter=true\" -input pipeInput -output pipeOutput -program bin/wordcount"
            ],
            "ExpectedBehavior": "The wordcount application should run successfully without permission errors.",
            "ObservedBehavior": "The application fails with a FileNotFoundException due to permission denied for the jobTokenPassword file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4102",
            "Title": "job counters not available in Jobhistory webui for killed jobs",
            "Description": "When a job is killed before it finishes, the job history web UI displays a '500 error' when attempting to view the job counters.",
            "StackTrace": [
                "Caused by: com.google.inject.ProvisionException: Guice provision errors:",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "1) Error injecting constructor, java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)",
                "Caused by: java.lang.NullPointerException at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)"
            ],
            "StepsToReproduce": [
                "Run a simple wordcount or sleep job.",
                "Kill the job before it finishes.",
                "Go to the job history web UI and click the 'Counters' link for that job."
            ],
            "ExpectedBehavior": "The job counters should be displayed correctly in the job history web UI.",
            "ObservedBehavior": "The job history web UI displays a '500 error' when trying to access job counters for killed jobs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6353",
            "Title": "Divide by zero error in MR AM when calculating available containers",
            "Description": "When running a sleep job with zero CPU vcores, an ArithmeticException occurs due to division by zero.",
            "StackTrace": [
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Run a sleep job with zero CPU vcores."
            ],
            "ExpectedBehavior": "The system should handle the case of zero CPU vcores without throwing an exception.",
            "ObservedBehavior": "An ArithmeticException is thrown, causing the job to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4913",
            "Title": "TestMRAppMaster#testMRAppMasterMissingStaging occasionally exits",
            "Description": "The testMRAppMasterMissingStaging can cause the JVM to exit due to an error from AsyncDispatcher, leading to build failures.",
            "StackTrace": [
                "2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye.."
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The test should complete without causing the JVM to exit unexpectedly.",
            "ObservedBehavior": "The JVM exits due to an unhandled event, causing the test process to terminate without proper unregistration.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6492",
            "Title": "AsyncDispatcher exit with NPE on TaskAttemptImpl#sendJHStartEventForAssignedFailTask",
            "Description": "A NullPointerException occurs in the AsyncDispatcher when handling task attempts that fail from the ASSIGNED state, specifically in the sendJHStartEventForAssignedFailTask method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle task attempts without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the AsyncDispatcher to exit unexpectedly.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5744",
            "Title": "Job hangs because RMContainerAllocator$AssignedRequests.preemptReduce() violates the comparator contract",
            "Description": "Tasks are not getting assigned due to repeated failures in RMContainerAllocator$AssignedRequests.preemptReduce() caused by a comparator contract violation.",
            "StackTrace": [
                "2014-02-06 16:43:45,183 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeLo(TimSort.java:747)",
                "at java.util.TimSort.mergeAt(TimSort.java:483)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:408)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Tasks should be assigned without errors.",
            "ObservedBehavior": "Tasks are not assigned due to an IllegalArgumentException in the comparator.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3583",
            "Title": "ProcfsBasedProcessTree#constructProcessInfo() may throw NumberFormatException",
            "Description": "HBase PreCommit builds frequently gave a NumberFormatException due to a 64-bit positive integer being parsed as a Java long, which is signed and can only handle 63-bit positive integers.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"18446743988060683582\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)",
                "at java.lang.Long.parseLong(Long.java:422)",
                "at java.lang.Long.parseLong(Long.java:468)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)",
                "at org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)",
                "at org.apache.hadoop.mapred.Task.initialize(Task.java:536)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle large integers without throwing a NumberFormatException.",
            "ObservedBehavior": "A NumberFormatException is thrown when attempting to parse a 64-bit positive integer.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-2238",
            "Title": "Undeletable build directories",
            "Description": "The MR hudson job is failing due to a test chmod'ing a build directory, preventing the checkout from cleaning the build directory.",
            "StackTrace": [
                "hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7",
                "at hudson.FilePath.act(FilePath.java:749)",
                "at hudson.FilePath.act(FilePath.java:735)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)",
                "at hudson.model.AbstractProject.checkout(AbstractProject.java:1116)",
                "at hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild.java:479)",
                "at hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild.java:411)",
                "at hudson.model.Run.run(Run.java:1324)",
                "at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)",
                "at hudson.model.ResourceController.execute(ResourceController.java:88)",
                "at hudson.model.Executor.run(Executor.java:139)",
                "Caused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The build directory should be cleaned successfully during the checkout process.",
            "ObservedBehavior": "The build directory cannot be deleted, causing the MR hudson job to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6410",
            "Title": "Aggregated Logs Deletion doesn't work after refreshing Log Retention Settings in secure cluster",
            "Description": "The issue occurs when attempting to delete aggregated logs after executing the command to refresh log retention settings in a secure cluster, resulting in a GSSException.",
            "StackTrace": [
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)",
                "Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]"
            ],
            "StepsToReproduce": [
                "1. Startup historyserver in secure cluster.",
                "2. Log deletion happens as per expectation.",
                "3. Execute mapred hsadmin -refreshLogRetentionSettings command to refresh the configuration value.",
                "4. All subsequent attempts of log deletion fail with GSSException."
            ],
            "ExpectedBehavior": "Log deletion should succeed after refreshing log retention settings.",
            "ObservedBehavior": "Log deletion fails with GSSException after refreshing log retention settings.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6693",
            "Title": "ArrayIndexOutOfBoundsException occurs when the length of the job name is equal to mapreduce.jobhistory.jobname.limit",
            "Description": "An ArrayIndexOutOfBoundsException is thrown when the job name exceeds the character limit defined by mapreduce.jobhistory.jobname.limit.",
            "StackTrace": [
                "java.lang.ArrayIndexOutOfBoundsException: 50",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The system should handle job names of varying lengths without throwing an exception.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown when the job name reaches the character limit.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5912",
            "Title": "Task.calculateOutputSize does not handle Windows files after MAPREDUCE-5196",
            "Description": "The method Task.calculateOutputSize fails to correctly handle Windows local output files, causing them to be routed through HDFS.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)",
                "at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)",
                "at org.apache.hadoop.mapred.Task.done(Task.java:1048)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The method should correctly identify and handle Windows file paths without routing them through HDFS.",
            "ObservedBehavior": "The method throws an IllegalArgumentException for Windows file paths, indicating they are not valid DFS filenames.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5952",
            "Title": "LocalContainerLauncher#renameMapOutputForReduce incorrectly assumes a single dir for mapOutIndex",
            "Description": "The javadoc comment for renameMapOutputForReduce incorrectly refers to a single map output directory, whereas this depends on LOCAL_DIRS. The method mapOutIndex should be set to subMapOutputFile.getOutputIndexFile().",
            "StackTrace": [
                "2014-06-30 14:48:35,574 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The renameMapOutputForReduce method should correctly handle multiple map output directories based on LOCAL_DIRS.",
            "ObservedBehavior": "The method incorrectly assumes a single directory for mapOutIndex, leading to a FileNotFoundException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3306",
            "Title": "Cannot run apps after MAPREDUCE-2989",
            "Description": "The NodeManager logs show a fatal error when trying to run jobs, indicating a NoSuchElementException.",
            "StackTrace": [
                "java.util.NoSuchElementException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:822)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "Applications should run without fatal errors in the NodeManager logs.",
            "ObservedBehavior": "A fatal error occurs in the NodeManager logs, preventing applications from running.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6554",
            "Title": "MRAppMaster servicestart failing with NPE in MRAppMaster#parsePreviousJobHistory",
            "Description": "The MRAppMaster fails to start due to a NullPointerException when attempting to recover the previous job history file.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.io.StringReader.<init>(StringReader.java:50)",
                "at org.apache.avro.Schema$Parser.parse(Schema.java:917)",
                "at org.apache.avro.Schema.parse(Schema.java:966)",
                "at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)"
            ],
            "StepsToReproduce": [
                "Create scenario so that MR app master gets preempted.",
                "On next MRAppMaster launch, try to recover previous job history file."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully recover the previous job history and start without errors.",
            "ObservedBehavior": "The MRAppMaster fails to start and throws a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4457",
            "Title": "mr job invalid transition TA_TOO_MANY_FETCH_FAILURE at FAILED",
            "Description": "A job transitioned into the ERROR state due to an invalid state transition caused by multiple TA_TOO_MANY_FETCH_FAILURE events.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_TOO_MANY_FETCH_FAILURE at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The job should handle state transitions correctly without entering an invalid state.",
            "ObservedBehavior": "The job enters an ERROR state due to invalid state transitions triggered by multiple fetch failure events.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-2716",
            "Title": "MR279: MRReliabilityTest job fails because of missing job-file.",
            "Description": "The ApplicationReport should include the jobFile, which is currently hardcoded to an empty string in TypeConverter.java, causing jobs like MRReliabilityTest to fail.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Can not create a Path from an empty string",
                "at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:96)",
                "at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)",
                "at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)",
                "at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)",
                "at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)",
                "at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)",
                "at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ApplicationReport should include the jobFile path, allowing jobs to run successfully.",
            "ObservedBehavior": "Jobs fail with an IllegalArgumentException due to the jobFile being an empty string.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6702.json",
        "creation_time": "2016-05-17T22:11:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6702",
            "Title": "TestMiniMRChildTask.testTaskEnv and TestMiniMRChildTask.testTaskOldEnv are failing",
            "Description": "Two tests in the MiniMRChildTask are failing due to environment variable issues in Hadoop 3.",
            "StackTrace": [
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)",
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The tests should pass without assertion errors.",
            "ObservedBehavior": "The tests fail with assertion errors indicating the environment checker job failed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4748",
            "Title": "Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
            "Description": "An error occurs when running a large pig script, indicating that the task cannot handle the event due to an invalid state transition.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The task should handle the event correctly without throwing an exception.",
            "ObservedBehavior": "The task fails to handle the event due to an invalid state transition.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3062",
            "Title": "YARN NM/RM fail to start",
            "Description": "The ResourceManager fails to start due to an invalid configuration for the admin address.",
            "StackTrace": [
                "2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager",
                "java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The ResourceManager should start successfully without errors.",
            "ObservedBehavior": "The ResourceManager fails to start with a fatal error regarding the admin address configuration.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5724",
            "Title": "JobHistoryServer does not start if HDFS is not running",
            "Description": "Starting JHS without HDFS running fails with a YarnRuntimeException due to an inability to create a done directory.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused"
            ],
            "StepsToReproduce": [
                "Start JobHistoryServer (JHS) without HDFS running."
            ],
            "ExpectedBehavior": "JobHistoryServer should start successfully.",
            "ObservedBehavior": "JobHistoryServer fails to start with an error indicating it cannot create the done directory.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5358",
            "Title": "MRAppMaster throws invalid transitions for JobImpl",
            "Description": "The MRAppMaster is unable to handle certain job events due to invalid state transitions, leading to errors in job execution.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The MRAppMaster should correctly handle job events and transitions without throwing errors.",
            "ObservedBehavior": "The MRAppMaster throws errors indicating it cannot handle job events due to invalid state transitions.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5837",
            "Title": "MRAppMaster fails when checking on uber mode",
            "Description": "The MRAppMaster encounters a NoClassDefError when determining if a job should run in uber mode due to missing dependent jars.",
            "StackTrace": [
                "java.lang.NoClassDefFoundError: scala/Function1",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The MRAppMaster should successfully determine if the job can run in uber mode without throwing errors.",
            "ObservedBehavior": "The MRAppMaster throws a NoClassDefError when the required class is not found due to missing jars.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3058",
            "Title": "Sometimes task keeps on running while its Syslog says that it is shutdown",
            "Description": "While running GridMixV3, one of the jobs got stuck for 15 hours. The syslog indicated that the task was supposed to stop within 20 seconds, but it remained active.",
            "StackTrace": [
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The task should stop within 20 seconds.",
            "ObservedBehavior": "The task remained stuck for more than 15 hours despite syslog indicating it should have stopped.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5088",
            "Title": "MR Client gets an renewer token exception while Oozie is submitting a job",
            "Description": "After the fix for HADOOP-9299, an UninitializedMessageException occurs in Oozie when submitting a job, indicating a missing required field 'renewer'.",
            "StackTrace": [
                "org.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer",
                "at org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)",
                "Caused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer",
                "at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)"
            ],
            "StepsToReproduce": null,
            "ExpectedBehavior": "The job should submit successfully without any exceptions.",
            "ObservedBehavior": "An UninitializedMessageException is thrown due to a missing 'renewer' field.",
            "Resolution": "Fixed"
        }
    }
]