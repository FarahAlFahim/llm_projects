[
    {
        "filename": "HADOOP-6989.json",
        "creation_time": "2010-10-04T23:55:16.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException in MapFile Writer Initialization",
            "Description": "An IllegalArgumentException is thrown when attempting to initialize a MapFile Writer without setting the key class or comparator option. This occurs during the execution of the testSetFile method in the TestSetFile class.",
            "StackTrace": [
                "Caused by: java.lang.IllegalArgumentException: key class or comparator option must be set",
                "at org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)",
                "at org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)",
                "at org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)",
                "at org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)"
            ],
            "StepsToReproduce": [
                "Run the testSetFile method in the TestSetFile class.",
                "Ensure that the writeTest method is called with the appropriate parameters."
            ],
            "ExpectedBehavior": "The MapFile Writer should initialize successfully without throwing an exception, allowing the test to proceed with writing and reading data.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the key class or comparator option must be set, preventing the test from completing successfully.",
            "AdditionalDetails": "The writeTest method is called with parameters that do not include the necessary key class or comparator, which leads to the exception. The source code for writeTest does not show any handling for this requirement."
        }
    },
    {
        "filename": "HADOOP-10823.json",
        "creation_time": "2014-07-15T07:20:00.000+0000",
        "bug_report": {
            "Title": "Assertion Error in ReloadingX509TrustManager Test Due to Incorrect Accepted Issuers Count",
            "Description": "The testReload() method in the ReloadingX509TrustManager class is failing due to an assertion error. The test expects the number of accepted issuers to be 2 after reloading the trust store, but it is only returning 1. This indicates that the trust store may not be reloading correctly or that the new certificate is not being added as expected.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: expected:<2> but was:<1>",
                "at junit.framework.Assert.fail(Assert.java:50)",
                "at junit.framework.Assert.failNotEquals(Assert.java:287)",
                "at junit.framework.Assert.assertEquals(Assert.java:67)",
                "at junit.framework.Assert.assertEquals(Assert.java:199)",
                "at junit.framework.Assert.assertEquals(Assert.java:205)",
                "at org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)",
                "java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "at sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)",
                "at sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)",
                "at java.security.KeyStore.load(KeyStore.java:1185)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the testReload() method in the TestReloadingX509TrustManager class.",
                "Ensure that the trust store is created and populated with the initial certificate.",
                "Wait for the reload interval to pass and add a new certificate to the trust store.",
                "Check the number of accepted issuers after the reload."
            ],
            "ExpectedBehavior": "The test should pass, asserting that the number of accepted issuers is 2 after reloading the trust store with the new certificate.",
            "ObservedBehavior": "The test fails with an assertion error, indicating that the number of accepted issuers is 1 instead of the expected 2.",
            "AdditionalDetails": "The loadTrustManager() method is likely failing to load the updated trust store correctly, which may be causing the EOFException seen in the stack trace. This could be due to issues with the trust store file or the way it is being modified."
        }
    },
    {
        "filename": "HADOOP-9125.json",
        "creation_time": "2012-12-10T02:07:52.000+0000",
        "bug_report": {
            "Title": "CommunicationException due to LDAP connection closure",
            "Description": "The application encounters a CommunicationException when attempting to retrieve user groups from an LDAP server. The root cause appears to be an IOException indicating that the connection was closed unexpectedly.",
            "StackTrace": [
                "javax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'",
                "at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)",
                "at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)",
                "at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)",
                "at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)",
                "at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)",
                "at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)",
                "at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)",
                "at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)",
                "at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)",
                "at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)",
                "at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)",
                "at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)",
                "at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)",
                "at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)",
                "at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)",
                "at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)",
                "at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)",
                "Caused by: java.io.IOException: connection closed",
                "at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)",
                "at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)",
                "at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)",
                "... 28 more"
            ],
            "StepsToReproduce": [
                "Attempt to retrieve user groups using the getGroups(String user) method.",
                "Ensure that the LDAP server is reachable and configured correctly.",
                "Simulate a scenario where the LDAP connection is closed unexpectedly."
            ],
            "ExpectedBehavior": "The application should successfully retrieve the user groups from the LDAP server without throwing a CommunicationException.",
            "ObservedBehavior": "The application throws a CommunicationException indicating that the connection was closed, preventing the retrieval of user groups.",
            "AdditionalDetails": "The issue may be related to network instability or misconfiguration of the LDAP server. Further investigation into the LDAP server logs may provide additional insights."
        }
    },
    {
        "filename": "HADOOP-10252.json",
        "creation_time": "2014-01-22T16:43:27.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException when initializing HttpServer due to null property value",
            "Description": "An IllegalArgumentException is thrown when the HttpServer is initialized with a null property value. This occurs during the configuration setup in the HMaster class, specifically when the set method of the Configuration class is called with a null value.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:958)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:940)",
                "at org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)",
                "at org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)",
                "at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "Attempt to initialize the HttpServer without providing a valid property value in the Configuration.",
                "Ensure that the property value being set is null."
            ],
            "ExpectedBehavior": "The HttpServer should initialize successfully without throwing an exception, provided that all required properties are set correctly.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the property value must not be null, preventing the HttpServer from initializing.",
            "AdditionalDetails": "The issue arises from the set method in the Configuration class, which checks for null values using Preconditions.checkArgument. The stack trace indicates that the HttpServer's initializeWebServer method is called during the initialization process, which leads to the exception when a null value is passed."
        }
    },
    {
        "filename": "HADOOP-12239.json",
        "creation_time": "2015-07-15T18:06:14.000+0000",
        "bug_report": {
            "Title": "IOException during log splitting due to lease on blob",
            "Description": "An IOException occurs when attempting to split logs for a worker node due to an existing lease on the blob. The system fails to write the RenamePending file, which is necessary for the atomic rename operation, leading to a retry mechanism being triggered.",
            "StackTrace": [
                "java.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436448556374, will retry",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.IOException: Unable to write RenamePending file for folder rename from hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374 to hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374-splitting",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1998)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:325)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:412)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:390)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:288)",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:204)",
                "... 4 more",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2598)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2609)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1366)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:255)",
                "... 11 more",
                "Caused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)",
                "at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)",
                "at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)",
                "at com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2892)",
                "at org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2593)"
            ],
            "StepsToReproduce": [
                "1. Initiate a log splitting operation for a worker node in HBase.",
                "2. Ensure that a lease is held on the blob being accessed.",
                "3. Observe the logs for the IOException indicating failure to write the RenamePending file."
            ],
            "ExpectedBehavior": "The log splitting operation should complete successfully without any IOException, allowing the RenamePending file to be written and the logs to be processed.",
            "ObservedBehavior": "An IOException is thrown indicating that the system is unable to write the RenamePending file due to an existing lease on the blob, causing the operation to fail and trigger a retry.",
            "AdditionalDetails": "The issue arises from the Azure storage system where a lease is held on the blob, preventing modifications. The RenamePending file is crucial for managing atomic renames in HBase, and its failure leads to operational issues."
        }
    },
    {
        "filename": "HADOOP-11878.json",
        "creation_time": "2015-04-27T08:06:22.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FileContext.delete Method",
            "Description": "A NullPointerException is thrown in the FileContext class when attempting to delete a file. The issue arises from the fixRelativePart method, which is called during the deletion process. This indicates that a null value is being passed to fixRelativePart, leading to the exception.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)",
                "at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the delete method on a FileContext instance with a null Path argument.",
                "Ensure that the deletion process is triggered, possibly through the DefaultContainerExecutor."
            ],
            "ExpectedBehavior": "The delete method should successfully delete the specified file or directory without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a null value is being processed in the fixRelativePart method.",
            "AdditionalDetails": "The delete method in FileContext is expected to handle null values gracefully. The fixRelativePart method should include checks to prevent null arguments from causing exceptions."
        }
    },
    {
        "filename": "HADOOP-14949.json",
        "creation_time": "2017-10-13T23:44:09.000+0000",
        "bug_report": {
            "Title": "AssertionError in TestKMS: Reencryption of Encrypted Key Failed",
            "Description": "An AssertionError is thrown during the execution of the TestKMS unit test, indicating that the system incorrectly allowed the re-encryption of an already encrypted key. This suggests a potential flaw in the key management logic or the test's assumptions about the system's behavior.",
            "StackTrace": [
                "java.lang.AssertionError: Should not have been able to reencryptEncryptedKey",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)",
                "at org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)"
            ],
            "StepsToReproduce": [
                "Run the TestKMS unit tests.",
                "Ensure that the test case related to re-encrypting an encrypted key is executed.",
                "Observe the failure due to the AssertionError."
            ],
            "ExpectedBehavior": "The system should prevent the re-encryption of an already encrypted key and throw an appropriate exception or error.",
            "ObservedBehavior": "The system incorrectly allows the re-encryption of an encrypted key, leading to an AssertionError in the test.",
            "AdditionalDetails": "The failure occurs in the context of a privileged action being executed, as indicated by the use of the 'doAs' method in the stack trace. The logPrivilegedAction method is invoked to log the action being performed, which may provide additional context for debugging."
        }
    },
    {
        "filename": "HADOOP-10540.json",
        "creation_time": "2014-04-11T05:14:07.000+0000",
        "bug_report": {
            "Title": "IOException due to Incorrect Command Line Arguments in HardLink Creation",
            "Description": "An IOException is thrown when attempting to create a hard link due to incorrect command line arguments. This issue occurs during the initialization of the DataNode, specifically when linking blocks as part of the upgrade process.",
            "StackTrace": [
                "java.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.",
                "at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)",
                "at org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)",
                "at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:274)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:815)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "Attempt to initialize a DataNode with incorrect command line arguments for hard link creation.",
                "Ensure that the command line arguments do not match the expected format: 'hardlink create [LINKNAME] [FILENAME]'."
            ],
            "ExpectedBehavior": "The DataNode should initialize successfully without throwing an IOException related to command line arguments.",
            "ObservedBehavior": "An IOException is thrown indicating incorrect command line arguments when attempting to create a hard link during DataNode initialization.",
            "AdditionalDetails": "The issue arises in the 'createHardLinkMult' method of the 'HardLink' class, which is called during the block linking process in the DataStorage class. The command line arguments must be validated before invoking this method to prevent such exceptions."
        }
    },
    {
        "filename": "HADOOP-7629.json",
        "creation_time": "2011-09-09T17:45:14.000+0000",
        "bug_report": {
            "Title": "NoSuchMethodException for FsPermission Constructor in Hadoop IPC",
            "Description": "A RuntimeException is thrown due to a NoSuchMethodException when attempting to instantiate an inner class of FsPermission. This occurs during the deserialization process of an ObjectWritable in the Hadoop IPC framework.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)",
                "at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)",
                "at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)",
                "at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)",
                "at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)",
                "at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)",
                "at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)",
                "at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)",
                "Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "at java.lang.Class.getConstructor0(Class.java:2706)",
                "at java.lang.Class.getDeclaredConstructor(Class.java:1985)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)"
            ],
            "StepsToReproduce": [
                "1. Attempt to deserialize an ObjectWritable that contains an instance of org.apache.hadoop.fs.permission.FsPermission$2.",
                "2. Ensure that the Hadoop IPC server is running and processing RPC calls.",
                "3. Trigger the RPC call that leads to the deserialization of the ObjectWritable."
            ],
            "ExpectedBehavior": "The ObjectWritable should be successfully deserialized without throwing a NoSuchMethodException.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the constructor for org.apache.hadoop.fs.permission.FsPermission$2 cannot be found.",
            "AdditionalDetails": "The issue seems to stem from the ReflectionUtils.newInstance method, which attempts to access a constructor that does not exist for the specified inner class. This could be due to the inner class not being properly defined or not having a no-argument constructor."
        }
    },
    {
        "filename": "HADOOP-15060.json",
        "creation_time": "2017-11-22T00:18:55.000+0000",
        "bug_report": {
            "Title": "AssertionError in Group Resolution Test due to Missing Timeout Log",
            "Description": "The test 'testFiniteGroupResolutionTime' in the 'TestShellBasedUnixGroupsMapping' class is failing due to an AssertionError. The test expects a log message indicating a command timeout when a non-existing user is queried for group resolution. However, the actual log message indicates a failure to find the user instead of a timeout, leading to the assertion failure.",
            "StackTrace": [
                "java.lang.AssertionError: Expected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser",
                "PartialGroupNameException The user name 'foobarnonexistinguser' is not found.",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)",
                "at org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)"
            ],
            "StepsToReproduce": [
                "Run the test 'testFiniteGroupResolutionTime' in the 'TestShellBasedUnixGroupsMapping' class.",
                "Ensure that the configuration for the command timeout is set to a low value (e.g., 1 second).",
                "Use a non-existing username (e.g., 'foobarnonexistinguser') to trigger the timeout condition."
            ],
            "ExpectedBehavior": "The test should log a message indicating that the command ran longer than the configured timeout limit when querying for groups of a non-existing user.",
            "ObservedBehavior": "The log message indicates that the user 'foobarnonexistinguser' could not be found, rather than indicating a timeout, leading to an assertion failure.",
            "AdditionalDetails": "The method 'getUnixGroups' is expected to handle timeouts correctly by logging a specific message when the command execution exceeds the timeout limit. However, in this case, it appears to be falling back to a user-not-found error instead."
        }
    },
    {
        "filename": "HADOOP-10937.json",
        "creation_time": "2014-08-04T21:22:55.000+0000",
        "bug_report": {
            "Title": "NullPointerException in KMSClientProvider during Key Decryption",
            "Description": "A NullPointerException occurs in the KMSClientProvider class when attempting to decrypt an encrypted key. This issue arises during the process of creating a wrapped output stream in the DFSClient, indicating that a required object is not properly initialized before being accessed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)"
            ],
            "StepsToReproduce": [
                "1. Attempt to create a new file in the Hadoop Distributed File System (HDFS) using a valid path.",
                "2. Ensure that the file creation process involves encrypted keys.",
                "3. Observe the logs for any NullPointerException thrown during the key decryption process."
            ],
            "ExpectedBehavior": "The system should successfully decrypt the encrypted key and create the file in HDFS without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that an object required for decryption is null, preventing the file from being created.",
            "AdditionalDetails": "The issue likely stems from the 'decryptEncryptedKey' method in the KMSClientProvider class, which may not be handling null values correctly. Further investigation is needed to ensure that all necessary objects are initialized before this method is called."
        }
    },
    {
        "filename": "HADOOP-9103.json",
        "creation_time": "2012-04-20T01:07:25.000+0000",
        "bug_report": {
            "Title": "IOException due to Non-existent File Lease in HDFS",
            "Description": "An IOException is thrown when the system attempts to load a lease for a non-existent file during the checkpointing process in HDFS. This indicates a potential issue with file management or cleanup in the Hadoop Distributed File System.",
            "StackTrace": [
                "java.io.IOException: Found lease for non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Initiate a checkpoint in HDFS while there are temporary files present.",
                "2. Ensure that the temporary files are not properly cleaned up or are in a state that leads to a lease being held.",
                "3. Observe the logs for the IOException indicating a lease for a non-existent file."
            ],
            "ExpectedBehavior": "The system should successfully complete the checkpointing process without throwing an IOException related to non-existent files.",
            "ObservedBehavior": "An IOException is thrown indicating that a lease was found for a non-existent file, which interrupts the checkpointing process.",
            "AdditionalDetails": "This issue may arise due to improper handling of temporary files during the checkpointing process. It is essential to ensure that all temporary files are either finalized or cleaned up before initiating a checkpoint."
        }
    },
    {
        "filename": "HADOOP-11151.json",
        "creation_time": "2014-09-29T08:18:04.000+0000",
        "bug_report": {
            "Title": "AuthenticationException: Anonymous Requests Disallowed",
            "Description": "The application throws an AuthenticationException when attempting to authenticate requests that are not properly authenticated. This occurs due to the configuration that disallows anonymous requests, leading to failures in the authentication process.",
            "StackTrace": [
                "org.apache.hadoop.security.authentication.client.AuthenticationException: Anonymous requests are disallowed",
                "at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)",
                "at org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)",
                "at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)",
                "at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)",
                "at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)",
                "at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)",
                "at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)",
                "at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)",
                "at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)",
                "at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)",
                "at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)",
                "at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)",
                "at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Send an HTTP request to the application without proper authentication credentials.",
                "Ensure that the application is configured to disallow anonymous requests.",
                "Observe the response from the server."
            ],
            "ExpectedBehavior": "The application should authenticate the request successfully if valid credentials are provided, or return an appropriate error message if the credentials are invalid.",
            "ObservedBehavior": "The application throws an AuthenticationException indicating that anonymous requests are disallowed, preventing the request from being processed.",
            "AdditionalDetails": "The issue arises from the PseudoAuthenticationHandler which is configured to reject anonymous requests. This behavior is expected based on the security configuration of the application."
        }
    },
    {
        "filename": "HADOOP-8031.json",
        "creation_time": "2012-02-07T20:22:24.000+0000",
        "bug_report": {
            "Title": "Configuration Load Failure Due to Missing core-site.xml",
            "Description": "The application fails to start a component for the resource 'NameNode' because it cannot find the 'core-site.xml' configuration file. This results in a RuntimeException being thrown, indicating that the required configuration file is missing.",
            "StackTrace": [
                "org.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].",
                "Caused by: java.lang.RuntimeException: core-site.xml not found",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:438)"
            ],
            "StepsToReproduce": [
                "Attempt to start the application that requires the 'core-site.xml' configuration file.",
                "Ensure that 'core-site.xml' is not present in the expected configuration directory."
            ],
            "ExpectedBehavior": "The application should start successfully if all required configuration files, including 'core-site.xml', are present.",
            "ObservedBehavior": "The application fails to start and throws a RuntimeException indicating that 'core-site.xml' is not found.",
            "AdditionalDetails": "The method 'loadResource' in the Configuration class is responsible for loading the configuration files. It attempts to locate 'core-site.xml' but fails, leading to the RuntimeException. The absence of this file prevents the application from obtaining necessary configuration properties."
        }
    },
    {
        "filename": "HADOOP-15411.json",
        "creation_time": "2018-04-24T23:18:39.000+0000",
        "bug_report": {
            "Title": "Yarn NodeManager Web Server Fails to Start Due to ConcurrentModificationException",
            "Description": "The Yarn NodeManager fails to start its web server due to a ConcurrentModificationException occurring during the initialization of the HttpServer2. This issue arises when the configuration is being iterated while it is being modified concurrently, leading to a runtime exception that prevents the web server from starting.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.",
                " at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)",
                " at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                " at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)",
                " at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                " at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)",
                " at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)",
                "Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)",
                " at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)",
                " ... 5 more",
                "Caused by: java.io.IOException: java.util.ConcurrentModificationException",
                " at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)",
                " at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)",
                " at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)",
                " ... 8 more",
                "Caused by: java.util.ConcurrentModificationException",
                " at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)",
                " at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)",
                " at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)",
                " at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)",
                " at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)",
                " at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)"
            ],
            "StepsToReproduce": [
                "Start the Yarn NodeManager service.",
                "Ensure that the configuration is being modified concurrently while the NodeManager attempts to start its web server."
            ],
            "ExpectedBehavior": "The Yarn NodeManager should start successfully, initializing the web server without any exceptions.",
            "ObservedBehavior": "The Yarn NodeManager fails to start, throwing a YarnRuntimeException due to an underlying ConcurrentModificationException.",
            "AdditionalDetails": "The issue seems to stem from the way the configuration is being accessed and modified concurrently. The method 'getFilterConfigMap' in 'AuthenticationFilterInitializer' is likely being called while the configuration is being modified, leading to the ConcurrentModificationException."
        }
    },
    {
        "filename": "HADOOP-15850.json",
        "creation_time": "2018-10-13T14:24:29.000+0000",
        "bug_report": {
            "Title": "IOException due to Inconsistent Sequence File in CopyCommitter",
            "Description": "An IOException is thrown during the execution of the CopyCommitter's commitJob method when it encounters an inconsistent sequence file. This occurs when the current chunk file does not match the prior entry, indicating a potential issue with the file chunking process.",
            "StackTrace": [
                "java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesnt match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)"
            ],
            "StepsToReproduce": [
                "Run a job using the CopyCommitter class with a sequence file that has inconsistent chunk entries.",
                "Ensure that the sequence file contains chunks that do not follow the expected consecutive order."
            ],
            "ExpectedBehavior": "The CopyCommitter should successfully concatenate file chunks without throwing an IOException, indicating that all chunks are consistent and properly ordered.",
            "ObservedBehavior": "An IOException is thrown indicating an inconsistency in the sequence file, specifically that the current chunk file does not match the prior entry.",
            "AdditionalDetails": "The issue arises in the concatFileChunks method where it checks if two neighboring chunks are consecutive. If they are not, it throws an IOException with a message detailing the inconsistency. This suggests that there may be a problem with how files are being chunked or how the sequence file is being generated."
        }
    },
    {
        "filename": "HADOOP-11693.json",
        "creation_time": "2015-03-05T23:19:13.000+0000",
        "bug_report": {
            "Title": "Azure Storage Exception: Server Busy During Log Splitting in HBase",
            "Description": "An AzureException is thrown indicating that the server is busy when attempting to rename files during log splitting in HBase. This issue occurs when the HBase master attempts to split logs and rename them in Azure Blob Storage, leading to a failure in processing the log splitting operation.",
            "StackTrace": [
                "org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)",
                "at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)",
                "at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)",
                "at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)",
                "at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)",
                "at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)",
                "at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)",
                "... 8 more"
            ],
            "StepsToReproduce": [
                "1. Start HBase with Azure Blob Storage configured as the file system.",
                "2. Trigger a log splitting operation in HBase.",
                "3. Monitor the logs for any AzureException indicating that the server is busy."
            ],
            "ExpectedBehavior": "The log splitting operation should complete successfully without any exceptions, and the logs should be renamed in Azure Blob Storage.",
            "ObservedBehavior": "The log splitting operation fails with an AzureException indicating that the server is busy, causing retries and potential delays in HBase operations.",
            "AdditionalDetails": "This issue may be related to Azure's service availability or throttling limits. Further investigation into Azure's service status during the time of the exception may be required."
        }
    },
    {
        "filename": "HADOOP-12441.json",
        "creation_time": "2015-09-25T23:26:43.000+0000",
        "bug_report": {
            "Title": "ExitCodeException due to Invalid Process ID in Container Management",
            "Description": "An ExitCodeException is thrown when attempting to signal a container process with an invalid process ID. The error message indicates that the process ID is garbage ('--'), which is not a valid integer. This issue arises during the cleanup phase of container management in the YARN NodeManager.",
            "StackTrace": [
                "ExitCodeException exitCode=1: ERROR: garbage process ID '--'.",
                "Usage:",
                "  kill pid ...              Send SIGTERM to every process listed.",
                "  kill signal pid ...       Send a signal to every process listed.",
                "  kill -s signal pid ...    Send a signal to every process listed.",
                "  kill -l                   List all signal names.",
                "  kill -L                   List all signal names in a nice table.",
                "  kill -l signal            Convert between signal numbers and names.",
                "        at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)",
                "        at org.apache.hadoop.util.Shell.run(Shell.java:461)",
                "        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)",
                "        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)",
                "        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a YARN application that launches containers.",
                "2. Simulate a scenario where a container fails or is terminated unexpectedly.",
                "3. Observe the cleanup process in the NodeManager logs."
            ],
            "ExpectedBehavior": "The NodeManager should successfully signal the container process to terminate using a valid process ID.",
            "ObservedBehavior": "An ExitCodeException is thrown indicating an invalid process ID ('--'), preventing the NodeManager from signaling the container process.",
            "AdditionalDetails": "The issue likely stems from improper handling of process IDs during the container lifecycle management. The process ID should be validated before attempting to signal it. Further investigation into the container management logic in DefaultContainerExecutor and ContainerLaunch classes is recommended."
        }
    },
    {
        "filename": "HADOOP-11685.json",
        "creation_time": "2015-03-06T22:00:14.000+0000",
        "bug_report": {
            "Title": "IOException during HLogSplitter log file processing due to Azure lease issue",
            "Description": "An IOException is thrown when the HLogSplitter attempts to process log files in HBase, specifically when trying to create directories in Azure Blob Storage. The underlying cause is a lease on the blob that prevents the operation from completing successfully.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1019)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:223)",
                "at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:142)",
                "at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:79)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: java.io.IOException",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1477)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1862)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1812)",
                "at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getRegionSplitEditsPath(HLogSplitter.java:502)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.createWAP(HLogSplitter.java:1211)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.getWriterAndPath(HLogSplitter.java:1200)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.append(HLogSplitter.java:1243)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:851)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:843)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:813)",
                "Caused by: java.io.IOException",
                "at com.microsoft.windowsazure.storage.core.Utility.initIOException(Utility.java:493)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:282)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1472)",
                "... 10 more",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)",
                "at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)",
                "at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)",
                "at com.microsoft.windowsazure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:248)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:319)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:279)"
            ],
            "StepsToReproduce": [
                "1. Set up HBase with Azure Blob Storage as the WAL storage.",
                "2. Start HBase and generate log files.",
                "3. Attempt to split the log files using HLogSplitter.",
                "4. Ensure that a lease is active on the blob being accessed."
            ],
            "ExpectedBehavior": "The HLogSplitter should successfully process the log files and create necessary directories in Azure Blob Storage without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating that there is a lease on the blob, preventing the HLogSplitter from completing its operation.",
            "AdditionalDetails": "The issue appears to be related to Azure Blob Storage lease management. The HLogSplitter's attempt to create directories fails due to an active lease on the blob, which requires a lease ID to be specified in the request."
        }
    },
    {
        "filename": "HADOOP-8589.json",
        "creation_time": "2012-07-11T23:27:00.000+0000",
        "bug_report": {
            "Title": "FileAlreadyExistsException when initializing ViewFileSystem",
            "Description": "The application throws a FileAlreadyExistsException when attempting to create a link in the ViewFileSystem due to an existing directory at the specified path. This occurs during the setup phase of the ViewFileSystem tests.",
            "StackTrace": [
                "org.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here",
                "at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)",
                "at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)",
                "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem$Cache.java:2128)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)",
                "at org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)"
            ],
            "StepsToReproduce": [
                "Run the ViewFileSystem tests that involve the setupForViewFileSystem method.",
                "Ensure that the directory /var already exists in the file system."
            ],
            "ExpectedBehavior": "The ViewFileSystem should initialize without throwing an exception, creating necessary links as specified in the configuration.",
            "ObservedBehavior": "The ViewFileSystem initialization fails with a FileAlreadyExistsException when it attempts to create a link at a path that already exists as a directory.",
            "AdditionalDetails": "The issue arises in the createLink method of the InodeTree class, which is called during the initialization of the ViewFileSystem. The setupForViewFileSystem method attempts to create links without checking if the target path already exists."
        }
    },
    {
        "filename": "HADOOP-11754.json",
        "creation_time": "2015-03-26T05:22:54.000+0000",
        "bug_report": {
            "Title": "RuntimeException on HTTP Server Start Due to Missing Signature Secret File",
            "Description": "The application fails to start the HTTP server due to a missing signature secret file. The error occurs during the initialization of the AuthenticationFilter, which is unable to read the specified secret file, leading to a RuntimeException. This, in turn, causes a cascading failure that prevents the HTTP server from starting.",
            "StackTrace": [
                "javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)",
                "at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)",
                "at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)",
                "at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)",
                "at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)",
                "at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)",
                "at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)",
                "at org.mortbay.jetty.Server.doStart(Server.java:224)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)",
                "Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)",
                "... 23 more",
                "org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)",
                "Caused by: java.io.IOException: Problem in starting http server. Server handlers failed",
                "at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)"
            ],
            "StepsToReproduce": [
                "Attempt to start the Hadoop YARN ResourceManager.",
                "Ensure that the signature secret file is missing from the specified path: /Users/sjlee/hadoop-http-auth-signature-secret."
            ],
            "ExpectedBehavior": "The HTTP server should start successfully without any exceptions.",
            "ObservedBehavior": "The HTTP server fails to start, throwing a RuntimeException due to the missing signature secret file.",
            "AdditionalDetails": "The issue arises in the 'initializeSecretProvider' method of the 'AuthenticationFilter' class, which is unable to read the required signature secret file. This leads to a failure in starting the HTTP server, as indicated by the IOException in the stack trace."
        }
    },
    {
        "filename": "HADOOP-8225.json",
        "creation_time": "2012-03-13T20:33:53.000+0000",
        "bug_report": {
            "Title": "SecurityException Thrown on System.exit Call in DistCp",
            "Description": "A SecurityException is thrown when the DistCp tool attempts to call System.exit(-999). This is intercepted by the LauncherSecurityManager, which prevents the application from terminating as expected. This behavior disrupts the normal execution flow of the DistCp operation.",
            "StackTrace": [
                "java.lang.SecurityException: Intercepted System.exit(-999)",
                "at org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)",
                "at java.lang.Runtime.exit(Runtime.java:88)",
                "at java.lang.System.exit(System.java:904)",
                "at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "StepsToReproduce": [
                "Run the DistCp tool with any valid arguments that would lead to a call to System.exit.",
                "Observe the logs for the SecurityException being thrown."
            ],
            "ExpectedBehavior": "The DistCp tool should complete its operation and exit normally without throwing a SecurityException.",
            "ObservedBehavior": "The DistCp tool throws a SecurityException when attempting to call System.exit, preventing the application from terminating as expected.",
            "AdditionalDetails": "The issue arises from the security manager in place (LauncherSecurityManager) that intercepts exit calls. This is likely a protective measure to prevent unauthorized termination of the application, but it interferes with the expected behavior of the DistCp tool."
        }
    },
    {
        "filename": "HADOOP-10866.json",
        "creation_time": "2014-07-18T22:00:15.000+0000",
        "bug_report": {
            "Title": "IOException when accessing non-symbolic link in TestSymlinkLocalFS",
            "Description": "The test method 'testDanglingLink' in the 'TestSymlinkLocalFS' class is throwing an IOException when attempting to retrieve the symlink of a path that is not a symbolic link. This indicates that the test is not correctly handling the creation and validation of symbolic links.",
            "StackTrace": [
                "java.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link",
                "at org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)",
                "at org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)",
                "at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)",
                "at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)",
                "at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)",
                "at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)"
            ],
            "StepsToReproduce": [
                "Run the test method 'testDanglingLink' in the 'TestSymlinkLocalFS' class.",
                "Ensure that the path 'linkToFile' is created and is not a symbolic link before the test runs."
            ],
            "ExpectedBehavior": "The test should handle the case of a dangling link gracefully, without throwing an IOException when attempting to retrieve the symlink.",
            "ObservedBehavior": "An IOException is thrown indicating that the specified path is not a symbolic link, which causes the test to fail.",
            "AdditionalDetails": "The method 'getSymlink()' in the 'FileStatus' class checks if the path is a symlink using the 'isSymlink()' method. If 'isSymlink()' returns false, an IOException is thrown. The test method 'testDanglingLink()' does not properly create a symbolic link before attempting to access it, leading to this exception."
        }
    },
    {
        "filename": "HADOOP-12089.json",
        "creation_time": "2015-06-15T17:34:46.000+0000",
        "bug_report": {
            "Title": "Azure Blob Lease Exception During Folder Deletion",
            "Description": "An exception occurs when attempting to delete a folder in Azure Blob Storage due to an active lease on the blob. The system fails to specify a lease ID in the request, resulting in a StorageException.",
            "StackTrace": [
                "org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)",
                "at org.apache.hadoop.hbase.Chore.run(Chore.java:80)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)",
                "at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)",
                "at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)",
                "at com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2991)",
                "at org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2597)"
            ],
            "StepsToReproduce": [
                "1. Attempt to delete a folder in Azure Blob Storage that has an active lease.",
                "2. Ensure that no lease ID is specified in the delete request.",
                "3. Observe the exception thrown during the deletion process."
            ],
            "ExpectedBehavior": "The folder should be deleted successfully if there are no active leases on the blobs within it.",
            "ObservedBehavior": "The deletion fails with a StorageException indicating that there is an active lease on the blob and no lease ID was specified.",
            "AdditionalDetails": "The method 'updateFolderLastModifiedTime' is called during the deletion process, which attempts to update the last modified time of the folder. This method does not handle the case where a lease is active on the blob, leading to the exception."
        }
    },
    {
        "filename": "HADOOP-11934.json",
        "creation_time": "2015-05-07T00:38:53.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CredentialProvider Initialization",
            "Description": "A NullPointerException occurs during the initialization of the CredentialProvider in the Hadoop security module. The stack trace indicates that the issue arises when attempting to retrieve the FileSystem instance, which fails due to a null configuration object.",
            "StackTrace": [
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)",
                "at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)",
                "at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)",
                "at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)",
                "at org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:70)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:66)",
                "at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)",
                "at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)",
                "at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)",
                "at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)",
                "at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)"
            ],
            "StepsToReproduce": [
                "1. Initialize a Configuration object without setting necessary parameters.",
                "2. Attempt to create a CredentialProvider using the uninitialized Configuration.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The CredentialProvider should be initialized successfully without throwing an exception, allowing for secure password retrieval.",
            "ObservedBehavior": "A NullPointerException is thrown during the initialization of the CredentialProvider due to a null configuration object.",
            "AdditionalDetails": "The issue seems to stem from the ensureInitialized() method in UserGroupInformation, which does not properly initialize the configuration before it is used in the CredentialProviderFactory."
        }
    },
    {
        "filename": "HADOOP-11722.json",
        "creation_time": "2015-03-16T21:39:07.000+0000",
        "bug_report": {
            "Title": "RuntimeException when removing expired delegation token due to NoNodeException",
            "Description": "A RuntimeException is thrown when attempting to remove a stored delegation token that does not exist in ZooKeeper. This occurs during the execution of the expired token removal process, specifically when the token is not found in the expected ZooKeeper path.",
            "StackTrace": [
                "java.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28",
                "at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)",
                "at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)",
                "... 4 more"
            ],
            "StepsToReproduce": [
                "1. Create a delegation token and store it in ZooKeeper.",
                "2. Wait for the token to expire based on the configured expiration time.",
                "3. Trigger the expired token removal process, which calls removeExpiredToken().",
                "4. Observe the logs for the RuntimeException indicating the token could not be removed."
            ],
            "ExpectedBehavior": "The expired delegation token should be removed from ZooKeeper without any exceptions, even if it has already been deleted or does not exist.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the stored token could not be removed due to a NoNodeException, suggesting that the token does not exist in ZooKeeper.",
            "AdditionalDetails": "The issue arises from the assumption that the token will always exist in ZooKeeper when attempting to remove it. Proper error handling should be implemented to gracefully handle the case where the token does not exist."
        }
    },
    {
        "filename": "HADOOP-15331.json",
        "creation_time": "2018-03-21T01:15:56.000+0000",
        "bug_report": {
            "Title": "Stream Closed Exception During Configuration Parsing",
            "Description": "The application encounters a 'Stream closed' exception when attempting to parse configuration files. This issue arises during the initialization of the ResourceManager service in Hadoop, specifically when loading resources and properties.",
            "StackTrace": [
                "2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346",
                "com.ctc.wstx.exc.WstxIOException: Stream closed",
                "at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)",
                "at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)",
                "at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)",
                "at org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)",
                "at org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:165)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(AbstractService.java:121)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)"
            ],
            "StepsToReproduce": [
                "Start the Hadoop ResourceManager service.",
                "Ensure that the configuration files are accessible and correctly formatted.",
                "Observe the logs for any errors related to configuration parsing."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any errors related to configuration parsing.",
            "ObservedBehavior": "The ResourceManager fails to start, logging a 'Stream closed' exception during the configuration parsing process.",
            "AdditionalDetails": "The issue seems to stem from the 'loadResource' method in the Configuration class, which attempts to read from a closed InputStream. This could indicate that the InputStream is being closed prematurely or not being properly managed."
        }
    },
    {
        "filename": "HADOOP-14062.json",
        "creation_time": "2016-12-20T00:35:48.000+0000",
        "bug_report": {
            "Title": "EOFException during RPC communication in Hadoop",
            "Description": "An EOFException is thrown during the RPC communication between the application master and the resource manager in Hadoop. This indicates that the connection was unexpectedly closed, leading to a failure in resource allocation.",
            "StackTrace": [
                "java.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:422)",
                "at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1428)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1338)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy80.allocate(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy81.allocate(Unknown Source)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:392)",
                "at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)",
                "at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)",
                "at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)"
            ],
            "StepsToReproduce": [
                "Start the Hadoop application master.",
                "Attempt to allocate resources from the resource manager.",
                "Observe the logs for any EOFException errors."
            ],
            "ExpectedBehavior": "The application master should successfully allocate resources from the resource manager without any exceptions.",
            "ObservedBehavior": "An EOFException is thrown, indicating that the connection was unexpectedly closed, preventing resource allocation.",
            "AdditionalDetails": "The EOFException suggests a potential issue with network connectivity or configuration between the application master and the resource manager. Further investigation into network stability and Hadoop configuration settings may be required."
        }
    },
    {
        "filename": "HADOOP-11149.json",
        "creation_time": "2014-09-27T16:37:06.000+0000",
        "bug_report": {
            "Title": "Timeout Exception in ZKFailoverController during Graceful Failover Test",
            "Description": "The test for graceful failover in the ZKFailoverController is timing out after 25 seconds, indicating that the failover process is not completing as expected. This issue may be related to the waitForActiveAttempt method not returning within the specified timeout period.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 25000 milliseconds",
                "at java.lang.Object.wait(Native Method)",
                "at org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)",
                "at org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)",
                "at org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)",
                "at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)",
                "at org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)",
                "at org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)",
                "at org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)",
                "at org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)"
            ],
            "StepsToReproduce": [
                "Run the testGracefulFailover() method in the TestZKFailoverController class.",
                "Ensure that the cluster is properly set up and that automatic failover is enabled.",
                "Observe the test execution and note the timeout exception."
            ],
            "ExpectedBehavior": "The graceful failover process should complete successfully within the timeout period, allowing the test to pass without timing out.",
            "ObservedBehavior": "The test times out after 25 seconds, indicating that the waitForActiveAttempt method is not returning as expected, leading to a failure in the graceful failover process.",
            "AdditionalDetails": "The waitForActiveAttempt method is expected to wait for an active attempt to be available, but it appears to be stuck or taking longer than the specified timeout. Further investigation into the conditions under which this method returns is needed."
        }
    },
    {
        "filename": "HADOOP-15059.json",
        "creation_time": "2017-11-21T20:54:52.000+0000",
        "bug_report": {
            "Title": "RuntimeException: Unable to determine current user due to token storage issue",
            "Description": "The application encounters a RuntimeException when attempting to determine the current user. This is caused by an IOException while reading the token storage file, which indicates an unknown version in the token storage. The issue arises during the initialization of the MRAppMaster, specifically when it tries to access user credentials.",
            "StackTrace": [
                "java.lang.RuntimeException: Unable to determine current user",
                "at org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)",
                "at org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)",
                "at org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)",
                "at org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)",
                "Caused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens",
                "at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)",
                "at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)",
                "at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)",
                "at org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)",
                "... 4 more",
                "Caused by: java.io.IOException: Unknown version 1 in token storage.",
                "at org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)",
                "at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)"
            ],
            "StepsToReproduce": [
                "Start the MRAppMaster application.",
                "Ensure that the token storage file at /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens is present.",
                "Attempt to access the current user credentials."
            ],
            "ExpectedBehavior": "The application should successfully determine the current user and proceed without exceptions.",
            "ObservedBehavior": "The application throws a RuntimeException indicating it is unable to determine the current user due to an IOException related to the token storage file.",
            "AdditionalDetails": "The root cause of the issue is an unknown version in the token storage file, which suggests that the token storage format may have changed or is incompatible with the current version of the application. The relevant method 'readTokenStorageStream' checks for the version of the token storage and throws an IOException if it encounters an unknown version."
        }
    },
    {
        "filename": "HADOOP-15307.json",
        "creation_time": "2018-03-12T14:56:59.000+0000",
        "bug_report": {
            "Title": "UnsupportedOperationException in Verifier when reading RPC reply",
            "Description": "An UnsupportedOperationException is thrown when the system encounters an unsupported verifier flavor while processing an RPC reply. This occurs during the reading of the RPC reply, specifically when the flavor is not recognized by the Verifier class.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: Unsupported verifier flavorAUTH_SYS",
                "at org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)",
                "at org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)",
                "at org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)"
            ],
            "StepsToReproduce": [
                "1. Attempt to start the NFS service using the Nfs3 class.",
                "2. Ensure that the RPC reply contains an unsupported verifier flavor (e.g., AUTH_SYS).",
                "3. Observe the exception thrown during the reading of the RPC reply."
            ],
            "ExpectedBehavior": "The system should handle unsupported verifier flavors gracefully, either by ignoring them or by providing a meaningful error message without throwing an exception.",
            "ObservedBehavior": "An UnsupportedOperationException is thrown, indicating that the verifier flavor is unsupported, which leads to a failure in processing the RPC reply.",
            "AdditionalDetails": "The Verifier class currently only supports AUTH_NONE and RPCSEC_GSS flavors. Any other flavor, such as AUTH_SYS, will result in an exception being thrown. This indicates a need for either extending the supported flavors or improving error handling."
        }
    },
    {
        "filename": "HADOOP-11446.json",
        "creation_time": "2014-12-23T22:15:23.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError when uploading files to S3 using TransferManager",
            "Description": "The application encounters an OutOfMemoryError when attempting to upload files to Amazon S3 using the TransferManager. This issue arises due to the inability to create new native threads, likely caused by an excessive number of concurrent uploads or insufficient system resources.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:713)",
                "at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)",
                "at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)",
                "at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)",
                "at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)",
                "at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)"
            ],
            "StepsToReproduce": [
                "1. Initialize a TransferManager instance for uploading files to S3.",
                "2. Attempt to upload a large file or multiple files concurrently.",
                "3. Monitor the application for memory usage and thread creation.",
                "4. Observe the OutOfMemoryError being thrown."
            ],
            "ExpectedBehavior": "The application should successfully upload files to S3 without encountering an OutOfMemoryError.",
            "ObservedBehavior": "The application throws an OutOfMemoryError, indicating that it is unable to create new native threads, which halts the upload process.",
            "AdditionalDetails": "The issue may be related to the configuration of the ThreadPoolExecutor used by the TransferManager. It is advisable to review the maximum pool size and the system's available resources."
        }
    },
    {
        "filename": "HADOOP-12689.json",
        "creation_time": "2016-01-04T23:30:49.000+0000",
        "bug_report": {
            "Title": "IOException: Specified path does not exist in S3",
            "Description": "An IOException is thrown when attempting to access a file at the specified path '/test', which does not exist in the S3 file system. This issue arises during the execution of a map task in a Hadoop job, specifically when the CopyMapper tries to retrieve the file status.",
            "StackTrace": [
                "java.io.IOException: /test doesn't exist",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)",
                "at org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "StepsToReproduce": [
                "1. Attempt to run a Hadoop job that references the path '/test' in S3.",
                "2. Ensure that the specified path does not exist in the S3 bucket.",
                "3. Observe the job execution to see the IOException being thrown."
            ],
            "ExpectedBehavior": "The Hadoop job should successfully access the specified file in S3 without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating that the specified path '/test' does not exist in the S3 file system.",
            "AdditionalDetails": "The issue is likely due to a missing file or incorrect path specification in the Hadoop job configuration. The method 'getFileStatus' in 'S3FileSystem' is responsible for checking the file's existence, which leads to the IOException when the file is not found."
        }
    },
    {
        "filename": "HADOOP-13132.json",
        "creation_time": "2016-05-11T11:42:30.000+0000",
        "bug_report": {
            "Title": "ClassCastException in LoadBalancingKMSClientProvider during Key Decryption",
            "Description": "A ClassCastException occurs when attempting to cast an AuthenticationException to a GeneralSecurityException in the LoadBalancingKMSClientProvider class. This issue arises during the decryption of an encrypted key, leading to a failure in the key management system.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException",
                "at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)",
                "at org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)",
                "at org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:59)",
                "at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:683)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:679)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.create(FileContext.java:679)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:382)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:377)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:456)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:421)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:384)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Attempt to decrypt an encrypted key using the LoadBalancingKMSClientProvider.",
                "Ensure that the key being decrypted is associated with an AuthenticationException."
            ],
            "ExpectedBehavior": "The decryption process should complete successfully without throwing a ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating that an AuthenticationException cannot be cast to a GeneralSecurityException.",
            "AdditionalDetails": "The issue appears to stem from the handling of exceptions in the decryptEncryptedKey method of LoadBalancingKMSClientProvider. The method should properly handle AuthenticationException without attempting to cast it to GeneralSecurityException."
        }
    },
    {
        "filename": "HADOOP-15121.json",
        "creation_time": "2017-12-15T07:41:39.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DecayRpcScheduler MetricsProxy",
            "Description": "A NullPointerException occurs in the getMetrics method of the DecayRpcScheduler's MetricsProxy class. This issue arises when the metrics system attempts to access metrics that have not been properly initialized, leading to a failure in the metrics collection process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)",
                "        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)",
                "        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)",
                "        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)",
                "        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)",
                "        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)",
                "        at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)",
                "        at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)",
                "        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)",
                "        at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)",
                "        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)",
                "        at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)",
                "        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)",
                "        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)",
                "        at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)",
                "        at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)",
                "        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
                "        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)",
                "        at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)",
                "        at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)",
                "        at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)",
                "        at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)",
                "        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)",
                "        at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)",
                "        at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)",
                "        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)",
                "        at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)",
                "        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)",
                "        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)",
                "        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)",
                "        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)",
                "        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop NameNode service.",
                "2. Ensure that the metrics system is enabled and configured.",
                "3. Monitor the logs for any NullPointerException related to metrics collection."
            ],
            "ExpectedBehavior": "The metrics system should initialize properly and collect metrics without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the metrics system is trying to access uninitialized metrics.",
            "AdditionalDetails": "The issue seems to stem from the MetricsProxy class not being able to retrieve metrics due to a potential failure in the initialization sequence. The getMetrics method in the MetricsProxy class is likely trying to access a metric that has not been set up correctly."
        }
    },
    {
        "filename": "HADOOP-8110.json",
        "creation_time": "2012-02-24T18:49:27.000+0000",
        "bug_report": {
            "Title": "Assertion Failure in Trash Functionality Test",
            "Description": "The test case for the trash functionality in Hadoop's file system is failing due to an unexpected value returned from the expunge operation. The test expected the expunge count to be 0, but it returned 1, indicating that an item was incorrectly marked as expunged.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>",
                "at junit.framework.Assert.fail(Assert.java:47)",
                "at junit.framework.Assert.failNotEquals(Assert.java:283)",
                "at junit.framework.Assert.assertEquals(Assert.java:64)",
                "at junit.framework.Assert.assertEquals(Assert.java:195)",
                "at org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)",
                "at org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)"
            ],
            "StepsToReproduce": [
                "Run the test case 'testTrash' in the 'TestViewFsTrash' class.",
                "Ensure that the trash functionality is properly set up and that the file system is in a clean state before the test.",
                "Observe the assertion failure indicating the mismatch in expected and actual expunge counts."
            ],
            "ExpectedBehavior": "The expunge operation should result in an expected count of 0, indicating that no items were expunged from the trash.",
            "ObservedBehavior": "The expunge operation returned a count of 1, indicating that one item was incorrectly marked as expunged.",
            "AdditionalDetails": "The failure occurs in the 'trashShell' method of the 'TestTrash' class, which is invoked by the 'testTrash' method in the 'TestViewFsTrash' class. The issue may stem from the state of the file system or the configuration used during the test."
        }
    },
    {
        "filename": "HADOOP-11400.json",
        "creation_time": "2014-12-12T10:05:52.000+0000",
        "bug_report": {
            "Title": "MetricsException: Error flushing metrics due to Broken Pipe",
            "Description": "The application encounters a MetricsException when attempting to flush metrics to the Graphite sink. The underlying cause is a SocketException indicating a 'Broken pipe', which suggests that the connection to the Graphite server was lost or closed unexpectedly during the flush operation.",
            "StackTrace": [
                "org.apache.hadoop.metrics2.MetricsException: Error flushing metrics",
                "at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)",
                "at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)",
                "at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)",
                "Caused by: java.net.SocketException: Broken pipe",
                "at java.net.SocketOutputStream.socketWrite0(Native Method)",
                "at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)",
                "at java.net.SocketOutputStream.write(SocketOutputStream.java:159)",
                "at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)",
                "at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)",
                "at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)",
                "at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)",
                "at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)",
                "at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)"
            ],
            "StepsToReproduce": [
                "1. Configure the application to send metrics to a Graphite server.",
                "2. Start the application and allow it to generate metrics.",
                "3. Ensure that the connection to the Graphite server is unstable or interrupted.",
                "4. Observe the logs for the MetricsException indicating a 'Broken pipe'."
            ],
            "ExpectedBehavior": "The application should successfully flush metrics to the Graphite sink without encountering any exceptions.",
            "ObservedBehavior": "The application throws a MetricsException due to a SocketException indicating a 'Broken pipe', preventing metrics from being flushed.",
            "AdditionalDetails": "The issue may be related to network instability or the Graphite server being unavailable. The 'consume' method in the MetricsSinkAdapter attempts to flush metrics after processing them, but if the connection is lost, it results in the observed exception."
        }
    },
    {
        "filename": "HADOOP-9865.json",
        "creation_time": "2013-08-12T23:22:58.000+0000",
        "bug_report": {
            "Title": "HadoopIllegalArgumentException: Path is relative",
            "Description": "An exception is thrown when a relative path is provided to the Hadoop file system operations. The method 'checkNotRelative' in the 'Path' class is designed to ensure that paths are absolute, but it fails when a relative path is passed, leading to a 'HadoopIllegalArgumentException'. This issue occurs during the execution of the 'ContainerLaunch' process in the YARN NodeManager.",
            "StackTrace": [
                "org.apache.hadoop.HadoopIllegalArgumentException: Path is relative",
                "at org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)",
                "at org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)",
                "at org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)",
                "at org.apache.hadoop.fs.Globber.glob(Globber.java:128)",
                "at org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)",
                "at org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Attempt to launch a container in YARN with a relative path specified for the classpath.",
                "2. Observe the exception thrown during the launch process."
            ],
            "ExpectedBehavior": "The system should accept absolute paths for file operations and handle relative paths appropriately, either by converting them to absolute paths or by throwing a more informative error.",
            "ObservedBehavior": "The system throws a 'HadoopIllegalArgumentException' indicating that the provided path is relative, which halts the container launch process.",
            "AdditionalDetails": "The method 'checkNotRelative' is called to validate the path, and it is crucial that the input path is absolute. The issue may arise from user input or configuration settings that do not enforce absolute paths."
        }
    },
    {
        "filename": "HADOOP-9977.json",
        "creation_time": "2013-09-17T22:11:54.000+0000",
        "bug_report": {
            "Title": "IOException during NameNode HTTP Server Initialization",
            "Description": "The application encounters an IOException caused by an UnrecoverableKeyException when attempting to initialize the HTTP server for the NameNode. This issue arises during the SSL configuration process, specifically when the application fails to recover the key from the keystore.",
            "StackTrace": [
                "java.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key",
                "    at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)",
                "    at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)",
                "Caused by: java.security.UnrecoverableKeyException: Cannot recover key",
                "    at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)",
                "    at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)",
                "    at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)",
                "    at java.security.KeyStore.getKey(KeyStore.java:792)",
                "    at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)",
                "    at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)",
                "    at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:259)",
                "    at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)",
                "    at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)",
                "    at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)"
            ],
            "StepsToReproduce": [
                "1. Configure the NameNode with SSL settings in the configuration file.",
                "2. Ensure that the keystore file is present and correctly referenced in the configuration.",
                "3. Start the NameNode service."
            ],
            "ExpectedBehavior": "The NameNode should start successfully, initializing the HTTP server without any exceptions.",
            "ObservedBehavior": "The NameNode fails to start, throwing an IOException due to an UnrecoverableKeyException when trying to recover the key from the keystore.",
            "AdditionalDetails": "This issue may be related to incorrect keystore passwords or missing keys in the keystore. Ensure that the keystore is properly configured and accessible."
        }
    },
    {
        "filename": "HADOOP-12611.json",
        "creation_time": "2015-12-01T17:14:38.000+0000",
        "bug_report": {
            "Title": "AssertionError in TestZKSignerSecretProvider due to unexpected non-null value",
            "Description": "The test case 'testMultipleInit' in the 'TestZKSignerSecretProvider' class is failing due to an AssertionError. The test expects a null value but receives a non-null byte array instead. This indicates that the initialization process is not correctly setting the expected state, leading to unexpected behavior in the ZKSignerSecretProvider.",
            "StackTrace": [
                "java.lang.AssertionError: expected null, but was:<[B@142bad79>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotNull(Assert.java:664)",
                "at org.junit.Assert.assertNull(Assert.java:646)",
                "at org.junit.Assert.assertNull(Assert.java:656)",
                "at org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)",
                "java.lang.IllegalStateException: instance must be started before calling this method",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.CGLIB$rollSecret$2(<generated>)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8$$FastClassByMockitoWithCGLIB$$6f94a716.invoke(<generated>)",
                "at org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216)",
                "at org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10)",
                "at org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22)",
                "at org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27)",
                "at org.mockito.internal.invocation.Invocation.callRealMethod(Invocation.callRealMethod.java:211)",
                "at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36)",
                "at org.mockito.internal.MockHandler.handle(MockHandler.java:99)",
                "at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider$$EnhancerByMockitoWithCGLIB$$575f06d8.rollSecret(<generated>)",
                "at org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider$1.run(RolloverSignerSecretProvider.java:97)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Run the test case 'testMultipleInit' in the 'TestZKSignerSecretProvider' class.",
                "Ensure that the ZKSignerSecretProvider is properly initialized before the test."
            ],
            "ExpectedBehavior": "The test case should pass, indicating that the expected value is null after initialization.",
            "ObservedBehavior": "The test case fails with an AssertionError, indicating that a non-null byte array was returned instead of null.",
            "AdditionalDetails": "The failure may be related to the 'rollSecret' method, which calls 'pullFromZK' without ensuring that the instance is started, leading to an IllegalStateException. This suggests that the initialization sequence may not be correctly implemented."
        }
    },
    {
        "filename": "HADOOP-10142.json",
        "creation_time": "2013-12-03T06:33:32.000+0000",
        "bug_report": {
            "Title": "User Not Found Exception in Shell Command Execution",
            "Description": "An exception is thrown when attempting to retrieve Unix groups for a user that does not exist. The error message indicates that the user 'dr.who' could not be found, leading to an ExitCodeException in the Shell command execution.",
            "StackTrace": [
                "org.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:417)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)",
                "at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)",
                "at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)",
                "at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)",
                "at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)",
                "at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(ResourceHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)",
                "at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "Attempt to retrieve Unix groups for a non-existent user (e.g., 'dr.who').",
                "Ensure that the Shell command execution is invoked in the context of user group retrieval."
            ],
            "ExpectedBehavior": "The system should handle the case where a user does not exist gracefully, possibly returning an empty list of groups or a specific error message without throwing an exception.",
            "ObservedBehavior": "An ExitCodeException is thrown indicating that the user 'dr.who' does not exist, causing the command execution to fail.",
            "AdditionalDetails": "The issue arises in the method getUnixGroups, which calls execCommand to retrieve groups for the specified user. If the user does not exist, the command fails, leading to the observed exception."
        }
    },
    {
        "filename": "HADOOP-14727.json",
        "creation_time": "2017-08-02T21:56:38.000+0000",
        "bug_report": {
            "Title": "Exception in BlockReaderFactory during Job History Retrieval",
            "Description": "An exception is thrown in the BlockReaderFactory when attempting to retrieve job history data. This issue appears to stem from a failure in the configuration loading process, which is critical for accessing the necessary resources for job history.",
            "StackTrace": [
                "java.lang.Exception: test",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)",
                "at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)",
                "at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)",
                "at java.io.DataInputStream.read(DataInputStream.java:149)",
                "at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)",
                "at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)",
                "at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)",
                "at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)",
                "at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)",
                "at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:647)",
                "at com.ctc.wstx.stax.WstxInputFactory.createXMLStreamReader(WstxInputFactory.java:366)",
                "at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)",
                "at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)",
                "at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1126)",
                "at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1344)",
                "at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)",
                "at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)",
                "at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)",
                "at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.<init>(CompletedJob.java:105)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:100)",
                "at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)",
                "at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)",
                "at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)",
                "at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)",
                "at com.google.common.cache.LocalCache.get(LocalCache.java:3965)",
                "at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)",
                "at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)",
                "at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)",
                "at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$1.call(GuiceFilter.java:203)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1552)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.doScope(ServletHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "Attempt to retrieve job history data from the Hadoop Job History Server.",
                "Ensure that the configuration files are correctly set up and accessible."
            ],
            "ExpectedBehavior": "The job history data should be retrieved successfully without throwing an exception.",
            "ObservedBehavior": "An exception is thrown during the retrieval process, indicating a failure in loading the necessary configuration resources.",
            "AdditionalDetails": "The issue may be related to the configuration loading mechanism in the Hadoop framework, particularly in the 'getProps()' method where properties are loaded from resources."
        }
    }
]