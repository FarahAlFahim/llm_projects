[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException in LzoDecompressor during MapReduce Shuffle",
            "Description": "An ArrayIndexOutOfBoundsException occurs in the LzoDecompressor class while attempting to decompress data during the shuffle phase of a MapReduce job. This error leads to a failure in fetching map outputs, causing the reduce task to fail.",
            "StackTrace": [
                "Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)",
                "at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)",
                "at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)",
                "at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)",
                "at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job that uses LZO compression.",
                "2. Ensure that the job has a sufficient number of map tasks to generate output.",
                "3. Monitor the reduce tasks during the shuffle phase."
            ],
            "ExpectedBehavior": "The reduce task should successfully fetch and decompress the map outputs without any exceptions.",
            "ObservedBehavior": "The reduce task fails with an ArrayIndexOutOfBoundsException during the decompression of map outputs, leading to a shuffle error.",
            "AdditionalDetails": "The issue appears to stem from the LzoDecompressor's setInput method, which may be attempting to access an index that is out of bounds due to incorrect handling of the input data size or buffer management."
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "bug_report": {
            "Title": "RuntimeException: Native LZ4 Library Not Available",
            "Description": "The application throws a RuntimeException indicating that the native LZ4 library is not available when attempting to use LZ4 compression. This occurs during the initialization of the IFile Writer in the MapTask, which relies on the LZ4 codec for compression.",
            "StackTrace": [
                "java.lang.RuntimeException: native lz4 library not available",
                "at org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Attempt to run a MapReduce job that uses LZ4 compression.",
                "2. Ensure that the native LZ4 library is not available in the environment.",
                "3. Observe the RuntimeException thrown during the job execution."
            ],
            "ExpectedBehavior": "The application should successfully use the LZ4 compression codec without throwing an exception, provided that the native library is available.",
            "ObservedBehavior": "The application throws a RuntimeException indicating that the native LZ4 library is not available, preventing the use of LZ4 compression.",
            "AdditionalDetails": "The issue arises from the method 'getCompressorType()' in the Lz4Codec class, which checks if the native code is loaded using 'isNativeCodeLoaded()'. If not, it throws a RuntimeException. This indicates that the native library must be included in the classpath or properly installed for LZ4 compression to function."
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "bug_report": {
            "Title": "NotFoundException when accessing MapReduce task URI",
            "Description": "A NotFoundException is thrown when attempting to access a specific MapReduce task URI. This indicates that the requested resource is not found, which may be due to an incorrect URI or the task not being available.",
            "StackTrace": [
                "com.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:123)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1069)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)"
            ],
            "StepsToReproduce": [
                "Send a request to the URI: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000",
                "Observe the response from the server."
            ],
            "ExpectedBehavior": "The server should return the details of the specified MapReduce task if it exists.",
            "ObservedBehavior": "The server throws a NotFoundException indicating that the resource is not found.",
            "AdditionalDetails": "The issue may stem from the task ID being incorrect or the task not being available in the system. Further investigation into the task management and URI generation logic is needed."
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "bug_report": {
            "Title": "MetricsException: QueueMetrics already exists during ResourceManager startup",
            "Description": "The ResourceManager fails to start due to a MetricsException indicating that the MBean for QueueMetrics already exists. This occurs when the ResourceManager attempts to register its metrics during initialization, leading to a conflict in the MBean registration process.",
            "StackTrace": [
                "org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)",
                "at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:93)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)",
                "at $Proxy6.postStart(Unknown Source)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)",
                "Caused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)",
                "... 19 more",
                "javax.management.RuntimeOperationsException: Exception occurred trying to register the MBean",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:969)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)",
                "at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:57)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:93)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)",
                "at $Proxy6.postStart(Unknown Source)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)",
                "Caused by: java.lang.IllegalArgumentException: No object name specified",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:967)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service.",
                "Ensure that the QueueMetrics for the default queue is already registered in the metrics system."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any exceptions related to MBean registration.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing a MetricsException indicating that the MBean for QueueMetrics already exists.",
            "AdditionalDetails": "This issue may occur if the ResourceManager is started multiple times without proper cleanup of previously registered MBeans. It is recommended to check the MBean registration logic in the MetricsSourceAdapter and ensure that MBeans are unregistered before re-registration."
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to Invalid Enum Constant in JobState",
            "Description": "An IllegalArgumentException is thrown when attempting to retrieve a job state from the JobState enum. The error indicates that the value '0' is not a valid constant in the JobState enum, which leads to a failure in the job history retrieval process.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0",
                "at java.lang.Enum.valueOf(Enum.java:236)",
                "at org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)",
                "at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1223)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "Attempt to retrieve job history via the JobHistory service.",
                "Ensure that the job state being queried includes an invalid state represented by '0'."
            ],
            "ExpectedBehavior": "The job state should be retrieved successfully without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that '0' is not a valid enum constant in JobState.",
            "AdditionalDetails": "The issue arises from the use of an invalid enum value when attempting to access job states. The enum JobState should be checked to ensure that only valid constants are used."
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError when starting container in Hadoop YARN",
            "Description": "An OutOfMemoryError occurs when attempting to start a container in Hadoop YARN, leading to a failure in setting up IO streams. This issue is likely caused by the system being unable to create new native threads, which is a critical requirement for establishing connections.",
            "StackTrace": [
                "java.lang.reflect.UndeclaredThrowableException",
                "    at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)",
                "    at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "    at java.lang.Thread.run(Thread.java:619)",
                "Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)",
                "    at $Proxy20.startContainer(Unknown Source)",
                "    at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)",
                "    ... 4 more",
                "Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "    at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)",
                "    at org.apache.hadoop.ipc.Client.call(Client.java:1089)",
                "    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)",
                "    ... 6 more",
                "Caused by: java.io.IOException: Couldn't set up IO streams",
                "    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)",
                "    at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)",
                "    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)",
                "    at org.apache.hadoop.ipc.Client.call(Client.java:1065)",
                "    ... 7 more",
                "Caused by: java.lang.OutOfMemoryError: unable to create new native thread",
                "    at java.lang.Thread.start0(Native Method)",
                "    at java.lang.Thread.start(Thread.java:597)",
                "    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)"
            ],
            "StepsToReproduce": [
                "Attempt to start a container in Hadoop YARN under high load conditions or with limited system resources."
            ],
            "ExpectedBehavior": "The container should start successfully without throwing an OutOfMemoryError, and IO streams should be set up correctly.",
            "ObservedBehavior": "An OutOfMemoryError is thrown, preventing the container from starting and resulting in a failure to set up IO streams.",
            "AdditionalDetails": "The root cause appears to be related to the system's inability to create new threads, which may be due to resource limits or configuration settings. It is advisable to check the system's memory and thread limits."
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "bug_report": {
            "Title": "RpcNoSuchMethodException: Unknown method setErasureCodingPolicy",
            "Description": "The application encounters a RemoteException indicating that the method 'setErasureCodingPolicy' is not recognized by the ClientProtocol interface. This issue arises during the execution of a Hadoop job, specifically when attempting to set the erasure coding policy for a distributed file system.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol.",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:180)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1437)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler$Call.java:165)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)",
                "at com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)",
                "at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)",
                "at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)",
                "at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)",
                "at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:304)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:218)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hadoop job that involves setting the erasure coding policy.",
                "2. Monitor the logs for any RemoteException related to 'setErasureCodingPolicy'."
            ],
            "ExpectedBehavior": "The erasure coding policy should be set successfully without any exceptions.",
            "ObservedBehavior": "A RemoteException is thrown indicating that the method 'setErasureCodingPolicy' is unknown, leading to job failure.",
            "AdditionalDetails": "The issue may stem from a mismatch between the client and server versions of Hadoop, where the method 'setErasureCodingPolicy' is not available in the server's ClientProtocol implementation."
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "bug_report": {
            "Title": "DiskErrorException: Job XML Not Found in Local Directories",
            "Description": "The application encounters a DiskErrorException when attempting to locate the job.xml file for a specific job in the configured local directories. This issue arises during the initialization of a job in the TaskTracker component of the Hadoop framework.",
            "StackTrace": [
                "org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)",
                "at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)",
                "at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)",
                "at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to the Hadoop TaskTracker.",
                "2. Ensure that the job is configured to use local directories for job caching.",
                "3. Monitor the TaskTracker logs for any DiskErrorException related to job.xml file retrieval."
            ],
            "ExpectedBehavior": "The TaskTracker should successfully locate the job.xml file in the configured local directories and initialize the job without errors.",
            "ObservedBehavior": "The TaskTracker throws a DiskErrorException indicating that it could not find the job.xml file in any of the configured local directories, preventing the job from being initialized.",
            "AdditionalDetails": "The issue may be related to incorrect configuration of local directories or the job not being properly cached. Further investigation into the configuration settings and the job submission process may be required."
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "bug_report": {
            "Title": "IOException: Spill failed during MapReduce task execution",
            "Description": "An IOException occurred during the execution of a MapReduce task, specifically during the spill phase of the MapTask. The root cause appears to be an EOFException while reading data, which indicates that the data stream was unexpectedly closed or incomplete.",
            "StackTrace": [
                "java.io.IOException: Spill failed",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)",
                "at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "at org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)",
                "at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)"
            ],
            "StepsToReproduce": [
                "Run a MapReduce job using the WordCount example.",
                "Ensure that the input data is large enough to trigger the spill phase.",
                "Monitor the job execution for any IOException related to spill failures."
            ],
            "ExpectedBehavior": "The MapReduce job should complete successfully without any IOException during the spill phase.",
            "ObservedBehavior": "The MapReduce job fails with an IOException indicating that the spill process failed due to an EOFException.",
            "AdditionalDetails": "The issue may be related to the handling of data streams during the spill process. The EOFException suggests that the data being read was incomplete or the stream was closed unexpectedly. Further investigation into the data source and the configuration of the MapReduce job may be necessary."
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError in Hadoop due to excessive memory usage",
            "Description": "The application encounters a java.lang.OutOfMemoryError: Java heap space in multiple threads, indicating that the Java Virtual Machine (JVM) has run out of memory. This issue arises during the processing of responses from datanodes and while scheduling speculative tasks in the MapReduce framework.",
            "StackTrace": [
                "Exception in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space",
                "at com.google.protobuf.CodedInputStream.<init>(CodedInputStream.java:538)",
                "at com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)",
                "at org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)",
                "Exception in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space",
                "at java.util.HashMap.resize(HashMap.java:462)",
                "at java.util.HashMap.addEntry(HashMap.java:755)",
                "at java.util.HashMap.put(HashMap.java:385)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Exception in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space",
                "Exception in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space"
            ],
            "StepsToReproduce": [
                "Run a Hadoop job that processes a large amount of data.",
                "Monitor the memory usage of the JVM during the job execution.",
                "Observe the logs for any OutOfMemoryError exceptions."
            ],
            "ExpectedBehavior": "The application should process data without running out of memory, efficiently handling responses and speculative tasks.",
            "ObservedBehavior": "The application throws OutOfMemoryError exceptions in multiple threads, indicating that it has exceeded the allocated heap space.",
            "AdditionalDetails": "The issue may be related to the handling of large data structures in memory, particularly in the methods 'readFields' and 'getTasks'. The 'ResponseProcessor' and 'DefaultSpeculator' classes are heavily involved in the processing and scheduling of tasks, which may lead to excessive memory consumption if not managed properly."
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException in JobBuilder.process due to unknown event type",
            "Description": "An IllegalArgumentException is thrown when the JobBuilder.process method encounters an unknown event type while processing job history events. This indicates that the event type being processed is not recognized by the JobBuilder, which may lead to incomplete processing of job history data.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type",
                "at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:189)"
            ],
            "StepsToReproduce": [
                "Run the TraceBuilder tool with a job history file that contains an unknown event type."
            ],
            "ExpectedBehavior": "The JobBuilder should process all event types correctly without throwing an exception, allowing for complete job history analysis.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating an unknown event type, which interrupts the processing of job history.",
            "AdditionalDetails": "The issue may arise from the job history file containing events that are not accounted for in the JobBuilder's event type handling logic. Further investigation is needed to identify the specific event types that are causing this exception."
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "bug_report": {
            "Title": "IOException: Filesystem closed during HDFS read operation",
            "Description": "An IOException is thrown indicating that the filesystem is closed when attempting to read data from HDFS. This occurs in the context of a MapReduce task, specifically during the execution of the getSplitDetails method in the MapTask class.",
            "StackTrace": [
                "java.io.IOException: Filesystem closed",
                "at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)",
                "at java.io.DataInputStream.readByte(DataInputStream.java:265)",
                "at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)",
                "at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)",
                "at org.apache.hadoop.io.Text.readString(Text.java:464)",
                "at org.apache.hadoop.io.Text.readString(Text.java:457)",
                "at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job that reads from HDFS.",
                "2. Ensure that the filesystem is closed before the job attempts to read data.",
                "3. Observe the exception thrown during the read operation."
            ],
            "ExpectedBehavior": "The read operation should successfully retrieve data from HDFS without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown with the message 'Filesystem closed', indicating that the read operation cannot proceed.",
            "AdditionalDetails": "The issue arises from the checkOpen method in the DFSClient class, which verifies if the filesystem is open before proceeding with read operations. If the filesystem is closed, it throws an IOException."
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CryptoOutputStream Initialization",
            "Description": "A NullPointerException is thrown during the initialization of the CryptoOutputStream, which is causing the LocalJobRunner to fail when running tasks. This issue appears to stem from a missing or improperly configured parameter in the CryptoUtils.wrapIfNecessary method.",
            "StackTrace": [
                "java.lang.Exception: java.lang.NullPointerException",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)",
                "at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)",
                "at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Configure a Hadoop job that requires encryption.",
                "2. Execute the job using the LocalJobRunner.",
                "3. Observe the logs for a NullPointerException during task execution."
            ],
            "ExpectedBehavior": "The job should run successfully without throwing a NullPointerException, and the CryptoOutputStream should be initialized correctly.",
            "ObservedBehavior": "A NullPointerException is thrown during the initialization of the CryptoOutputStream, causing the job to fail.",
            "AdditionalDetails": "The issue likely arises from the parameters passed to the CryptoUtils.wrapIfNecessary method. It is important to ensure that the Configuration object and the FSDataOutputStream are properly initialized before calling this method."
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "bug_report": {
            "Title": "ExitCodeException Thrown During Container Launch",
            "Description": "An ExitCodeException is thrown when attempting to launch a container using the DefaultContainerExecutor in the YARN NodeManager. The exception indicates that the command executed by the shell returned a non-zero exit code, which typically signifies an error in execution.",
            "StackTrace": [
                "ExitCodeException exitCode=1:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Attempt to launch a container using the YARN NodeManager.",
                "2. Ensure that the command executed by the shell is valid and executable.",
                "3. Observe the logs for any ExitCodeException being thrown."
            ],
            "ExpectedBehavior": "The container should launch successfully without throwing an ExitCodeException, indicating a successful execution of the command.",
            "ObservedBehavior": "An ExitCodeException is thrown with exit code 1, indicating that the command executed by the shell failed.",
            "AdditionalDetails": "The runCommand() method in the Shell class is responsible for executing the command. The method checks the exit code of the process and throws an ExitCodeException if the exit code is not zero. The error message from the command execution can be found in the errMsg StringBuffer."
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AppController.attempts() Method",
            "Description": "A NullPointerException is thrown in the AppController.attempts() method when attempting to join task type and attempt state strings. This occurs when either the task type or attempt state is not provided, leading to a failure in the join operation.",
            "StackTrace": [
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at com.google.common.base.Joiner.toString(Joiner.java:317)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:97)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:127)",
                "at com.google.common.base.Joiner.join(Joiner.java:158)",
                "at com.google.common.base.Joiner.join(Joiner.java:166)",
                "at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)"
            ],
            "StepsToReproduce": [
                "1. Call the attempts() method without setting the TASK_TYPE or ATTEMPT_STATE parameters.",
                "2. Ensure that the job ID is provided but the required parameters are missing.",
                "3. Observe the NullPointerException being thrown."
            ],
            "ExpectedBehavior": "The system should handle missing parameters gracefully and return a meaningful error message without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to join null values for task type or attempt state, causing the application to crash.",
            "AdditionalDetails": "The issue arises in the attempts() method where the join() function is called with potentially null values. Proper validation should be implemented to check for null or empty values before attempting to join them."
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "bug_report": {
            "Title": "ClosedChannelException during IPC response processing",
            "Description": "A ClosedChannelException is thrown when attempting to write to a closed socket channel during the processing of an IPC response in the Hadoop framework. This indicates that the server is trying to send a response to a client that is no longer connected.",
            "StackTrace": [
                "java.nio.channels.ClosedChannelException",
                "at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)",
                "at org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)",
                "at org.apache.hadoop.ipc.Server.access$2000(Server.java:98)",
                "at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)",
                "at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)",
                "org.apache.hadoop.util.Shell$ExitCodeException:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:182)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(ShellCommandExecutor.java:375)",
                "at org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)",
                "at org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)",
                "at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)",
                "at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)",
                "at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop server and ensure it is running properly.",
                "2. Initiate a client connection to the server.",
                "3. Simulate a scenario where the client disconnects unexpectedly (e.g., network failure or client crash).",
                "4. Attempt to send a response from the server to the now disconnected client."
            ],
            "ExpectedBehavior": "The server should handle the disconnection gracefully without throwing an exception, possibly by logging the event and not attempting to write to the closed channel.",
            "ObservedBehavior": "The server throws a ClosedChannelException when it attempts to write to a socket channel that has already been closed, leading to potential instability in the server process.",
            "AdditionalDetails": "The issue seems to originate from the 'channelWrite' method in the Server class, which is called during the response processing. The server should implement better error handling to manage cases where the client is no longer connected."
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "bug_report": {
            "Title": "NullPointerException in TaskAttemptImpl during State Transition",
            "Description": "A NullPointerException occurs in the TaskAttemptImpl class when transitioning states. This issue arises during the handling of task attempt events, specifically when the transition method is invoked without proper initialization of certain objects.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)",
                "at org.apache.hadoop.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)",
                "at org.apache.hadoop.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job that triggers task attempts.",
                "2. Ensure that the task attempts encounter a scenario that leads to state transitions.",
                "3. Monitor the logs for NullPointerException in the TaskAttemptImpl class."
            ],
            "ExpectedBehavior": "The task attempt should transition states without throwing a NullPointerException, and all necessary objects should be properly initialized.",
            "ObservedBehavior": "A NullPointerException is thrown during the state transition process, indicating that an object was not initialized correctly.",
            "AdditionalDetails": "The issue likely stems from the 'transition' method in TaskAttemptImpl, where certain fields (like 'container') may not be initialized before being accessed. Further investigation into the initialization sequence of TaskAttemptImpl is required."
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "bug_report": {
            "Title": "IOException: Broken pipe during Hadoop streaming task execution",
            "Description": "An IOException with the message 'Broken pipe' occurs when a Hadoop streaming task attempts to write data to an output stream that has been closed on the other end. This typically indicates that the process reading from the output stream has terminated unexpectedly, leading to a failure in the data writing process.",
            "StackTrace": [
                "Error: java.io.IOException: Broken pipe",
                "at java.io.FileOutputStream.writeBytes(Native Method)",
                "at java.io.FileOutputStream.write(FileOutputStream.java:282)",
                "at java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "at org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)",
                "at org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)",
                "at org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:394)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:329)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop streaming job that writes output to a pipe.",
                "2. Ensure that the process reading from the pipe is terminated before the writing process completes.",
                "3. Observe the logs for the IOException indicating a broken pipe."
            ],
            "ExpectedBehavior": "The Hadoop streaming task should successfully write data to the output stream without encountering an IOException.",
            "ObservedBehavior": "The Hadoop streaming task fails with an IOException: 'Broken pipe', indicating that the output stream was closed unexpectedly.",
            "AdditionalDetails": "This issue may occur if the downstream process (the one reading from the output stream) crashes or is terminated before the upstream process (the one writing to the output stream) finishes writing. It is important to ensure that the reading process is stable and can handle the data being sent."
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "bug_report": {
            "Title": "IOException: File already exists when creating output file in Azure Blob Storage",
            "Description": "An IOException is thrown when attempting to create a file in Azure Blob Storage that already exists. This occurs during the execution of a Hadoop job, specifically in the LevelReducer class when trying to write output using MultipleOutputs.",
            "StackTrace": [
                "java.io.IOException: File already exists: wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "StepsToReproduce": [
                "Run a Hadoop job that uses MultipleOutputs to write output to Azure Blob Storage.",
                "Ensure that the output path specified already contains a file with the same name as the one being created."
            ],
            "ExpectedBehavior": "The job should either overwrite the existing file if the overwrite flag is set to true or create a new file if it does not exist.",
            "ObservedBehavior": "An IOException is thrown indicating that the file already exists, preventing the job from completing successfully.",
            "AdditionalDetails": "The issue arises in the LevelReducer class, specifically in the processValue method, where it attempts to write output using MultipleOutputs. The create method in the NativeAzureFileSystem does not allow overwriting existing files unless explicitly specified."
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "bug_report": {
            "Title": "ApplicationNotFoundException when retrieving job status",
            "Description": "An IOException is thrown indicating that the application with the specified ID does not exist in the Resource Manager (RM). This occurs when attempting to retrieve the application report for a job that may have already completed or failed, leading to an ApplicationNotFoundException.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)",
                "at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)",
                "at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)",
                "at java.lang.Thread.run(Thread.java:662)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)"
            ],
            "StepsToReproduce": [
                "Submit a job to the YARN Resource Manager.",
                "Attempt to retrieve the job status after the job has completed or failed.",
                "Observe the exception thrown when the application ID is no longer found."
            ],
            "ExpectedBehavior": "The job status should be retrievable without exceptions, even if the job has completed or failed.",
            "ObservedBehavior": "An IOException is thrown indicating that the application with the specified ID does not exist in the Resource Manager.",
            "AdditionalDetails": "This issue may occur if the job has been removed from the Resource Manager's tracking system after completion or failure. It is important to handle such cases gracefully in the application logic."
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to ERROR Job State in Hadoop MapReduce",
            "Description": "An IllegalArgumentException is thrown when the job state is set to ERROR in the Hadoop MapReduce framework. This occurs during the transition of job states, specifically when the job is marked as finished with an ERROR state, leading to an unhandled exception.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Illegal job state: ERROR",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to the Hadoop MapReduce framework.",
                "2. Ensure that the job encounters an error during execution.",
                "3. Observe the job state transition to ERROR.",
                "4. Check the logs for the IllegalArgumentException."
            ],
            "ExpectedBehavior": "The job should handle the ERROR state gracefully without throwing an IllegalArgumentException.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating an illegal job state when the job is marked as ERROR.",
            "AdditionalDetails": "The issue arises in the 'finished' method of the JobImpl class, where the job state is set to ERROR without proper handling of this state transition. The transition method attempts to process an event that is not valid for the current job state."
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "bug_report": {
            "Title": "ClassCastException in LongSumReducer due to IntWritable to LongWritable cast",
            "Description": "A ClassCastException occurs in the LongSumReducer when attempting to reduce values of type IntWritable instead of LongWritable. This indicates a type mismatch in the data being processed by the reducer.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable",
                "at org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)",
                "at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that uses LongSumReducer with input data containing IntWritable values.",
                "2. Monitor the job execution until it reaches the reduce phase."
            ],
            "ExpectedBehavior": "The LongSumReducer should successfully reduce the values of type LongWritable without any ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown indicating that an IntWritable cannot be cast to LongWritable, causing the job to fail.",
            "AdditionalDetails": "The LongSumReducer is designed to work with LongWritable values. The input data should be validated to ensure that it contains the correct type before being processed by the reducer."
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ShuffleSchedulerImpl during MapReduce Shuffle Phase",
            "Description": "A NullPointerException occurs in the ShuffleSchedulerImpl class while attempting to copy data from a host during the shuffle phase of a MapReduce job. This error leads to a failure in the shuffle process, causing the reduce task to fail.",
            "StackTrace": [
                "Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job that involves a shuffle phase.",
                "2. Ensure that there are hosts with incomplete map outputs.",
                "3. Monitor the logs for the reduce task to observe the shuffle process."
            ],
            "ExpectedBehavior": "The shuffle process should successfully copy map outputs from the specified hosts without encountering any exceptions.",
            "ObservedBehavior": "The shuffle process fails with a NullPointerException, causing the reduce task to terminate unexpectedly.",
            "AdditionalDetails": "The NullPointerException is likely caused by the 'host' parameter being null when passed to the 'copyFromHost' method. This can occur if the scheduler fails to allocate a valid host for shuffling."
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "bug_report": {
            "Title": "ClassCastException in OutputCommitter during Task Recovery",
            "Description": "A ClassCastException occurs when the OutputCommitter attempts to recover a task using an incompatible TaskAttemptContext implementation. The error arises from trying to cast an instance of TaskAttemptContextImpl to TaskAttemptContext, which leads to a runtime exception.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext",
                "at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job that requires task recovery.",
                "2. Ensure that the job encounters a failure that triggers the recovery process.",
                "3. Observe the logs for a ClassCastException related to TaskAttemptContext."
            ],
            "ExpectedBehavior": "The task recovery process should complete successfully without throwing a ClassCastException, allowing the job to continue processing.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating that the system is attempting to cast an incompatible TaskAttemptContext implementation, which halts the recovery process.",
            "AdditionalDetails": "The issue arises from the use of different context implementations in the Hadoop MapReduce framework. The OutputCommitter expects a TaskAttemptContext from the old API (org.apache.hadoop.mapred), while the RecoveryService is using the new API (org.apache.hadoop.mapreduce). This mismatch leads to the ClassCastException."
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "bug_report": {
            "Title": "Deadlock in Hadoop Reduce Task Execution",
            "Description": "A deadlock occurs in the Hadoop MapReduce framework during the execution of a reduce task. The EventFetcher thread is in a TIMED_WAITING state while waiting for a task to be fetched, and the Shuffle thread is waiting on the EventFetcher, leading to a deadlock situation.",
            "StackTrace": [
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "        at java.lang.Thread.sleep(Native Method)",
                "        at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)",
                "",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "        at java.lang.Object.wait(Native Method)",
                "        - waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)",
                "        at java.lang.Thread.join(Thread.java:1143)",
                "        - locked <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)",
                "        at java.lang.Thread.join(Thread.java:1196)",
                "        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)",
                "        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)",
                "        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "        at java.security.AccessController.doPrivileged(Native Method)",
                "        at javax.security.auth.Subject.doAs(Subject.java:396)",
                "        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job with a reduce task that requires fetching events.",
                "2. Monitor the threads during execution to observe the states.",
                "3. Identify the EventFetcher and Shuffle threads to confirm the deadlock."
            ],
            "ExpectedBehavior": "The reduce task should complete successfully without any deadlocks, allowing the EventFetcher to fetch events and the Shuffle to process them.",
            "ObservedBehavior": "The reduce task enters a deadlock state where the EventFetcher is waiting to fetch events while the Shuffle thread is waiting for the EventFetcher to complete, causing the entire task to hang indefinitely.",
            "AdditionalDetails": "The deadlock appears to be caused by the EventFetcher not being able to fetch new tasks due to the Shuffle thread holding a lock on it. This could be related to the way tasks are being polled and executed in the Hadoop framework."
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "bug_report": {
            "Title": "NodeManager Fails to Start Due to Duplicate Registration",
            "Description": "The NodeManager fails to start because it encounters a YarnRemoteException indicating a duplicate registration from the node. This issue arises during the registration process with the ResourceManager, which is triggered in the NodeManager's start method.",
            "StackTrace": [
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)",
                "at $Proxy13.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager service.",
                "Ensure that the NodeManager attempts to register with the ResourceManager.",
                "Observe the logs for any YarnRemoteException indicating duplicate registration."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and register with the ResourceManager without any exceptions.",
            "ObservedBehavior": "The NodeManager fails to start and throws a YarnException due to a duplicate registration error.",
            "AdditionalDetails": "The issue may be caused by multiple instances of NodeManager trying to register with the same node ID. The registration process is handled in the registerWithRM() method, which is called during the start() method of NodeManager. The duplicate registration error suggests that the NodeManager is already registered with the ResourceManager before the current start attempt."
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException in Hadoop YARN during Task Cleanup",
            "Description": "An InvalidStateTransitionException is thrown in the Hadoop YARN framework when an event is processed in an invalid state. This occurs specifically when a task attempt fails to launch and during job counter updates while the job is in an error state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)",
                "at java.lang.Thread.run(Thread.java:619)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:657)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:111)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:848)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:844)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "Submit a job to the Hadoop YARN framework.",
                "Simulate a failure in task container launch.",
                "Observe the state transitions and events being processed."
            ],
            "ExpectedBehavior": "The system should handle task failures gracefully without throwing an InvalidStateTransitionException, allowing for proper state management and recovery.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when processing events related to task launch failures and job counter updates while in an invalid state.",
            "AdditionalDetails": "The exception indicates that the event being processed is not valid for the current state of the task or job. The relevant methods in the source code include 'doTransition' in the StateMachineFactory, which is responsible for managing state transitions based on events. The 'handle' methods in TaskAttemptImpl and JobImpl are also critical as they process the events leading to the exception."
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "bug_report": {
            "Title": "Deadlock in Container Launching Process",
            "Description": "The application is experiencing a deadlock during the container launching process, where the thread is stuck in a WAITING state while trying to start a container. This issue arises from the interaction between the Client and the ContainerManager, leading to a failure in launching the application master container.",
            "StackTrace": [
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Object.wait(Object.java:485)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1076)",
                "- locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)",
                "at $Proxy76.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the YARN ResourceManager.",
                "2. Monitor the application master logs during the container launch phase.",
                "3. Observe the thread state of the application master, which should show it in a WAITING state."
            ],
            "ExpectedBehavior": "The application master should successfully launch the container and transition to the RUNNING state without entering a WAITING state indefinitely.",
            "ObservedBehavior": "The application master enters a WAITING state while trying to start the container, leading to a deadlock and preventing the application from progressing.",
            "AdditionalDetails": "The issue may be related to the implementation of the 'startContainer' method in the ContainerManager, which is invoked by the AMLauncher. The deadlock could be caused by contention for the Client's lock during the RPC call."
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "bug_report": {
            "Title": "NodeManager Fails to Start Due to Invalid ResourceManager Address Configuration",
            "Description": "The NodeManager fails to start because it encounters an invalid configuration for the ResourceManager address. The error indicates that the configuration for 'yarn.resourcemanager.resource-tracker.address' is not a valid host:port pair, leading to a failure in creating a socket address.",
            "StackTrace": [
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more",
                "java.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED",
                "at org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)",
                "at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:95)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:85)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)",
                "at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)",
                "java.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED",
                "at org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)",
                "at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:87)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)",
                "at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)"
            ],
            "StepsToReproduce": [
                "1. Configure the NodeManager with an invalid value for 'yarn.resourcemanager.resource-tracker.address'.",
                "2. Attempt to start the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and register with the ResourceManager using the provided address.",
            "ObservedBehavior": "The NodeManager fails to start, throwing an exception indicating that the ResourceManager address is not a valid host:port pair.",
            "AdditionalDetails": "Ensure that the configuration for 'yarn.resourcemanager.resource-tracker.address' is set correctly in the configuration files. The expected format is 'hostname:port'."
        }
    },
    {
        "filename": "MAPREDUCE-3123.json",
        "creation_time": "2011-09-29T19:03:45.000+0000",
        "bug_report": {
            "Title": "Syntax Error in Task Script Causes Container Launch Failure",
            "Description": "A syntax error in the task script generated during the container launch process leads to an ExitCodeException, preventing the container from starting successfully. The error occurs due to an unexpected token in the shell command being executed.",
            "StackTrace": [
                "org.apache.hadoop.util.Shell$ExitCodeException:",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:",
                "line 26: syntax error near unexpected token `-_+='",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:",
                "line 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir test",
                "ink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:188)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(ShellCommandExecutor.java:381)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:174)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:62)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that generates a task script with a command containing unexpected tokens.",
                "2. Monitor the container launch process in the Hadoop YARN environment.",
                "3. Observe the failure due to the syntax error in the generated task script."
            ],
            "ExpectedBehavior": "The container should launch successfully without any syntax errors in the task script.",
            "ObservedBehavior": "The container fails to launch due to a syntax error in the task script, resulting in an ExitCodeException.",
            "AdditionalDetails": "The error occurs specifically at line 26 of the task script, where an unexpected token is present. This indicates a potential issue with how commands are being constructed or sanitized before being written to the script."
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "bug_report": {
            "Title": "InterruptedException during Application Master allocation in YARN",
            "Description": "An InterruptedException is thrown during the allocation process in the YARN Application Master service. This issue occurs when the AsyncDispatcher's GenericEventHandler attempts to handle an event but is interrupted while trying to acquire a lock on a blocking queue.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "Start the YARN Application Master service.",
                "Trigger an allocation request to the Application Master.",
                "Interrupt the thread handling the allocation request during the event processing."
            ],
            "ExpectedBehavior": "The Application Master should handle allocation requests without throwing an InterruptedException.",
            "ObservedBehavior": "An InterruptedException is thrown, causing the allocation process to fail.",
            "AdditionalDetails": "The issue seems to stem from the handling of events in the AsyncDispatcher, particularly when acquiring locks on the LinkedBlockingQueue. This could indicate a problem with thread management or event handling in the YARN framework."
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "bug_report": {
            "Title": "File Not Found Exception in TaskLog Syncing",
            "Description": "The application encounters a 'No such file or directory' error when attempting to write to an index file during the logging process. This issue arises in the Hadoop framework when the system tries to access a file that does not exist, leading to a failure in the task logging mechanism.",
            "StackTrace": [
                "ENOENT: No such file or directory",
                "at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)",
                "at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)",
                "at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:229)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop job that generates logs.",
                "2. Ensure that the log directory specified does not exist or is inaccessible.",
                "3. Monitor the logs for the 'No such file or directory' error."
            ],
            "ExpectedBehavior": "The system should create the necessary log files and write to the index file without encountering a file not found error.",
            "ObservedBehavior": "The system throws an ENOENT error indicating that it cannot find the specified file or directory when attempting to write to the index file.",
            "AdditionalDetails": "The issue may be related to the permissions of the directory or the absence of the directory itself. The method 'createForWrite' in 'SecureIOUtils' is expected to create the file if it does not exist, but it fails due to the missing directory."
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException when attempting to list application logs",
            "Description": "The application encounters a FileNotFoundException when trying to access the log file for a specific application. This occurs during the log aggregation process, specifically when the LogDumper attempts to list the status of the log directory.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)",
                "at org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)",
                "at org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)",
                "at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)",
                "at org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)"
            ],
            "StepsToReproduce": [
                "Run the LogDumper application with an application ID that does not have associated logs in the specified directory.",
                "Ensure that the log directory path is set to /tmp/logs/ and the application ID is valid but does not exist."
            ],
            "ExpectedBehavior": "The LogDumper should handle the absence of the log file gracefully, either by providing a user-friendly error message or by skipping the log aggregation process without throwing an exception.",
            "ObservedBehavior": "The application throws a FileNotFoundException, causing the LogDumper to terminate unexpectedly.",
            "AdditionalDetails": "The issue arises in the 'listStatusIterator' method of the Hdfs class, which is called during the log aggregation process in the LogDumper. The method attempts to list the status of a directory that does not exist, leading to the exception."
        }
    },
    {
        "filename": "MAPREDUCE-6815.json",
        "creation_time": "2016-12-02T00:30:37.000+0000",
        "bug_report": {
            "Title": "AssertionError in Job State Validation During Task Kill Test",
            "Description": "An AssertionError occurs in the testKillTask method of the TestKill class when the job state is expected to be SUCCEEDED but is instead ERROR. This indicates a failure in the job state transition logic after a task is killed.",
            "StackTrace": [
                "java.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)"
            ],
            "StepsToReproduce": [
                "Run the testKillTask method in the TestKill class.",
                "Ensure that the BlockingMRApp is properly initialized with the required parameters.",
                "Observe the job state after sending a kill signal to one of the tasks."
            ],
            "ExpectedBehavior": "The job should transition to the SUCCEEDED state after one task is killed and the other task completes successfully.",
            "ObservedBehavior": "The job transitions to the ERROR state instead of SUCCEEDED, leading to an AssertionError.",
            "AdditionalDetails": "The waitForState method in MRApp is expected to validate the job state against the final expected state. The failure indicates that the job's state management logic may not be handling task failures correctly, particularly in scenarios where tasks are killed."
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ApplicationMaster Registration",
            "Description": "The application fails to register with the Resource Manager due to a NullPointerException occurring in the ClientRMProtocolService. This issue arises when the ApplicationMaster attempts to call the registerApplicationMaster method, leading to an UndeclaredThrowableException.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)",
                "at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)",
                "at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)",
                "at kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)",
                "Caused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:130)",
                "at $Proxy6.registerApplicationMaster(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:101)",
                "... 3 more",
                "Caused by: java.lang.NullPointerException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1084)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)",
                "... 5 more",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)"
            ],
            "StepsToReproduce": [
                "Start the ApplicationMaster.",
                "Attempt to register with the Resource Manager."
            ],
            "ExpectedBehavior": "The ApplicationMaster should successfully register with the Resource Manager without any exceptions.",
            "ObservedBehavior": "The ApplicationMaster fails to register, throwing a NullPointerException and an UndeclaredThrowableException.",
            "AdditionalDetails": "The NullPointerException occurs in the getRequestPrototype method of the ClientRMProtocolService, indicating that there may be an issue with the request being sent to the Resource Manager."
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "bug_report": {
            "Title": "AccessControlException when canceling delegation token",
            "Description": "An AccessControlException is thrown when a user attempts to cancel a delegation token without the necessary permissions. This issue arises in the cancelToken method of the AbstractDelegationTokenSecretManager class.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)",
                "at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)"
            ],
            "StepsToReproduce": [
                "1. Attempt to cancel a delegation token as a user without the necessary permissions.",
                "2. Observe the AccessControlException thrown."
            ],
            "ExpectedBehavior": "The user should be able to cancel the delegation token if they have the appropriate permissions.",
            "ObservedBehavior": "An AccessControlException is thrown indicating that the user is not authorized to cancel the token.",
            "AdditionalDetails": "The issue is likely related to the permission checks implemented in the cancelToken method of the AbstractDelegationTokenSecretManager class. The user must have the correct authorization to perform this action."
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "bug_report": {
            "Title": "CircularRedirectException in WebAppProxyServlet",
            "Description": "The application encounters a CircularRedirectException when attempting to proxy a link through the WebAppProxyServlet. This issue arises when the servlet tries to redirect to a URL that leads back to itself, causing an infinite loop of redirects.",
            "StackTrace": [
                "org.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'",
                "at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
            ],
            "StepsToReproduce": [
                "1. Send a request to the WebAppProxyServlet with a URI that leads to a circular redirect.",
                "2. Observe the response and the resulting CircularRedirectException."
            ],
            "ExpectedBehavior": "The WebAppProxyServlet should handle the redirect properly without entering an infinite loop, returning a valid response or an appropriate error message.",
            "ObservedBehavior": "The application throws a CircularRedirectException, indicating that it is stuck in a loop of redirects.",
            "AdditionalDetails": "The issue likely stems from the logic in the proxyLink method, which does not adequately handle cases where the target URL is the same as the original request URL, leading to a circular redirect."
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "bug_report": {
            "Title": "UnsatisfiedLinkError in NativeIO.access0 Method",
            "Description": "The application encounters an UnsatisfiedLinkError when attempting to access a file using the NativeIO class on a Windows environment. This error indicates that the native method 'access0' could not be found, which is critical for checking file access permissions.",
            "StackTrace": [
                "java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)",
                "at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)",
                "at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)",
                "at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)",
                "at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)",
                "at org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)",
                "at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)"
            ],
            "StepsToReproduce": [
                "Run the application on a Windows environment.",
                "Attempt to access a file or directory that requires permission checks.",
                "Observe the error in the logs."
            ],
            "ExpectedBehavior": "The application should successfully check file access permissions without throwing an UnsatisfiedLinkError.",
            "ObservedBehavior": "The application throws an UnsatisfiedLinkError indicating that the native method 'access0' could not be found, preventing file access checks from completing.",
            "AdditionalDetails": "This issue may arise if the native library required for the NativeIO class is not properly loaded or is missing. Ensure that the native libraries for Hadoop are correctly installed and accessible in the environment."
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException: Wrong FS when moving files from local to HDFS",
            "Description": "An IllegalArgumentException is thrown when attempting to move a file from the local filesystem to HDFS. The error indicates a mismatch between the expected filesystem type and the actual filesystem type being used.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)",
                "at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)",
                "at org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Attempt to move a file from the local filesystem to HDFS using the moveFromLocalFile method.",
                "2. Ensure that the source path is a local file and the destination path is an HDFS path.",
                "3. Observe the exception thrown during the operation."
            ],
            "ExpectedBehavior": "The file should be successfully moved from the local filesystem to the specified HDFS location without any exceptions.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a mismatch between the expected filesystem (file:/// for local) and the actual filesystem (hdfs://10.18.52.146:9000).",
            "AdditionalDetails": "The issue arises in the checkPath method of the FileSystem class, which validates the filesystem type of the provided path. The method expects the source path to be a local file (file:///), but it receives an HDFS path instead."
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException: Invalid key to HMAC computation in ContainerTokenSecretManager",
            "Description": "An IllegalArgumentException is thrown during the HMAC computation in the ContainerTokenSecretManager when attempting to create a password. The root cause appears to be an invalid key being passed to the HMAC computation, leading to a failure in container creation.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Invalid key to HMAC computation",
                "at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)",
                "at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Caused by: java.security.InvalidKeyException: Secret key expected",
                "at com.sun.crypto.provider.HmacCore.a(DashoA13*..)",
                "at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)",
                "at javax.crypto.Mac.init(DashoA13*..)",
                "at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)"
            ],
            "StepsToReproduce": [
                "1. Attempt to create a container using the LeafQueue's createContainer method.",
                "2. Ensure that the parameters passed to the method include an invalid or null key for HMAC computation."
            ],
            "ExpectedBehavior": "The container should be created successfully without throwing an IllegalArgumentException.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating an invalid key for HMAC computation, preventing the container from being created.",
            "AdditionalDetails": "The issue seems to stem from the createPassword method in the SecretManager class, which is unable to initialize the HMAC due to an invalid key. This could be related to the configuration of security tokens or the initialization of the secret manager."
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "bug_report": {
            "Title": "IOException due to Resource Modification on HDFS",
            "Description": "An IOException is thrown when attempting to copy a resource from HDFS to the local filesystem. The error indicates that the resource has changed on the source filesystem, which is unexpected during the copy operation. This issue arises when the modification time of the resource on HDFS does not match the expected timestamp.",
            "StackTrace": [
                "java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)",
                "at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.lang.Thread.run(Thread.java:619)",
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)",
                "at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)",
                "at org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)",
                "at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that requires copying a resource from HDFS.",
                "2. Ensure that the resource on HDFS is modified after the job submission but before the copy operation completes.",
                "3. Observe the logs for the IOException indicating the resource has changed."
            ],
            "ExpectedBehavior": "The resource should be copied successfully from HDFS to the local filesystem without any exceptions.",
            "ObservedBehavior": "An IOException is thrown indicating that the resource on the source filesystem has changed, causing the copy operation to fail.",
            "AdditionalDetails": "The issue is likely caused by concurrent modifications to the resource on HDFS while the copy operation is in progress. The method 'copy(Path sCopy, Path dstdir)' checks the modification time of the source file against an expected timestamp, and if they do not match, it throws an IOException."
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "bug_report": {
            "Title": "IllegalMonitorStateException in IndexCache.getIndexInformation",
            "Description": "An IllegalMonitorStateException is thrown when attempting to call wait() on an object without holding the object's monitor. This issue occurs in the IndexCache class when trying to retrieve index information, which is part of the shuffle handling process in Hadoop.",
            "StackTrace": [
                "java.lang.IllegalMonitorStateException",
                "at java.lang.Object.wait(Native Method)",
                "at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)",
                "at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)",
                "at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Trigger a shuffle operation in Hadoop that requires index information retrieval.",
                "Ensure that the IndexCache is accessed concurrently without proper synchronization."
            ],
            "ExpectedBehavior": "The system should retrieve index information without throwing an IllegalMonitorStateException.",
            "ObservedBehavior": "An IllegalMonitorStateException is thrown, indicating that wait() was called without holding the object's monitor.",
            "AdditionalDetails": "The method getIndexInformation in IndexCache is expected to handle synchronization properly to avoid this exception. Review the implementation to ensure that wait() is called only when the monitor is held."
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException: Invalid NodeId format in YARN job",
            "Description": "An IllegalArgumentException is thrown when the YARN job attempts to process a NodeId that does not conform to the expected 'host:port' format. This occurs during the recovery process of a task attempt, leading to job failures.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port",
                "at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:985)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:128)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:845)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "Submit a YARN job with an invalid NodeId format (e.g., missing port or incorrect format).",
                "Monitor the job's recovery process to observe the exception being thrown."
            ],
            "ExpectedBehavior": "The YARN job should handle NodeId parsing gracefully and either correct the format or log an appropriate error message without crashing.",
            "ObservedBehavior": "The job fails with an IllegalArgumentException indicating an invalid NodeId format, causing the job to terminate unexpectedly.",
            "AdditionalDetails": "The issue arises in the 'toNodeId' method of the ConverterUtils class, which expects a valid 'host:port' format. The error suggests that the NodeId being processed does not meet this requirement, leading to the exception."
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "bug_report": {
            "Title": "IOException due to ClosedByInterruptException in Hadoop IPC Client",
            "Description": "An IOException is thrown when the Hadoop IPC Client attempts to communicate with a remote server, but the connection is interrupted. This is caused by a ClosedByInterruptException, indicating that the thread was interrupted while trying to write data to the socket channel.",
            "StackTrace": [
                "java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1062)",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)",
                "at $Proxy0.statusUpdate(Unknown Source)",
                "at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)",
                "at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)",
                "at java.io.DataOutputStream.flush(DataOutputStream.java:106)",
                "at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1040)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop server and ensure it is listening on the specified port (35400).",
                "2. Initiate a task that requires communication with the Hadoop IPC Client.",
                "3. Interrupt the thread that is responsible for sending the status update to the server."
            ],
            "ExpectedBehavior": "The Hadoop IPC Client should successfully send the status update to the server without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating that the call to the server failed due to a ClosedByInterruptException, which occurs when the thread is interrupted during a write operation.",
            "AdditionalDetails": "The issue seems to stem from the way the thread handling the communication is managed. If the thread is interrupted while performing I/O operations, it leads to a ClosedByInterruptException, which is not handled gracefully in the current implementation."
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "bug_report": {
            "Title": "NullPointerException in JvmManager during Task Execution",
            "Description": "A NullPointerException is thrown in the JvmManager class while attempting to retrieve JVM details during task execution. This issue occurs when the TaskRunner tries to launch a JVM for a task, leading to a failure in task execution.",
            "StackTrace": [
                "java.lang.Throwable: Child Error",
                "at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)",
                "at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)",
                "at org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)",
                "at org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)",
                "at org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)",
                "at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop job that requires task execution.",
                "2. Monitor the logs for any JVM-related errors.",
                "3. Observe the NullPointerException in the logs when the TaskRunner attempts to launch a JVM."
            ],
            "ExpectedBehavior": "The TaskRunner should successfully launch a JVM for task execution without throwing a NullPointerException.",
            "ObservedBehavior": "The TaskRunner throws a NullPointerException when trying to retrieve JVM details, causing the task execution to fail.",
            "AdditionalDetails": "The source code for the JvmManager class is not provided, but the stack trace indicates that the issue arises from the getDetails method, which likely attempts to access an object that has not been initialized."
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitonException when handling JOB_TASK_ATTEMPT_COMPLETED event",
            "Description": "An InvalidStateTransitonException is thrown when the system attempts to handle a JOB_TASK_ATTEMPT_COMPLETED event while in a FAILED state. This indicates that the event is not valid for the current state of the job, leading to a failure in processing the event.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to the Hadoop YARN framework.",
                "2. Ensure that the job transitions to a FAILED state.",
                "3. Trigger a JOB_TASK_ATTEMPT_COMPLETED event for the job."
            ],
            "ExpectedBehavior": "The system should handle the JOB_TASK_ATTEMPT_COMPLETED event gracefully, either by ignoring it or by transitioning to a valid state if applicable.",
            "ObservedBehavior": "The system throws an InvalidStateTransitonException, indicating that the event cannot be processed in the current FAILED state.",
            "AdditionalDetails": "The exception occurs in the doTransition method of the StateMachineFactory, which is responsible for managing state transitions. The handle method in JobImpl attempts to process the event but fails due to the invalid state. The system logs an error and adds a diagnostic message indicating the invalid event."
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AppSchedulingInfo.allocateNodeLocal",
            "Description": "A NullPointerException occurs in the method allocateNodeLocal of the AppSchedulingInfo class when attempting to allocate resources for a node. This issue arises when the method is called with a null SchedulerNode, leading to a failure in resource allocation.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Trigger a resource allocation request in the YARN ResourceManager.",
                "2. Ensure that the SchedulerNode passed to the allocateNodeLocal method is null.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The system should handle the null SchedulerNode gracefully, either by skipping the allocation or by logging an appropriate error message without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the resource allocation process to fail and potentially impacting the overall scheduling functionality.",
            "AdditionalDetails": "The allocateNodeLocal method is expected to allocate resources for a given node. If the node is null, the method should include a null check to prevent the exception. The relevant code snippet for allocateNodeLocal is not provided, but it should be reviewed to ensure proper null handling."
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "bug_report": {
            "Title": "ClosedChannelException during Job History Event Handling",
            "Description": "A ClosedChannelException is thrown when the JobHistoryEventHandler attempts to handle a job event after the output stream has been closed. This occurs during the shutdown process of the MRAppMaster, specifically when it tries to write job history events after the event writer has been closed.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)",
                "Caused by: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)",
                "at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:107)",
                "at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)",
                "at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)",
                "at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to the Hadoop cluster.",
                "2. Allow the job to complete.",
                "3. Trigger the shutdown of the MRAppMaster, especially during the cleanup phase.",
                "4. Observe the logs for the ClosedChannelException."
            ],
            "ExpectedBehavior": "The JobHistoryEventHandler should handle job events without throwing exceptions, even during the shutdown process.",
            "ObservedBehavior": "A ClosedChannelException is thrown, indicating that the output stream for writing job history events is closed when the event handler attempts to write an event.",
            "AdditionalDetails": "The issue seems to stem from the timing of the shutdown process where the event writer is closed before all events are processed. This can lead to attempts to write to a closed stream, resulting in the exception."
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "bug_report": {
            "Title": "Kerberos Authentication Failure in Hadoop IPC Client",
            "Description": "The application encounters an IOException when attempting to connect to the Hadoop IPC server due to a failure in Kerberos authentication. The error indicates that no valid credentials were provided, leading to a GSSException.",
            "StackTrace": [
                "java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1097)",
                "at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)",
                "at $Proxy7.getProtocolVersion(Unknown Source)",
                "at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)",
                "at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)",
                "at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)",
                "at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)",
                "at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)",
                "at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)",
                "at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:543)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:488)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:590)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:187)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1228)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1072)",
                "... 20 more",
                "Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:385)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1200(Client.java:187)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:583)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:580)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:579)",
                "... 23 more",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)"
            ],
            "StepsToReproduce": [
                "Attempt to connect to the Hadoop IPC server at 192.168.7.80:8020 without valid Kerberos credentials.",
                "Ensure that the Hadoop configuration is set to use Kerberos authentication."
            ],
            "ExpectedBehavior": "The application should successfully authenticate using Kerberos and establish a connection to the Hadoop IPC server.",
            "ObservedBehavior": "The application fails to connect due to a GSSException indicating that no valid credentials were provided.",
            "AdditionalDetails": "This issue may occur if the Kerberos ticket is not available or has expired. Ensure that the user has a valid Kerberos ticket before attempting to connect."
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AppSchedulingInfo.allocateNodeLocal",
            "Description": "A NullPointerException occurs in the allocateNodeLocal method of the AppSchedulingInfo class when attempting to allocate resources for a node. This issue arises when the method is called with a null SchedulerNode, leading to a failure in resource allocation.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Trigger a resource allocation request in the YARN ResourceManager.",
                "2. Ensure that the SchedulerNode passed to the allocateNodeLocal method is null.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should handle null SchedulerNode gracefully, either by skipping the allocation or by logging an appropriate error message without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the resource allocation process to fail and potentially impacting the overall scheduling functionality.",
            "AdditionalDetails": "The allocateNodeLocal method is expected to allocate resources for a given node. If the node is null, the method should ideally check for nullity before proceeding with resource allocation. This indicates a lack of null checks in the code."
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException in TaskAttempt Handling",
            "Description": "An InvalidStateTransitionException is thrown when an event of type TA_COMMIT_PENDING is processed in the COMMIT_PENDING state of a TaskAttempt. This indicates that the event cannot be handled in the current state, leading to a failure in the task attempt processing.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that transitions to the COMMIT_PENDING state.",
                "2. Trigger an event of type TA_COMMIT_PENDING for the task attempt.",
                "3. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The task attempt should handle the TA_COMMIT_PENDING event appropriately without throwing an exception, allowing the task to transition to the next state.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the TA_COMMIT_PENDING event cannot be processed in the COMMIT_PENDING state, leading to a failure in task attempt handling.",
            "AdditionalDetails": "The exception occurs in the doTransition method of the StateMachineFactory, which suggests that the state machine does not allow the transition from COMMIT_PENDING with the given event. This may require a review of the state transition logic to ensure that all valid events are handled correctly in their respective states."
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "bug_report": {
            "Title": "UnknownServiceException Thrown During Job End Notification",
            "Description": "An UnknownServiceException is thrown when the JobEndNotifier attempts to notify a URL after a job has finished. This occurs because the URL connection does not have a valid content-type, leading to a failure in retrieving the input stream.",
            "StackTrace": [
                "java.net.UnknownServiceException: no content-type",
                "at java.net.URLConnection.getContentHandler(URLConnection.java:1192)",
                "at java.net.URLConnection.getContent(URLConnection.java:689)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)"
            ],
            "StepsToReproduce": [
                "1. Configure a job with a notification URL that does not return a valid content-type.",
                "2. Run the job to completion.",
                "3. Observe the logs for the UnknownServiceException during the job end notification."
            ],
            "ExpectedBehavior": "The job end notification should succeed without throwing an exception, regardless of the content-type returned by the URL.",
            "ObservedBehavior": "An UnknownServiceException is thrown, indicating that the URL connection does not have a valid content-type, causing the job end notification to fail.",
            "AdditionalDetails": "The issue arises in the notifyURLOnce() method of the JobEndNotifier class, specifically when calling conn.getContent(). The method should handle cases where the content-type is not set, possibly by checking the content-type before attempting to retrieve the input stream."
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException due to Permission Denied on Local File System",
            "Description": "The application encounters a FileNotFoundException when attempting to create a local file for storing job token passwords. The error indicates that the application does not have the necessary permissions to write to the specified directory.",
            "StackTrace": [
                "java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1026)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:703)",
                "at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)",
                "at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)"
            ],
            "StepsToReproduce": [
                "1. Attempt to run a Hadoop job that requires writing a job token password to the local file system.",
                "2. Ensure that the user running the job does not have write permissions to the directory '/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/'.",
                "3. Observe the FileNotFoundException in the logs."
            ],
            "ExpectedBehavior": "The application should successfully create the job token password file without any permission issues.",
            "ObservedBehavior": "The application throws a FileNotFoundException indicating 'Permission denied' when trying to create the job token password file.",
            "AdditionalDetails": "The issue is likely related to the permissions set on the directory '/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/'. Ensure that the user has the appropriate write permissions to this directory."
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CountersBlock Constructor",
            "Description": "A NullPointerException is thrown when attempting to instantiate the CountersBlock class due to a failure in the dependency injection process. This occurs when the getCounters method attempts to access counters that are not properly initialized.",
            "StackTrace": [
                "Caused by: com.google.inject.ProvisionException: Guice provision errors:",
                "java.lang.reflect.InvocationTargetException",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "    at java.lang.reflect.Method.invoke(Method.java:597)",
                "...",
                "1) Error injecting constructor, java.lang.NullPointerException",
                "  at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)",
                "  while locating org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock",
                "...",
                "Caused by: java.lang.NullPointerException",
                "    at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)",
                "    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)",
                "    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)",
                "    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)"
            ],
            "StepsToReproduce": [
                "1. Attempt to create an instance of CountersBlock.",
                "2. Ensure that the necessary context (AppContext) is provided.",
                "3. Observe the NullPointerException being thrown during the constructor invocation."
            ],
            "ExpectedBehavior": "The CountersBlock should be instantiated without throwing any exceptions, and the counters should be properly initialized.",
            "ObservedBehavior": "A NullPointerException is thrown during the instantiation of the CountersBlock, indicating that the counters are not initialized correctly.",
            "AdditionalDetails": "The issue seems to stem from the getCounters method, which fails to retrieve the necessary job and task information, leading to uninitialized counters being passed to the incrAllCounters method."
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "bug_report": {
            "Title": "ArithmeticException in ResourceCalculatorUtils.computeAvailableContainers",
            "Description": "An ArithmeticException occurs when the computeAvailableContainers method attempts to divide by zero. This issue arises during the assignment of containers in the RMContainerAllocator class, specifically when the available resources are insufficient, leading to a division by zero error.",
            "StackTrace": [
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop MapReduce job with insufficient resources allocated.",
                "2. Monitor the ResourceManager and ApplicationMaster logs.",
                "3. Observe the logs for the ArithmeticException when the RMContainerAllocator attempts to assign containers."
            ],
            "ExpectedBehavior": "The system should handle resource allocation gracefully without throwing an ArithmeticException, even when resources are insufficient.",
            "ObservedBehavior": "An ArithmeticException is thrown, indicating a division by zero error in the computeAvailableContainers method, causing the container assignment process to fail.",
            "AdditionalDetails": "The computeAvailableContainers method is called with available resources that may not meet the required resources, leading to a division by zero. This indicates a need for validation of resource availability before performing calculations."
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "bug_report": {
            "Title": "Missing Event Handler for AM_STARTED EventType",
            "Description": "The application encounters an exception when attempting to dispatch an event of type 'AM_STARTED' due to the absence of a registered event handler for the 'EventType' class. This results in a failure to process the event, which may lead to further issues in the application flow.",
            "StackTrace": [
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Trigger an event of type 'AM_STARTED' in the application.",
                "Ensure that the event dispatching mechanism is active.",
                "Observe the logs for the exception indicating no handler is registered."
            ],
            "ExpectedBehavior": "The application should successfully dispatch the 'AM_STARTED' event to the appropriate handler without throwing an exception.",
            "ObservedBehavior": "An exception is thrown indicating that there is no registered handler for the 'AM_STARTED' event type, preventing the event from being processed.",
            "AdditionalDetails": "The dispatch method in the AsyncDispatcher class attempts to retrieve an EventHandler for the event type but fails, leading to the exception. It is necessary to ensure that all event types, including 'AM_STARTED', have corresponding handlers registered in the eventDispatchers map."
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "bug_report": {
            "Title": "NullPointerException in TaskAttemptImpl during Task Failure Handling",
            "Description": "A NullPointerException occurs in the sendJHStartEventForAssignedFailTask method of TaskAttemptImpl when handling a task failure event. This issue arises when the task attempt is not properly initialized or when an expected object is null, leading to a failure in sending job history start events for failed tasks.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job that is expected to fail due to fetch failures.",
                "2. Monitor the job's task attempts and observe the handling of failure events.",
                "3. Check the logs for a NullPointerException in the TaskAttemptImpl class."
            ],
            "ExpectedBehavior": "The system should handle task failure events gracefully, sending appropriate job history events without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that an expected object is null when attempting to send job history start events for failed tasks.",
            "AdditionalDetails": "The issue may be related to the initialization of the TaskAttempt object or the event handling mechanism. Further investigation is needed to ensure that all necessary objects are properly instantiated before they are accessed."
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException in RMContainerAllocator during Reduce Preemption",
            "Description": "An IllegalArgumentException is thrown in the RMContainerAllocator class when attempting to preempt reduce tasks due to insufficient memory for map tasks. The exception indicates that the comparison method used in sorting violates its general contract, which typically occurs when the comparator is inconsistent with equals.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeLo(TimSort.java:747)",
                "at java.util.TimSort.mergeAt(TimSort.java:483)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:408)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop MapReduce job with a configuration that leads to high memory usage for reduce tasks.",
                "2. Ensure that there are unassigned map tasks while reduce tasks are scheduled.",
                "3. Monitor the RMContainerAllocator's heartbeat method to trigger preemption.",
                "4. Observe the logs for the IllegalArgumentException."
            ],
            "ExpectedBehavior": "The RMContainerAllocator should successfully preempt reduce tasks to allocate resources for map tasks without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the comparison method violates its general contract, leading to a failure in the preemption process.",
            "AdditionalDetails": "The issue likely stems from the comparator used in the sorting process within the preemptReduce method. It is essential to ensure that the comparator is consistent with equals and adheres to the contract specified in the Comparator interface."
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "bug_report": {
            "Title": "NumberFormatException when parsing long value in ProcfsBasedProcessTree",
            "Description": "A NumberFormatException occurs when the application attempts to parse a string representation of a long integer that exceeds the maximum value for a long in Java. This issue arises in the ProcfsBasedProcessTree class when retrieving process information.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"18446743988060683582\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)",
                "at java.lang.Long.parseLong(Long.java:422)",
                "at java.lang.Long.parseLong(Long.java:468)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)",
                "at org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)",
                "at org.apache.hadoop.mapred.Task.initialize(Task.java:536)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "Run a Hadoop job that retrieves process information using ProcfsBasedProcessTree.",
                "Ensure that the process ID or resource value being parsed exceeds the maximum long value (9223372036854775807)."
            ],
            "ExpectedBehavior": "The application should handle large values gracefully, either by using a different data type or by implementing error handling to avoid crashing.",
            "ObservedBehavior": "The application throws a NumberFormatException, causing the job to fail when attempting to parse a long value that exceeds the maximum limit.",
            "AdditionalDetails": "The issue is likely due to the assumption that all process IDs or resource values can be represented as a long. Consider reviewing the logic in the constructProcessInfo method of ProcfsBasedProcessTree to handle larger values or implement appropriate error handling."
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "bug_report": {
            "Title": "IOException during Subversion Checkout in Hudson",
            "Description": "An IOException occurs when attempting to perform a remote file operation during the checkout process of a Subversion SCM in Hudson. The error indicates that the system is unable to delete a specific user log file, which prevents the checkout from completing successfully.",
            "StackTrace": [
                "hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7",
                "at hudson.FilePath.act(FilePath.java:749)",
                "at hudson.FilePath.act(FilePath.java:735)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)",
                "at hudson.model.AbstractProject.checkout(AbstractProject.java:1116)",
                "at hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild$AbstractRunner.java:479)",
                "at hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild$AbstractRunner.java:411)",
                "at hudson.model.Run.run(Run.java:1324)",
                "at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)",
                "at hudson.model.ResourceController.execute(ResourceController.java:88)",
                "at hudson.model.Executor.run(Executor.java:139)",
                "Caused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
            ],
            "StepsToReproduce": [
                "1. Set up a Hudson job configured to use Subversion SCM.",
                "2. Trigger a build for the job.",
                "3. Observe the build process where the checkout operation is performed."
            ],
            "ExpectedBehavior": "The Subversion checkout should complete successfully without any IOException, allowing the build to proceed.",
            "ObservedBehavior": "The checkout fails with an IOException indicating that a specific user log file could not be deleted, halting the build process.",
            "AdditionalDetails": "The issue may be related to file permissions or the state of the file system on the Hudson slave node. Further investigation into the file system and user permissions is recommended."
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "bug_report": {
            "Title": "Kerberos Authentication Failure in Hadoop Client",
            "Description": "The application encounters an IOException due to a failure in establishing a SASL connection using Kerberos authentication. The error indicates that no valid credentials were provided, leading to a GSSException.",
            "StackTrace": [
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy9.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy10.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystem filesys, final Path path)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)",
                "at java.util.TimerThread.mainLoop(Timer.java:555)",
                "at java.util.TimerThread.run(Timer.java:505)",
                "Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)",
                "at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1381)",
                "... 21 more",
                "Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:411)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client$Connection.java:550)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:367)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:716)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:712)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)",
                "... 24 more",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)"
            ],
            "StepsToReproduce": [
                "Attempt to connect to a Hadoop cluster using Kerberos authentication without valid credentials.",
                "Invoke a method that requires listing files or directories in HDFS."
            ],
            "ExpectedBehavior": "The application should successfully authenticate using Kerberos and list the requested files or directories in HDFS.",
            "ObservedBehavior": "The application throws an IOException indicating a failure in establishing a SASL connection due to missing valid Kerberos credentials.",
            "AdditionalDetails": "Ensure that the Kerberos ticket granting ticket (TGT) is available and valid before attempting to connect to the Hadoop cluster."
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException in FileNameIndexUtils",
            "Description": "An ArrayIndexOutOfBoundsException occurs in the FileNameIndexUtils class when attempting to trim a URL-encoded string. This issue arises during the processing of done files in the JobHistoryEventHandler.",
            "StackTrace": [
                "java.lang.ArrayIndexOutOfBoundsException: 50",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Submit a job to the Hadoop MapReduce framework.",
                "Ensure that the job generates a done file that exceeds the expected length.",
                "Monitor the job history events to observe the processing of done files."
            ],
            "ExpectedBehavior": "The system should process done files without throwing an ArrayIndexOutOfBoundsException, regardless of the length of the job name or other parameters.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown when the length of the job name or related parameters exceeds the expected limit, causing the job history processing to fail.",
            "AdditionalDetails": "The issue seems to stem from the method 'getDoneFileName' in the FileNameIndexUtils class, which is called with parameters that may not be properly validated for length. The job name limit is configurable, and the current implementation does not handle cases where the job name exceeds this limit."
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to Invalid DFS Filename",
            "Description": "An IllegalArgumentException is thrown when attempting to resolve a path that is not a valid DFS filename. This occurs during the execution of a Hadoop task when it tries to access a file in the Hadoop Distributed File System (HDFS). The exception indicates that the pathname provided does not conform to the expected format for DFS filenames.",
            "StackTrace": [
                "Exception running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)",
                "at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)",
                "at org.apache.hadoop.mapred.Task.done(Task.java:1048)"
            ],
            "StepsToReproduce": [
                "1. Submit a Hadoop job that attempts to write output to a path that includes invalid characters for a DFS filename.",
                "2. Monitor the job execution and observe the logs for any exceptions thrown.",
                "3. Specifically look for the IllegalArgumentException related to the pathname."
            ],
            "ExpectedBehavior": "The system should validate the pathname and either correct it or provide a meaningful error message without throwing an IllegalArgumentException.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the provided pathname is not a valid DFS filename, causing the job to fail.",
            "AdditionalDetails": "The method 'getPathName(Path file)' checks the validity of the path and throws an IllegalArgumentException if the path does not conform to DFS naming conventions. The path in question appears to be a Windows-style path, which may not be compatible with HDFS."
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException when renaming map output file",
            "Description": "The application throws a FileNotFoundException when attempting to rename a map output file during the execution of a MapReduce job. The error indicates that the specified output file does not exist, which prevents the job from completing successfully.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop MapReduce job that generates output files.",
                "2. Execute the job and ensure it attempts to rename the output files.",
                "3. Observe the logs for any FileNotFoundException related to the output files."
            ],
            "ExpectedBehavior": "The application should successfully rename the output files generated by the MapReduce job without throwing a FileNotFoundException.",
            "ObservedBehavior": "The application throws a FileNotFoundException indicating that the specified output file does not exist, preventing the job from completing successfully.",
            "AdditionalDetails": "The issue may arise if the output file is not created due to a previous error in the job execution or if the file path is incorrectly specified. The method 'deprecatedGetFileStatus' in 'RawLocalFileSystem' is responsible for checking the existence of the file, and it throws a FileNotFoundException if the file does not exist."
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "bug_report": {
            "Title": "NoSuchElementException in ApplicationImpl Transition Handling",
            "Description": "A NoSuchElementException is thrown when transitioning the application state in the ApplicationImpl class. This occurs when the application ID is not found in the context's applications map, leading to an attempt to iterate over an empty collection.",
            "StackTrace": [
                "java.util.NoSuchElementException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:822)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Trigger an application state transition in the ApplicationImpl class.",
                "Ensure that the application ID being transitioned does not exist in the context's applications map."
            ],
            "ExpectedBehavior": "The application state should transition without throwing an exception, even if the application ID is not found in the context.",
            "ObservedBehavior": "A NoSuchElementException is thrown, indicating that the application ID was not found during the transition process.",
            "AdditionalDetails": "The issue likely arises from the 'transition' method in ApplicationImpl, where the application ID is removed from the context's applications map. If the application ID is not present, the subsequent operations that rely on this ID may lead to a NoSuchElementException when iterating over the map."
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "bug_report": {
            "Title": "NullPointerException in MRAppMaster during Job History Parsing",
            "Description": "A NullPointerException occurs in the MRAppMaster class when attempting to parse job history files. The exception is thrown during the initialization of the EventReader, which indicates that a required input stream is null.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at java.io.StringReader.<init>(StringReader.java:50)",
                "        at org.apache.avro.Schema$Parser.parse(Schema.java:917)",
                "        at org.apache.avro.Schema.parse(Schema.java:966)",
                "        at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)",
                "        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)",
                "        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)",
                "        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)",
                "        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)",
                "        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)",
                "        at java.security.AccessController.doPrivileged(Native Method)",
                "        at javax.security.auth.Subject.doAs(Subject.java:422)",
                "        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)",
                "        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)",
                "        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)"
            ],
            "StepsToReproduce": [
                "1. Start the MRAppMaster with a configuration that points to a job history file.",
                "2. Ensure that the job history file is either missing or corrupted, leading to a null input stream.",
                "3. Observe the logs for the NullPointerException during the parsing process."
            ],
            "ExpectedBehavior": "The MRAppMaster should handle the absence or corruption of the job history file gracefully, either by logging an appropriate error message or by providing a fallback mechanism.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the MRAppMaster to fail during the initialization process, which prevents the application from starting correctly.",
            "AdditionalDetails": "The issue arises in the 'parsePreviousJobHistory' method, specifically when calling 'getPreviousJobHistoryStream'. If this method returns null, it leads to a failure in initializing the EventReader, which expects a valid input stream."
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException in TaskAttempt Handling",
            "Description": "An InvalidStateTransitionException is thrown when a task attempt receives an event indicating too many fetch failures while in a FAILED state. This indicates that the state machine is not handling the event correctly, leading to an unhandled exception.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_TOO_MANY_FETCH_FAILURE at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that is expected to fail due to too many fetch failures.",
                "2. Monitor the task attempts and observe the state transitions.",
                "3. Check the logs for the InvalidStateTransitionException when the event TA_TOO_MANY_FETCH_FAILURE is dispatched."
            ],
            "ExpectedBehavior": "The task attempt should handle the TA_TOO_MANY_FETCH_FAILURE event gracefully, transitioning to an appropriate state or logging the error without throwing an exception.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the event cannot be processed in the current FAILED state, leading to potential job failures and lack of proper error handling.",
            "AdditionalDetails": "The handle(TaskAttemptEvent event) method in TaskAttemptImpl is responsible for processing task events. The state machine should be updated to handle the TA_TOO_MANY_FETCH_FAILURE event appropriately when in a FAILED state."
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException when creating Path from empty string",
            "Description": "An IllegalArgumentException is thrown when attempting to create a Path object with an empty string. This occurs during the execution of a job in the Hadoop MapReduce framework, specifically when retrieving job configurations.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Can not create a Path from an empty string",
                "at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:96)",
                "at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)",
                "at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)",
                "at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)",
                "at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)",
                "at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)",
                "at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)",
                "at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)",
                "at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)",
                "at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:192)"
            ],
            "StepsToReproduce": [
                "1. Attempt to run a Hadoop job without providing a valid path configuration.",
                "2. Ensure that the job configuration is set up in such a way that it results in an empty string being passed to the Path constructor."
            ],
            "ExpectedBehavior": "The Path object should be created successfully without throwing an exception, or a meaningful error message should be provided if the path is invalid.",
            "ObservedBehavior": "An IllegalArgumentException is thrown with the message 'Can not create a Path from an empty string'.",
            "AdditionalDetails": "The issue arises in the 'checkPathArg' method of the Path class, which explicitly disallows the creation of a Path from an empty string. This indicates that the job configuration is not being set correctly, leading to an empty string being passed."
        }
    },
    {
        "filename": "MAPREDUCE-6702.json",
        "creation_time": "2016-05-17T22:11:38.000+0000",
        "bug_report": {
            "Title": "AssertionError in Environment Checker Job during MiniMRChildTask Tests",
            "Description": "The test cases for the MiniMRChildTask are failing due to an AssertionError indicating that the environment checker job has failed. This issue arises in both the testTaskEnv and testTaskOldEnv methods, suggesting a potential problem with the environment setup or configuration used during the tests.",
            "StackTrace": [
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)",
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)"
            ],
            "StepsToReproduce": [
                "Run the test cases for TestMiniMRChildTask, specifically testTaskEnv() and testTaskOldEnv().",
                "Ensure that the environment is set up correctly with the necessary configurations.",
                "Observe the output for AssertionError indicating the environment checker job failure."
            ],
            "ExpectedBehavior": "The test cases should pass without any assertion errors, indicating that the environment checker job completed successfully.",
            "ObservedBehavior": "The test cases fail with an AssertionError stating that the environment checker job failed, indicating a problem with the environment setup.",
            "AdditionalDetails": "The methods testTaskEnv() and testTaskOldEnv() both initialize a JobConf object and set up input and output directories. The failure occurs during the execution of runTestTaskEnv(), which suggests that the issue may lie in the configuration or the environment setup for the Hadoop job."
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException in Task State Machine",
            "Description": "An InvalidStateTransitionException is thrown when an event of type T_ATTEMPT_SUCCEEDED is received while the task is in the SUCCEEDED state. This indicates that the state machine is not handling this event correctly, leading to an unhandled exception.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that includes tasks that can succeed.",
                "2. Monitor the task states and ensure that a task reaches the SUCCEEDED state.",
                "3. Trigger an event of type T_ATTEMPT_SUCCEEDED for the task that is already in the SUCCEEDED state."
            ],
            "ExpectedBehavior": "The task state machine should handle the T_ATTEMPT_SUCCEEDED event gracefully without throwing an exception, regardless of the current state.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the event cannot be processed in the current state (SUCCEEDED).",
            "AdditionalDetails": "The state machine's transition logic does not account for the scenario where a T_ATTEMPT_SUCCEEDED event is received after the task has already succeeded. This may require an update to the state machine's event handling logic to prevent such exceptions."
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "bug_report": {
            "Title": "RuntimeException: Not a host:port pair in ResourceManager initialization",
            "Description": "The ResourceManager fails to initialize due to a RuntimeException indicating that the provided address is not a valid host:port pair. This occurs during the initialization of the AdminService, which is part of the ResourceManager's startup process.",
            "StackTrace": [
                "java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)"
            ],
            "StepsToReproduce": [
                "1. Configure the ResourceManager with an invalid address for 'yarn.resourcemanager.admin.address'.",
                "2. Start the ResourceManager.",
                "3. Observe the RuntimeException in the logs."
            ],
            "ExpectedBehavior": "The ResourceManager should initialize successfully without throwing any exceptions, provided that the configuration parameters are valid.",
            "ObservedBehavior": "The ResourceManager fails to start and throws a RuntimeException indicating that the address for 'yarn.resourcemanager.admin.address' is not a valid host:port pair.",
            "AdditionalDetails": "The issue likely stems from the configuration file where 'yarn.resourcemanager.admin.address' is set. It should be in the format 'hostname:port'. The method 'createSocketAddr' is responsible for parsing this address, and it expects a valid format to create an InetSocketAddress."
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "bug_report": {
            "Title": "YarnRuntimeException: Error creating done directory due to Connection Refused",
            "Description": "The application encounters a YarnRuntimeException when attempting to create a done directory in HDFS. The root cause appears to be a connection refusal to the HDFS namenode at localhost:8020, which prevents the HistoryFileManager from initializing properly.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)",
                "Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1410)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1359)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:185)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)",
                "at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1722)",
                "at org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:124)",
                "at org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1106)",
                "at org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1102)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1102)",
                "at org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1514)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:561)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:502)",
                "... 8 more",
                "Caused by: java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1377)"
            ],
            "StepsToReproduce": [
                "Start the Hadoop YARN Job History Server.",
                "Ensure that the HDFS namenode is not running or is unreachable.",
                "Attempt to submit a job that requires the Job History Server to create a done directory."
            ],
            "ExpectedBehavior": "The Job History Server should initialize successfully and create the done directory in HDFS without any exceptions.",
            "ObservedBehavior": "The Job History Server fails to initialize due to a YarnRuntimeException caused by a connection refusal to the HDFS namenode.",
            "AdditionalDetails": "The error indicates that the application is trying to connect to the HDFS namenode at localhost:8020, which is not reachable. This could be due to the namenode not running or incorrect configuration in the Hadoop setup."
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException on Job State Handling",
            "Description": "The application encounters an InvalidStateTransitionException when processing job events related to task attempts and job completion. This occurs when the application attempts to transition the job state to a state that is not valid based on the current state of the job.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Submit a job that includes task attempts and map tasks.",
                "Monitor the job state transitions and events being dispatched.",
                "Observe the logs for InvalidStateTransitionException when the job attempts to process JOB_TASK_ATTEMPT_COMPLETED or JOB_MAP_TASK_RESCHEDULED events while in the SUCCEEDED state."
            ],
            "ExpectedBehavior": "The job should handle task completion and rescheduling events appropriately without throwing an InvalidStateTransitionException, allowing for smooth state transitions.",
            "ObservedBehavior": "The application throws an InvalidStateTransitionException when it receives JOB_TASK_ATTEMPT_COMPLETED and JOB_MAP_TASK_RESCHEDULED events while the job is in the SUCCEEDED state, indicating that these events are not valid in the current state.",
            "AdditionalDetails": "The issue arises from the state machine's inability to handle certain events when the job is already marked as SUCCEEDED. The 'handle' method in JobImpl attempts to transition the state based on the event type, but the current state does not allow for these transitions, leading to the exception."
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "bug_report": {
            "Title": "NoClassDefFoundError for scala.Function1 in Hadoop MapReduce Job",
            "Description": "A NoClassDefFoundError is thrown when attempting to execute a Hadoop MapReduce job due to the missing scala.Function1 class. This error occurs during the initialization of the JobImpl class, specifically when checking if the job is a chain job.",
            "StackTrace": [
                "java.lang.NoClassDefFoundError: scala/Function1",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)",
                "Caused by: java.lang.ClassNotFoundException: scala.Function1",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:358)"
            ],
            "StepsToReproduce": [
                "1. Configure a Hadoop MapReduce job that requires Scala classes.",
                "2. Attempt to run the job using the Hadoop framework.",
                "3. Observe the error in the logs indicating NoClassDefFoundError for scala.Function1."
            ],
            "ExpectedBehavior": "The Hadoop MapReduce job should execute successfully without any class loading errors.",
            "ObservedBehavior": "The job fails to start due to a NoClassDefFoundError for the scala.Function1 class, indicating that the Scala library is not available in the classpath.",
            "AdditionalDetails": "The error suggests that the Scala library, which is required for certain functionalities in the Hadoop MapReduce framework, is not included in the job's classpath. Ensure that the Scala library is properly included in the Hadoop environment."
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "bug_report": {
            "Title": "EOFException during Data Streaming in HDFS",
            "Description": "An EOFException is thrown when attempting to read from a DataOutputStream in HDFS, indicating that the stream has ended unexpectedly without a proper length prefix. This issue arises during the data transfer process, particularly when adding a datanode to an existing pipeline or recovering from a datanode error.",
            "StackTrace": [
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)"
            ],
            "StepsToReproduce": [
                "1. Initiate a data streaming operation in HDFS.",
                "2. Simulate a failure in the datanode pipeline.",
                "3. Attempt to recover the pipeline by adding a new datanode."
            ],
            "ExpectedBehavior": "The data streaming operation should continue without interruption, and the new datanode should be added to the pipeline successfully.",
            "ObservedBehavior": "An EOFException is thrown, indicating that the data stream has ended prematurely, preventing the addition of the new datanode and halting the data transfer process.",
            "AdditionalDetails": "The issue seems to occur in the 'addDatanode2ExistingPipeline' method, which is responsible for managing the datanode pipeline during data transfer. The EOFException suggests that the data being read does not conform to the expected format, possibly due to a failure in the previous datanode or a network issue."
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "bug_report": {
            "Title": "UninitializedMessageException due to missing 'renewer' field in GetDelegationTokenRequest",
            "Description": "An UninitializedMessageException is thrown when attempting to submit a job in Oozie due to a missing required field 'renewer' in the GetDelegationTokenRequest. This issue arises during the job submission process, specifically when the system attempts to build the request for a delegation token.",
            "StackTrace": [
                "org.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer",
                "at org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)",
                "at org.apache.oozie.command.XCommand.call(XCommand.java:277)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)",
                "at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer",
                "at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)",
                "at org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)",
                "at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)",
                "at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:581)",
                "at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:576)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:576)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)"
            ],
            "StepsToReproduce": [
                "1. Attempt to submit a job using Oozie.",
                "2. Ensure that the job configuration does not include the 'renewer' field in the delegation token request.",
                "3. Observe the exception thrown during the job submission process."
            ],
            "ExpectedBehavior": "The job should be submitted successfully without throwing an UninitializedMessageException.",
            "ObservedBehavior": "An UninitializedMessageException is thrown indicating that the 'renewer' field is missing from the message.",
            "AdditionalDetails": "The issue seems to stem from the construction of the GetDelegationTokenRequestProto, which requires the 'renewer' field to be set. This indicates a potential oversight in the job configuration or the way the delegation token request is being built."
        }
    }
]