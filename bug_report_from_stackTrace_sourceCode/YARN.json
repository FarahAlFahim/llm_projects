[
    {
        "filename": "YARN-5918.json",
        "creation_time": "2016-11-20T14:19:00.000+0000",
        "bug_report": {
            "Title": "NullPointerException in OpportunisticContainerAllocatorAMService",
            "Description": "A NullPointerException occurs in the OpportunisticContainerAllocatorAMService class when attempting to convert NodeId to RemoteNode. This issue arises when the method getLeastLoadedNodes is called, leading to a failure in the allocation process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1857)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2539)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager.",
                "2. Submit an application that triggers the allocation process.",
                "3. Monitor the logs for NullPointerException in the OpportunisticContainerAllocatorAMService."
            ],
            "ExpectedBehavior": "The system should successfully convert NodeId to RemoteNode and allocate resources without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a required object is null during the conversion of NodeId to RemoteNode.",
            "AdditionalDetails": "The issue likely arises from the getNode method returning null for a NodeId that does not exist in the scheduler's context. This can happen if the node has been removed or is not yet registered."
        }
    },
    {
        "filename": "YARN-8629.json",
        "creation_time": "2018-08-07T00:14:14.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException in CGroupsHandlerImpl when deleting cgroup",
            "Description": "The application encounters a FileNotFoundException when attempting to delete a cgroup due to a missing 'tasks' file in the specified directory. This issue arises in the CGroupsHandlerImpl class during the cleanup process of a container.",
            "StackTrace": [
                "java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:93)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Launch a container using the Hadoop YARN framework.",
                "2. Ensure that the container's cgroup is created at the path '/sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/'.",
                "3. Simulate a scenario where the 'tasks' file is deleted or not created.",
                "4. Trigger the cleanup process for the container."
            ],
            "ExpectedBehavior": "The application should successfully delete the cgroup without throwing a FileNotFoundException, even if the 'tasks' file is missing.",
            "ObservedBehavior": "The application throws a FileNotFoundException indicating that the 'tasks' file does not exist, which prevents the cgroup from being deleted.",
            "AdditionalDetails": "The method 'checkAndDeleteCgroup' attempts to read the 'tasks' file to determine if it can delete the cgroup. If the file is missing, it results in an IOException, which is logged but does not handle the absence of the file gracefully."
        }
    },
    {
        "filename": "YARN-4431.json",
        "creation_time": "2015-12-07T18:31:36.000+0000",
        "bug_report": {
            "Title": "Connection Refused Exception During NodeManager Unregistration",
            "Description": "The application encounters a java.net.ConnectException when attempting to unregister a NodeManager from the ResourceManager. This issue arises when the NodeManager service is stopped, and it fails to connect to the ResourceManager at the specified address.",
            "StackTrace": [
                "java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:408)",
                "at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1452)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1385)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)",
                "at com.sun.proxy.$Proxy74.unRegisterNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)",
                "at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:255)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)",
                "at com.sun.proxy.$Proxy75.unRegisterNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop(NodeStatusUpdaterImpl.java:245)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:377)"
            ],
            "StepsToReproduce": [
                "1. Start the NodeManager service.",
                "2. Attempt to stop the NodeManager service while it is trying to unregister from the ResourceManager.",
                "3. Observe the logs for the connection exception."
            ],
            "ExpectedBehavior": "The NodeManager should successfully unregister from the ResourceManager without any connection exceptions.",
            "ObservedBehavior": "The NodeManager fails to unregister from the ResourceManager, resulting in a java.net.ConnectException indicating that the connection was refused.",
            "AdditionalDetails": "The exception occurs because the NodeManager is trying to connect to the ResourceManager at 0.0.0.0:8031, which is not a valid address. This could indicate a misconfiguration in the network settings or that the ResourceManager is not running."
        }
    },
    {
        "filename": "YARN-2273.json",
        "creation_time": "2014-07-10T18:38:53.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FairScheduler during Node Resource Comparison",
            "Description": "A NullPointerException occurs in the FairScheduler's NodeAvailableResourceComparator when attempting to compare node resources during the continuous scheduling process. This issue arises when the nodes collection contains a null reference, leading to a failure in resource comparison.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)",
                "at java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)",
                "at java.util.TimSort.sort(TimSort.java:203)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager with a configuration that includes nodes.",
                "2. Ensure that at least one node in the nodes collection is null or has not been properly initialized.",
                "3. Trigger the continuous scheduling process by allowing the ResourceManager to run.",
                "4. Observe the logs for a NullPointerException in the FairScheduler."
            ],
            "ExpectedBehavior": "The FairScheduler should successfully compare the available resources of all nodes and schedule containers without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when the FairScheduler attempts to compare resources of nodes, causing the scheduling process to fail.",
            "AdditionalDetails": "The issue likely stems from the 'compare' method in the NodeAvailableResourceComparator, which does not handle null nodes properly. The method attempts to access resources of nodes without checking if the node references are null, leading to the exception."
        }
    },
    {
        "filename": "YARN-2834.json",
        "creation_time": "2014-11-09T06:07:01.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CapacityScheduler during Application Attempt Recovery",
            "Description": "A NullPointerException occurs in the CapacityScheduler class when attempting to add an application attempt during the recovery process. This issue arises when the application attempt ID is null, leading to a failure in the addApplicationAttempt method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1005)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:843)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:701)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1091)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1226)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service.",
                "Trigger the recovery process for an application that has failed attempts.",
                "Observe the logs for a NullPointerException in the CapacityScheduler."
            ],
            "ExpectedBehavior": "The CapacityScheduler should successfully add the application attempt without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the addApplicationAttempt method of the CapacityScheduler, indicating that the application attempt ID is null.",
            "AdditionalDetails": "The issue likely stems from the application attempt ID not being properly initialized or passed during the recovery process. Further investigation into the RMAppAttempt and RMApp classes may be necessary to ensure that valid IDs are being used."
        }
    },
    {
        "filename": "YARN-370.json",
        "creation_time": "2013-02-01T04:02:58.000+0000",
        "bug_report": {
            "Title": "Unauthorized Request to Start Container Due to Insufficient Resources",
            "Description": "The application fails to launch due to an unauthorized request error when attempting to start a container. The error indicates that the requested resources (memory and vCores) do not match the expected values, leading to a YarnRemoteException.",
            "StackTrace": [
                "Error launching appattempt_1359688216672_0001_000001. Got exception: RemoteTrace: at LocalTrace: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: RemoteTrace: at LocalTrace: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unauthorized request to start container. Expected resource <memory:2048, vCores:1> but found <memory:1536, vCores:1>",
                "at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:383)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:400)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:68)",
                "at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1735)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1731)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1729)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)",
                "at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)",
                "at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:123)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:109)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:255)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "Attempt to launch an application with a container request for memory:2048 and vCores:1.",
                "Ensure that the available resources on the node manager are set to memory:1536 and vCores:1.",
                "Observe the error message during the application launch."
            ],
            "ExpectedBehavior": "The application should successfully launch the container with the requested resources if they are available.",
            "ObservedBehavior": "The application fails to launch due to an unauthorized request error, indicating a mismatch in the expected and provided resources.",
            "AdditionalDetails": "The issue appears to stem from a configuration mismatch between the requested resources and the available resources on the node manager. Adjusting the resource allocation settings may resolve the issue."
        }
    },
    {
        "filename": "YARN-3675.json",
        "creation_time": "2015-05-18T22:38:39.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FairScheduler during Container Completion",
            "Description": "A NullPointerException occurs in the FairScheduler's unreserve method when attempting to complete a container. This issue arises when the reserved container is null, leading to a failure in the unreserve process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserve(FSAppAttempt.java:469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:815)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:763)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the FairScheduler.",
                "2. Ensure that the application attempts to complete a container that has been reserved.",
                "3. Trigger the completion of the container while the reserved container is null."
            ],
            "ExpectedBehavior": "The FairScheduler should successfully unreserve the container and complete the application attempt without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the FairScheduler attempts to unreserve a container that is null, leading to a failure in processing the completed container.",
            "AdditionalDetails": "The issue seems to stem from the unreserve method in FSAppAttempt, where it attempts to access the reserved container without checking if it is null. This can occur if the node's reserved container is not properly set before the unreserve call."
        }
    },
    {
        "filename": "YARN-4763.json",
        "creation_time": "2016-03-04T10:03:56.000+0000",
        "bug_report": {
            "Title": "NullPointerException in RMAppsBlock.renderData",
            "Description": "A NullPointerException occurs in the RMAppsBlock.renderData method when attempting to render application data. This issue arises when the application reports are not properly initialized or are null, leading to a failure in rendering the application table.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)",
                "    at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)",
                "    at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "    at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "    at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "    at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)",
                "    at org.apache.hadoop.yarn.webapp.hamlet.Hamlet._(Hamlet.java:30354)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics.render(AppsBlockWithMetrics.java:30)",
                "    at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "    at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "    at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "    at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "    at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "    at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:848)",
                "    at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "    at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "    at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)",
                "    at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:156)",
                "    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "    at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager.",
                "2. Access the ResourceManager web UI.",
                "3. Navigate to the applications page.",
                "4. Observe the error in the application table rendering."
            ],
            "ExpectedBehavior": "The application table should render correctly with all application reports displayed without any errors.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the application table from rendering and resulting in an error page.",
            "AdditionalDetails": "The issue likely stems from the 'appReports' variable in the renderData method being null or not properly initialized. Additional checks should be implemented to ensure that 'appReports' is valid before attempting to iterate over it."
        }
    },
    {
        "filename": "YARN-8202.json",
        "creation_time": "2018-04-24T15:52:00.000+0000",
        "bug_report": {
            "Title": "InvalidResourceRequestException due to Resource Allocation Limits",
            "Description": "An InvalidResourceRequestException is thrown when a resource request exceeds the maximum allowed allocation for a resource type. The request for 'resource1' is invalid as it exceeds the maximum allocation defined by the scheduler based on the registered NodeManagers.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation. Requested resource=<memory:200, vCores:1, resource1: 500M>, maximum allowed allocation=<memory:6144, vCores:8, resource1: 5G>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:8192, resource1: 9223372036854775807G>",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)",
                "at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:433)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)"
            ],
            "StepsToReproduce": [
                "Submit a resource request with 'resource1' set to 500M.",
                "Ensure that the maximum allowed allocation for 'resource1' is set to 5G.",
                "Observe the response from the resource manager."
            ],
            "ExpectedBehavior": "The resource request should be accepted if it is within the defined limits.",
            "ObservedBehavior": "An InvalidResourceRequestException is thrown indicating that the requested resource exceeds the maximum allowed allocation.",
            "AdditionalDetails": "The maximum allowed allocation is dynamically calculated based on the resources of registered NodeManagers, which may differ from the configured maximum allocation. Ensure that the NodeManagers are properly configured to avoid such exceptions."
        }
    },
    {
        "filename": "YARN-7118.json",
        "creation_time": "2017-08-29T12:04:01.000+0000",
        "bug_report": {
            "Title": "NullPointerException in WebServices.getApps()",
            "Description": "A NullPointerException is thrown in the WebServices.getApps() method when attempting to retrieve application data. This issue occurs when the method is called without the necessary context or data, leading to a failure in the application history service.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)"
            ],
            "StepsToReproduce": [
                "Invoke the WebServices.getApps() method without the required application context.",
                "Ensure that the application history service is active and accessible.",
                "Monitor the logs for any NullPointerException being thrown."
            ],
            "ExpectedBehavior": "The getApps() method should return a list of applications without throwing an exception, even if no applications are currently available.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the method is attempting to access a null reference.",
            "AdditionalDetails": "The source code for the methods involved is not provided, but it is likely that the getApps() method in WebServices is not properly handling cases where the application data is not initialized or is missing."
        }
    },
    {
        "filename": "YARN-4743.json",
        "creation_time": "2016-02-27T09:12:28.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException in FairScheduler due to Comparator Violation",
            "Description": "An IllegalArgumentException is thrown during the scheduling process in the FairScheduler when attempting to sort child queues. The exception indicates that the comparison method used in the sorting violates its general contract, which can lead to unpredictable behavior in the scheduling logic.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:868)",
                "at java.util.TimSort.mergeAt(TimSort.java:485)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:410)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Trigger a scheduling event in the FairScheduler.",
                "2. Ensure that the child queues have a comparator that violates the comparison contract.",
                "3. Observe the IllegalArgumentException being thrown during the sorting process."
            ],
            "ExpectedBehavior": "The FairScheduler should successfully sort the child queues without throwing an exception, allowing for proper scheduling of resources.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the comparison method violates its general contract, preventing the scheduling process from completing successfully.",
            "AdditionalDetails": "The issue likely stems from the comparator used in the 'policy.getComparator()' method, which needs to be reviewed to ensure it adheres to the requirements of the Comparator interface."
        }
    },
    {
        "filename": "YARN-2414.json",
        "creation_time": "2014-08-12T23:48:48.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AppBlock.render() Method",
            "Description": "A NullPointerException is thrown in the AppBlock.render() method when attempting to render an application block in the YARN ResourceManager web application. This issue occurs during the processing of a web request, leading to a failure in rendering the application view.",
            "StackTrace": [
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:84)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:460)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1191)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:55)"
            ],
            "StepsToReproduce": [
                "Send a request to the YARN ResourceManager web application that triggers the rendering of an application block.",
                "Ensure that the application block being rendered has a null reference that leads to the NullPointerException."
            ],
            "ExpectedBehavior": "The application block should render successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application block rendering to fail.",
            "AdditionalDetails": "The issue appears to originate from the AppBlock.render() method, specifically at line 116. It is likely that a required object is not initialized or is null when the render method is called."
        }
    },
    {
        "filename": "YARN-3878.json",
        "creation_time": "2015-07-02T00:20:59.000+0000",
        "bug_report": {
            "Title": "InterruptedException during Application Attempt State Update",
            "Description": "An InterruptedException is thrown when attempting to update the application attempt state in the ResourceManager. This occurs during the handling of events in the AsyncDispatcher, specifically when trying to put an event into a LinkedBlockingQueue.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.access$3300(RMAppAttemptImpl.java:109)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1619)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:838)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager in a Hadoop YARN environment.",
                "2. Submit an application that triggers multiple events.",
                "3. Monitor the logs for any InterruptedException during the event handling."
            ],
            "ExpectedBehavior": "The application attempt state should be updated without throwing an InterruptedException.",
            "ObservedBehavior": "An InterruptedException is thrown, causing the application attempt state update to fail.",
            "AdditionalDetails": "The issue seems to stem from the handling of events in the AsyncDispatcher, particularly when the event queue is full, leading to potential interruptions during the locking mechanism."
        }
    },
    {
        "filename": "YARN-6683.json",
        "creation_time": "2017-06-02T00:29:13.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException on COLLECTOR_UPDATE Event",
            "Description": "An InvalidStateTransitionException is thrown when attempting to process a COLLECTOR_UPDATE event while the application is in the KILLED state. This indicates that the event is not valid for the current state of the application, leading to a failure in handling the event.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the YARN ResourceManager.",
                "2. Transition the application to the KILLED state.",
                "3. Attempt to send a COLLECTOR_UPDATE event to the application."
            ],
            "ExpectedBehavior": "The application should handle the COLLECTOR_UPDATE event appropriately, or ignore it without throwing an exception.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the COLLECTOR_UPDATE event cannot be processed in the KILLED state.",
            "AdditionalDetails": "The exception occurs in the doTransition method of the StateMachineFactory, which is responsible for managing state transitions. The handle method in RMAppImpl attempts to process the event but fails due to the invalid state transition."
        }
    },
    {
        "filename": "YARN-2910.json",
        "creation_time": "2014-11-27T06:19:00.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in FairScheduler during Resource Allocation",
            "Description": "A ConcurrentModificationException is thrown when the FairScheduler attempts to allocate resources. This occurs due to concurrent modifications of the lists holding runnable and non-runnable applications while iterating over them in the getResourceUsage() method.",
            "StackTrace": [
                "java.util.ConcurrentModificationException: java.util.ConcurrentModificationException",
                "at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)",
                "at java.util.ArrayList$Itr.next(ArrayList.java:831)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager with multiple applications.",
                "2. Simultaneously submit and cancel applications to create concurrent modifications to the runnableApps and nonRunnableApps lists.",
                "3. Monitor the logs for any ConcurrentModificationException during resource allocation."
            ],
            "ExpectedBehavior": "The FairScheduler should allocate resources without throwing any exceptions, even when applications are being added or removed concurrently.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, causing the resource allocation process to fail.",
            "AdditionalDetails": "The issue arises from the lack of synchronization when iterating over the lists of runnable and non-runnable applications in the getResourceUsage() method. The getHeadroom() method is also synchronized, but it does not prevent modifications to the lists during iteration."
        }
    },
    {
        "filename": "YARN-192.json",
        "creation_time": "2012-11-01T05:00:41.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FSSchedulerApp.unreserve Method",
            "Description": "A NullPointerException occurs in the FSSchedulerApp.unreserve method when attempting to unreserve resources for an application. This issue arises when the application or its associated resources are not properly initialized or are null, leading to a failure in resource management within the FairScheduler.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.unreserve(FSSchedulerApp.java:356)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.unreserve(AppSchedulable.java:214)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:266)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:330)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueSchedulable.assignContainer(FSQueueSchedulable.java:161)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:759)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:836)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:329)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with the FairScheduler enabled.",
                "2. Submit an application that requires resource allocation.",
                "3. Trigger a scenario where the application attempts to unreserve resources (e.g., by removing nodes or changing resource availability).",
                "4. Observe the logs for a NullPointerException in the FSSchedulerApp.unreserve method."
            ],
            "ExpectedBehavior": "The system should successfully unreserve resources for the application without throwing a NullPointerException, allowing for proper resource management and scheduling.",
            "ObservedBehavior": "A NullPointerException is thrown in the FSSchedulerApp.unreserve method, indicating that an expected object (likely the application or its resources) is null, leading to a failure in the resource unreservation process.",
            "AdditionalDetails": "The unreserve method is expected to handle the unreservation of resources for a given application. The stack trace indicates that the issue may stem from the application or its associated resources not being properly initialized before the unreserve call is made. Further investigation into the application lifecycle and resource management logic is needed to identify the root cause."
        }
    },
    {
        "filename": "YARN-4581.json",
        "creation_time": "2016-01-12T03:37:40.000+0000",
        "bug_report": {
            "Title": "IOException and OutOfMemoryError during Application History Logging",
            "Description": "The application encounters an IOException indicating that the output file is not at zero offset, followed by an OutOfMemoryError when attempting to create new native threads. This issue arises during the application history logging process in the Hadoop YARN framework.",
            "StackTrace": [
                "java.io.IOException: Output file not at zero offset.",
                "    at org.apache.hadoop.io.file.tfile.BCFile$Writer.<init>(BCFile.java:288)",
                "    at org.apache.hadoop.io.file.tfile.TFile$Writer.<init>(TFile.java:288)",
                "    at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:728)",
                "    at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "    at java.lang.Thread.run(Thread.java:745)",
                "java.lang.OutOfMemoryError: unable to create new native thread",
                "    at java.lang.Thread.start0(Native Method)",
                "    at java.lang.Thread.start(Thread.java:714)",
                "    at org.apache.hadoop.hdfs.DFSOutputStream.start(DFSOutputStream.java:2033)",
                "    at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForAppend(DFSOutputStream.java:1652)",
                "    at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1573)",
                "    at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1603)",
                "    at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1591)",
                "    at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:328)",
                "    at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:324)",
                "    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "    at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:324)",
                "    at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)",
                "    at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:723)",
                "    at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "    at java.lang.Thread.run(Thread.java:745)",
                "java.lang.Thread.State: TIMED_WAITING (on object monitor)",
                "    at java.lang.Object.wait(Native Method)",
                "    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:502)",
                "    - locked <0x0000000745f88b98> (a java.util.LinkedList)"
            ],
            "StepsToReproduce": [
                "Start an application that utilizes the Hadoop YARN framework.",
                "Ensure that the application generates a significant amount of history logging data.",
                "Monitor the application for any IOException or OutOfMemoryError messages."
            ],
            "ExpectedBehavior": "The application should log history events without encountering IOException or OutOfMemoryError.",
            "ObservedBehavior": "The application fails to log history events due to an IOException indicating the output file is not at zero offset, followed by an OutOfMemoryError when trying to create new threads.",
            "AdditionalDetails": "The issue may be related to the configuration of the Hadoop environment, particularly the limits on the number of threads and memory allocation. Further investigation into the application history logging mechanism and resource management is recommended."
        }
    },
    {
        "filename": "YARN-7786.json",
        "creation_time": "2018-01-22T14:29:46.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AMLauncher during Token Setup",
            "Description": "A NullPointerException occurs in the AMLauncher class when attempting to set up tokens for the Application Master. This issue arises during the launch process of the Application Master, specifically in the setupTokens method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                " at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)",
                " at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)",
                " at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)",
                " at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)",
                " at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                " at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                " at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Application Master using the AMLauncher.",
                "2. Ensure that the necessary context and container IDs are provided.",
                "3. Observe the logs for any NullPointerException during the token setup phase."
            ],
            "ExpectedBehavior": "The Application Master should successfully set up tokens without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a required object is not initialized or is null during the token setup process.",
            "AdditionalDetails": "The issue likely stems from the setupTokens method being called with a null reference. Further investigation is needed to determine which object is null at line 205 in the setupTokens method."
        }
    },
    {
        "filename": "YARN-8035.json",
        "creation_time": "2018-03-16T12:02:04.000+0000",
        "bug_report": {
            "Title": "MetricsException: Tag ContainerPid already exists",
            "Description": "The application encounters a MetricsException indicating that a tag with the name 'ContainerPid' already exists when attempting to record process IDs for containers. This issue arises during the monitoring of container resources in the YARN NodeManager.",
            "StackTrace": [
                "org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448)"
            ],
            "StepsToReproduce": [
                "Start the YARN NodeManager with multiple containers.",
                "Ensure that the containers are attempting to register their process IDs.",
                "Monitor the logs for the MetricsException related to the 'ContainerPid' tag."
            ],
            "ExpectedBehavior": "The system should successfully record the process IDs for each container without throwing an exception about existing tags.",
            "ObservedBehavior": "The system throws a MetricsException indicating that the tag 'ContainerPid' already exists, preventing the recording of process IDs.",
            "AdditionalDetails": "The issue likely arises from the fact that the 'tag' method in the MetricsRegistry checks for existing tags before adding a new one. If multiple containers attempt to register their process IDs simultaneously, it can lead to this exception being thrown. The 'recordProcessId' method in the ContainerMetrics class calls the 'tag' method, which leads to this exception when the tag already exists."
        }
    },
    {
        "filename": "YARN-4152.json",
        "creation_time": "2015-09-12T15:02:22.000+0000",
        "bug_report": {
            "Title": "NullPointerException in LogAggregationService during Container Stop",
            "Description": "A NullPointerException occurs in the LogAggregationService when attempting to stop a container. This issue arises when the log aggregation for the specified container is not initialized, leading to a failure in the stopContainer method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start an application that utilizes the YARN resource manager.",
                "2. Ensure that a container is created and then finished.",
                "3. Trigger the log aggregation process for the finished container.",
                "4. Observe the logs for a NullPointerException in the LogAggregationService."
            ],
            "ExpectedBehavior": "The log aggregation service should successfully stop the container and aggregate logs without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the log aggregation service attempts to stop a container that has not been initialized, resulting in a failure to aggregate logs.",
            "AdditionalDetails": "The issue seems to stem from the stopContainer method where it checks for the AppLogAggregator associated with the container's application ID. If the aggregator is null, a warning is logged, but the method continues to execute, leading to a NullPointerException when trying to access methods on the null aggregator."
        }
    },
    {
        "filename": "YARN-3697.json",
        "creation_time": "2015-05-21T18:05:38.000+0000",
        "bug_report": {
            "Title": "InterruptedException during Container Scheduling in YARN",
            "Description": "An InterruptedException is thrown during the container scheduling process in the YARN ResourceManager. This occurs when the system attempts to acquire a lock on a blocking queue, but the thread is interrupted before it can proceed. This can lead to unexpected behavior in resource allocation and scheduling.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:249)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)"
            ],
            "StepsToReproduce": [
                "Start the YARN ResourceManager.",
                "Submit a job that requires container allocation.",
                "Interrupt the thread responsible for scheduling containers."
            ],
            "ExpectedBehavior": "The container should be allocated without throwing an InterruptedException.",
            "ObservedBehavior": "An InterruptedException is thrown, causing potential issues in resource allocation and scheduling.",
            "AdditionalDetails": "The issue seems to stem from the handling of events in the AsyncDispatcher, particularly when the thread is interrupted while trying to acquire a lock on a blocking queue."
        }
    },
    {
        "filename": "YARN-2340.json",
        "creation_time": "2014-07-23T15:18:38.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CapacityScheduler when adding application attempt",
            "Description": "A NullPointerException occurs in the CapacityScheduler class when attempting to add an application attempt. This issue arises from a null reference being accessed in the addApplicationAttempt method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                " at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:568)",
                " at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:916)",
                " at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:101)",
                " at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:602)",
                " at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Trigger an event of type APP_ATTEMPT_ADDED in the CapacityScheduler.",
                "Ensure that the ApplicationAttemptId passed to the addApplicationAttempt method is null or contains a null reference."
            ],
            "ExpectedBehavior": "The CapacityScheduler should successfully add an application attempt without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application attempt addition process to fail.",
            "AdditionalDetails": "The issue likely stems from the addApplicationAttempt method not properly handling null values for the ApplicationAttemptId or related parameters. Further investigation is needed to ensure that all necessary checks are in place before accessing these values."
        }
    },
    {
        "filename": "YARN-8022.json",
        "creation_time": "2018-03-10T19:29:27.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AppBlock Rendering",
            "Description": "A NullPointerException occurs in the AppBlock class while rendering the application block. This issue arises when the application attempts to access a null reference during the rendering process, leading to a failure in the web application.",
            "StackTrace": [
                "java.lang.NullPointerException",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:283)",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:280)",
                " at java.security.AccessController.doPrivileged(Native Method)",
                " at javax.security.auth.Subject.doAs(Subject.java:422)",
                " at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock.render(AppBlock.java:279)",
                " at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock.render(RMAppBlock.java:71)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                " at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                " at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)",
                " at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)",
                " at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                " at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                " at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:54)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager.",
                "2. Attempt to access the application block via the web interface.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The application block should render correctly without throwing any exceptions, displaying the relevant application information.",
            "ObservedBehavior": "A NullPointerException is thrown during the rendering process, causing the application block to fail to display.",
            "AdditionalDetails": "The issue likely stems from a null reference in the AppBlock class, specifically in the run method at line 283. The context() method may not be properly initialized, leading to a null context when attempting to render the application block."
        }
    },
    {
        "filename": "YARN-3793.json",
        "creation_time": "2015-06-10T20:52:38.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FileContext.delete Method",
            "Description": "A NullPointerException occurs in the FileContext class when attempting to delete a file. The exception is thrown during the execution of the delete method, which is called by the DefaultContainerExecutor's deleteAsUser method. This indicates that a null reference is being accessed, likely due to an improperly initialized or missing Path object.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "at org.apache.hadoop.fs.FileContext.delete(FileContext.java:755)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)",
                "at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)"
            ],
            "StepsToReproduce": [
                "1. Attempt to delete a file using the delete method in FileContext.",
                "2. Ensure that the Path object passed to the delete method is null or improperly initialized.",
                "3. Observe the NullPointerException being thrown."
            ],
            "ExpectedBehavior": "The delete method should successfully delete the specified file or directory without throwing an exception, even if the Path object is invalid.",
            "ObservedBehavior": "A NullPointerException is thrown when the delete method is called with a null or invalid Path object, causing the deletion process to fail.",
            "AdditionalDetails": "The issue likely arises from the fixRelativePart method, which is called within the delete method. If the Path object is null, it may lead to a failure in resolving the relative path, resulting in a NullPointerException."
        }
    },
    {
        "filename": "YARN-6102.json",
        "creation_time": "2017-01-17T09:36:29.000+0000",
        "bug_report": {
            "Title": "No Handler Registered for RMNodeEventType in AsyncDispatcher",
            "Description": "An exception is thrown in the AsyncDispatcher when attempting to dispatch an event of type RMNodeEventType. The system fails to find a registered handler for this event type, leading to a runtime exception.",
            "StackTrace": [
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Trigger an event of type RMNodeEventType in the system.",
                "Ensure that the AsyncDispatcher is active and processing events.",
                "Observe the logs for the exception indicating no handler is registered."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should successfully dispatch the RMNodeEventType event to the appropriate handler without throwing an exception.",
            "ObservedBehavior": "The AsyncDispatcher throws an exception indicating that there is no registered handler for the RMNodeEventType, resulting in a failure to process the event.",
            "AdditionalDetails": "The dispatch method in AsyncDispatcher checks for a handler in the eventDispatchers map. If no handler is found for the event type, it throws an exception. This indicates that the system lacks a proper registration for handling RMNodeEventType events."
        }
    },
    {
        "filename": "YARN-8409.json",
        "creation_time": "2018-06-08T20:36:32.000+0000",
        "bug_report": {
            "Title": "Connection Refused Leading to NullPointerException in ActiveStandbyElector",
            "Description": "The application encounters a java.net.ConnectException indicating that the connection was refused when attempting to connect to a ZooKeeper instance. This failure leads to a subsequent NullPointerException in the ActiveStandbyElector class, specifically during the execution of the zkDoWithRetries method. The root cause appears to be the failure to establish a connection to the ZooKeeper service, which is critical for the ActiveStandbyElector's operation.",
            "StackTrace": [
                "java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)",
                "at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the ZooKeeper service is not running or is unreachable.",
                "2. Start the ResourceManager service.",
                "3. Observe the logs for the connection refused error followed by the NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully connect to the ZooKeeper service and initialize without errors.",
            "ObservedBehavior": "The ResourceManager fails to connect to the ZooKeeper service, resulting in a Connection refused error, which subsequently leads to a NullPointerException in the ActiveStandbyElector.",
            "AdditionalDetails": "The ActiveStandbyElector relies on ZooKeeper for leader election and coordination. The failure to connect to ZooKeeper indicates a potential misconfiguration or an issue with the ZooKeeper service itself. Further investigation into the ZooKeeper logs and configuration may be necessary."
        }
    },
    {
        "filename": "YARN-8223.json",
        "creation_time": "2018-04-27T11:49:02.000+0000",
        "bug_report": {
            "Title": "ClassNotFoundException for Auxiliary Service Class",
            "Description": "The application fails to start due to a ClassNotFoundException for the class 'org.apache.auxtest.AuxServiceFromLocal'. This issue arises during the initialization of the NodeManager in a Hadoop YARN environment, specifically when attempting to load auxiliary services.",
            "StackTrace": [
                "java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:381)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:424)",
                "at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:357)",
                "at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)",
                "at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:348)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)"
            ],
            "StepsToReproduce": [
                "1. Attempt to start the NodeManager in a Hadoop YARN environment.",
                "2. Ensure that the configuration is set to use the auxiliary service 'org.apache.auxtest.AuxServiceFromLocal'.",
                "3. Observe the logs for the ClassNotFoundException."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and initialize all configured auxiliary services without any ClassNotFoundException.",
            "ObservedBehavior": "The NodeManager fails to start due to a ClassNotFoundException for the class 'org.apache.auxtest.AuxServiceFromLocal', indicating that the class is not available in the classpath.",
            "AdditionalDetails": "Ensure that the class 'org.apache.auxtest.AuxServiceFromLocal' is included in the application's classpath. This may involve checking the build configuration or the deployment package to confirm that the necessary libraries are present."
        }
    },
    {
        "filename": "YARN-8331.json",
        "creation_time": "2018-05-21T05:19:35.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException when handling CONTAINER_LAUNCHED event",
            "Description": "An InvalidStateTransitionException is thrown when the system attempts to handle a CONTAINER_LAUNCHED event while in the DONE state. This indicates that the state machine is not correctly managing state transitions for container events.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start a container and transition it to the DONE state.",
                "2. Attempt to send a CONTAINER_LAUNCHED event to the container after it has reached the DONE state."
            ],
            "ExpectedBehavior": "The system should handle the CONTAINER_LAUNCHED event appropriately without throwing an exception, regardless of the current state of the container.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the CONTAINER_LAUNCHED event cannot be processed in the DONE state.",
            "AdditionalDetails": "The issue arises from the state machine's inability to handle certain events when the container is in a terminal state (DONE). The state machine should be reviewed to ensure that it can gracefully handle or ignore events that are not valid in the current state."
        }
    },
    {
        "filename": "YARN-2931.json",
        "creation_time": "2014-12-08T21:09:13.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException when creating directory in Yarn",
            "Description": "A FileNotFoundException is thrown when attempting to create a directory in the Yarn file system. The error indicates that the specified file path does not exist, which prevents the directory creation process from completing successfully.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:720)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)",
                "at org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Attempt to create a directory at the path /data/yarn/nm/filecache using the Yarn file system.",
                "Ensure that the parent directory /data/yarn/nm does not exist or is not accessible."
            ],
            "ExpectedBehavior": "The directory should be created successfully if the parent directory exists and is accessible.",
            "ObservedBehavior": "A FileNotFoundException is thrown indicating that the file /data/yarn/nm/filecache does not exist, preventing the directory creation.",
            "AdditionalDetails": "The method 'deprecatedGetFileStatus' checks if the file exists and throws a FileNotFoundException if it does not. This indicates that the directory creation process is dependent on the existence of the parent directory."
        }
    },
    {
        "filename": "YARN-6837.json",
        "creation_time": "2017-07-18T11:17:55.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ResourceSet.addResources",
            "Description": "A NullPointerException occurs in the ResourceSet.addResources method when attempting to add resources to a container. This issue arises during the resource request transition of a container, indicating that a required resource or object is not properly initialized.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.addResources(ResourceSet.java:84)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:868)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:819)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1684)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:96)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1418)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1411)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN NodeManager.",
                "2. Submit a job that requests resources for a container.",
                "3. Monitor the logs for the NodeManager to observe the NullPointerException."
            ],
            "ExpectedBehavior": "The system should successfully add resources to the container without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a required resource or object is null when attempting to add resources to the container.",
            "AdditionalDetails": "The issue likely stems from the ResourceSet.addResources method, which is called during the resource request transition of a container. It is essential to ensure that all required resources are properly initialized before this method is invoked."
        }
    },
    {
        "filename": "YARN-4762.json",
        "creation_time": "2016-03-04T02:24:47.000+0000",
        "bug_report": {
            "Title": "Yarn NodeManager Fails to Initialize Container Executor",
            "Description": "The Yarn NodeManager fails to initialize the container executor due to an IOException indicating that the Linux container runtime could not be initialized. This issue prevents the NodeManager from starting properly, leading to a YarnRuntimeException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)",
                "Caused by: java.io.IOException: Failed to initialize linux container runtime(s)!",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)"
            ],
            "StepsToReproduce": [
                "Start the Yarn NodeManager service.",
                "Ensure that the configuration is set to use the Linux container executor.",
                "Check for the availability of the required Linux container runtime."
            ],
            "ExpectedBehavior": "The Yarn NodeManager should initialize the container executor successfully and start without errors.",
            "ObservedBehavior": "The Yarn NodeManager fails to start, throwing a YarnRuntimeException due to an IOException related to the Linux container runtime initialization.",
            "AdditionalDetails": "The issue may be related to the configuration of the Linux container runtime or the absence of necessary permissions or executables required for its initialization. The relevant method in the source code is 'LinuxContainerExecutor.init()', which is responsible for initializing the container executor."
        }
    },
    {
        "filename": "YARN-2823.json",
        "creation_time": "2014-11-06T21:38:47.000+0000",
        "bug_report": {
            "Title": "NullPointerException in SchedulerApplicationAttempt during Application Attempt Handling",
            "Description": "A NullPointerException occurs in the method transferStateFromPreviousAttempt of the SchedulerApplicationAttempt class when handling application attempts in the CapacityScheduler. This issue arises when the application attempt is expected to transfer state from a previous attempt, but the previous attempt is null, leading to a failure in the state transfer process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Submit an application attempt to the CapacityScheduler.",
                "2. Ensure that the application attempt is configured to transfer state from a previous attempt.",
                "3. Trigger the event that leads to the handling of the application attempt."
            ],
            "ExpectedBehavior": "The application attempt should successfully transfer its state from the previous attempt without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the previous application attempt is null, which prevents the state transfer from occurring.",
            "AdditionalDetails": "The issue likely stems from the addApplicationAttempt method in the CapacityScheduler, which calls transferStateFromPreviousAttempt. If the previous attempt is not properly initialized or is null, it leads to this exception. Further investigation is needed to ensure that application attempts are correctly initialized before being added."
        }
    },
    {
        "filename": "YARN-5098.json",
        "creation_time": "2016-05-17T00:43:08.000+0000",
        "bug_report": {
            "Title": "Hadoop RemoteException: Invalid HDFS Delegation Token",
            "Description": "The application encounters a RemoteException indicating that a specific HDFS delegation token cannot be found in the cache. This issue arises during the SASL connection setup process, which is critical for secure communication between the client and the HDFS server.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:583)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:398)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:752)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:748)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:747)",
                "at org.apache.hadoop.ipc.Client$Connection.access$3100(Client.java:398)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1597)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1439)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1386)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:240)",
                "at com.sun.proxy.$Proxy83.getServerDefaults(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy84.getServerDefaults(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:1018)",
                "at org.apache.hadoop.fs.Hdfs.getServerDefaults(Hdfs.java:156)",
                "at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:550)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:687)"
            ],
            "StepsToReproduce": [
                "1. Attempt to connect to the HDFS using a client that requires a valid HDFS delegation token.",
                "2. Ensure that the token has expired or is not present in the cache.",
                "3. Observe the exception thrown during the SASL connection setup."
            ],
            "ExpectedBehavior": "The client should successfully establish a SASL connection with the HDFS server using a valid delegation token.",
            "ObservedBehavior": "The client fails to connect, throwing a RemoteException indicating that the specified HDFS delegation token cannot be found in the cache.",
            "AdditionalDetails": "This issue may occur if the token has expired or if there is a misconfiguration in the token management system. It is essential to ensure that the token is valid and properly cached before attempting to establish a connection."
        }
    },
    {
        "filename": "YARN-3971.json",
        "creation_time": "2015-07-24T10:17:05.000+0000",
        "bug_report": {
            "Title": "IOException when removing node label due to active queue dependency",
            "Description": "An IOException is thrown when attempting to remove a node label that is currently in use by an active queue. The error message indicates that the label cannot be removed until it is detached from the queue. This issue arises during the initialization of the node label store in the ResourceManager.",
            "StackTrace": [
                "java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue(RMNodeLabelsManager.java:104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels(RMNodeLabelsManager.java:118)",
                "at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.recover(FileSystemNodeLabelsStore.java:221)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:232)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:245)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:964)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1005)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1001)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1666)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1001)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:832)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:422)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with a queue that has a node label assigned.",
                "2. Attempt to remove the node label that is currently assigned to the queue.",
                "3. Observe the IOException thrown during the initialization of the node label store."
            ],
            "ExpectedBehavior": "The node label should be removed successfully if it is not in use by any queue.",
            "ObservedBehavior": "An IOException is thrown indicating that the label cannot be removed because it is still in use by an active queue.",
            "AdditionalDetails": "The issue occurs in the 'checkRemoveFromClusterNodeLabelsOfQueue' method, which checks if the label can be removed from the queue before proceeding with the removal. The current implementation does not handle the case where a label is still in use, leading to the exception."
        }
    },
    {
        "filename": "YARN-6948.json",
        "creation_time": "2017-08-04T08:23:46.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException when adding attempt in FINAL_SAVING state",
            "Description": "An InvalidStateTransitionException is thrown when an attempt is added to an application that is in the FINAL_SAVING state. This indicates that the application is not in a valid state to handle the ATTEMPT_ADDED event.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the ResourceManager.",
                "2. Allow the application to reach the FINAL_SAVING state.",
                "3. Attempt to add a new application attempt while the application is in the FINAL_SAVING state."
            ],
            "ExpectedBehavior": "The application should be able to handle the ATTEMPT_ADDED event without throwing an exception, regardless of its current state.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the application cannot handle the ATTEMPT_ADDED event while in the FINAL_SAVING state.",
            "AdditionalDetails": "The state machine for the application does not allow transitions from FINAL_SAVING to handle ATTEMPT_ADDED events. This may require a review of the state transition logic to ensure that the application can appropriately handle events in all states."
        }
    },
    {
        "filename": "YARN-1409.json",
        "creation_time": "2013-11-13T11:25:56.000+0000",
        "bug_report": {
            "Title": "RejectedExecutionException in LogHandler due to ScheduledThreadPoolExecutor Shutdown",
            "Description": "A RejectedExecutionException is thrown when attempting to schedule a log deletion task in the NonAggregatingLogHandler. This occurs because the ScheduledThreadPoolExecutor is shutting down and cannot accept new tasks.",
            "StackTrace": [
                "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Start the application that utilizes the NonAggregatingLogHandler.",
                "2. Trigger an event that leads to the application finishing, which should invoke the log deletion scheduling.",
                "3. Ensure that the ScheduledThreadPoolExecutor is in the process of shutting down when the log deletion task is scheduled."
            ],
            "ExpectedBehavior": "The log deletion task should be scheduled successfully without any exceptions, allowing logs to be deleted after the specified delay.",
            "ObservedBehavior": "A RejectedExecutionException is thrown, indicating that the task cannot be scheduled because the ScheduledThreadPoolExecutor is shutting down.",
            "AdditionalDetails": "The issue arises in the handle method of NonAggregatingLogHandler when it attempts to schedule a LogDeleterRunnable after an application finishes. The executor's shutdown state prevents new tasks from being accepted, leading to the exception."
        }
    },
    {
        "filename": "YARN-5545.json",
        "creation_time": "2016-08-21T12:57:35.000+0000",
        "bug_report": {
            "Title": "YARN Application Submission Failure Due to Access Control Exception",
            "Description": "An IOException occurs when attempting to submit a job to YARN, indicating that the submission cannot proceed due to an AccessControlException. The error message specifies that the queue 'root.default' already has 0 applications and cannot accept the submission of the specified application.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)",
                "at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:301)"
            ],
            "StepsToReproduce": [
                "Attempt to submit a job to YARN using the 'submitJob' method.",
                "Ensure that the queue 'root.default' has 0 applications at the time of submission."
            ],
            "ExpectedBehavior": "The job should be submitted successfully to the YARN queue without any exceptions.",
            "ObservedBehavior": "An IOException is thrown indicating that the queue 'root.default' cannot accept the submission due to having 0 applications.",
            "AdditionalDetails": "The issue seems to stem from the YARN queue configuration, which may not allow submissions when it is empty. Further investigation into the queue settings and permissions may be required."
        }
    },
    {
        "filename": "YARN-301.json",
        "creation_time": "2013-01-01T05:40:18.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in FairScheduler during Container Assignment",
            "Description": "A ConcurrentModificationException is thrown in the FairScheduler when attempting to assign a container to an application. This occurs due to concurrent modifications of the appScheds list while it is being iterated over, leading to instability in the scheduling process.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)",
                "at java.util.TreeMap$KeyIterator.next(TreeMap.java:1154)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:181)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:780)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:842)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:340)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with FairScheduler enabled.",
                "2. Submit multiple applications that require container resources.",
                "3. Trigger node updates that may lead to concurrent modifications of the appScheds list."
            ],
            "ExpectedBehavior": "The FairScheduler should assign containers to applications without throwing exceptions, ensuring that all applications receive the resources they require in a stable manner.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, causing the scheduling process to fail and potentially leading to resource allocation issues.",
            "AdditionalDetails": "The issue likely arises from the lack of synchronization when modifying the appScheds list during the assignment process. The methods involved in the stack trace, particularly 'assignContainer' and 'nodeUpdate', should ensure that modifications to shared data structures are properly synchronized to prevent concurrent modification issues."
        }
    },
    {
        "filename": "YARN-7942.json",
        "creation_time": "2018-02-16T19:09:39.000+0000",
        "bug_report": {
            "Title": "NoPathPermissionsException when attempting to delete a service in YARN",
            "Description": "An attempt to delete a service in YARN resulted in a NoPathPermissionsException due to insufficient authorization to access the specified path in the registry. This indicates a potential issue with ACLs (Access Control Lists) not being properly configured for the service being deleted.",
            "StackTrace": [
                "org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:412)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:390)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:722)",
                "at org.apache.hadoop.registry.client.impl.RegistryOperationsService.delete(RegistryOperationsService.java:162)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionDestroy(ServiceClient.java:462)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:253)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:243)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.stopService(ApiServer.java:243)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.deleteService(ApiServer.java:223)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:89)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)",
                "at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$Context.java:203)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.AuthenticationWithProxyUserFilter.doFilter(AuthenticationWithProxyUserFilter.java:101)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1617)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "Attempt to delete the service located at `/registry/users/hbase/services/yarn-service/hbase-app-test` using the YARN API.",
                "Ensure that the user performing the action does not have the necessary ACL permissions for the specified path."
            ],
            "ExpectedBehavior": "The service should be deleted successfully if the user has the appropriate permissions to access the specified path.",
            "ObservedBehavior": "The deletion fails with a NoPathPermissionsException indicating that the user is not authorized to access the specified path.",
            "AdditionalDetails": "The exception indicates that the ACLs for the path are not set correctly, leading to authorization issues. Further investigation into the ACL configuration for the registry paths is required."
        }
    },
    {
        "filename": "YARN-7692.json",
        "creation_time": "2017-12-29T06:00:34.000+0000",
        "bug_report": {
            "Title": "AccessControlException when submitting/updating YARN application",
            "Description": "The application submission process fails due to an AccessControlException, indicating that the user 'hrt_qa' does not have the necessary permissions to submit or update the specified application. This issue arises during the recovery phase of the ResourceManager, specifically when checking application priorities.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2348)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:358)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:567)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1390)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1143)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1183)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1179)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1179)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:611)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)",
                "Caused by: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0"
            ],
            "StepsToReproduce": [
                "Attempt to submit or update an application in YARN as the user 'hrt_qa'.",
                "Ensure that the application ID is 'application_1514268754125_0001'.",
                "Observe the logs for any AccessControlException related to permissions."
            ],
            "ExpectedBehavior": "The user 'hrt_qa' should be able to submit or update the application without encountering permission errors.",
            "ObservedBehavior": "The user 'hrt_qa' receives an AccessControlException indicating insufficient permissions to submit or update the specified application.",
            "AdditionalDetails": "This issue may be related to the configuration of user permissions in YARN. It is recommended to check the access control settings for the user 'hrt_qa' and ensure that they have the necessary permissions to interact with the application."
        }
    },
    {
        "filename": "YARN-3917.json",
        "creation_time": "2015-07-11T00:41:28.000+0000",
        "bug_report": {
            "Title": "UnsupportedOperationException: Could not determine OS in SysInfo.newInstance()",
            "Description": "The application throws an UnsupportedOperationException when attempting to initialize the NodeManager due to an inability to determine the operating system. This occurs in the SysInfo.newInstance() method, which is called during the initialization of the ResourceCalculatorPlugin.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: Could not determine OS",
                "at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager service.",
                "Ensure that the environment does not match any known operating systems (Linux or Windows)."
            ],
            "ExpectedBehavior": "The NodeManager should initialize successfully and determine the operating system correctly.",
            "ObservedBehavior": "The NodeManager fails to initialize and throws an UnsupportedOperationException indicating that the operating system could not be determined.",
            "AdditionalDetails": "The SysInfo.newInstance() method checks for specific operating systems (Linux and Windows) and throws an exception if neither is detected. This indicates that the application may not be running in a supported environment."
        }
    },
    {
        "filename": "YARN-3537.json",
        "creation_time": "2015-04-23T11:34:23.000+0000",
        "bug_report": {
            "Title": "NullPointerException in NodeManager during service stop",
            "Description": "A NullPointerException occurs in the NodeManager's stopRecoveryStore method when attempting to stop the recovery store. This issue arises during the shutdown process of the NodeManager service.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore(NodeManager.java:181)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:326)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:106)"
            ],
            "StepsToReproduce": [
                "1. Start the NodeManager service.",
                "2. Trigger a shutdown of the NodeManager service.",
                "3. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The NodeManager should stop gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the stopRecoveryStore method, causing the shutdown process to fail.",
            "AdditionalDetails": "The NullPointerException likely occurs because the 'nmStore' or 'context' variable is null when stopRecoveryStore is called. Further investigation is needed to ensure these variables are properly initialized before the stop method is invoked."
        }
    },
    {
        "filename": "YARN-7962.json",
        "creation_time": "2018-02-22T22:32:20.000+0000",
        "bug_report": {
            "Title": "RejectedExecutionException in DelegationTokenRenewer",
            "Description": "The application encounters a RejectedExecutionException when attempting to process a DelegationTokenRenewer event after the thread pool has been terminated. This indicates that the system is trying to execute a task on a thread pool that is no longer accepting new tasks.",
            "StackTrace": [
                "java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the YARN ResourceManager.",
                "2. Wait for the application to complete.",
                "3. Observe the logs for any RejectedExecutionException related to DelegationTokenRenewer."
            ],
            "ExpectedBehavior": "The DelegationTokenRenewer should process the event without throwing an exception, even after the application has finished.",
            "ObservedBehavior": "A RejectedExecutionException is thrown, indicating that the task cannot be executed because the thread pool has been terminated.",
            "AdditionalDetails": "The issue arises in the DelegationTokenRenewer's applicationFinished method, which attempts to process a DelegationTokenRenewerEvent after the thread pool has already been shut down. This suggests that there may be a race condition or improper handling of application lifecycle events."
        }
    },
    {
        "filename": "YARN-8357.json",
        "creation_time": "2018-05-24T16:46:57.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ServiceClient.actionStart Method",
            "Description": "A NullPointerException is thrown in the actionStart method of the ServiceClient class when attempting to start a service. This occurs when the service name provided is either invalid or leads to a null service status.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:644)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)"
            ],
            "StepsToReproduce": [
                "Invoke the actionStart method of ServiceClient with an invalid or null service name.",
                "Ensure that the service is not in the UPGRADING state.",
                "Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The actionStart method should successfully start the service or return an appropriate error message if the service name is invalid or the service is in an incorrect state.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the method is attempting to access a property or method on a null object.",
            "AdditionalDetails": "The actionStart method calls getStatus(serviceName) which may return null if the service does not exist or is not properly initialized. This leads to a NullPointerException when the method attempts to access properties of the liveService object."
        }
    },
    {
        "filename": "YARN-6534.json",
        "creation_time": "2017-04-26T21:43:52.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException during ResourceManager Initialization",
            "Description": "The ResourceManager fails to initialize due to a missing SSL keystore file. The application throws a FileNotFoundException when attempting to load the keystore from the specified path, which leads to a ServiceStateException.",
            "StackTrace": [
                "org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher.serviceInit(AbstractSystemMetricsPublisher.java:59)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.serviceInit(TimelineServiceV1Publisher.java:67)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1453)",
                "Caused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:168)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:86)",
                "at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:219)",
                "at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:179)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.getSSLFactory(TimelineConnector.java:176)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit(TimelineConnector.java:106)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service.",
                "Ensure that the keystore file '/etc/security/clientKeys/all.jks' does not exist."
            ],
            "ExpectedBehavior": "The ResourceManager should initialize successfully without throwing exceptions.",
            "ObservedBehavior": "The ResourceManager fails to initialize and throws a ServiceStateException due to a missing keystore file.",
            "AdditionalDetails": "The missing keystore file is critical for SSL configuration. Ensure that the file exists at the specified path or update the configuration to point to the correct keystore location."
        }
    },
    {
        "filename": "YARN-4227.json",
        "creation_time": "2015-10-06T04:59:10.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FairScheduler during Container Completion",
            "Description": "A NullPointerException occurs in the FairScheduler class when attempting to complete a container. This issue arises when the completedContainer method is called with a null RMContainer, leading to a failure in the resource manager's scheduling process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:849)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1273)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:122)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:585)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Trigger a container completion event in the FairScheduler.",
                "Ensure that the RMContainer associated with the event is null.",
                "Observe the resulting NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The FairScheduler should handle the container completion event gracefully, without throwing a NullPointerException, even if the RMContainer is null.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the scheduling process to fail and potentially impacting resource management.",
            "AdditionalDetails": "The issue likely stems from the completedContainer method being called with a null RMContainer, which is not handled properly. This can occur during the processing of completed containers in the nodeUpdate method, where the container information is pulled and processed."
        }
    },
    {
        "filename": "YARN-2649.json",
        "creation_time": "2014-10-06T22:57:46.000+0000",
        "bug_report": {
            "Title": "AppAttempt State Mismatch in ResourceManager Test",
            "Description": "The test 'testAMRMUnusableNodes' in the ResourceManager fails due to an unexpected state of the application attempt. The expected state was 'ALLOCATED', but the actual state was 'SCHEDULED'. This indicates a potential issue in the state transition logic of the application attempts within the ResourceManager.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>",
                "at junit.framework.Assert.fail(Assert.java:50)",
                "at junit.framework.Assert.failNotEquals(Assert.java:287)",
                "at junit.framework.Assert.assertEquals(Assert.java:67)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)",
                "at org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)",
                "attempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(670)) - appattempt_1412569506932_0001_000001 State change from NEW to SUBMITTED",
                "event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE",
                "rmnode.RMNodeImpl (RMNodeImpl.java:handle(384)) - Processing 127.0.0.1:1234 of type STATUS_UPDATE",
                "event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent.EventType: APP_ATTEMPT_ADDED",
                "event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE",
                "event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent.EventType: ATTEMPT_ADDED",
                "attempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(658)) - Processing event for appattempt_1412569506932_0001_000001 of type ATTEMPT_ADDED",
                "attempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(670)) - appattempt_1412569506932_0001_000001 State change from SUBMITTED to SCHEDULED"
            ],
            "StepsToReproduce": [
                "Run the test 'testAMRMUnusableNodes' in the ResourceManager.",
                "Observe the state transitions of the application attempt.",
                "Check the final state of the application attempt after the test execution."
            ],
            "ExpectedBehavior": "The application attempt should reach the 'ALLOCATED' state after being launched and registered.",
            "ObservedBehavior": "The application attempt reached the 'SCHEDULED' state instead of the expected 'ALLOCATED' state.",
            "AdditionalDetails": "The issue may stem from the state transition logic in the RMAppAttemptImpl class, particularly in the handling of events related to application attempts. Further investigation into the event dispatching and state management is required."
        }
    },
    {
        "filename": "YARN-4288.json",
        "creation_time": "2015-10-22T12:30:16.000+0000",
        "bug_report": {
            "Title": "Connection Reset by Peer Error in NodeManager Registration",
            "Description": "The NodeManager fails to register with the ResourceManager due to a connection reset by the peer. This results in an IOException being thrown, which disrupts the normal operation of the NodeManager.",
            "StackTrace": [
                "java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1473)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)",
                "at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)",
                "Caused by: java.io.IOException: Connection reset by peer",
                "at sun.nio.ch.FileDispatcherImpl.read0(Native Method)",
                "at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)",
                "at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)",
                "at sun.nio.ch.IOUtil.read(IOUtil.java:197)",
                "at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)",
                "at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)",
                "at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)",
                "at java.io.FilterInputStream.read(FilterInputStream.java:133)",
                "at java.io.FilterInputStream.read(FilterInputStream.java:133)",
                "at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:514)",
                "at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)",
                "at java.io.BufferedInputStream.read(BufferedInputStream.java:254)",
                "at java.io.DataInputStream.readInt(DataInputStream.java:387)",
                "at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1072)",
                "at org.apache.hadoop.ipc.Client$Connection.run(Client.java:967)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager service on host 172.27.62.28.",
                "Ensure that the ResourceManager is running on host 172.27.62.57:8025.",
                "Observe the logs for any connection issues during the NodeManager registration process."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager without any connection errors.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager, resulting in an IOException due to a connection reset by peer.",
            "AdditionalDetails": "The issue may be related to network instability or firewall settings between the NodeManager and ResourceManager. Further investigation into network configurations and logs on both hosts is recommended."
        }
    },
    {
        "filename": "YARN-1032.json",
        "creation_time": "2013-08-05T21:10:46.000+0000",
        "bug_report": {
            "Title": "NullPointerException in RackResolver during Container Assignment",
            "Description": "A NullPointerException is thrown in the RackResolver class when attempting to resolve a hostname during the container assignment process in the RMContainerAllocator. This issue occurs when the hostname passed to the coreResolve method is null, leading to an attempt to access an element from a null list.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)",
                "at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Initialize the Hadoop YARN environment.",
                "2. Trigger a container assignment that requires hostname resolution.",
                "3. Ensure that a null hostname is passed to the RackResolver."
            ],
            "ExpectedBehavior": "The RackResolver should handle null hostnames gracefully, either by returning a default value or by logging an error without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application to crash during the container assignment process.",
            "AdditionalDetails": "The coreResolve method attempts to access the first element of the rNameList without checking if it is empty, which leads to the NullPointerException when the hostname is null. This indicates a lack of validation for the input parameters."
        }
    },
    {
        "filename": "YARN-5837.json",
        "creation_time": "2016-11-04T16:06:59.000+0000",
        "bug_report": {
            "Title": "NullPointerException in NodeCLI.printNodeStatus",
            "Description": "A NullPointerException occurs in the NodeCLI class when attempting to print the status of a node. This issue arises when the method printNodeStatus is called with a null or invalid nodeIdStr, leading to an unhandled exception.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)"
            ],
            "StepsToReproduce": [
                "Run the NodeCLI with the 'status' command without providing a valid node ID.",
                "Example command: java -cp <classpath> org.apache.hadoop.yarn.client.cli.NodeCLI -status"
            ],
            "ExpectedBehavior": "The application should handle the case where the node ID is null or invalid gracefully, providing a user-friendly error message instead of throwing a NullPointerException.",
            "ObservedBehavior": "The application throws a NullPointerException when the printNodeStatus method is called with a null or invalid node ID.",
            "AdditionalDetails": "The printNodeStatus method does not currently check if the nodeIdStr is null before attempting to use it, which leads to the NullPointerException. Adding a null check at the beginning of this method could prevent this issue."
        }
    },
    {
        "filename": "YARN-6827.json",
        "creation_time": "2017-07-15T05:14:25.000+0000",
        "bug_report": {
            "Title": "NullPointerException in TimelineClientImpl when putting entities",
            "Description": "A NullPointerException is thrown in the TimelineClientImpl class when attempting to put entities into the timeline service. This issue occurs during the execution of the putEntities method, indicating that a null reference is being accessed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:178)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.putEntity(TimelineServiceV1Publisher.java:368)"
            ],
            "StepsToReproduce": [
                "Invoke the putEntity method of the TimelineServiceV1Publisher class with a null TimelineEntity.",
                "Ensure that the putEntities method in TimelineClientImpl is called as part of the process."
            ],
            "ExpectedBehavior": "The system should handle null entities gracefully, either by ignoring them or by throwing a controlled exception with a clear message.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application to fail without a clear indication of the issue.",
            "AdditionalDetails": "The putEntity method in TimelineServiceV1Publisher attempts to log the entity details before calling putEntities. If the entity is null, this will lead to a NullPointerException when trying to access entity properties such as getEntityId() or getEntityType(). Proper null checks should be implemented to prevent this issue."
        }
    },
    {
        "filename": "YARN-3832.json",
        "creation_time": "2015-06-19T13:31:18.000+0000",
        "bug_report": {
            "Title": "IOException during file rename operation in Hadoop",
            "Description": "An IOException is thrown when attempting to rename a file in Hadoop's file system due to the destination directory being non-empty. This occurs in the FSDownload class when trying to move a temporary file to its final destination.",
            "StackTrace": [
                "java.io.IOException: Rename cannot overwrite non empty destination directory /opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39",
                "at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:735)",
                "at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:244)",
                "at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:678)",
                "at org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Initiate a file download operation using Hadoop's FSDownload class.",
                "2. Ensure that the destination directory '/opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39' is not empty.",
                "3. Observe the exception thrown during the rename operation."
            ],
            "ExpectedBehavior": "The file should be renamed successfully if the destination directory is empty or if the overwrite option is enabled.",
            "ObservedBehavior": "An IOException is thrown indicating that the rename operation cannot overwrite a non-empty destination directory.",
            "AdditionalDetails": "The issue arises in the renameInternal method of the AbstractFileSystem class, which is called by the FSDownload class when attempting to rename a temporary file to its final destination. The rename operation fails because the destination directory is not empty, which is not handled in the current implementation."
        }
    },
    {
        "filename": "YARN-2409.json",
        "creation_time": "2014-08-12T10:53:06.000+0000",
        "bug_report": {
            "Title": "Invalid State Transition in YARN ResourceManager",
            "Description": "The YARN ResourceManager is encountering an InvalidStateTransitionException when processing application attempt events. Specifically, the events STATUS_UPDATE and CONTAINER_ALLOCATED are being received while the application attempt is in the LAUNCHED state, which is not a valid transition according to the state machine's rules.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:662)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Submit an application to the YARN ResourceManager.",
                "Ensure the application reaches the LAUNCHED state.",
                "Trigger a STATUS_UPDATE or CONTAINER_ALLOCATED event for the application attempt."
            ],
            "ExpectedBehavior": "The ResourceManager should handle STATUS_UPDATE and CONTAINER_ALLOCATED events appropriately without throwing an exception, transitioning the application attempt to the correct state.",
            "ObservedBehavior": "The ResourceManager throws an InvalidStateTransitionException when attempting to process STATUS_UPDATE and CONTAINER_ALLOCATED events while in the LAUNCHED state.",
            "AdditionalDetails": "The state machine for application attempts does not allow STATUS_UPDATE or CONTAINER_ALLOCATED events to be processed in the LAUNCHED state, indicating a potential issue in event handling logic or state management."
        }
    },
    {
        "filename": "YARN-8116.json",
        "creation_time": "2018-04-04T15:30:52.000+0000",
        "bug_report": {
            "Title": "NumberFormatException during NodeManager recovery process",
            "Description": "The NodeManager encounters a NumberFormatException when attempting to recover container states from the state store. This occurs because an empty string is being parsed as a long value, which is not valid.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)",
                "at java.lang.Long.parseLong(Long.java:601)",
                "at java.lang.Long.parseLong(Long.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager service.",
                "Ensure that the state store contains an entry with an empty string for a container ID.",
                "Observe the logs for the NumberFormatException."
            ],
            "ExpectedBehavior": "The NodeManager should successfully recover the container states without throwing an exception.",
            "ObservedBehavior": "The NodeManager throws a NumberFormatException when it encounters an empty string while trying to parse container IDs during recovery.",
            "AdditionalDetails": "The issue arises in the method 'loadContainerState' of the 'NMLeveldbStateStoreService' class, specifically at line 350 where it attempts to parse a container ID from the state store. The empty string input indicates a potential data corruption or misconfiguration in the state store."
        }
    },
    {
        "filename": "YARN-8403.json",
        "creation_time": "2018-06-06T22:34:42.000+0000",
        "bug_report": {
            "Title": "YarnException: Download and unpack failed due to Permission Denied",
            "Description": "The application fails to download and unpack resources due to a permission issue when trying to create a file in the local file cache directory. The stack trace indicates that a FileNotFoundException is thrown because the application does not have the necessary permissions to write to the specified path.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed",
                "at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)",
                "at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:409)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:66)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:408)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:399)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:381)",
                "at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:298)",
                "... 9 more"
            ],
            "StepsToReproduce": [
                "Attempt to run a Yarn application that requires downloading resources.",
                "Ensure that the local file cache directory (/grid/0/hadoop/yarn/local/filecache/) has restricted permissions for the user running the application."
            ],
            "ExpectedBehavior": "The application should successfully download and unpack the required resources without any permission issues.",
            "ObservedBehavior": "The application fails with a YarnException indicating that the download and unpack process failed due to a FileNotFoundException caused by permission denial.",
            "AdditionalDetails": "The issue is likely related to the permissions set on the local file cache directory. Ensure that the user running the Yarn application has the necessary write permissions to the directory."
        }
    },
    {
        "filename": "YARN-1458.json",
        "creation_time": "2013-11-29T03:31:39.000+0000",
        "bug_report": {
            "Title": "Deadlock in FairScheduler during Application Removal",
            "Description": "A deadlock occurs in the FairScheduler when attempting to remove an application while another thread is trying to access application weights. This leads to blocked threads and can cause the ResourceManager to become unresponsive.",
            "StackTrace": [
                "java.lang.Thread.State: BLOCKED (on object monitor)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplication(FairScheduler.java:671)",
                "        - waiting to lock <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1023)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:440)",
                "        at java.lang.Thread.run(Thread.java:744)",
                "java.lang.Thread.State: RUNNABLE",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getAppWeight(FairScheduler.java:545)",
                "        - locked <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.getWeights(AppSchedulable.java:129)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShare(ComputeFairShares.java:143)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.resourceUsedWithWeightToResourceRatio(ComputeFairShares.java:131)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShares(ComputeFairShares.java:102)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy.computeShares(FairSharePolicy.java:119)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.recomputeShares(FSLeafQueue.java:100)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.recomputeShares(FSParentQueue.java:62)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:282)",
                "        - locked <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:255)",
                "        at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with FairScheduler enabled.",
                "2. Submit multiple applications to the ResourceManager.",
                "3. Trigger the removal of an application while the scheduler is actively computing shares for other applications."
            ],
            "ExpectedBehavior": "The application should be removed without causing any deadlocks, and the scheduler should continue to function normally.",
            "ObservedBehavior": "The ResourceManager becomes unresponsive due to blocked threads, leading to a deadlock situation.",
            "AdditionalDetails": "The deadlock occurs because the `removeApplication` method is synchronized, which prevents other threads from accessing the scheduler's state while it is being modified. This can lead to situations where one thread is waiting for a lock held by another thread that is also waiting for a lock held by the first thread."
        }
    },
    {
        "filename": "YARN-8209.json",
        "creation_time": "2018-04-26T00:22:23.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DockerClient while executing Docker commands",
            "Description": "A NullPointerException is thrown in the DockerClient class when attempting to write a command to a temporary file. This occurs during the execution of Docker commands, specifically when trying to remove a Docker container.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile(DockerClient.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand(DockerCommandExecutor.java:85)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeStatusCommand(DockerCommandExecutor.java:192)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.getContainerStatus(DockerCommandExecutor.java:128)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer(LinuxContainerExecutor.java:935)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask.run(DockerContainerDeletionTask.java:61)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Attempt to remove a Docker container using the LinuxContainerExecutor.",
                "2. Ensure that the DockerCommand object passed to the writeCommandToTempFile method is null or improperly initialized.",
                "3. Observe the NullPointerException being thrown."
            ],
            "ExpectedBehavior": "The system should successfully execute the Docker command and remove the specified Docker container without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a required object is null when attempting to write a command to a temporary file.",
            "AdditionalDetails": "The issue likely arises from the DockerCommand object being null or improperly initialized before being passed to the writeCommandToTempFile method in the DockerClient class. Further investigation is needed to ensure that the DockerCommand is correctly instantiated and passed through the execution flow."
        }
    },
    {
        "filename": "YARN-3804.json",
        "creation_time": "2015-06-15T08:54:42.000+0000",
        "bug_report": {
            "Title": "ResourceManager Transition to Active Fails Due to Permission Issue",
            "Description": "The ResourceManager (RM) fails to transition to the Active state due to a permission issue with the user 'yarn'. The exception indicates that the user does not have the necessary permissions to call the 'refreshAdminAcls' method, which is required during the transition process.",
            "StackTrace": [
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:645)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:518)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Can not execute refreshAdminAcls",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "... 4 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAdminAcls(AdminService.java:465)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:295)",
                "... 5 more",
                "Caused by: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:182)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:148)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAccess(AdminService.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:228)"
            ],
            "StepsToReproduce": [
                "Attempt to transition the ResourceManager to Active state while the user 'yarn' is logged in.",
                "Ensure that the user 'yarn' does not have the necessary permissions to call 'refreshAdminAcls'."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the Active state without any permission-related exceptions.",
            "ObservedBehavior": "The ResourceManager fails to transition to Active, throwing a ServiceFailedException due to insufficient permissions for the user 'yarn'.",
            "AdditionalDetails": "The issue arises from the 'checkAcls' method in the AdminService class, which verifies user permissions before executing the 'refreshAdminAcls' method. The user 'yarn' lacks the required permissions, leading to an AccessControlException."
        }
    },
    {
        "filename": "YARN-1839.json",
        "creation_time": "2014-03-14T23:52:29.000+0000",
        "bug_report": {
            "Title": "Invalid NMToken Error in Container Management Protocol",
            "Description": "The application encounters an InvalidToken exception when attempting to communicate with the Container Management Protocol. This issue arises due to the absence of a valid NMToken for the specified host, which prevents the application from launching containers successfully.",
            "StackTrace": [
                "org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Attempt to launch a container on the specified host (<host>:45454).",
                "2. Ensure that no valid NMToken is available for the host.",
                "3. Observe the application logs for the InvalidToken exception."
            ],
            "ExpectedBehavior": "The application should successfully launch the container and communicate with the Container Management Protocol without any token-related errors.",
            "ObservedBehavior": "The application throws an InvalidToken exception indicating that no NMToken was sent for the specified host, preventing the container from launching.",
            "AdditionalDetails": "The issue may be related to the configuration of the token management system or the initialization of the NMToken. Further investigation into the token generation and distribution process is recommended."
        }
    },
    {
        "filename": "YARN-6714.json",
        "creation_time": "2017-06-15T09:56:15.000+0000",
        "bug_report": {
            "Title": "IllegalStateException when unreserving resources in YARN Capacity Scheduler",
            "Description": "An IllegalStateException is thrown when attempting to unreserve resources for an application that is currently reserved for another application. This issue occurs in the YARN Capacity Scheduler when handling completed containers.",
            "StackTrace": [
                "java.lang.IllegalStateException: Trying to unreserve  for application appattempt_1495188831758_0121_000002 when currently reserved  for application application_1495188831758_0121 on node host: node1:45454 #containers=2 available=... used=...",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:152)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the YARN Capacity Scheduler.",
                "2. Reserve resources for the application.",
                "3. Attempt to unreserve resources for a different application that is currently reserved on the same node.",
                "4. Observe the IllegalStateException being thrown."
            ],
            "ExpectedBehavior": "The system should successfully unreserve resources for the application without throwing an exception, or handle the situation gracefully if the resources are reserved for another application.",
            "ObservedBehavior": "An IllegalStateException is thrown indicating that the system is trying to unreserve resources for an application that is currently reserved for another application.",
            "AdditionalDetails": "The issue seems to stem from the unreserveResource method in the FiCaSchedulerNode class, which does not properly handle the case where an application attempts to unreserve resources that are still reserved for another application."
        }
    },
    {
        "filename": "YARN-3351.json",
        "creation_time": "2015-03-16T14:19:59.000+0000",
        "bug_report": {
            "Title": "BindException Occurs When Attempting to Proxy Link in WebAppProxyServlet",
            "Description": "A BindException is thrown when the WebAppProxyServlet attempts to create a socket connection to a specified proxy host. This issue arises during the execution of the proxyLink method, which is called from the doGet method of the WebAppProxyServlet. The error indicates that the application is unable to bind to the requested address, likely due to an invalid or unavailable network configuration.",
            "StackTrace": [
                "java.net.BindException: Cannot assign requested address",
                "at java.net.PlainSocketImpl.socketBind(Native Method)",
                "at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)",
                "at java.net.Socket.bind(Socket.java:631)",
                "at java.net.Socket.<init>(Socket.java:423)",
                "at java.net.Socket.<init>(Socket.java:280)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)",
                "at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)"
            ],
            "StepsToReproduce": [
                "1. Deploy the application containing the WebAppProxyServlet.",
                "2. Attempt to access a proxied link through the servlet.",
                "3. Observe the server logs for the BindException."
            ],
            "ExpectedBehavior": "The WebAppProxyServlet should successfully create a socket connection to the specified proxy host and forward the request without errors.",
            "ObservedBehavior": "A BindException is thrown, indicating that the application cannot assign the requested address, preventing the proxy link from being established.",
            "AdditionalDetails": "This issue may be related to network configuration, such as an incorrect proxy host address or insufficient permissions to bind to the specified address. Further investigation into the network settings and the parameters passed to the proxyLink method is recommended."
        }
    },
    {
        "filename": "YARN-2813.json",
        "creation_time": "2014-11-05T22:29:46.000+0000",
        "bug_report": {
            "Title": "NullPointerException in TimelineWebServices.getDomains",
            "Description": "A NullPointerException is thrown when the getDomains method is invoked in the TimelineWebServices class. This issue occurs during the processing of a web request, leading to a failure in the application.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:96)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:572)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:269)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:542)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1204)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)",
                "at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:713)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:353)"
            ],
            "StepsToReproduce": [
                "Send a request to the TimelineWebServices endpoint that triggers the getDomains method."
            ],
            "ExpectedBehavior": "The getDomains method should return a list of domains without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the web application to fail to respond properly.",
            "AdditionalDetails": "The issue appears to originate from the MemoryTimelineStore.getDomains method, which may be attempting to access a null object. Further investigation into the state of the MemoryTimelineStore at the time of the request is needed."
        }
    },
    {
        "filename": "YARN-1550.json",
        "creation_time": "2013-12-30T03:58:32.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FairSchedulerAppsBlock.render() Method",
            "Description": "A NullPointerException is thrown in the FairSchedulerAppsBlock.render() method, which is part of the Apache Hadoop YARN ResourceManager web application. This issue occurs when the method attempts to access a property or method on a null object reference.",
            "StackTrace": [
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock.render(FairSchedulerAppsBlock.java:96)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:66)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:76)"
            ],
            "StepsToReproduce": [
                "1. Start the Apache Hadoop YARN ResourceManager.",
                "2. Access the FairSchedulerAppsBlock web interface.",
                "3. Trigger the rendering of the FairSchedulerAppsBlock."
            ],
            "ExpectedBehavior": "The FairSchedulerAppsBlock should render without throwing any exceptions, displaying the relevant information about the applications managed by the Fair Scheduler.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the rendering process to fail and potentially leading to a broken web interface.",
            "AdditionalDetails": "The issue likely arises from the block() method returning a null block object, which is then used in the render() method. The block() method initializes the block only if it is null, but if the initialization fails or is not properly handled, it can lead to a NullPointerException during rendering."
        }
    },
    {
        "filename": "YARN-5006.json",
        "creation_time": "2016-04-28T08:26:38.000+0000",
        "bug_report": {
            "Title": "ConnectionLossException in ZKRMStateStore during Application State Storage",
            "Description": "The application encounters a ConnectionLossException when attempting to store application state in ZKRMStateStore. This issue arises during the execution of the multi() method in ZooKeeper, indicating a loss of connection to the ZooKeeper ensemble.",
            "StackTrace": [
                "org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:936)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1075)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1096)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:947)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.createWithRetries(ZKRMStateStore.java:956)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:626)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:138)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:123)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:860)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:855)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager with a ZooKeeper ensemble.",
                "2. Submit an application that requires state storage.",
                "3. Monitor the logs for any ConnectionLossException during the state storage process."
            ],
            "ExpectedBehavior": "The application state should be stored successfully in ZooKeeper without any exceptions.",
            "ObservedBehavior": "A ConnectionLossException is thrown, indicating that the connection to ZooKeeper was lost during the state storage operation.",
            "AdditionalDetails": "The issue may be related to network instability or misconfiguration of the ZooKeeper ensemble. Further investigation into the ZooKeeper logs and network conditions is recommended."
        }
    },
    {
        "filename": "YARN-5728.json",
        "creation_time": "2016-10-13T05:16:28.000+0000",
        "bug_report": {
            "Title": "Test Timeout in Node Utilization Update",
            "Description": "The test 'testUpdateNodeUtilization' in the MiniYarnClusterNodeUtilization class is timing out after 60 seconds, indicating a potential issue with the node heartbeat process or the retry mechanism in the Hadoop YARN framework.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 60000 milliseconds",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.processWaitTimeAndRetryInfo(RetryInvocationHandler.java:130)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:107)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy85.nodeHeartbeat(Unknown Source)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:113)"
            ],
            "StepsToReproduce": [
                "Run the test 'testUpdateNodeUtilization' in the TestMiniYarnClusterNodeUtilization class.",
                "Observe the test execution for timeout after 60 seconds."
            ],
            "ExpectedBehavior": "The test should complete successfully without timing out, indicating that the node utilization update process is functioning correctly.",
            "ObservedBehavior": "The test times out after 60 seconds, suggesting that the node heartbeat process is not completing as expected.",
            "AdditionalDetails": "The timeout may be related to the retry mechanism in the 'RetryInvocationHandler' class, specifically in the 'processWaitTimeAndRetryInfo' method, which handles the wait time and retry logic for node heartbeats."
        }
    },
    {
        "filename": "YARN-2805.json",
        "creation_time": "2014-11-04T20:37:09.000+0000",
        "bug_report": {
            "Title": "YarnRuntimeException: Failed to login due to keytab issue",
            "Description": "The ResourceManager fails to start due to a login failure when attempting to authenticate using a keytab file. The exception indicates that the system is unable to obtain a password for the specified user from the keytab, which prevents the ResourceManager from initializing properly.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)",
                "Caused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the keytab file /etc/security/keytabs/rm.service.keytab exists.",
                "2. Attempt to start the ResourceManager using the command: java -cp <classpath> org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                "3. Observe the logs for the YarnRuntimeException indicating a login failure."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any login errors, allowing it to manage resources as intended.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing a YarnRuntimeException due to a login failure when attempting to authenticate with the provided keytab.",
            "AdditionalDetails": "The issue may be related to the configuration of the keytab or the user permissions associated with it. Ensure that the user 'rm' has the correct permissions to access the keytab and that the keytab is valid."
        }
    },
    {
        "filename": "YARN-4744.json",
        "creation_time": "2016-02-29T10:08:57.000+0000",
        "bug_report": {
            "Title": "PrivilegedOperationException during Container Cleanup",
            "Description": "A PrivilegedOperationException is thrown when attempting to signal a container during the cleanup process in the YARN NodeManager. The exception indicates that the exit code from the privileged operation is 9, which typically signifies a failure in executing the command due to insufficient permissions or a missing resource.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=9:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer(DefaultLinuxContainerRuntime.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.signalContainer(DelegatingLinuxContainerRuntime.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.signalContainer(LinuxContainerExecutor.java:513)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: ExitCodeException exitCode=9:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:150)"
            ],
            "StepsToReproduce": [
                "1. Launch a container using YARN NodeManager.",
                "2. Trigger a cleanup operation for the container.",
                "3. Observe the logs for any PrivilegedOperationException."
            ],
            "ExpectedBehavior": "The container cleanup process should complete successfully without throwing any exceptions.",
            "ObservedBehavior": "The cleanup process fails with a PrivilegedOperationException indicating an exit code of 9, suggesting a failure in executing the privileged operation.",
            "AdditionalDetails": "The exit code 9 may indicate a permission issue or a missing resource required for the cleanup operation. Further investigation into the permissions of the user executing the NodeManager and the resources available on the system may be necessary."
        }
    },
    {
        "filename": "YARN-1752.json",
        "creation_time": "2014-02-22T05:51:42.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException when handling UNREGISTERED event",
            "Description": "An InvalidStateTransitionException is thrown when the system attempts to handle an UNREGISTERED event while in the LAUNCHED state. This indicates a flaw in the state transition logic of the ResourceManager's application attempt handling.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "1. Start a YARN application and ensure it transitions to the LAUNCHED state.",
                "2. Trigger an UNREGISTERED event for the application attempt.",
                "3. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The system should handle the UNREGISTERED event gracefully without throwing an exception, possibly by transitioning to a valid state or ignoring the event.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException indicating that the UNREGISTERED event cannot be processed while in the LAUNCHED state.",
            "AdditionalDetails": "The exception occurs in the doTransition method of the StateMachineFactory, which suggests that the state machine does not allow the transition from LAUNCHED to a state that can handle UNREGISTERED events. This may require a review of the state transition rules defined in the state machine."
        }
    },
    {
        "filename": "YARN-6629.json",
        "creation_time": "2017-05-22T08:31:16.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AppSchedulingInfo.allocate Method",
            "Description": "A NullPointerException occurs in the AppSchedulingInfo.allocate method when attempting to allocate resources for an application. This issue arises during the resource allocation process in the Hadoop YARN ResourceManager, specifically when the application attempt is not found or is null.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:446)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply(FiCaSchedulerApp.java:516)",
                "at org.apache.hadoop.yarn.client.TestNegativePendingResource$1.answer(TestNegativePendingResource.java:225)",
                "at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:31)",
                "at org.mockito.internal.MockHandler.handle(MockHandler.java:97)",
                "at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp$$EnhancerByMockitoWithCGLIB$$29eb8afc.apply(<generated>)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit(CapacityScheduler.java:2396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.submitResourceCommitRequest(CapacityScheduler.java:2281)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1247)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1236)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1325)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:987)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1367)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:143)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Trigger a resource allocation request in the YARN ResourceManager.",
                "2. Ensure that the application attempt ID is not valid or is null.",
                "3. Observe the logs for a NullPointerException in the AppSchedulingInfo.allocate method."
            ],
            "ExpectedBehavior": "The resource allocation process should handle null application attempts gracefully, either by skipping the allocation or by logging an appropriate error message without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the resource allocation process to fail and potentially disrupt the scheduling of resources in the YARN ResourceManager.",
            "AdditionalDetails": "The issue seems to stem from the 'apply' method in the FiCaSchedulerApp class, which is called during the allocation process. If the application attempt is not found, it leads to a null reference being accessed in the AppSchedulingInfo.allocate method."
        }
    },
    {
        "filename": "YARN-3493.json",
        "creation_time": "2015-04-15T22:03:19.000+0000",
        "bug_report": {
            "Title": "InvalidResourceRequestException during ResourceManager startup",
            "Description": "The ResourceManager fails to start due to an InvalidResourceRequestException being thrown. The exception indicates that the requested memory exceeds the maximum configured memory limit. This issue arises during the recovery of applications when the ResourceManager attempts to validate resource requests.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager with a configuration that sets maxMemory to 2048.",
                "Submit an application that requests 3072 memory."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully and validate resource requests without throwing exceptions.",
            "ObservedBehavior": "The ResourceManager fails to start and throws an InvalidResourceRequestException due to a memory request exceeding the maximum configured limit.",
            "AdditionalDetails": "The exception is thrown during the recovery process of applications, specifically in the validateResourceRequest method, which checks if the requested memory is within the allowed limits. The configuration for maximum memory needs to be reviewed to ensure it aligns with the resource requests being made."
        }
    },
    {
        "filename": "YARN-7645.json",
        "creation_time": "2017-12-12T21:19:53.000+0000",
        "bug_report": {
            "Title": "AssertionError in AM Restart Tests due to Incorrect Attempt State",
            "Description": "An AssertionError is thrown during the execution of the AM restart tests in the ResourceManager, indicating that the expected state of the attempt is 'ALLOCATED' but the actual state is 'SCHEDULED'. This suggests a potential issue in the state management of container attempts during the restart process.",
            "StackTrace": [
                "java.lang.AssertionError: Attempt state is not correct (timeout). expected:<ALLOCATED> but was:<SCHEDULED>",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.amRestartTests(TestContainerResourceUsage.java:275)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.testUsageAfterAMRestartWithMultipleContainers(TestContainerResourceUsage.java:254)"
            ],
            "StepsToReproduce": [
                "Run the test suite for the ResourceManager, specifically targeting the TestContainerResourceUsage class.",
                "Execute the testUsageAfterAMRestartWithMultipleContainers method.",
                "Observe the assertion failure indicating the incorrect state of the attempt."
            ],
            "ExpectedBehavior": "The state of the container attempt should be 'ALLOCATED' after the Application Master (AM) restart when keepRunningContainers is set to false.",
            "ObservedBehavior": "The state of the container attempt is 'SCHEDULED' instead of the expected 'ALLOCATED', leading to an AssertionError.",
            "AdditionalDetails": "The method amRestartTests(boolean keepRunningContainers) is responsible for managing the state of the container attempts during the AM restart process. The failure indicates a potential flaw in the logic that handles state transitions."
        }
    },
    {
        "filename": "YARN-6054.json",
        "creation_time": "2017-01-04T20:58:59.000+0000",
        "bug_report": {
            "Title": "ServiceStateException due to LevelDB Corruption in ApplicationHistoryServer",
            "Description": "The ApplicationHistoryServer fails to initialize due to a ServiceStateException caused by corruption in the LevelDB store. The error indicates that there are missing files in the LevelDB store, which prevents the server from starting correctly.",
            "StackTrace": [
                "org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:182)",
                "Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)",
                "at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)",
                "at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)",
                "at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:229)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)"
            ],
            "StepsToReproduce": [
                "Attempt to start the ApplicationHistoryServer.",
                "Ensure that the LevelDB store at <levelDbStorePath> is corrupted or has missing files."
            ],
            "ExpectedBehavior": "The ApplicationHistoryServer should initialize successfully without throwing a ServiceStateException.",
            "ObservedBehavior": "The ApplicationHistoryServer fails to initialize and throws a ServiceStateException due to missing files in the LevelDB store.",
            "AdditionalDetails": "The error indicates that there are 9 missing files in the LevelDB store, which suggests that the database may have been improperly shut down or corrupted. The relevant method 'serviceInit' in 'LeveldbTimelineStore' is responsible for initializing the LevelDB store, and it fails when it attempts to open the corrupted database."
        }
    },
    {
        "filename": "YARN-196.json",
        "creation_time": "2012-01-16T09:52:45.000+0000",
        "bug_report": {
            "Title": "Connection Refused Exception in NodeManager Startup",
            "Description": "The NodeManager fails to start due to a connection refusal when attempting to register with the ResourceManager. This issue is caused by a `java.net.ConnectException` indicating that the connection to the specified host and port is refused.",
            "StackTrace": [
                "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)",
                "... 3 more",
                "Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:131)",
                "at $Proxy23.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "... 5 more",
                "Caused by: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:857)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1141)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1100)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:128)",
                "... 7 more",
                "Caused by: java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:659)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:469)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:563)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:211)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1247)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1117)",
                "... 9 more",
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)",
                "at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:76)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager service.",
                "Ensure that the ResourceManager is not running or is unreachable.",
                "Observe the logs for connection errors."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager and start without errors.",
            "ObservedBehavior": "The NodeManager fails to start and throws a connection refused exception when trying to register with the ResourceManager.",
            "AdditionalDetails": "The connection refused error indicates that the ResourceManager is either not running or not reachable at the specified address (HOST-10-18-52-250:8025). Ensure that the ResourceManager is up and running and that network configurations allow for connections."
        }
    },
    {
        "filename": "YARN-8508.json",
        "creation_time": "2018-07-09T23:37:49.000+0000",
        "bug_report": {
            "Title": "ResourceHandlerException: Insufficient GPU Resources for Container Launch",
            "Description": "The system encountered a ResourceHandlerException while attempting to launch a container due to insufficient GPU resources. The request for 2 GPUs could not be fulfilled as only 1 GPU was available, leading to the failure of the container launch process.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.internalAssignGpus(GpuResourceAllocator.java:225)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.assignGpus(GpuResourceAllocator.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl.preStart(GpuResourceHandlerImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.preStart(ResourceHandlerChain.java:75)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:509)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:479)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:494)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:306)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:103)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.internalAssignGpus(GpuResourceAllocator.java:225)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.assignGpus(GpuResourceAllocator.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl.preStart(GpuResourceHandlerImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.preStart(ResourceHandlerChain.java:75)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:509)"
            ],
            "StepsToReproduce": [
                "Request a container with 2 GPUs when only 1 GPU is available on the node."
            ],
            "ExpectedBehavior": "The container should launch successfully if the requested number of GPUs is available.",
            "ObservedBehavior": "The container launch fails with a ResourceHandlerException indicating insufficient GPU resources.",
            "AdditionalDetails": "The issue arises in the GpuResourceAllocator class, specifically in the internalAssignGpus method, where it checks for available GPUs against the requested amount. The system currently does not handle cases where the requested resources exceed the available resources gracefully."
        }
    },
    {
        "filename": "YARN-2308.json",
        "creation_time": "2014-07-17T10:01:57.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CapacityScheduler when adding application attempt",
            "Description": "A NullPointerException occurs in the CapacityScheduler class when attempting to add an application attempt. This issue arises during the handling of application events, specifically when the application attempt ID is null or not properly initialized.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the ResourceManager.",
                "2. Ensure that the application attempt ID is not properly initialized or is null.",
                "3. Observe the logs for a NullPointerException in the CapacityScheduler."
            ],
            "ExpectedBehavior": "The CapacityScheduler should handle the application attempt without throwing a NullPointerException, even if the application attempt ID is not initialized.",
            "ObservedBehavior": "A NullPointerException is thrown in the addApplicationAttempt method of the CapacityScheduler, causing the application handling process to fail.",
            "AdditionalDetails": "The issue likely stems from the handling of application events where the application attempt ID is expected to be non-null. The method addApplicationAttempt is called without proper validation of the application attempt ID, leading to the exception."
        }
    },
    {
        "filename": "YARN-933.json",
        "creation_time": "2013-07-17T12:29:28.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException on Application Launch Failure",
            "Description": "An InvalidStateTransitionException is thrown when an application attempt fails to launch due to a connection timeout. This occurs in the ResourceManager's event handling process, indicating that the application is in an invalid state to handle the LAUNCH_FAILED event.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: LAUNCH_FAILED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:630)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:99)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:495)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:476)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=host-10-18-40-15/10.18.40.59:8020]",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:573)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)"
            ],
            "StepsToReproduce": [
                "Attempt to launch an application in the Hadoop YARN environment.",
                "Ensure that the target host (10.18.40.59) is unreachable or has a high latency.",
                "Monitor the ResourceManager logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should transition to a failed state gracefully and handle the LAUNCH_FAILED event without throwing an InvalidStateTransitionException.",
            "ObservedBehavior": "The application throws an InvalidStateTransitionException when it receives a LAUNCH_FAILED event while in a FAILED state, indicating that the event cannot be processed in the current state.",
            "AdditionalDetails": "The exception is caused by a ConnectTimeoutException, which suggests that the application was unable to establish a connection within the specified timeout period (20000 milliseconds). This indicates potential network issues or misconfiguration in the cluster setup."
        }
    },
    {
        "filename": "YARN-1374.json",
        "creation_time": "2013-10-30T11:49:49.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in ResourceManager Initialization",
            "Description": "A ConcurrentModificationException is thrown during the initialization of the ResourceManager in the Hadoop YARN service. This occurs when the service attempts to iterate over a collection that has been modified concurrently, leading to instability in the service startup process.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)",
                "at java.util.AbstractList$Itr.next(AbstractList.java:343)",
                "at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service with a configuration that includes multiple services being added to the CompositeService.",
                "Ensure that the configuration or the services being added are modified concurrently during the initialization process."
            ],
            "ExpectedBehavior": "The ResourceManager should initialize successfully without throwing any exceptions, allowing the service to start and manage resources as intended.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, preventing the ResourceManager from completing its initialization and leading to a failure in starting the service.",
            "AdditionalDetails": "The issue likely arises from the use of unmodifiable collections in the CompositeService's serviceInit method. If any service being added modifies the collection concurrently, it can lead to this exception. Review the synchronization mechanisms around the collection modifications to ensure thread safety."
        }
    },
    {
        "filename": "YARN-174.json",
        "creation_time": "2012-10-19T17:25:40.000+0000",
        "bug_report": {
            "Title": "Invalid Yarn Log Directory Path Causes NodeManager Initialization Failure",
            "Description": "The NodeManager fails to initialize due to an invalid log directory path specified in the configuration. The error message indicates that the path '${yarn.log.dir}/userlogs' is not valid, as it should either have a file scheme or be without a scheme. This issue prevents the NodeManager from starting properly.",
            "StackTrace": [
                "org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService.init(NodeHealthCheckerService.java:48)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged(NodeManager.java:256)",
                "at org.apache.hadoop.yarn.service.AbstractService.changeState(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:112)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.reboot(NodeStatusUpdaterImpl.java:157)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$900(NodeStatusUpdaterImpl.java:63)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:357)"
            ],
            "StepsToReproduce": [
                "1. Set the yarn.log.dir configuration to an invalid path such as '${yarn.log.dir}/userlogs'.",
                "2. Start the NodeManager service.",
                "3. Observe the initialization failure due to the invalid path."
            ],
            "ExpectedBehavior": "The NodeManager should initialize successfully and start without errors if a valid log directory path is provided.",
            "ObservedBehavior": "The NodeManager fails to initialize and throws a YarnException indicating that the specified log directory path is invalid.",
            "AdditionalDetails": "The issue arises in the LocalDirsHandlerService class, specifically in the validatePaths method, which checks the validity of the log directory paths. The configuration should be corrected to provide a valid path."
        }
    },
    {
        "filename": "YARN-6448.json",
        "creation_time": "2017-04-05T18:39:49.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException in FairScheduler due to Comparator Violation",
            "Description": "An IllegalArgumentException is thrown during the sorting of nodes in the FairScheduler's continuous scheduling attempt. The exception indicates that the comparison method used in the sorting process violates its general contract, which can lead to unpredictable behavior in the scheduling algorithm.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:899)",
                "at java.util.TimSort.mergeAt(TimSort.java:516)",
                "at java.util.TimSort.mergeForceCollapse(TimSort.java:457)",
                "at java.util.TimSort.sort(TimSort.java:254)",
                "at java.util.Arrays.sort(Arrays.java:1512)",
                "at java.util.ArrayList.sort(ArrayList.java:1454)",
                "at java.util.Collections.sort(Collections.java:175)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)"
            ],
            "StepsToReproduce": [
                "1. Start the FairScheduler in a Hadoop YARN environment.",
                "2. Ensure that the nodes being tracked have a comparator that does not adhere to the general contract of the compare method.",
                "3. Observe the logs for the IllegalArgumentException during the continuous scheduling attempt."
            ],
            "ExpectedBehavior": "The nodes should be sorted without throwing any exceptions, allowing the FairScheduler to function correctly.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, indicating that the comparison method used in sorting violates its general contract, which disrupts the scheduling process.",
            "AdditionalDetails": "The issue likely stems from the comparator passed to the sortedNodeList method. It is essential to ensure that the comparator is consistent with the equals method and adheres to the contract defined in the Comparator interface."
        }
    },
    {
        "filename": "YARN-4530.json",
        "creation_time": "2015-12-30T15:19:19.000+0000",
        "bug_report": {
            "Title": "IOException due to Resource Modification Time Mismatch",
            "Description": "An IOException is thrown when attempting to copy a resource from HDFS to the local filesystem. The error indicates that the modification time of the resource on the source filesystem has changed since it was last checked, leading to a mismatch between the expected and actual timestamps.",
            "StackTrace": [
                "java.io.IOException: Resource hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar changed on src filesystem (expected 1451380519452, was 1451380611793",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:176)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:276)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:50)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)"
            ],
            "StepsToReproduce": [
                "1. Attempt to copy the resource located at 'hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar'.",
                "2. Ensure that the resource's modification time is changed on the source filesystem after the initial check.",
                "3. Observe the exception thrown during the copy operation."
            ],
            "ExpectedBehavior": "The resource should be copied successfully from HDFS to the local filesystem without any exceptions.",
            "ObservedBehavior": "An IOException is thrown indicating that the resource has changed on the source filesystem, leading to a modification time mismatch.",
            "AdditionalDetails": "The issue arises in the 'copy' method of the FSDownload class, where it checks the modification time of the resource against an expected timestamp. If they do not match, an IOException is thrown. This behavior is intended to prevent inconsistencies when resources are modified during the copy process."
        }
    },
    {
        "filename": "YARN-7737.json",
        "creation_time": "2018-01-11T19:35:01.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException when accessing container log file",
            "Description": "The application throws a FileNotFoundException when attempting to access a log file for a container that does not exist. This occurs during the container launch process, specifically when the system tries to retrieve the status of the log file associated with a container that has exited with a failure code.",
            "StackTrace": [
                "java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a container using the YARN resource manager.",
                "2. Ensure that the container fails during execution.",
                "3. Check the logs for the container after it has exited."
            ],
            "ExpectedBehavior": "The system should handle the absence of the log file gracefully, either by logging a warning or providing a default log output.",
            "ObservedBehavior": "The system throws a FileNotFoundException, causing the application to fail when it attempts to access a non-existent log file.",
            "AdditionalDetails": "The issue arises in the method 'deprecatedGetFileStatus' where it checks for the existence of the file. If the file does not exist, it throws a FileNotFoundException. This behavior is triggered during the handling of container exit codes in the 'handleContainerExitCode' method."
        }
    },
    {
        "filename": "YARN-5136.json",
        "creation_time": "2016-05-24T15:34:28.000+0000",
        "bug_report": {
            "Title": "IllegalStateException when removing non-existent application from FairScheduler",
            "Description": "An IllegalStateException is thrown when attempting to remove an application from the FairScheduler that does not exist in the specified queue. This indicates a potential issue with the application's lifecycle management or the state of the queue.",
            "StackTrace": [
                "java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the FairScheduler.",
                "2. Attempt to remove the application from the scheduler after it has already been removed or does not exist in the queue.",
                "3. Observe the IllegalStateException being thrown."
            ],
            "ExpectedBehavior": "The application should be removed from the scheduler without throwing an exception, or a proper message should indicate that the application does not exist.",
            "ObservedBehavior": "An IllegalStateException is thrown indicating that the application to be removed does not exist in the specified queue.",
            "AdditionalDetails": "The issue arises in the `removeApp` method of the `FSLeafQueue` class, where it checks for the existence of the application in the `runnableApps` and `nonRunnableApps` lists. If the application is not found in either list, it throws an IllegalStateException. This suggests that there may be a race condition or a logic error in the application lifecycle management."
        }
    },
    {
        "filename": "YARN-8211.json",
        "creation_time": "2018-04-26T02:13:22.000+0000",
        "bug_report": {
            "Title": "BufferUnderflowException in RegistryDNS when reading from SocketChannel",
            "Description": "A BufferUnderflowException occurs in the RegistryDNS class when attempting to read from a SocketChannel. This happens when the method getMessgeLength is called with a ByteBuffer that does not contain enough data to read the expected message length.",
            "StackTrace": [
                "java.nio.BufferUnderflowException",
                "at java.nio.Buffer.nextGetIndex(Buffer.java:500)",
                "at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the DNS service that utilizes the RegistryDNS class.",
                "2. Send a TCP request to the service that results in a response with a message length that exceeds the available data in the ByteBuffer.",
                "3. Observe the exception being thrown in the logs."
            ],
            "ExpectedBehavior": "The method should correctly read the message length from the ByteBuffer and handle cases where the buffer does not contain enough data without throwing an exception.",
            "ObservedBehavior": "A BufferUnderflowException is thrown when the method getMessgeLength attempts to read from the ByteBuffer that does not have enough data.",
            "AdditionalDetails": "The getMessgeLength method reads two bytes from the ByteBuffer to determine the message length. If the buffer does not contain at least two bytes, a BufferUnderflowException will occur. This indicates that the data being read is insufficient, which may be due to an error in the data transmission or handling."
        }
    },
    {
        "filename": "YARN-2124.json",
        "creation_time": "2014-06-05T07:44:27.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Resource Management during Preemption Policy Execution",
            "Description": "A NullPointerException occurs in the Resource Management module of the Hadoop YARN framework when executing the preemption policy. This issue arises specifically in the `greaterThan` method, which is called during the computation of ideal resource distribution.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager.",
                "2. Configure a preemption policy that utilizes the ProportionalCapacityPreemptionPolicy.",
                "3. Trigger a condition that requires resource preemption.",
                "4. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The system should compute the ideal resource distribution without throwing exceptions, allowing for proper resource preemption.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the preemption policy to fail and potentially leading to resource allocation issues.",
            "AdditionalDetails": "The issue likely stems from a null reference being passed to the `greaterThan` method, which is used to compare resource values. Further investigation is needed to identify which resource parameters are null during the execution of the preemption policy."
        }
    },
    {
        "filename": "YARN-7849.json",
        "creation_time": "2018-01-29T23:49:33.000+0000",
        "bug_report": {
            "Title": "AssertionError in Node Utilization Propagation Test",
            "Description": "The test 'testUpdateNodeUtilization' in the 'TestMiniYarnClusterNodeUtilization' class fails due to an AssertionError indicating that the expected containers utilization was not propagated to the RMNode. The expected utilization was <<pmem:1024, vmem:2048, vCores:11.0>>, but the actual value was <null>.",
            "StackTrace": [
                "java.lang.AssertionError: Containers Utillization not propagated to RMNode expected:<<pmem:1024, vmem:2048, vCores:11.0>> but was:<null>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.verifySimulatedUtilization(TestMiniYarnClusterNodeUtilization.java:227)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:116)"
            ],
            "StepsToReproduce": [
                "Run the test 'testUpdateNodeUtilization' in the 'TestMiniYarnClusterNodeUtilization' class.",
                "Ensure that the node status is updated with the expected containers utilization.",
                "Observe the failure due to the assertion in 'verifySimulatedUtilization' method."
            ],
            "ExpectedBehavior": "The containers utilization should be correctly propagated to the RMNode, and the assertion should pass without throwing an AssertionError.",
            "ObservedBehavior": "The containers utilization was expected to be <<pmem:1024, vmem:2048, vCores:11.0>>, but it was <null>, leading to an AssertionError.",
            "AdditionalDetails": "The failure occurs in the 'verifySimulatedUtilization' method, which checks if the node utilization is updated after a heartbeat. The method iterates over RMNodes and asserts that the aggregated containers utilization matches the expected value. The root cause may be related to the node status not being updated correctly or a failure in the heartbeat mechanism."
        }
    },
    {
        "filename": "YARN-8591.json",
        "creation_time": "2018-07-27T05:56:26.000+0000",
        "bug_report": {
            "Title": "NullPointerException in TimelineReaderWebServices during Entity Retrieval",
            "Description": "A NullPointerException is thrown in the TimelineReaderWebServices class when attempting to retrieve entities. This occurs during the handling of a web request, indicating that a required object is not initialized before being accessed.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter.doFilter(TimelineReaderWhitelistAuthorizationFilter.java:85)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:98)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1604)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)"
            ],
            "StepsToReproduce": [
                "Send a request to the TimelineReaderWebServices endpoint that triggers the getEntities method.",
                "Ensure that the request context is set up in a way that leads to a NullPointerException (e.g., missing required parameters or context)."
            ],
            "ExpectedBehavior": "The getEntities method should return a list of entities without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, resulting in a WebApplicationException being returned to the client.",
            "AdditionalDetails": "The NullPointerException is specifically occurring in the checkAccess method of the TimelineReaderWebServices class, indicating that an expected object is not initialized. Further investigation is needed to identify which object is null and under what conditions."
        }
    },
    {
        "filename": "YARN-6649.json",
        "creation_time": "2017-05-25T20:36:08.000+0000",
        "bug_report": {
            "Title": "RuntimeException: Unable to Encode Value Class from Code 1000 in TimelineWebServices",
            "Description": "A RuntimeException occurs in the TimelineWebServices class when attempting to retrieve an entity. The root cause is an IOException indicating that the system is unable to encode a value class from a specific code (1000). This issue arises during the deserialization process of an object, which suggests that there may be a mismatch in the expected class type or an issue with the serialization configuration.",
            "StackTrace": [
                "javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)",
                "at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:636)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:294)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:588)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:95)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1352)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.util.FSTUtil.rethrow(FSTUtil.java:122)",
                "at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:879)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:414)",
                "at org.apache.hadoop.yarn.server.timeline.EntityFileTimelineStore.getEntity(EntityFileTimelineStore.java:911)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.doGetEntity(TimelineDataManager.java:215)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getEntity(TimelineDataManager.java:202)",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:155)",
                "... 52 more",
                "Caused by: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:240)",
                "at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:877)",
                "... 58 more",
                "Caused by: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTClazzNameRegistry.decodeClass(FSTClazzNameRegistry.java:173)",
                "at org.nustaq.serialization.coders.FSTStreamDecoder.readClass(FSTStreamDecoder.java:431)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:338)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)",
                "at org.nustaq.serialization.serializers.FSTArrayListSerializer.instantiate(FSTArrayListSerializer.java:63)",
                "at org.nustaq.serialization.FSTObjectInput.instantiateAndReadWithSer(FSTObjectInput.java:459)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:354)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:304)",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:238)"
            ],
            "StepsToReproduce": [
                "Invoke the getEntity method in the TimelineWebServices class with an entity type and ID that triggers the serialization process.",
                "Ensure that the entity being retrieved has a class type that is not properly registered or is missing in the serialization configuration."
            ],
            "ExpectedBehavior": "The system should successfully retrieve the entity without throwing a RuntimeException, and the entity should be properly deserialized.",
            "ObservedBehavior": "A RuntimeException is thrown indicating an inability to encode a value class from code 1000, leading to an IOException.",
            "AdditionalDetails": "The issue may be related to the serialization configuration in the FST (Fast Serialization) library, particularly regarding the registration of classes and their corresponding codes."
        }
    },
    {
        "filename": "YARN-3742.json",
        "creation_time": "2015-05-29T06:00:38.000+0000",
        "bug_report": {
            "Title": "ZKClient Creation Timeout in ResourceManager",
            "Description": "The application encounters a timeout while waiting for the ZKClient to be created, leading to an IOException. This issue arises during the state update of an application in the ResourceManager, specifically when interacting with the ZKRMStateStore.",
            "StackTrace": [
                "java.io.IOException: Wait for ZKClient creation timed out",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with ZKRMStateStore configured.",
                "2. Attempt to update the application state while the ZKClient is not available or takes too long to initialize.",
                "3. Observe the logs for the IOException indicating a timeout."
            ],
            "ExpectedBehavior": "The ZKClient should be created successfully within the expected time frame, allowing the ResourceManager to update the application state without errors.",
            "ObservedBehavior": "The application fails to update its state due to a timeout while waiting for the ZKClient to be created, resulting in an IOException.",
            "AdditionalDetails": "The issue may be related to network latency or Zookeeper configuration. Further investigation into the ZKClient initialization process and its timeout settings may be necessary."
        }
    },
    {
        "filename": "YARN-4984.json",
        "creation_time": "2016-04-21T19:16:03.000+0000",
        "bug_report": {
            "Title": "HDFS Delegation Token Not Found in Cache",
            "Description": "The application encounters a RemoteException indicating that the HDFS delegation token cannot be found in the cache. This issue arises during the log aggregation process in the YARN NodeManager, specifically when attempting to check the existence of a file in HDFS.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be found in cache",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1427)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1358)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)",
                "at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)",
                "at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystem filesys, final Path path)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.access$100(LogAggregationService.java:67)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createAppDir(LogAggregationService.java:261)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator(LogAggregationService.java:367)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:320)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:447)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)"
            ],
            "StepsToReproduce": [
                "1. Start a YARN application that requires log aggregation.",
                "2. Ensure that the application attempts to write logs to HDFS.",
                "3. Monitor the NodeManager logs for any RemoteException related to HDFS delegation tokens."
            ],
            "ExpectedBehavior": "The application should successfully retrieve the HDFS delegation token from the cache and proceed with log aggregation without errors.",
            "ObservedBehavior": "The application fails with a RemoteException indicating that the specified HDFS delegation token cannot be found in the cache, preventing log aggregation from completing.",
            "AdditionalDetails": "This issue may be related to the lifecycle of the HDFS delegation tokens and their caching mechanism. It is important to ensure that tokens are valid and available in the cache when required by the log aggregation service."
        }
    },
    {
        "filename": "YARN-4584.json",
        "creation_time": "2016-01-12T09:08:31.000+0000",
        "bug_report": {
            "Title": "NullPointerException during ResourceManager Recovery",
            "Description": "A NullPointerException occurs in the ResourceManager when attempting to recover applications. This issue arises during the recovery process of the ResourceManager, specifically in the RMAppAttemptImpl.recover method, indicating that a required object is not initialized or is null.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover(RMAppAttemptImpl.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:953)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:946)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recoverApplication(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recover(RMAppManager.java:464)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1232)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:594)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1022)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1062)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1058)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1705)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1058)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:323)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:127)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:877)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager in a Hadoop YARN cluster.",
                "Trigger a recovery process for applications managed by the ResourceManager.",
                "Observe the logs for a NullPointerException during the recovery phase."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover all applications without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the recovery process, indicating that a required object is null.",
            "AdditionalDetails": "The issue likely stems from the RMAppAttemptImpl.recover method, which may be attempting to access an uninitialized object. Further investigation is needed to identify which specific object is null and under what conditions."
        }
    },
    {
        "filename": "YARN-2846.json",
        "creation_time": "2014-11-11T15:30:08.000+0000",
        "bug_report": {
            "Title": "IOException: Interrupted while waiting for process to exit in ContainerExecutor",
            "Description": "An IOException is thrown when the ContainerExecutor attempts to reacquire a container, indicating that the process was interrupted while waiting for it to exit. This is caused by an InterruptedException during a sleep operation in the reacquireContainer method.",
            "StackTrace": [
                "java.io.IOException: Interrupted while waiting for process 20001 to exit",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:46)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)"
            ],
            "StepsToReproduce": [
                "1. Start a container using the YARN NodeManager.",
                "2. Simulate an interruption during the container's execution (e.g., by sending an interrupt signal).",
                "3. Observe the logs for the IOException indicating the process was interrupted."
            ],
            "ExpectedBehavior": "The ContainerExecutor should successfully reacquire the container and wait for it to exit without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating that the process was interrupted while waiting for it to exit, leading to potential issues in container management.",
            "AdditionalDetails": "The issue arises in the reacquireContainer method, specifically during the sleep operation that is interrupted. This could be due to external factors such as system resource constraints or manual interruption."
        }
    },
    {
        "filename": "YARN-7890.json",
        "creation_time": "2018-02-03T21:10:43.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ContainerStartContext.getFilecacheDirs()",
            "Description": "A NullPointerException is thrown when attempting to retrieve the file cache directories in the ContainerStartContext class. This occurs during the container launch process, specifically when the method getFilecacheDirs() is called, which attempts to return an unmodifiable list of file cache directories. If the filecacheDirs list is null, this results in a NullPointerException.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)",
                "at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)",
                "at java.util.Collections.unmodifiableList(Collections.java:1287)",
                "at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:49)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "Attempt to launch a container using the LinuxContainerExecutor.",
                "Ensure that the filecacheDirs property in ContainerStartContext is not initialized (i.e., it is null).",
                "Observe the logs for a NullPointerException when getFilecacheDirs() is called."
            ],
            "ExpectedBehavior": "The method getFilecacheDirs() should return an unmodifiable list of file cache directories without throwing an exception, even if the list is empty.",
            "ObservedBehavior": "A NullPointerException is thrown when getFilecacheDirs() is called, indicating that the filecacheDirs list is null.",
            "AdditionalDetails": "The getFilecacheDirs() method does not handle the case where filecacheDirs is null. It should be initialized to an empty list if no directories are set, to prevent this exception from occurring."
        }
    },
    {
        "filename": "YARN-139.json",
        "creation_time": "2012-10-01T19:51:20.000+0000",
        "bug_report": {
            "Title": "InterruptedException during AsyncDispatcher stop process",
            "Description": "An InterruptedException is thrown when attempting to stop the AsyncDispatcher, indicating that the thread was interrupted while waiting for another thread to join. This issue arises during the shutdown process of the MRAppMaster, specifically when handling job finish events.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "at java.lang.Thread.join(Thread.java:1196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:105)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:437)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:402)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start a job using MRAppMaster.",
                "2. Allow the job to complete.",
                "3. Trigger the shutdown process of the AsyncDispatcher.",
                "4. Observe the logs for InterruptedException."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should stop gracefully without throwing an InterruptedException.",
            "ObservedBehavior": "An InterruptedException is thrown during the stop process of the AsyncDispatcher, indicating that the thread was interrupted while waiting.",
            "AdditionalDetails": "The stop method in AsyncDispatcher attempts to join threads that may have been interrupted, leading to this exception. The service's state should be checked to ensure it is not already stopped before attempting to stop it."
        }
    },
    {
        "filename": "YARN-42.json",
        "creation_time": "2012-05-14T11:38:55.000+0000",
        "bug_report": {
            "Title": "YarnException: Failed to initialize LocalizationService due to IOException",
            "Description": "The application fails to initialize the LocalizationService in the Yarn NodeManager due to an IOException when attempting to create a directory for user cache. This results in a failure to start the NodeManager, which is critical for managing container resources.",
            "StackTrace": [
                "org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:166)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:268)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:284)",
                "Caused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)",
                "at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2325)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)",
                "... 6 more",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop(NonAggregatingLogHandler.java:82)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stop(ContainerManagerImpl.java:266)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:182)",
                "at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "StepsToReproduce": [
                "Start the Yarn NodeManager service.",
                "Ensure that the directory /mrv2/tmp/nm-local-dir/usercache is not accessible or does not exist.",
                "Observe the logs for the Yarn NodeManager."
            ],
            "ExpectedBehavior": "The LocalizationService should initialize successfully, allowing the NodeManager to start without errors.",
            "ObservedBehavior": "The LocalizationService fails to initialize due to an IOException when trying to create the user cache directory, leading to a failure in starting the NodeManager.",
            "AdditionalDetails": "The IOException indicates that the application does not have the necessary permissions to create the directory or the path is invalid. Further investigation into the filesystem permissions and the existence of the specified path is required."
        }
    },
    {
        "filename": "YARN-7453.json",
        "creation_time": "2017-11-07T09:46:28.000+0000",
        "bug_report": {
            "Title": "NoAuthException during ResourceManager transition to active state",
            "Description": "The ResourceManager fails to transition to the active state due to a NoAuthException from ZooKeeper. This indicates that the operation attempted requires authentication, which is not provided or is invalid.",
            "StackTrace": [
                "org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1006)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:910)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:159)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.access$200(CuratorTransactionImpl.java:44)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:129)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:125)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:122)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction.commit(ZKCuratorManager.java:403)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager.safeSetData(ZKCuratorManager.java:372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.getAndIncrementEpoch(ZKRMStateStore.java:493)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1202)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1198)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1198)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:607)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager in a high-availability (HA) configuration.",
                "Attempt to transition the ResourceManager to the active state.",
                "Ensure that the ZooKeeper ensemble is configured with authentication enabled."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the active state without any authentication errors.",
            "ObservedBehavior": "The ResourceManager fails to transition to the active state, throwing a NoAuthException indicating insufficient authentication.",
            "AdditionalDetails": "The issue may be related to the ZooKeeper ACLs or the authentication configuration in the Hadoop cluster. Ensure that the correct credentials are provided and that the ResourceManager has the necessary permissions to perform the required operations."
        }
    },
    {
        "filename": "YARN-3369.json",
        "creation_time": "2015-03-18T23:29:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AppSchedulingInfo during Resource Allocation",
            "Description": "A NullPointerException occurs in the AppSchedulingInfo class when attempting to check for application deactivation during resource allocation. This issue arises when the method checkForDeactivation is called, and it attempts to access a null reference, leading to the exception.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Trigger resource allocation in the YARN ResourceManager.",
                "2. Ensure that the application has no active resource requests.",
                "3. Observe the logs for a NullPointerException in the AppSchedulingInfo class."
            ],
            "ExpectedBehavior": "The application should be deactivated without throwing a NullPointerException, allowing for proper resource management.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the resource allocation process to fail.",
            "AdditionalDetails": "The checkForDeactivation method checks for active resource requests but may not handle cases where the priorities or resource requests are null, leading to the exception. Further investigation is needed to ensure that all necessary objects are properly initialized before this method is called."
        }
    },
    {
        "filename": "YARN-945.json",
        "creation_time": "2013-07-19T22:59:06.000+0000",
        "bug_report": {
            "Title": "AccessControlException: SIMPLE authentication is not enabled",
            "Description": "The application encounters an AccessControlException indicating that SIMPLE authentication is not enabled when attempting to initialize the authentication context during an IPC connection. This issue arises in the context of Hadoop's IPC server communication.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN]",
                "at org.apache.hadoop.ipc.Server$Connection.initializeAuthContext(Server.java:1531)",
                "at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1482)",
                "at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:788)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:587)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:562)"
            ],
            "StepsToReproduce": [
                "Attempt to connect to the Hadoop IPC server without enabling SIMPLE authentication.",
                "Ensure that the server is configured to require authentication.",
                "Observe the server logs for the AccessControlException."
            ],
            "ExpectedBehavior": "The server should allow connections with SIMPLE authentication enabled, or provide a clear error message indicating the need for authentication.",
            "ObservedBehavior": "The server throws an AccessControlException stating that SIMPLE authentication is not enabled, preventing the connection from being established.",
            "AdditionalDetails": "The issue likely stems from the method 'initializeAuthContext(int authType)' being called with an unsupported authentication type. The server's configuration should be checked to ensure that SIMPLE authentication is enabled if it is required."
        }
    },
    {
        "filename": "YARN-6072.json",
        "creation_time": "2017-01-08T09:21:12.000+0000",
        "bug_report": {
            "Title": "NullPointerException during ResourceManager transition to Active",
            "Description": "A NullPointerException occurs in the AdminService class when attempting to refresh service ACLs during the transition of the ResourceManager to an active state. This leads to a ServiceFailedException, indicating that the ResourceManager could not transition to Active.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:552)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "org.apache.hadoop.ha.ServiceFailedException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)",
                "        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Error on refreshAll during transition to Active",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:311)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "... 4 more",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "... 5 more"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager in a high-availability setup.",
                "2. Trigger a transition to the active state for the ResourceManager.",
                "3. Observe the logs for any NullPointerException during the transition."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the active state without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, leading to a ServiceFailedException indicating that the ResourceManager could not transition to Active.",
            "AdditionalDetails": "The issue seems to originate from the refreshServiceAcls method, which may be attempting to access a null object. Further investigation is needed to identify the specific object that is null."
        }
    },
    {
        "filename": "YARN-7663.json",
        "creation_time": "2017-12-15T01:52:46.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException when transitioning application state to START from KILLED",
            "Description": "An InvalidStateTransitionException is thrown when attempting to transition an application state to START while it is currently in the KILLED state. This indicates that the application is not allowed to transition to the START state from KILLED, which is a violation of the expected state machine behavior.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: START at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:805)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:885)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the YARN ResourceManager.",
                "2. Transition the application to the KILLED state.",
                "3. Attempt to transition the application to the START state."
            ],
            "ExpectedBehavior": "The application should not be allowed to transition to the START state from the KILLED state, and an appropriate error message should be logged without throwing an exception.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the transition from KILLED to START is invalid.",
            "AdditionalDetails": "The state machine for the application does not allow transitioning from KILLED to START, which is expected behavior. The application should handle this scenario gracefully without throwing an exception."
        }
    },
    {
        "filename": "YARN-5873.json",
        "creation_time": "2016-11-12T09:54:20.000+0000",
        "bug_report": {
            "Title": "NullPointerException in WritingContainerStartEvent.hashCode()",
            "Description": "A NullPointerException occurs in the hashCode method of the WritingContainerStartEvent class when attempting to access the application ID of a container that is not properly initialized. This issue arises during the handling of container start events in the ResourceManager's application history writer.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:210)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.doAllocation(RegularContainerAllocator.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:832)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:865)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:931)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:690)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:508)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1475)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1470)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1346)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1601)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:149)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with a configuration that allows container allocation.",
                "2. Trigger the allocation of a container that has not been properly initialized.",
                "3. Observe the logs for a NullPointerException in the WritingContainerStartEvent.hashCode() method."
            ],
            "ExpectedBehavior": "The system should handle container start events without throwing a NullPointerException, ensuring that all necessary fields are properly initialized before being accessed.",
            "ObservedBehavior": "A NullPointerException is thrown when the hashCode method of WritingContainerStartEvent attempts to access the application ID of a container that is not initialized, leading to a failure in processing container start events.",
            "AdditionalDetails": "The issue likely stems from the container not being fully initialized before the WritingContainerStartEvent is created. This could be due to a race condition or improper handling of container states during allocation."
        }
    },
    {
        "filename": "YARN-3227.json",
        "creation_time": "2015-02-19T16:58:01.000+0000",
        "bug_report": {
            "Title": "IOException during Delegation Token Renewal",
            "Description": "An IOException is thrown when attempting to renew a delegation token due to an unauthorized HTTP response. This issue occurs in the DelegationTokenRenewer class when handling application submission events.",
            "StackTrace": [
                "java.io.IOException: Failed to renew token: Kind: TIMELINE_DELEGATION_TOKEN, Service: timelineserver.example.com:4080, Ident: (owner=user, renewer=rmuser, realUser=oozie, issueDate=1423248845528, maxDate=1423853645528, sequenceNumber=9716, masterKeyId=9)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:443)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:808)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:789)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: java.io.IOException: HTTP status [401], message [Unauthorized]",
                "at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:286)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.renewDelegationToken(DelegationTokenAuthenticator.java:211)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.renewDelegationToken(DelegationTokenAuthenticatedURL.java:414)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:374)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:360)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$4.run(TimelineClientImpl.java:429)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:161)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.operateDelegationToken(TimelineClientImpl.java:444)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.renewDelegationToken(TimelineClientImpl.java:378)",
                "at org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier$Renewer.renew(TimelineDelegationTokenIdentifier.java:81)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:532)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:529)"
            ],
            "StepsToReproduce": [
                "Submit an application that requires a timeline delegation token.",
                "Ensure that the token is valid and has not expired.",
                "Trigger the renewal process for the delegation token."
            ],
            "ExpectedBehavior": "The delegation token should be renewed successfully without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown with an HTTP 401 Unauthorized status, indicating that the renewal process failed.",
            "AdditionalDetails": "The issue may be related to incorrect credentials or permissions for the user attempting to renew the token. Further investigation into the authentication mechanism and user permissions is required."
        }
    },
    {
        "filename": "YARN-4235.json",
        "creation_time": "2015-10-07T19:26:24.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException in FairScheduler when assigning application to queue",
            "Description": "An IndexOutOfBoundsException occurs in the FairScheduler component of the Hadoop YARN ResourceManager when attempting to assign an application to a queue. The exception is thrown because the code attempts to access an element from an empty list, indicating that the expected queue configuration is missing or not properly initialized.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager with an empty queue configuration.",
                "2. Submit an application to the ResourceManager.",
                "3. Observe the logs for the IndexOutOfBoundsException."
            ],
            "ExpectedBehavior": "The application should be assigned to a valid queue based on the configured queue placement rules.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown, indicating that the application could not be assigned to any queue due to an empty list of configured queues.",
            "AdditionalDetails": "The issue seems to stem from the method 'getQueueForApp' in the 'QueuePlacementRule$PrimaryGroup' class, which attempts to access the first element of a list that is empty. This suggests that the queue configuration is not being properly initialized or is missing entirely."
        }
    },
    {
        "filename": "YARN-4833.json",
        "creation_time": "2016-03-17T13:22:23.000+0000",
        "bug_report": {
            "Title": "AccessControlException when submitting application to YARN queue",
            "Description": "The application submission fails with an AccessControlException indicating that the user 'hdfs' does not have permission to submit the application to the default queue. This issue arises during the application submission process in a Hadoop YARN environment.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.submitApplication:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:483)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2360)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2356)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2356)",
                "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.submitApplication:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1449)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1386)"
            ],
            "StepsToReproduce": [
                "1. Attempt to submit an application to the YARN resource manager using the user 'hdfs'.",
                "2. Ensure that the application is configured to submit to the default queue."
            ],
            "ExpectedBehavior": "The application should be submitted successfully to the YARN queue without any permission errors.",
            "ObservedBehavior": "The application submission fails with an AccessControlException indicating that the user 'hdfs' does not have permission to submit to the default queue.",
            "AdditionalDetails": "This issue may be related to the configuration of YARN queues and user permissions. It is recommended to check the YARN configuration files (e.g., capacity-scheduler.xml) to ensure that the user 'hdfs' has the necessary permissions to submit applications to the default queue."
        }
    },
    {
        "filename": "YARN-1689.json",
        "creation_time": "2014-02-05T19:16:00.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AbstractYarnScheduler during Application Master Registration",
            "Description": "A NullPointerException occurs in the AbstractYarnScheduler class when attempting to retrieve transferred containers during the registration of an application master. This issue arises when the application master is in an invalid state, leading to an InvalidStateTransitonException.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)",
                "        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)",
                "        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)",
                "        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "        at java.security.AccessController.doPrivileged(Native Method)",
                "        at javax.security.auth.Subject.doAs(Subject.java:396)",
                "        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED",
                "        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:624)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:81)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:656)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:640)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "        at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Attempt to register an application master when the application is in a KILLED state.",
                "2. Observe the logs for a NullPointerException in AbstractYarnScheduler."
            ],
            "ExpectedBehavior": "The application master should register successfully without throwing a NullPointerException, even if the application is in a KILLED state.",
            "ObservedBehavior": "A NullPointerException is thrown in AbstractYarnScheduler when trying to get transferred containers during the registration of the application master, leading to an InvalidStateTransitonException.",
            "AdditionalDetails": "The issue seems to stem from the state management of the application, where an invalid event is being processed while the application is in a KILLED state. This indicates a potential flaw in the state transition logic."
        }
    },
    {
        "filename": "YARN-5594.json",
        "creation_time": "2016-08-30T15:14:19.000+0000",
        "bug_report": {
            "Title": "InvalidProtocolBufferException during ResourceManager State Load",
            "Description": "The ResourceManager fails to start due to an InvalidProtocolBufferException caused by an invalid tag (zero) in the protocol message. This occurs when attempting to read the RMDelegationTokenIdentifierData from the state store.",
            "StackTrace": [
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).",
                "at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4644)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4740)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4735)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:5075)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:4955)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:337)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:267)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:210)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadState(FileSystemRMStateStore.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1007)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1044)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service.",
                "Ensure that the state store contains an invalid RMDelegationTokenIdentifierData.",
                "Observe the logs for the InvalidProtocolBufferException."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully and load its state from the state store without any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to start and throws an InvalidProtocolBufferException due to an invalid tag in the protocol message.",
            "AdditionalDetails": "The issue likely arises from corrupted or improperly formatted data in the state store, specifically related to RMDelegationTokenIdentifierData. The method 'loadRMDTSecretManagerState' attempts to read this data, leading to the exception."
        }
    },
    {
        "filename": "YARN-7511.json",
        "creation_time": "2017-11-16T11:41:43.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Resource Localization Failure Handling",
            "Description": "A NullPointerException occurs in the resource localization failure handling process within the YARN NodeManager. This issue arises when attempting to remove a resource from a ConcurrentHashMap, which indicates that the resource being referenced is null.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)",
                "at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": [
                "Trigger a resource localization failure in the YARN NodeManager.",
                "Ensure that the LocalResourceRequest being processed is null or improperly initialized.",
                "Observe the logs for a NullPointerException in the stack trace provided."
            ],
            "ExpectedBehavior": "The system should gracefully handle resource localization failures without throwing a NullPointerException, ensuring that all resources are properly managed.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to remove a null resource from the ConcurrentHashMap, leading to a failure in the resource localization process.",
            "AdditionalDetails": "The method 'resourceLocalizationFailed(LocalResourceRequest request)' attempts to remove a resource from 'pendingResources' without checking if 'request' is null. This indicates a potential flaw in the error handling logic."
        }
    },
    {
        "filename": "YARN-3790.json",
        "creation_time": "2015-06-10T04:53:40.000+0000",
        "bug_report": {
            "Title": "AssertionError in TestWorkPreservingRMRestart: Expected Memory Allocation Mismatch",
            "Description": "The test case 'testSchedulerRecovery' in the 'TestWorkPreservingRMRestart' class is failing due to an assertion error. The expected memory allocation of 6144 MB does not match the actual allocation of 8192 MB, indicating a potential issue in resource management during the recovery of applications in the ResourceManager.",
            "StackTrace": [
                "java.lang.AssertionError: expected:<6144> but was:<8192>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.junit.Assert.assertEquals(Assert.java:555)",
                "at org.junit.Assert.assertEquals(Assert.java:542)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.assertMetrics(TestWorkPreservingRMRestart.java:853)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.checkFSQueue(TestWorkPreservingRMRestart.java:342)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.testSchedulerRecovery(TestWorkPreservingRMRestart.java:241)"
            ],
            "StepsToReproduce": [
                "Run the test case 'testSchedulerRecovery' in the 'TestWorkPreservingRMRestart' class.",
                "Ensure that the ResourceManager is configured with the appropriate settings for memory allocation.",
                "Observe the assertion failure due to the mismatch in expected and actual memory allocation."
            ],
            "ExpectedBehavior": "The expected memory allocation for the application should be 6144 MB after recovery, matching the assertion in the test case.",
            "ObservedBehavior": "The actual memory allocation reported is 8192 MB, leading to an AssertionError in the test case.",
            "AdditionalDetails": "The issue may stem from the resource recovery logic in the ResourceManager, particularly in how it calculates available resources after restarting. The test case relies on the correct implementation of resource metrics, which may not be accurately reflecting the state post-recovery."
        }
    },
    {
        "filename": "YARN-6068.json",
        "creation_time": "2017-01-07T03:16:07.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException during Application Log Handling",
            "Description": "An InvalidStateTransitionException is thrown when the application attempts to handle an event related to application log handling while in the RUNNING state. This indicates that the application is trying to process an event that is not valid for its current state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start an application in the YARN cluster.",
                "2. Trigger an event related to application log handling while the application is in the RUNNING state.",
                "3. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should handle the log handling event appropriately without throwing an exception, or it should transition to a valid state that allows for log handling.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the application cannot handle the APPLICATION_LOG_HANDLING_FAILED event while in the RUNNING state.",
            "AdditionalDetails": "The exception is thrown from the doTransition method in the StateMachineFactory, which suggests that the event type APPLICATION_LOG_HANDLING_FAILED is not valid for the RUNNING state. This may require a review of the state transition logic to ensure that all valid events are accounted for in the RUNNING state."
        }
    },
    {
        "filename": "YARN-903.json",
        "creation_time": "2013-07-07T08:35:30.000+0000",
        "bug_report": {
            "Title": "YarnException: Container not handled by NodeManager",
            "Description": "The application encounters a YarnException indicating that certain containers are not handled by the NodeManager. This issue arises when attempting to stop containers that are not recognized by the NodeManager, leading to failures in container management.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000002 is not handled by this NodeManager",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)",
                "at org.apache.hadoop.yarn.proto.ContainerManagementProtocol$ContainerManagementProtocolService$2.callBlockingMethod(ContainerManagementProtocol.java:85)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1868)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1864)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1862)",
                "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000001 is not handled by this NodeManager",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)",
                "at org.apache.hadoop.yarn.proto.ContainerManagementProtocol$ContainerManagementProtocolService$2.callBlockingMethod(ContainerManagementProtocol.java:85)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1868)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1864)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1862)",
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)",
                "at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)",
                "at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:281)"
            ],
            "StepsToReproduce": [
                "Attempt to stop a container that is not recognized by the NodeManager.",
                "Monitor the logs for YarnException indicating the container is not handled."
            ],
            "ExpectedBehavior": "The NodeManager should successfully stop the specified containers if they are recognized and managed by it.",
            "ObservedBehavior": "The NodeManager throws a YarnException stating that the specified containers are not handled, resulting in a failure to stop them.",
            "AdditionalDetails": "This issue may occur due to misconfiguration of the NodeManager or if the containers were started on a different NodeManager instance."
        }
    },
    {
        "filename": "YARN-8236.json",
        "creation_time": "2018-04-29T16:28:11.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ServiceClient when adding keytab resource",
            "Description": "A NullPointerException is thrown in the ServiceClient class when attempting to add a keytab resource during the submission of an application. This issue occurs in the addKeytabResourceIfSecure method, indicating that a required object is not properly initialized before being accessed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269)"
            ],
            "StepsToReproduce": [
                "1. Initialize a ServiceClient instance.",
                "2. Create a Service object without properly initializing its keytab resource.",
                "3. Call the submitApp method on the ServiceClient instance with the uninitialized Service object."
            ],
            "ExpectedBehavior": "The application should be submitted successfully without throwing a NullPointerException, even if the keytab resource is not provided.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a required object is null when attempting to add the keytab resource.",
            "AdditionalDetails": "The addKeytabResourceIfSecure method is expected to handle cases where the keytab resource is not provided. It should check for null values before attempting to access any properties or methods on the resource."
        }
    },
    {
        "filename": "YARN-2857.json",
        "creation_time": "2014-10-24T20:47:51.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in ContainerLogAppender during Log Configuration",
            "Description": "A ConcurrentModificationException is thrown when attempting to close the ContainerLogAppender. This occurs due to concurrent modifications to a LinkedList while iterating over it, specifically in the close() method of the ContainerLogAppender class.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)",
                "at org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)",
                "at org.apache.log4j.Category.removeAllAppenders(Category.java:891)",
                "at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:759)",
                "at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)",
                "at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)",
                "at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440)",
                "at org.apache.pig.Main.configureLog4J(Main.java:740)",
                "at org.apache.pig.Main.run(Main.java:384)",
                "at org.apache.pig.PigRunner.run(PigRunner.java:49)",
                "at org.apache.oozie.action.hadoop.PigMain.runPigJob(PigMain.java:283)",
                "at org.apache.oozie.action.hadoop.PigMain.run(PigMain.java:223)",
                "at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:37)",
                "at org.apache.oozie.action.hadoop.PigMain.main(PigMain.java:76)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:226)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "1. Start a Pig job that utilizes the ContainerLogAppender.",
                "2. Ensure that multiple threads are logging events concurrently.",
                "3. Observe the logs during the shutdown process of the Pig job."
            ],
            "ExpectedBehavior": "The ContainerLogAppender should close without throwing any exceptions, ensuring all logging events are processed correctly.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, indicating that the LinkedList is being modified while it is being iterated over, leading to potential loss of log events.",
            "AdditionalDetails": "The close() method in the ContainerLogAppender class attempts to iterate over the 'tail' LinkedList while it may be concurrently modified by other threads. This can be resolved by ensuring proper synchronization or using a thread-safe collection."
        }
    },
    {
        "filename": "YARN-2416.json",
        "creation_time": "2014-08-13T22:36:31.000+0000",
        "bug_report": {
            "Title": "Invalid State Transition Exceptions in ResourceManager",
            "Description": "The ResourceManager is encountering multiple InvalidStateTransitionExceptions when handling application attempt events. The exceptions indicate that certain events (REGISTERED, STATUS_UPDATE, CONTAINER_ALLOCATED) are being processed while the application attempt is in the ALLOCATED state, which is not a valid transition for these events.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: REGISTERED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "Submit an application to the ResourceManager.",
                "Ensure the application transitions to the ALLOCATED state.",
                "Trigger events such as REGISTERED, STATUS_UPDATE, or CONTAINER_ALLOCATED for the application attempt."
            ],
            "ExpectedBehavior": "The ResourceManager should handle application attempt events appropriately without throwing InvalidStateTransitionExceptions.",
            "ObservedBehavior": "The ResourceManager throws InvalidStateTransitionExceptions when handling certain events while in the ALLOCATED state.",
            "AdditionalDetails": "The handle method in RMAppAttemptImpl attempts to transition the state machine with events that are not valid for the current state. The state machine's valid transitions need to be reviewed to ensure that events are only sent when in the correct state."
        }
    },
    {
        "filename": "YARN-345.json",
        "creation_time": "2013-01-17T12:57:46.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException in YARN Application State Management",
            "Description": "The application encounters an InvalidStateTransitionException when attempting to process certain events in the YARN application state management. This occurs when the application is in a state where it cannot handle the specified event, leading to application failures.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at APPLICATION_RESOURCES_CLEANINGUP",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHING_CONTAINERS_WAIT",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_CONTAINER_FINISHED at FINISHED",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: INIT_CONTAINER at FINISHED"
            ],
            "StepsToReproduce": [
                "1. Submit a YARN application that transitions through multiple states.",
                "2. Trigger events that are not valid for the current application state, such as FINISH_APPLICATION or APPLICATION_CONTAINER_FINISHED when the application is already in a FINISHED state.",
                "3. Observe the logs for InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should handle state transitions smoothly without throwing exceptions, even when events are triggered.",
            "ObservedBehavior": "The application throws InvalidStateTransitionException when attempting to process events that are not valid for the current state, leading to potential application failures.",
            "AdditionalDetails": "The issue arises from the state machine's inability to handle certain events in specific states. The 'handle' method in ApplicationImpl attempts to transition states based on events, but if the event is invalid for the current state, it results in an exception. This indicates a need for better state management and validation of events before processing."
        }
    },
    {
        "filename": "YARN-3894.json",
        "creation_time": "2015-07-08T07:00:51.000+0000",
        "bug_report": {
            "Title": "IOException during CapacityScheduler reinitialization due to illegal queue capacity",
            "Description": "The system encounters an IOException when attempting to reinitialize the queues in the CapacityScheduler. The root cause is an IllegalArgumentException triggered by an illegal capacity value of 0.5 for the child queues of the root queue labeled 'node2'. This prevents the ResourceManager from transitioning to Active mode, leading to a ServiceFailedException.",
            "StackTrace": [
                "java.io.IOException: Failed to re-init queues",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:383)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:376)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:605)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "Caused by: java.lang.IllegalArgumentException: Illegal capacity of 0.5 for children of queue root for label=node2",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues(ParentQueue.java:159)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:639)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:503)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:379)",
                "    ... 8 more",
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Error when transitioning to Active mode",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:321)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "    ... 4 more",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: java.io.IOException: Failed to re-init queues",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:617)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)"
            ],
            "StepsToReproduce": [
                "Attempt to reinitialize the CapacityScheduler with a configuration that includes a child queue with a capacity of 0.5 for the root queue labeled 'node2'.",
                "Trigger the transition of the ResourceManager to Active mode."
            ],
            "ExpectedBehavior": "The CapacityScheduler should reinitialize successfully without throwing an IOException, allowing the ResourceManager to transition to Active mode.",
            "ObservedBehavior": "An IOException is thrown during the reinitialization of queues, preventing the ResourceManager from transitioning to Active mode.",
            "AdditionalDetails": "The issue arises from the configuration of queue capacities, specifically the child queue under the root queue. The capacity value of 0.5 is not acceptable, leading to the IllegalArgumentException."
        }
    },
    {
        "filename": "YARN-1903.json",
        "creation_time": "2014-04-04T20:51:24.000+0000",
        "bug_report": {
            "Title": "AssertionError in TestNMClient during Container Status Check",
            "Description": "An AssertionError is thrown in the TestNMClient class when the testGetContainerStatus method is executed. This indicates that the expected condition in the test is not met, leading to a failure in the unit test for container management in the YARN client.",
            "StackTrace": [
                "java.lang.AssertionError: 4: ",
                "at org.junit.Assert.fail(Assert.java:93)",
                "at org.junit.Assert.assertTrue(Assert.java:43)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testGetContainerStatus(TestNMClient.java:382)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testContainerManagement(TestNMClient.java:346)",
                "at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient(TestNMClient.java:226)"
            ],
            "StepsToReproduce": [
                "Run the unit tests in the TestNMClient class.",
                "Ensure that the testGetContainerStatus method is invoked.",
                "Observe the output for any AssertionError."
            ],
            "ExpectedBehavior": "The testGetContainerStatus method should pass without throwing an AssertionError, indicating that the container status is as expected.",
            "ObservedBehavior": "An AssertionError is thrown, indicating that the expected condition (likely related to the container status) is not met.",
            "AdditionalDetails": "The testGetContainerStatus method is likely checking the status of a container against expected values. The failure suggests that the actual status does not match the expected state, which could be due to incorrect container state management or setup in the test."
        }
    },
    {
        "filename": "YARN-4347.json",
        "creation_time": "2015-11-11T22:32:59.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CapacityScheduler during Application Attempt Recovery",
            "Description": "A NullPointerException occurs in the CapacityScheduler class when attempting to add an application attempt during the recovery process. This issue arises when the application attempt ID is null, leading to a failure in the addApplicationAttempt method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:755)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:839)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:854)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:844)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:719)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:411)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1219)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:593)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1026)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1067)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1063)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service.",
                "Trigger the recovery process for applications with attempts.",
                "Observe the logs for NullPointerException in the CapacityScheduler."
            ],
            "ExpectedBehavior": "The CapacityScheduler should successfully add application attempts during the recovery process without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the addApplicationAttempt method of the CapacityScheduler, indicating that an application attempt ID is null.",
            "AdditionalDetails": "The issue likely stems from the RMAppAttemptImpl not being properly initialized or having a null application attempt ID during the recovery process. Further investigation into the state of the application attempts during recovery is needed."
        }
    },
    {
        "filename": "YARN-1692.json",
        "creation_time": "2014-02-07T02:01:17.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in FairScheduler during Demand Update",
            "Description": "A ConcurrentModificationException is thrown when the FairScheduler attempts to update the demand for resources while iterating over the child queues. This issue arises due to concurrent modifications of the childQueues collection, which is not properly synchronized.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:954)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Start the FairScheduler with multiple queues and applications.",
                "2. Trigger concurrent updates to the child queues while the FairScheduler is running.",
                "3. Monitor the logs for the ConcurrentModificationException."
            ],
            "ExpectedBehavior": "The FairScheduler should update the demand for resources without throwing a ConcurrentModificationException, ensuring that all child queues are processed correctly.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, indicating that the childQueues collection was modified while it was being iterated over, leading to a failure in updating resource demands.",
            "AdditionalDetails": "The issue likely stems from the lack of synchronization when accessing the childQueues collection in the updateDemand() method. Consider using a synchronized collection or implementing proper locking mechanisms to prevent concurrent modifications."
        }
    },
    {
        "filename": "YARN-7697.json",
        "creation_time": "2018-01-03T19:28:50.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError in LogAggregationIndexedFileController",
            "Description": "The application encounters a java.lang.OutOfMemoryError while attempting to load indexed logs metadata in the LogAggregationIndexedFileController. This issue arises during the log aggregation process, specifically when initializing the writer for rolling logs.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:823)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:840)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling(LogAggregationIndexedFileController.java:293)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.access$600(LogAggregationIndexedFileController.java:98)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:216)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:205)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:312)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:284)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN application with a large number of containers or extensive log data.",
                "2. Trigger the log aggregation process.",
                "3. Monitor the application for memory usage.",
                "4. Observe the OutOfMemoryError occurring during the log aggregation phase."
            ],
            "ExpectedBehavior": "The log aggregation process should complete successfully without running out of memory, allowing logs to be uploaded for all containers.",
            "ObservedBehavior": "The application throws a java.lang.OutOfMemoryError, indicating that the Java heap space is insufficient to handle the log aggregation process.",
            "AdditionalDetails": "The issue may be related to the size of the logs being processed or the configuration of the Java heap space. It is recommended to review the memory settings and consider optimizing the log aggregation process to handle larger datasets."
        }
    },
    {
        "filename": "YARN-7382.json",
        "creation_time": "2017-10-23T23:36:59.000+0000",
        "bug_report": {
            "Title": "NoSuchElementException in FairScheduler during Container Assignment",
            "Description": "A NoSuchElementException is thrown when the FairScheduler attempts to retrieve the next pending ask for resource allocation, indicating that the schedulerKeys collection is empty. This occurs during the container assignment process, leading to a failure in scheduling applications.",
            "StackTrace": [
                "java.util.NoSuchElementException",
                "at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)",
                "at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:128)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager with a configured FairScheduler.",
                "2. Submit an application that requires resource allocation.",
                "3. Monitor the ResourceManager logs for scheduling events.",
                "4. Observe the logs for the occurrence of NoSuchElementException."
            ],
            "ExpectedBehavior": "The FairScheduler should successfully retrieve the next pending ask and allocate resources to the application without throwing an exception.",
            "ObservedBehavior": "The FairScheduler throws a NoSuchElementException when attempting to retrieve the next pending ask, indicating that there are no pending requests available for scheduling.",
            "AdditionalDetails": "The issue arises in the getNextPendingAsk() method of AppSchedulingInfo, which attempts to access the first key of an empty schedulerKeys collection. This suggests that the application may not have made any resource requests, or that the requests were cleared before the scheduling attempt."
        }
    },
    {
        "filename": "YARN-1094.json",
        "creation_time": "2013-08-23T19:06:17.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DelegationTokenRenewer during Application Submission",
            "Description": "A NullPointerException occurs in the DelegationTokenRenewer class when attempting to set a timer for token renewal during the application submission process. This issue arises when the ResourceManager attempts to recover its state and submit applications, leading to a failure in the application lifecycle.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.setTimerForTokenRenewal(DelegationTokenRenewer.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplication(DelegationTokenRenewer.java:307)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:819)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:613)",
                "        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:832)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service.",
                "Trigger the recovery process by enabling recovery in the configuration.",
                "Submit an application that requires delegation token renewal."
            ],
            "ExpectedBehavior": "The application should be submitted successfully, and the delegation token renewal timer should be set without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the application from being submitted and causing the ResourceManager to fail during the recovery process.",
            "AdditionalDetails": "The issue likely stems from a null reference in the DelegationTokenToRenew object being passed to the setTimerForTokenRenewal method. Further investigation is needed to ensure that all necessary objects are properly initialized before they are used."
        }
    },
    {
        "filename": "YARN-7269.json",
        "creation_time": "2017-09-28T23:56:42.000+0000",
        "bug_report": {
            "Title": "ServletException: Unable to Determine Proxy Server for Redirection",
            "Description": "The application throws a ServletException indicating that it could not determine the proxy server for redirection. This occurs in the AmIpFilter class when the findRedirectUrl method fails to find a valid proxy URL.",
            "StackTrace": [
                "javax.servlet.ServletException: Could not determine the proxy server for redirection",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:199)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:141)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1426)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(ServletHandler.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "1. Deploy the application with multiple Resource Managers (RM) configured.",
                "2. Attempt to access a resource that requires redirection through the proxy.",
                "3. Observe the logs for the ServletException indicating the inability to determine the proxy server."
            ],
            "ExpectedBehavior": "The application should successfully determine the proxy server for redirection and not throw a ServletException.",
            "ObservedBehavior": "The application throws a ServletException stating 'Could not determine the proxy server for redirection' when attempting to find a valid proxy URL.",
            "AdditionalDetails": "The findRedirectUrl method checks the size of proxyUriBases. If it is not equal to one, it attempts to retrieve valid URLs from the Resource Managers. If no valid URL is found, it throws a ServletException. This indicates a potential misconfiguration or lack of available proxy URLs."
        }
    },
    {
        "filename": "YARN-7249.json",
        "creation_time": "2017-09-25T16:49:46.000+0000",
        "bug_report": {
            "Title": "NullPointerException in CapacityScheduler during Container Completion",
            "Description": "A NullPointerException occurs in the CapacityScheduler's completedContainer method when attempting to complete a container. This issue arises when the scheduler tries to handle a reserved container that may no longer exist or has not been properly initialized.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705)"
            ],
            "StepsToReproduce": [
                "1. Reserve a container using the CapacityScheduler.",
                "2. Trigger the event to kill the reserved container.",
                "3. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The CapacityScheduler should successfully complete the reserved container without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the scheduler is attempting to access a method or property of a null object reference.",
            "AdditionalDetails": "The issue likely stems from the 'killReservedContainer' method, which calls 'completedContainer' with a potentially null RMContainer. The method should include checks to ensure that the RMContainer is not null before proceeding with the completion logic."
        }
    },
    {
        "filename": "YARN-4598.json",
        "creation_time": "2016-01-15T06:48:48.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException when handling RESOURCE_FAILED event",
            "Description": "An InvalidStateTransitionException is thrown when the system attempts to handle a RESOURCE_FAILED event while the container is in the CONTAINER_CLEANEDUP_AFTER_KILL state. This indicates that the event handling logic does not properly account for the current state of the container, leading to an invalid state transition.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:83)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1071)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start a YARN application that creates containers.",
                "2. Forcefully kill a container, transitioning it to the CONTAINER_CLEANEDUP_AFTER_KILL state.",
                "3. Trigger a RESOURCE_FAILED event for the killed container."
            ],
            "ExpectedBehavior": "The system should handle the RESOURCE_FAILED event gracefully without throwing an InvalidStateTransitionException, regardless of the container's current state.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when attempting to handle a RESOURCE_FAILED event while the container is in the CONTAINER_CLEANEDUP_AFTER_KILL state.",
            "AdditionalDetails": "The issue arises in the doTransition method of the StateMachineFactory, which does not allow the RESOURCE_FAILED event to be processed in the CONTAINER_CLEANEDUP_AFTER_KILL state. This indicates a potential flaw in the state transition logic that needs to be addressed."
        }
    },
    {
        "filename": "YARN-1149.json",
        "creation_time": "2013-09-04T21:46:58.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitionException during Application Log Handling",
            "Description": "An InvalidStateTransitionException is thrown when the application attempts to handle the APPLICATION_LOG_HANDLING_FINISHED event while in the RUNNING state. This indicates that the application state machine is not correctly handling the transition for this event type.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:689)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start an application in the YARN cluster.",
                "2. Trigger the APPLICATION_LOG_HANDLING_FINISHED event while the application is in the RUNNING state.",
                "3. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should handle the APPLICATION_LOG_HANDLING_FINISHED event without throwing an exception, transitioning to the appropriate state if necessary.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the application cannot handle the APPLICATION_LOG_HANDLING_FINISHED event while in the RUNNING state.",
            "AdditionalDetails": "The state machine for the application does not have a defined transition for the APPLICATION_LOG_HANDLING_FINISHED event when in the RUNNING state. This may require an update to the state machine configuration to handle this event appropriately."
        }
    },
    {
        "filename": "YARN-7818.json",
        "creation_time": "2018-01-25T18:42:55.000+0000",
        "bug_report": {
            "Title": "PrivilegedOperationException during Container Launch",
            "Description": "A PrivilegedOperationException is thrown when attempting to launch a container in the YARN NodeManager. The exception indicates that the operation exited with code 143, which typically signifies that the process was terminated due to a timeout or external signal.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.launchContainer(DefaultLinuxContainerRuntime.java:124)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.launchContainer(DelegatingLinuxContainerRuntime.java:152)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:549)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:285)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:95)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: ExitCodeException exitCode=143:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:902)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)"
            ],
            "StepsToReproduce": [
                "1. Submit a YARN application that requires container launch.",
                "2. Monitor the NodeManager logs for any container launch attempts.",
                "3. Observe the logs for the PrivilegedOperationException with exit code 143."
            ],
            "ExpectedBehavior": "The container should launch successfully without throwing a PrivilegedOperationException.",
            "ObservedBehavior": "The container fails to launch, and a PrivilegedOperationException is thrown with exit code 143, indicating a failure in executing a privileged operation.",
            "AdditionalDetails": "The exit code 143 typically indicates that the process was terminated by a signal (SIGTERM). This could be due to resource constraints or a timeout in the container launch process. Further investigation into the NodeManager's resource allocation and timeout settings may be necessary."
        }
    }
]