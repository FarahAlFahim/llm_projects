[
    {
        "filename": "STORM-2443.json",
        "creation_time": "2017-03-31T08:09:04.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Nimbus.setLogConfig",
            "Description": "A NullPointerException is thrown in the Nimbus.setLogConfig method when attempting to set the log configuration for a topology. This occurs when the topology ID provided is not valid or when the configuration object is null.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at org.apache.storm.daemon.nimbus.Nimbus.setLogConfig(Nimbus.java:2688) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3295) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3280) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_66]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_66]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]"
            ],
            "StepsToReproduce": [
                "Invoke the setLogConfig method on the Nimbus class with a null or invalid topology ID.",
                "Pass a null LogConfig object to the method."
            ],
            "ExpectedBehavior": "The setLogConfig method should successfully set the log configuration for the specified topology without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that an attempt was made to access a method or property of a null object.",
            "AdditionalDetails": "The issue may arise from the tryReadTopoConf method, which can return null if the topology ID is invalid. This can lead to a NullPointerException when the method attempts to access properties of the returned configuration."
        }
    },
    {
        "filename": "STORM-3213.json",
        "creation_time": "2018-09-05T16:16:45.000+0000",
        "bug_report": {
            "Title": "NullPointerException in getComponentPageInfo due to missing Bolt resources",
            "Description": "A NullPointerException is thrown when invoking the getComponentPageInfo method in the Nimbus class. This occurs when the method attempts to retrieve resources for a bolt that does not exist or is not properly initialized, leading to an internal error in processing the request.",
            "StackTrace": [
                "org.apache.storm.thrift.TApplicationException: Internal error processing getComponentPageInfo",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1359)",
                "at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1343)",
                "at org.apache.storm.daemon.ui.UIHelpers.getComponentPage(UIHelpers.java:1559)",
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4238) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4577) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4556) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:169) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.scheduler.resource.ResourceUtils.getBoltResources(ResourceUtils.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4192) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "StepsToReproduce": [
                "Invoke the getComponentPageInfo method on the Nimbus client with a topology ID and component ID that do not exist or are not properly initialized.",
                "Ensure that the Nimbus server is running and accessible.",
                "Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The getComponentPageInfo method should return a valid ComponentPageInfo object or an appropriate error message indicating that the component does not exist.",
            "ObservedBehavior": "A NullPointerException is thrown, leading to an internal error and preventing the retrieval of component page information.",
            "AdditionalDetails": "The issue seems to stem from the ResourceUtils.getBoltResources method, which is called within Nimbus.getComponentPageInfo. If the bolt resources are not properly initialized or the topology does not contain the specified component, a NullPointerException is thrown."
        }
    },
    {
        "filename": "STORM-2496.json",
        "creation_time": "2017-04-28T08:17:47.000+0000",
        "bug_report": {
            "Title": "AuthorizationException during Blob Download in Storm Localizer",
            "Description": "An AuthorizationException is thrown when attempting to download a blob in the Storm localizer, indicating that the user does not have the required READ access to the specified resource. This results in a failure to process events in the Storm supervisor, leading to a runtime exception that halts the process.",
            "StackTrace": [
                "org.apache.storm.generated.AuthorizationException: null",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]",
                "java.util.concurrent.ExecutionException: AuthorizationException(msg:<user> does not have READ access to dep-org.apache.curator-curator-framework-jar-2.10.0.jar)",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_112]",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_112]",
                "at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:380) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:740) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "Caused by: org.apache.storm.generated.AuthorizationException",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:1774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]"
            ],
            "StepsToReproduce": [
                "Attempt to download a blob using the Localizer in Apache Storm.",
                "Ensure that the user does not have READ access to the required resource (e.g., dep-org.apache.curator-curator-framework-jar-2.10.0.jar).",
                "Observe the logs for the AuthorizationException."
            ],
            "ExpectedBehavior": "The blob should be downloaded successfully if the user has the required READ access.",
            "ObservedBehavior": "An AuthorizationException is thrown, indicating that the user does not have READ access, which leads to a RuntimeException that halts the process.",
            "AdditionalDetails": "The issue arises in the Localizer class during the downloadBlob method, where the authorization check fails, preventing the blob from being downloaded and causing the supervisor to halt."
        }
    },
    {
        "filename": "STORM-2879.json",
        "creation_time": "2018-01-03T07:07:49.000+0000",
        "bug_report": {
            "Title": "KeyNotFoundException during Blob Download in Apache Storm",
            "Description": "A KeyNotFoundException is thrown when attempting to download a blob in Apache Storm. This occurs when the system cannot find the specified key, which leads to a failure in the blob download process. The issue is compounded by a FileNotFoundException indicating that the configuration file 'stormconf.ser' does not exist at the expected path.",
            "StackTrace": [
                "org.apache.storm.generated.KeyNotFoundException: null",
                "at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]",
                "java.io.FileNotFoundException: File '/opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785/stormconf.ser' does not exist",
                "at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:264) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:376) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:370) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:226) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:213) ~[storm-core-1.1.2-mt001.jar:?]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]"
            ],
            "StepsToReproduce": [
                "Attempt to download a blob using the Nimbus client.",
                "Ensure that the blob key being requested does not exist in the blob store.",
                "Check the configuration path for the storm configuration file 'stormconf.ser' to ensure it is correctly set."
            ],
            "ExpectedBehavior": "The blob should be downloaded successfully if the key exists and the configuration file is present.",
            "ObservedBehavior": "A KeyNotFoundException is thrown indicating that the specified key does not exist, followed by a FileNotFoundException for the missing 'stormconf.ser' file.",
            "AdditionalDetails": "The issue may be related to the configuration of the storm environment, specifically the paths set for blob storage and configuration files. Ensure that the paths are correctly configured and that the necessary files are present."
        }
    },
    {
        "filename": "STORM-3012.json",
        "creation_time": "2018-03-27T15:30:32.000+0000",
        "bug_report": {
            "Title": "PacemakerConnectionException: Timed out waiting for channel ready",
            "Description": "The application encounters a PacemakerConnectionException indicating a timeout while waiting for the channel to be ready. This issue arises during the heartbeat process of the Storm cluster, specifically when the Nimbus server attempts to clean up topologies. The root cause appears to be related to the PacemakerClient not being ready, leading to a NullPointerException in the Nimbus cleanup process.",
            "StackTrace": [
                "org.apache.storm.pacemaker.PacemakerConnectionException: Timed out waiting for channel ready.",
                "at org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0.y.jar:2.0.0.y]"
            ],
            "StepsToReproduce": [
                "Start the Storm cluster with the Pacemaker integration.",
                "Trigger a cleanup operation in the Nimbus server.",
                "Observe the logs for the PacemakerConnectionException."
            ],
            "ExpectedBehavior": "The Nimbus server should successfully communicate with the Pacemaker client and perform the cleanup operation without any exceptions.",
            "ObservedBehavior": "The Nimbus server throws a PacemakerConnectionException due to a timeout while waiting for the channel to be ready, leading to a NullPointerException during the cleanup process.",
            "AdditionalDetails": "The issue may be related to network connectivity or configuration settings for the Pacemaker client. The waitUntilReady method in the PacemakerClient class is responsible for ensuring the channel is ready before sending messages. If the channel is not ready, it throws a PacemakerConnectionException, which is not handled properly in the Nimbus cleanup process, resulting in a NullPointerException."
        }
    },
    {
        "filename": "STORM-3073.json",
        "creation_time": "2018-05-15T11:12:21.000+0000",
        "bug_report": {
            "Title": "Queue Full Exception in Storm Executor",
            "Description": "A RuntimeException is thrown due to an IllegalStateException indicating that the queue is full. This occurs during the processing of tuples in the Storm executor, specifically when trying to transfer messages to the worker's queue.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.IllegalStateException: Queue full",
                "at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:168) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:157) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.Utils$2.run(Utils.java:349) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "Caused by: java.lang.IllegalStateException: Queue full",
                "at java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:1.8.0_144]",
                "at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:516) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.sendSpoutMsg(SpoutOutputCollectorImpl.java:140) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.emit(SpoutOutputCollectorImpl.java:70) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:42) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.loadgen.LoadSpout.fail(LoadSpout.java:135) ~[stormjar.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor.failSpoutMsg(SpoutExecutor.java:360) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:120) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.RotatingMap.rotate(RotatingMap.java:63) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:295) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.Executor.accept(Executor.java:278) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "StepsToReproduce": [
                "1. Start the Storm application with a spout that generates a high volume of tuples.",
                "2. Monitor the queue size in the worker's transfer mechanism.",
                "3. Observe the logs for any 'Queue full' exceptions."
            ],
            "ExpectedBehavior": "The system should handle backpressure gracefully and not throw an exception when the queue is full.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the queue is full, leading to potential message loss and system instability.",
            "AdditionalDetails": "The issue seems to stem from the `tryTransferRemote` method in the `WorkerTransfer` class, which fails to add new tuples to the queue when it is full. This indicates a need for better backpressure handling in the spout's message emission logic."
        }
    },
    {
        "filename": "STORM-1672.json",
        "creation_time": "2016-03-31T19:24:18.000+0000",
        "bug_report": {
            "Title": "ClassCastException in StatsUtil.filterSysStreams",
            "Description": "A ClassCastException occurs when attempting to cast a Long object to a Map in the StatsUtil class, specifically in the filterSysStreams method. This issue arises during the aggregation of component execution statistics, leading to a failure in processing component page information.",
            "StackTrace": [
                "java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map",
                "at org.apache.storm.stats.StatsUtil.filterSysStreams(StatsUtil.java:1696)",
                "at org.apache.storm.stats.StatsUtil.aggPreMergeCompPageBolt(StatsUtil.java:240)",
                "at org.apache.storm.stats.StatsUtil.aggCompExecStats(StatsUtil.java:1130)",
                "at org.apache.storm.stats.StatsUtil.aggregateCompStats(StatsUtil.java:1108)",
                "at org.apache.storm.stats.StatsUtil.aggCompExecsStats(StatsUtil.java:1236)",
                "at org.apache.storm.daemon.nimbus$fn__3490$exec_fn__789__auto__$reify__3519.getComponentPageInfo(nimbus.clj:2130)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3826)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3810)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "Invoke the method aggPreMergeCompPageBolt with a Map that contains a Long value where a Map is expected.",
                "Ensure that the includeSys parameter is set to true or false as needed.",
                "Call the method aggregateCompStats with the appropriate parameters to trigger the execution flow leading to the error."
            ],
            "ExpectedBehavior": "The method filterSysStreams should process the input Map without throwing a ClassCastException, returning a filtered Map of system streams.",
            "ObservedBehavior": "A ClassCastException is thrown indicating that a Long value cannot be cast to a Map, causing the application to fail during the execution of component page information aggregation.",
            "AdditionalDetails": "The issue likely arises from the data structure being passed to the filterSysStreams method. The method expects a Map but receives a Long instead, which suggests that the input data may not be correctly formatted or populated prior to this method call."
        }
    },
    {
        "filename": "STORM-1520.json",
        "creation_time": "2016-02-03T02:48:58.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException: No matching method found for stateChanged in Storm Cluster State",
            "Description": "An IllegalArgumentException is thrown when the method 'stateChanged' is invoked on an instance of 'org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413'. The error indicates that there is no matching method found, which suggests that the method may not be defined or is not accessible in the context where it is being called.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: No matching method found: stateChanged for class org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:53)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660.stateChanged(zookeeper_state_factory.clj:145)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:259)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:255)",
                "at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)",
                "at org.apache.storm.shade.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)",
                "at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:84)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:253)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Initialize the Storm cluster state.",
                "2. Trigger a state change that invokes the 'stateChanged' method.",
                "3. Observe the exception thrown in the logs."
            ],
            "ExpectedBehavior": "The 'stateChanged' method should be invoked successfully without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that no matching method 'stateChanged' is found for the specified class.",
            "AdditionalDetails": "The source code for the method 'stateChanged' is not provided, which makes it difficult to determine if the method is missing or if there is an issue with method visibility or parameters."
        }
    },
    {
        "filename": "STORM-1977.json",
        "creation_time": "2016-07-17T09:07:06.000+0000",
        "bug_report": {
            "Title": "KeyNotFoundException when accessing blob metadata in LocalFsBlobStore",
            "Description": "The application encounters a KeyNotFoundException when attempting to retrieve blob metadata for specific keys in the LocalFsBlobStore. This issue arises when the requested blob does not exist in the local file system, leading to failures in blob replication and configuration retrieval.",
            "StackTrace": [
                "KeyNotFoundException(msg:production-topology-2-1468745167-stormcode.ser)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:268)",
                "...",
                "at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:498)",
                "at org.apache.storm.daemon.nimbus$get_cluster_info$iter__9520__9524$fn__9525.invoke(nimbus.clj:1427)",
                "...",
                "at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1401)",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9612.getClusterInfo(nimbus.clj:1838)",
                "at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3724)",
                "at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3708)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "KeyNotFoundException(msg:production-topology-2-1468745167-stormconf.ser)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:239)",
                "at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:271)",
                "at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:300)",
                "...",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.daemon.nimbus$read_storm_conf_as_nimbus.invoke(nimbus.clj:548)",
                "at org.apache.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:555)",
                "at org.apache.storm.daemon.nimbus$mk_assignments$iter__9205__9209$fn__9210.invoke(nimbus.clj:912)",
                "...",
                "at org.apache.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:911)",
                "at clojure.lang.RestFn.invoke(RestFn.java:410)",
                "at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770$fn__9781$fn__9782.invoke(nimbus.clj:2216)",
                "at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770.invoke(nimbus.clj:2215)",
                "at org.apache.storm.timer$schedule_recurring$this__1732.invoke(timer.clj:105)",
                "at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:50)",
                "at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)",
                "java.lang.RuntimeException: (\"Error when processing an event\")",
                "at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)",
                "at clojure.lang.RestFn.invoke(RestFn.java:423)",
                "at org.apache.storm.daemon.nimbus$nimbus_data$fn__8727.invoke(nimbus.clj:205)",
                "at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:71)",
                "at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)",
                "at clojure.lang.AFn.run(AFn.java:22)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Attempt to access blob metadata for the key 'production-topology-2-1468745167-stormcode.ser'.",
                "Attempt to access blob metadata for the key 'production-topology-2-1468745167-stormconf.ser'.",
                "Ensure that the specified keys do not exist in the LocalFsBlobStore."
            ],
            "ExpectedBehavior": "The application should successfully retrieve blob metadata and replication information without throwing a KeyNotFoundException.",
            "ObservedBehavior": "The application throws a KeyNotFoundException when attempting to access blob metadata for non-existent keys, leading to failures in processing events and retrieving cluster information.",
            "AdditionalDetails": "The issue appears to stem from the method 'getStoredBlobMeta(String key)' in the LocalFsBlobStore class, which throws a KeyNotFoundException if the specified key does not exist. This is further propagated through the call stack, affecting the Nimbus daemon's ability to retrieve necessary topology configurations."
        }
    },
    {
        "filename": "STORM-2988.json",
        "creation_time": "2018-03-07T14:55:22.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException during JMX Metrics Reporting Initialization",
            "Description": "An IllegalArgumentException is thrown when attempting to convert a configuration map to a String in the JmxStormReporter class. This occurs during the initialization of the metrics reporting system in Apache Storm, specifically when starting the StormMetricRegistry.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Don't know how to convert {\"class\" \"org.apache.storm.metrics2.reporters.JmxStormReporter\", \"daemons\" [\"supervisor\" \"nimbus\" \"worker\"], \"report.period\" 10, \"report.period.units\" \"SECONDS\"} + to String",
                "at org.apache.storm.utils.Utils.getString(Utils.java:848) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain(JmxStormReporter.java:70) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.reporters.JmxStormReporter.prepare(JmxStormReporter.java:51) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.StormMetricRegistry.startReporter(StormMetricRegistry.java:119) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.StormMetricRegistry.start(StormMetricRegistry.java:102) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.worker$fn__5545$exec_fn__1369__auto____5546.invoke(worker.clj:611) ~[storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.RestFn.invoke(RestFn.java:512) [storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]",
                "at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]",
                "java.lang.RuntimeException: (\"Error on initialization\")",
                "at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.RestFn.invoke(RestFn.java:512) [storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]",
                "at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]"
            ],
            "StepsToReproduce": [
                "Start the Apache Storm worker process with the provided configuration.",
                "Ensure that the JmxStormReporter is included in the metrics configuration.",
                "Observe the logs for the IllegalArgumentException during initialization."
            ],
            "ExpectedBehavior": "The JmxStormReporter should initialize successfully and start reporting metrics without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the system does not know how to convert the provided configuration map to a String, leading to a failure in the initialization process.",
            "AdditionalDetails": "The issue seems to stem from the way the configuration map is being processed in the Utils.getString method. It may require a proper implementation to handle the conversion of the map to a String format."
        }
    },
    {
        "filename": "STORM-2321.json",
        "creation_time": "2017-01-24T04:18:07.000+0000",
        "bug_report": {
            "Title": "NoNodeException in BlobStore Key Sequence Retrieval",
            "Description": "The application encounters a NoNodeException when attempting to retrieve the key sequence number from ZooKeeper. This occurs when the specified node does not exist in the ZooKeeper hierarchy, leading to a failure in the blob synchronization process.",
            "StackTrace": [
                "org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)",
                "at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:149)",
                "at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)",
                "at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3755)",
                "at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3740)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)",
                "at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Attempt to retrieve the key sequence number for a blob that does not exist in ZooKeeper.",
                "2. Ensure that the ZooKeeper node '/blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar' has not been created.",
                "3. Observe the application logs for the NoNodeException."
            ],
            "ExpectedBehavior": "The application should handle the absence of the specified ZooKeeper node gracefully, either by creating the node or returning a default value.",
            "ObservedBehavior": "The application throws a NoNodeException, causing the blob synchronization process to fail.",
            "AdditionalDetails": "The method 'getKeySequenceNumber' attempts to access a ZooKeeper node that does not exist, leading to the NoNodeException. This indicates a potential race condition or a failure to create the necessary nodes in ZooKeeper prior to accessing them."
        }
    },
    {
        "filename": "STORM-3013.json",
        "creation_time": "2018-03-28T04:47:28.000+0000",
        "bug_report": {
            "Title": "IllegalStateException: Consumer Already Closed in Kafka Consumer Metrics",
            "Description": "A RuntimeException is thrown due to an IllegalStateException indicating that the Kafka consumer has already been closed. This occurs during the metrics tick process when attempting to retrieve offsets from the Kafka consumer.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.IllegalStateException: This consumer has already been closed.",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:70) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$fn__4975$fn__4990$fn__5021.invoke(executor.clj:634) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]",
                "Caused by: java.lang.IllegalStateException: This consumer has already been closed.",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:1787) ~[stormjar.jar:?]",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:1622) ~[stormjar.jar:?]",
                "at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$metrics_tick$fn__4899.invoke(executor.clj:345) ~[storm-core-1.2.1.jar:1.2.1]",
                "at clojure.core$map$fn__4553.invoke(core.clj:2622) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$filter$fn__4580.invoke(core.clj:2679) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.RT.next(RT.java:674) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$next__4112.invoke(core.clj:64) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$fn__6523.invoke(protocols.clj:170) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$fn__6478$G__6473__6487.invoke(protocols.clj:19) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?]",
                "at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?]",
                "at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.executor$metrics_tick.invoke(executor.clj:349) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$fn__4975$tuple_action_fn__4981.invoke(executor.clj:522) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:471) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]"
            ],
            "StepsToReproduce": [
                "1. Start the Storm application with Kafka spouts.",
                "2. Ensure that the Kafka consumer is closed before the metrics tick is triggered.",
                "3. Observe the logs for the RuntimeException indicating the consumer has already been closed."
            ],
            "ExpectedBehavior": "The Kafka consumer should be open and able to retrieve offsets during the metrics tick process.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the Kafka consumer has already been closed, preventing the retrieval of offsets.",
            "AdditionalDetails": "The issue seems to stem from the `getValueAndReset()` method in the `KafkaOffsetMetric` class, where the consumer is expected to be open when calling `beginningOffsets()` and `endOffsets()`. Proper handling of consumer lifecycle is necessary to avoid this exception."
        }
    },
    {
        "filename": "STORM-3117.json",
        "creation_time": "2018-06-20T21:37:56.000+0000",
        "bug_report": {
            "Title": "KeyNotFoundException in Nimbus due to missing topology credentials",
            "Description": "The Nimbus service encounters a KeyNotFoundException when attempting to retrieve metadata for a topology. This issue arises because the required credentials for the topology are not found in the expected Zookeeper path, leading to a failure in processing events and halting the Nimbus process.",
            "StackTrace": [
                "org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormjar.jar",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483) [storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3990) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]",
                "java.lang.RuntimeException: KeyNotFoundException(msg:wc-topology-test-1-1529509694-stormcode.ser)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2822) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "Caused by: org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormcode.ser",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:420) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1517) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl(Nimbus.java:2675) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.sendClusterMetricsToExecutors(Nimbus.java:2686) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2819) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "java.lang.RuntimeException: Halting process: Error while processing event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:468) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:488) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "java.lang.Error: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.",
                "at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:603) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:582) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "Caused by: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.",
                "at org.apache.storm.zookeeper.AclEnforcement.getTopoAcl(AclEnforcement.java:194) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithTopoChildren(AclEnforcement.java:250) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithReadOnlyTopoChildren(AclEnforcement.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.zookeeper.AclEnforcement.verifyAcls(AclEnforcement.java:136) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1155) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1162) ~[storm-server-2.0.0.y.jar:2.0.0.y]"
            ],
            "StepsToReproduce": [
                "Deploy a topology with ID 'wc-topology-test-1-1529509694'.",
                "Ensure that the required credentials for the topology are not present in the Zookeeper path '/storms'.",
                "Start the Nimbus service and observe the logs for errors."
            ],
            "ExpectedBehavior": "The Nimbus service should successfully retrieve the metadata and credentials for the topology, allowing it to process events without errors.",
            "ObservedBehavior": "The Nimbus service fails to find the required credentials for the topology, resulting in a KeyNotFoundException and halting the process.",
            "AdditionalDetails": "The issue is likely related to the configuration of ACLs in Zookeeper. The Nimbus service expects certain credentials to be present for the topology, which are missing, leading to the failure."
        }
    },
    {
        "filename": "STORM-2993.json",
        "creation_time": "2018-03-12T19:04:16.000+0000",
        "bug_report": {
            "Title": "ClosedChannelException during HDFS write operation",
            "Description": "A ClosedChannelException is thrown when attempting to write data to HDFS using the HDFSWriter class. This indicates that the output stream has been closed unexpectedly, leading to failure in writing tuples.",
            "StackTrace": [
                "java.nio.channels.ClosedChannelException: null",
                "    at org.apache.hadoop.hdfs.ExceptionLastSeen.throwException4Close(ExceptionLastSeen.java:73) ~[stormjar.jar:?]",
                "    at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:153) ~[stormjar.jar:?]",
                "    at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:105) ~[stormjar.jar:?]",
                "    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57) ~[stormjar.jar:?]",
                "    at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_161]",
                "    at java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_161]",
                "    at org.apache.storm.hdfs.common.HDFSWriter.doWrite(HDFSWriter.java:48) ~[stormjar.jar:?]",
                "    at org.apache.storm.hdfs.common.AbstractHDFSWriter.write(AbstractHDFSWriter.java:40) ~[stormjar.jar:?]",
                "    at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.execute(AbstractHdfsBolt.java:158) [stormjar.jar:?]",
                "    at org.apache.storm.daemon.executor$fn__10189$tuple_action_fn__10191.invoke(executor.clj:745) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at org.apache.storm.daemon.executor$mk_task_receiver$fn__10108.invoke(executor.clj:473) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at org.apache.storm.disruptor$clojure_handler$reify__4115.onEvent(disruptor.clj:41) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:74) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at org.apache.storm.daemon.executor$fn__10189$fn__10202$fn__10257.invoke(executor.clj:868) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at org.apache.storm.util$async_loop$fn__1221.invoke(util.clj:484) [storm-core-1.2.1.3.0.0.0-1013.jar:1.2.1.3.0.0.0-1013]",
                "    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]",
                "    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]"
            ],
            "StepsToReproduce": [
                "1. Set up a Storm topology that writes data to HDFS using HDFSWriter.",
                "2. Ensure that the HDFS output stream is closed before the write operation is attempted.",
                "3. Trigger the execution of the bolt that writes to HDFS with a tuple."
            ],
            "ExpectedBehavior": "The HDFSWriter should successfully write the tuple data to HDFS without throwing an exception.",
            "ObservedBehavior": "A ClosedChannelException is thrown, indicating that the output stream is closed and the write operation cannot be completed.",
            "AdditionalDetails": "The issue may arise if the HDFS output stream is closed prematurely or if there are network issues causing the stream to become unavailable. Further investigation into the lifecycle of the HDFS output stream and error handling in the HDFSWriter class is recommended."
        }
    },
    {
        "filename": "STORM-1540.json",
        "creation_time": "2016-02-11T22:55:05.000+0000",
        "bug_report": {
            "Title": "NotSerializableException for ConsList during Storm processing",
            "Description": "A RuntimeException is thrown when attempting to serialize an instance of org.apache.storm.trident.tuple.ConsList. This occurs during the consumption of events in the DisruptorQueue, indicating that the ConsList class is not serializable, which is required for the processing of tuples in Apache Storm.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:448) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:414) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.disruptor$consume_loop_STAR_$fn__7651.invoke(disruptor.clj:83) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.util$async_loop$fn__554.invoke(util.clj:484) [storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_72]",
                "Caused by: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList",
                "at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:41) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568) ~[kryo-2.21.jar:?]",
                "at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:75) ~[kryo-2.21.jar:?]",
                "at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18) ~[kryo-2.21.jar:?]",
                "at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:486) ~[kryo-2.21.jar:?]",
                "at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:44) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:44) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.worker$mk_transfer_fn$transfer_fn__8346.invoke(worker.clj:186) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__8037.invoke(executor.clj:309) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.disruptor$clojure_handler$reify__7634.onEvent(disruptor.clj:40) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:435) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]"
            ],
            "StepsToReproduce": [
                "1. Set up an Apache Storm topology that utilizes ConsList in its tuples.",
                "2. Trigger the processing of events that include instances of ConsList.",
                "3. Observe the logs for the RuntimeException indicating NotSerializableException."
            ],
            "ExpectedBehavior": "The system should serialize all tuple components without throwing a NotSerializableException.",
            "ObservedBehavior": "A NotSerializableException is thrown for org.apache.storm.trident.tuple.ConsList, causing the processing to fail.",
            "AdditionalDetails": "The ConsList class does not implement Serializable, which is required for serialization in the context of Apache Storm's tuple processing. This issue may require modifying the ConsList class or using a different data structure that is serializable."
        }
    },
    {
        "filename": "STORM-2275.json",
        "creation_time": "2017-01-04T23:21:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Nimbus Transition Handling",
            "Description": "A NullPointerException occurs in the Nimbus class during the transition of topology states, leading to a RuntimeException that halts the process. This issue arises when the Nimbus attempts to process an event but encounters a null reference, specifically when trying to access the topology status.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "    at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1174)",
                "    at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83)",
                "Caused by: java.lang.NullPointerException",
                "    at org.apache.storm.daemon.nimbus.Nimbus.transition(Nimbus.java:1215)",
                "    at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1172)",
                "    ... 1 more",
                "java.lang.RuntimeException: Halting process: Error while processing event",
                "    at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792)",
                "    at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$15(Nimbus.java:1107)",
                "    at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:104)"
            ],
            "StepsToReproduce": [
                "1. Trigger a topology event that requires a state transition in the Nimbus class.",
                "2. Ensure that the topology ID provided does not correspond to an existing topology, leading to a null status.",
                "3. Observe the logs for the NullPointerException and subsequent RuntimeException."
            ],
            "ExpectedBehavior": "The Nimbus should handle the transition gracefully, logging an appropriate message without throwing a NullPointerException or halting the process.",
            "ObservedBehavior": "A NullPointerException is thrown when the Nimbus attempts to access the topology status, leading to a RuntimeException that halts the process.",
            "AdditionalDetails": "The issue seems to stem from the transition method where it checks the status of the topology. If the topology does not exist, the status is null, which is not handled properly, resulting in a NullPointerException."
        }
    },
    {
        "filename": "STORM-2873.json",
        "creation_time": "2017-12-29T18:44:56.000+0000",
        "bug_report": {
            "Title": "NoAuthException when attempting to delete Zookeeper node in Storm",
            "Description": "A RuntimeException occurs when the Storm application attempts to delete a Zookeeper node due to insufficient authentication. The error indicates that the operation is not authorized, leading to a failure in the backpressure handling mechanism.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721",
                "at backtype.storm.util$wrap_in_runtime.invoke(util.clj:52) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.zookeeper$delete_node.doInvoke(zookeeper.clj:110) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at clojure.lang.RestFn.invoke(RestFn.java:464) ~[clojure-1.6.0.jar:?]",
                "at backtype.storm.zookeeper$delete_recursive.invoke(zookeeper.clj:189) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4207.delete_node(zookeeper_state_factory.clj:117) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at sun.reflect.GeneratedMethodAccessor860.invoke(Unknown Source) ~[?:?]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]",
                "at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__4254.delete_node(pacemaker_state_factory.clj:174) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at sun.reflect.GeneratedMethodAccessor859.invoke(Unknown Source) ~[?:?]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]",
                "at backtype.storm.cluster$mk_storm_cluster_state$reify__3873.worker_backpressure_BANG_(cluster.clj:421) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at sun.reflect.GeneratedMethodAccessor857.invoke(Unknown Source) ~[?:?]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]",
                "at clojure.lang.Reflector.invoke(Reflector.java:93) ~[clojure-1.6.0.jar:?]",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]",
                "at backtype.storm.daemon.worker$mk_backpressure_handler$fn__7117.invoke(worker.clj:161) [storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.disruptor$worker_backpressure_handler$reify__6432.onEvent(disruptor.clj:57) [storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.utils.WorkerBackpressureThread.run(WorkerBackpressureThread.java:64) [storm-core-0.10.2.y.jar:0.10.2.y]"
            ],
            "StepsToReproduce": [
                "1. Start the Storm application with a topology that utilizes Zookeeper for backpressure management.",
                "2. Trigger a condition that requires the deletion of a Zookeeper node (e.g., scaling down the topology).",
                "3. Observe the logs for any RuntimeExceptions related to Zookeeper operations."
            ],
            "ExpectedBehavior": "The application should successfully delete the Zookeeper node without any authentication errors.",
            "ObservedBehavior": "A NoAuthException is thrown, indicating that the application does not have the necessary permissions to delete the specified Zookeeper node.",
            "AdditionalDetails": "This issue may be related to the Zookeeper configuration, specifically the authentication settings. Ensure that the Storm application has the correct permissions to perform delete operations on Zookeeper nodes."
        }
    },
    {
        "filename": "STORM-2279.json",
        "creation_time": "2017-01-05T20:59:11.000+0000",
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException in Nimbus.getComponentPageInfo",
            "Description": "An ArrayIndexOutOfBoundsException occurs in the Nimbus class when attempting to retrieve component page information. This issue arises when the method getComponentPageInfo is called with invalid parameters, leading to an attempt to access an invalid index in an ArrayList.",
            "StackTrace": [
                "java.lang.ArrayIndexOutOfBoundsException: -2",
                "at java.util.ArrayList.elementData(ArrayList.java:418)",
                "at java.util.ArrayList.get(ArrayList.java:431)",
                "at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:3606)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4097)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4081)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160)",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the getComponentPageInfo method in the Nimbus class with invalid parameters.",
                "Ensure that the parameters lead to an attempt to access an invalid index in the internal ArrayList."
            ],
            "ExpectedBehavior": "The getComponentPageInfo method should return valid component page information without throwing an exception.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown, indicating an attempt to access an invalid index in an ArrayList.",
            "AdditionalDetails": "The issue seems to stem from the parameters passed to the getComponentPageInfo method. It is crucial to validate the input parameters to prevent accessing invalid indices in the ArrayList."
        }
    },
    {
        "filename": "STORM-3079.json",
        "creation_time": "2018-05-17T19:29:10.000+0000",
        "bug_report": {
            "Title": "KeyNotFoundException in LocalFsBlobStore during Blob Retrieval",
            "Description": "A KeyNotFoundException is thrown when attempting to retrieve a blob from the LocalFsBlobStore. This occurs during the cleanup process in the Nimbus class, specifically when trying to read the topology associated with a given ID. The exception indicates that the requested blob key does not exist in the blob store.",
            "StackTrace": [
                "org.apache.storm.generated.KeyNotFoundException: null",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:393) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:670) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2333) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2387) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2674) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]"
            ],
            "StepsToReproduce": [
                "1. Deploy a topology in Apache Storm.",
                "2. Trigger a cleanup operation in the Nimbus daemon.",
                "3. Ensure that the blob associated with the topology has been deleted or is missing.",
                "4. Observe the logs for the KeyNotFoundException."
            ],
            "ExpectedBehavior": "The Nimbus should handle the absence of the requested blob gracefully, either by skipping the cleanup for that blob or logging a warning without throwing an exception.",
            "ObservedBehavior": "A KeyNotFoundException is thrown, causing the cleanup process to fail and potentially impacting the stability of the Nimbus daemon.",
            "AdditionalDetails": "The issue arises in the getStoredBlobMeta method, which attempts to read metadata for a blob that does not exist. This indicates a potential flaw in the cleanup logic where it does not account for missing blobs."
        }
    },
    {
        "filename": "STORM-3096.json",
        "creation_time": "2018-06-05T18:39:44.000+0000",
        "bug_report": {
            "Title": "WrappedKeyNotFoundException when accessing topology blobs",
            "Description": "The application throws a WrappedKeyNotFoundException when attempting to access topology-related blobs (both .ser and .jar files) in the LocalFsBlobStore. This occurs during operations that involve reading or cleaning up topology dependencies, indicating that the specified keys do not exist in the blob store.",
            "StackTrace": [
                "org.apache.storm.utils.WrappedKeyNotFoundException: topology-testHardCoreFaultTolerance-4-18-1528026822-stormcode.ser",
                " at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:259) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:394) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:680) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2389) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2443) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2730) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.StormTimer$1.run(StormTimer.java:111) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "org.apache.storm.utils.WrappedKeyNotFoundException: topology-testHardCoreFaultTolerance-4-18-1528026822-stormjar.jar",
                " at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:259) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:423) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1499) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.Nimbus.waitForDesiredCodeReplication(Nimbus.java:1509) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2982) [storm-server-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3508) [storm-client-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3487) [storm-client-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38) [libthrift-0.11.0.jar:0.11.0]",
                " at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.11.0.jar:0.11.0]",
                " at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]",
                " at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [libthrift-0.11.0.jar:0.11.0]",
                " at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                " at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                " at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]"
            ],
            "StepsToReproduce": [
                "Attempt to submit a topology with the name 'testHardCoreFaultTolerance' that requires specific blobs.",
                "Ensure that the blobs 'topology-testHardCoreFaultTolerance-4-18-1528026822-stormcode.ser' and 'topology-testHardCoreFaultTolerance-4-18-1528026822-stormjar.jar' are not present in the LocalFsBlobStore.",
                "Trigger the cleanup process or wait for the Nimbus server to perform its cleanup operations."
            ],
            "ExpectedBehavior": "The application should successfully access the required blobs and perform the necessary operations without throwing exceptions.",
            "ObservedBehavior": "The application throws WrappedKeyNotFoundException indicating that the specified blobs do not exist in the LocalFsBlobStore.",
            "AdditionalDetails": "The issue seems to stem from the Nimbus server's inability to find the specified blobs during cleanup and topology submission processes. This could be due to missing blobs or incorrect blob management."
        }
    },
    {
        "filename": "STORM-1642.json",
        "creation_time": "2016-03-21T07:34:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DisruptorQueue during Tuple Deserialization",
            "Description": "A NullPointerException is thrown in the DisruptorQueue class when attempting to consume a batch of tuples. This issue arises during the deserialization process of tuples using the KryoTupleDeserializer, which indicates that a null buffer is being set in the Input class.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) ~[storm-core-0.10.0.jar:0.10.0]",
                "    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) ~[storm-core-0.10.0.jar:0.10.0]",
                "    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.jar:0.10.0]",
                "    at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) ~[storm-core-0.10.0.jar:0.10.0]",
                "    at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) [storm-core-0.10.0.jar:0.10.0]",
                "    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]",
                "    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]",
                "Caused by: java.lang.NullPointerException",
                "    at com.esotericsoftware.kryo.io.Input.setBuffer(Input.java:57) ~[kryo-2.21.jar:?]",
                "    at backtype.storm.serialization.KryoTupleDeserializer.deserialize(KryoTupleDeserializer.java:47) ~[storm-core-0.10.0.jar:0.10.0]",
                "    at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:433) ~[storm-core-0.10.0.jar:0.10.0]",
                "    at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) ~[storm-core-0.10.0.jar:0.10.0]",
                "    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:132) ~[storm-core-0.10.0.jar:0.10.0]",
                "    ... 6 more"
            ],
            "StepsToReproduce": [
                "1. Start the Storm application with a topology that uses the DisruptorQueue.",
                "2. Ensure that the topology is processing tuples.",
                "3. Monitor the logs for any NullPointerException related to tuple deserialization."
            ],
            "ExpectedBehavior": "The DisruptorQueue should successfully consume and deserialize tuples without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the tuple deserialization process, causing the worker to die.",
            "AdditionalDetails": "The issue seems to originate from the KryoTupleDeserializer where a null buffer is being set in the Input class. This indicates that the input data being deserialized may not be properly initialized or is missing."
        }
    },
    {
        "filename": "STORM-2700.json",
        "creation_time": "2017-08-21T14:09:50.000+0000",
        "bug_report": {
            "Title": "AuthorizationException during Blob Localization in Storm Supervisor",
            "Description": "An AuthorizationException is thrown when the user 'ethan' attempts to access a resource (key1) without the necessary READ permissions. This leads to a RuntimeException that halts the process in the Storm supervisor.",
            "StackTrace": [
                "java.util.concurrent.ExecutionException: AuthorizationException(msg:ethan does not have READ access to key1)",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_131]",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_131]",
                "at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:410) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:305) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:789) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "Caused by: org.apache.storm.generated.AuthorizationException",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:527) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:68) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:497) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:473) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:437) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:823) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "StepsToReproduce": [
                "Attempt to access the resource 'key1' with user 'ethan' who does not have READ permissions.",
                "Observe the logs for the AuthorizationException."
            ],
            "ExpectedBehavior": "The user should have the necessary permissions to access the resource, allowing the blob localization to proceed without errors.",
            "ObservedBehavior": "The process halts due to an AuthorizationException indicating that the user 'ethan' does not have READ access to 'key1'.",
            "AdditionalDetails": "The issue arises in the Localizer class during the downloadBlob method, which is responsible for downloading resources. The authorization check fails, leading to the exception being thrown and subsequently causing the supervisor to halt the process."
        }
    },
    {
        "filename": "STORM-1663.json",
        "creation_time": "2016-03-29T06:07:27.000+0000",
        "bug_report": {
            "Title": "TTransportException during Topology Page Info Retrieval",
            "Description": "A TTransportException is thrown when attempting to retrieve topology page information from the Nimbus service. This issue appears to be related to network communication problems or misconfiguration in the Thrift transport layer.",
            "StackTrace": [
                "org.apache.storm.thrift.transport.TTransportException",
                "at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)",
                "at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getTopologyPageInfo(Nimbus.java:1243)",
                "at org.apache.storm.generated.Nimbus$Client.getTopologyPageInfo(Nimbus.java:1228)",
                "at org.apache.storm.ui.core$topology_page.invoke(core.clj:638)",
                "at org.apache.storm.ui.core$fn__3662.invoke(core.clj:987)",
                "at org.apache.storm.shade.compojure.core$make_route$fn__302.invoke(core.clj:93)",
                "at org.apache.storm.shade.compojure.core$if_route$fn__290.invoke(core.clj:39)",
                "at org.apache.storm.shade.compojure.core$if_method$fn__283.invoke(core.clj:24)",
                "at org.apache.storm.shade.compojure.core$routing$fn__308.invoke(core.clj:106)",
                "at clojure.core$some.invoke(core.clj:2570)",
                "at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:106)",
                "at clojure.lang.RestFn.applyTo(RestFn.java:139)",
                "at clojure.core$apply.invoke(core.clj:632)",
                "at org.apache.storm.shade.compojure.core$routes$fn__312.invoke(core.clj:111)",
                "at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1204.invoke(json.clj:56)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)",
                "at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__724.invoke(reload.clj:22)",
                "at org.apache.storm.ui.helpers$requests_middleware$fn__3091.invoke(helpers.clj:50)",
                "at org.apache.storm.ui.core$catch_errors$fn__3837.invoke(core.clj:1250)",
                "at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2852.invoke(keyword_params.clj:27)",
                "at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2892.invoke(nested_params.clj:65)",
                "at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2823.invoke(params.clj:55)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)",
                "at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__3075.invoke(flash.clj:14)",
                "at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__3063.invoke(session.clj:43)",
                "at org.apache.storm.shade.ring.middleware.cookies$wrap_cookies$fn__2991.invoke(cookies.clj:160)",
                "at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2729.invoke(servlet.clj:127)",
                "at org.apache.storm.shade.ring.util.servlet$servlet$fn__2733.invoke(servlet.clj:136)",
                "at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)",
                "at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)",
                "at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Attempt to access the topology page in the Storm UI.",
                "Observe the logs for any TTransportException errors."
            ],
            "ExpectedBehavior": "The topology page information should be retrieved successfully without any exceptions.",
            "ObservedBehavior": "A TTransportException is thrown, indicating a failure in reading data from the transport layer.",
            "AdditionalDetails": "This issue may be related to network connectivity or configuration issues with the Thrift transport layer. Further investigation into the Nimbus service and network settings is recommended."
        }
    },
    {
        "filename": "STORM-2518.json",
        "creation_time": "2017-05-17T06:26:37.000+0000",
        "bug_report": {
            "Title": "NullPointerException in BlobStoreAclHandler during ACL normalization",
            "Description": "A NullPointerException occurs in the BlobStoreAclHandler class when attempting to fix ACLs for a user. This issue arises during the normalization of settable ACLs, specifically when the method fixACLsForUser is called with a null or improperly initialized list of AccessControl objects.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.fixACLsForUser(BlobStoreAclHandler.java:382) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableACLs(BlobStoreAclHandler.java:357) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableBlobMeta(BlobStoreAclHandler.java:306) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:103) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9064.beginCreateBlob(nimbus.clj:2047) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3430) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3414) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]"
            ],
            "StepsToReproduce": [
                "Attempt to create a blob using the createBlob method in LocalFsBlobStore.",
                "Ensure that the ACLs passed to the method are null or improperly initialized."
            ],
            "ExpectedBehavior": "The system should handle null or empty ACL lists gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the fixACLsForUser method is invoked with a null or improperly initialized list of AccessControl objects.",
            "AdditionalDetails": "The issue likely stems from the normalizeSettableBlobMeta method, which calls normalizeSettableACLs with a null ACL list. Proper null checks should be implemented in the methods to prevent this exception."
        }
    },
    {
        "filename": "STORM-3124.json",
        "creation_time": "2018-06-27T13:28:01.000+0000",
        "bug_report": {
            "Title": "Pacemaker Connection Failure in Nimbus Cleanup Process",
            "Description": "The Nimbus service encounters a RuntimeException due to a failure in connecting to the Pacemaker service while attempting to clean up heartbeat information for topologies. This results in the Nimbus process halting unexpectedly.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2773) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:110) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "Caused by: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.",
                "at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:214) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:482) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:897) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2469) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2771) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "... 2 more"
            ],
            "StepsToReproduce": [
                "Start the Nimbus service.",
                "Ensure that the Pacemaker service is either down or unreachable.",
                "Trigger a cleanup operation in Nimbus, which attempts to retrieve heartbeat information from the Pacemaker."
            ],
            "ExpectedBehavior": "Nimbus should handle the Pacemaker connection failure gracefully, logging the error and continuing its operation without halting the process.",
            "ObservedBehavior": "Nimbus throws a RuntimeException and halts the process when it fails to connect to the Pacemaker service.",
            "AdditionalDetails": "The failure occurs in the `get_worker_hb_children` method of `PaceMakerStateStorage`, which is called during the cleanup process in Nimbus. The `sendAll` method in `PacemakerClientPool` is responsible for sending requests to the Pacemaker servers, and it throws a `PacemakerConnectionException` when no connections can be established."
        }
    },
    {
        "filename": "STORM-2095.json",
        "creation_time": "2016-09-14T16:00:30.000+0000",
        "bug_report": {
            "Title": "DirectoryNotEmptyException during Blob Deletion in LocalFsBlobStore",
            "Description": "A RuntimeException is thrown when attempting to delete a blob from the LocalFsBlobStore due to a DirectoryNotEmptyException. This occurs when the deleteBlob method is invoked, and the underlying file system indicates that the directory is not empty, preventing the deletion of the specified blob.",
            "StackTrace": [
                "java.lang.RuntimeException: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file",
                "at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:229)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.daemon.nimbus$setup_blobstore.invoke(nimbus.clj:1196)",
                "at org.apache.storm.daemon.nimbus$fn__7064$exec_fn__2461__auto____7065.invoke(nimbus.clj:1416)",
                "at clojure.lang.AFn.applyToHelper(AFn.java:156)",
                "at clojure.lang.AFn.applyTo(AFn.java:144)",
                "at clojure.core$apply.invoke(core.clj:630)",
                "at org.apache.storm.daemon.nimbus$fn__7064$service_handler__7308.doInvoke(nimbus.clj:1358)",
                "at clojure.lang.RestFn.invoke(RestFn.java:421)",
                "at org.apache.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:2206)",
                "at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2239)",
                "at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2262)",
                "at clojure.lang.AFn.applyToHelper(AFn.java:152)",
                "at clojure.lang.AFn.applyTo(AFn.java:144)",
                "at org.apache.storm.daemon.nimbus.main(Unknown Source)",
                "Caused by: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file",
                "at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)",
                "at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)",
                "at java.nio.file.Files.deleteIfExists(Files.java:1165)",
                "at org.apache.storm.blobstore.FileBlobStoreImpl.delete(FileBlobStoreImpl.java:239)",
                "at org.apache.storm.blobstore.FileBlobStoreImpl.deleteKey(FileBlobStoreImpl.java:178)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:226)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "1. Attempt to delete a blob using the deleteBlob method in LocalFsBlobStore.",
                "2. Ensure that the blob's directory is not empty (i.e., it contains files or subdirectories).",
                "3. Observe the exception thrown during the deletion process."
            ],
            "ExpectedBehavior": "The blob should be deleted successfully without any exceptions, even if the directory contains files.",
            "ObservedBehavior": "A DirectoryNotEmptyException is thrown, preventing the deletion of the blob.",
            "AdditionalDetails": "The deleteBlob method attempts to delete both the data and metadata associated with the blob. The failure occurs in the deleteKey method, which tries to delete the directory containing the blob. The underlying file system does not allow deletion of a non-empty directory, leading to the exception."
        }
    },
    {
        "filename": "STORM-2847.json",
        "creation_time": "2017-12-07T16:51:01.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException in KafkaSpout during Offset Commit",
            "Description": "An IllegalArgumentException is thrown when attempting to commit offsets for acknowledged tuples in the KafkaSpout. The exception indicates that the consumer is trying to check the position for partitions that are not assigned to it, which leads to a failure in the commitOffsetsForAckedTuples method.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: You can only check the position for partitions assigned to this consumer.",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1262)",
                "at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:473)"
            ],
            "StepsToReproduce": [
                "1. Set up a Kafka consumer with multiple partitions.",
                "2. Ensure that the consumer is not assigned to all partitions.",
                "3. Trigger the commitOffsetsForAckedTuples method after processing messages from the assigned partitions."
            ],
            "ExpectedBehavior": "The offsets for acknowledged tuples should be committed successfully without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, indicating that the consumer is trying to check the position for partitions that it is not assigned to.",
            "AdditionalDetails": "The issue arises in the commitOffsetsForAckedTuples method where the kafkaConsumer.position(tp) is called for a TopicPartition that is not assigned to the consumer. This can occur if the offsetManagers map contains entries for partitions that the consumer is not currently assigned to."
        }
    },
    {
        "filename": "STORM-1114.json",
        "creation_time": "2015-10-15T15:41:36.000+0000",
        "bug_report": {
            "Title": "NodeExistsException and NoNodeException in TransactionalState Management",
            "Description": "The application encounters a NodeExistsException when attempting to create a ZNode that already exists, and a NoNodeException when trying to delete a ZNode that does not exist. This indicates improper handling of ZNode existence checks in the TransactionalState class.",
            "StackTrace": [
                "Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /ignoreStoredMetadata",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:239) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:193) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.forPath(TransactionalState.java:83) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.createNode(TransactionalState.java:100) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:115) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "... 9 more",
                "Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /rainbowHdfsPath",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.delete(TransactionalState.java:126) ~[storm-core-0.10.1.y.jar:0.10.1.y]"
            ],
            "StepsToReproduce": [
                "Attempt to create a ZNode at '/ignoreStoredMetadata' when it already exists.",
                "Attempt to delete a ZNode at '/rainbowHdfsPath' when it does not exist."
            ],
            "ExpectedBehavior": "The application should handle existing ZNodes gracefully without throwing exceptions, and should not attempt to delete non-existent ZNodes.",
            "ObservedBehavior": "The application throws NodeExistsException when trying to create an existing ZNode and NoNodeException when trying to delete a non-existent ZNode.",
            "AdditionalDetails": "The method 'setData' in the TransactionalState class does not check for the existence of the ZNode before attempting to create it, leading to the NodeExistsException. Similarly, the 'delete' method does not handle the case where the ZNode does not exist, resulting in the NoNodeException."
        }
    },
    {
        "filename": "STORM-2811.json",
        "creation_time": "2017-11-12T08:37:10.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Nimbus when killing topology",
            "Description": "A NullPointerException occurs in the Nimbus class when attempting to kill a topology using the killTopologyWithOpts method. The exception is thrown because the getTopoId method returns an empty Optional, leading to a failure in the tryReadTopoConfFromName method.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at org.apache.storm.cluster.IStormClusterState.getTopoId(IStormClusterState.java:171) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConfFromName(Nimbus.java:1970) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.killTopologyWithOpts(Nimbus.java:2760) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3226) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3210) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:167) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.thrift.server.Invocation.run(Invocation.java:18) ~[libthrift-0.10.0.jar:0.10.0]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]"
            ],
            "StepsToReproduce": [
                "Invoke the killTopologyWithOpts method on the Nimbus class with a topology name that does not exist or is not active."
            ],
            "ExpectedBehavior": "The system should gracefully handle the request to kill a topology that does not exist, returning an appropriate error message without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application to crash or behave unexpectedly.",
            "AdditionalDetails": "The getTopoId method in IStormClusterState returns an empty Optional when the topology name does not match any active topologies. This leads to a failure in the tryReadTopoConfFromName method, which expects a valid topology ID."
        }
    },
    {
        "filename": "STORM-2903.json",
        "creation_time": "2018-01-19T17:10:01.000+0000",
        "bug_report": {
            "Title": "NullPointerException in AbstractAutoCreds.addTokensToUGI",
            "Description": "A NullPointerException is thrown in the method AbstractAutoCreds.addTokensToUGI when attempting to add tokens to the UserGroupInformation (UGI). This issue arises when the subject passed to the method is null, leading to a failure in the authentication process.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.common.AbstractAutoCreds.addTokensToUGI(AbstractAutoCreds.java:219) ~[storm-autocreds-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]",
                "at org.apache.storm.common.AbstractAutoCreds.populateSubject(AbstractAutoCreds.java:118) ~[storm-autocreds-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]",
                "at org.apache.storm.security.auth.AuthUtils.populateSubject(AuthUtils.java:228) ~[storm-core-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]"
            ],
            "StepsToReproduce": [
                "1. Call the method AbstractAutoCreds.addTokensToUGI with a null Subject.",
                "2. Ensure that the method is invoked as part of the authentication process."
            ],
            "ExpectedBehavior": "The method should handle a null Subject gracefully, either by initializing a new Subject or by throwing a controlled exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application to crash or fail the authentication process.",
            "AdditionalDetails": "The populateSubject method is designed to initialize a new Subject if the provided one is null. However, it appears that the addTokensToUGI method does not check for null before proceeding, leading to the exception. This indicates a potential oversight in error handling within the authentication flow."
        }
    },
    {
        "filename": "STORM-3168.json",
        "creation_time": "2018-08-01T19:31:42.000+0000",
        "bug_report": {
            "Title": "KeyNotFoundException during Blob Download in AsyncLocalizer",
            "Description": "The application encounters a KeyNotFoundException when attempting to download blobs in the AsyncLocalizer. This results in a RuntimeException being thrown, which is wrapped in a CompletableFuture's ExecutionException. The issue appears to stem from the Nimbus service not being able to find the requested blob metadata.",
            "StackTrace": [
                "java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not download...",
                "at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_131]",
                "at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_131]",
                "at org.apache.storm.localizer.AsyncLocalizer.updateBlobs(AsyncLocalizer.java:303) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_131]",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]",
                "Caused by: java.lang.RuntimeException: Could not download...",
                "at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:268) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_131]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_131]",
                "... 3 more",
                "Caused by: org.apache.storm.generated.KeyNotFoundException",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25853) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25821) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25752) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:798) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:785) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:85) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:122) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:252) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_131]",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_131]",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_131]"
            ],
            "StepsToReproduce": [
                "1. Start the Storm application with a topology that requires blob downloads.",
                "2. Ensure that the blob metadata for the required blobs is not present in the Nimbus service.",
                "3. Observe the logs for the ExecutionException and KeyNotFoundException."
            ],
            "ExpectedBehavior": "The application should successfully download the required blobs without throwing exceptions.",
            "ObservedBehavior": "The application throws a KeyNotFoundException, leading to a RuntimeException and an ExecutionException, indicating that the blob metadata could not be found.",
            "AdditionalDetails": "The issue seems to be related to the Nimbus service not having the required blob metadata available. The method getBlobMeta(String key) is expected to retrieve the metadata, but it fails due to the KeyNotFoundException."
        }
    },
    {
        "filename": "STORM-2986.json",
        "creation_time": "2018-03-05T21:41:24.000+0000",
        "bug_report": {
            "Title": "NullPointerException in LogCleaner during directory cleanup",
            "Description": "A NullPointerException is thrown in the LogCleaner class when attempting to select directories for cleanup. This occurs during the execution of the run method in the LogCleaner class, which is triggered by a StormTimer task.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at java.util.Arrays.stream(Arrays.java:5004) ~[?:1.8.0_131]",
                "at org.apache.storm.daemon.logviewer.utils.LogCleaner.selectDirsForCleanup(LogCleaner.java:217) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.logviewer.utils.LogCleaner.run(LogCleaner.java:135) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "StepsToReproduce": [
                "1. Start the Storm application with log cleanup enabled.",
                "2. Ensure that there are directories available for cleanup.",
                "3. Wait for the LogCleaner to trigger its run method via the StormTimer."
            ],
            "ExpectedBehavior": "The LogCleaner should successfully select directories for cleanup without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the LogCleaner to fail during the directory selection process.",
            "AdditionalDetails": "The issue likely arises from a null reference being passed to the Arrays.stream method in the selectDirsForCleanup method. Further investigation is needed to determine what specific variable is null at that point."
        }
    },
    {
        "filename": "STORM-2197.json",
        "creation_time": "2016-11-10T03:57:30.000+0000",
        "bug_report": {
            "Title": "GSS Initiate Failure in Kerberos SASL Transport",
            "Description": "The application encounters a GSS initiate failure when attempting to establish a connection using Kerberos SASL transport. This results in a TTransportException being thrown, which prevents the application from successfully connecting to the Nimbus server.",
            "StackTrace": [
                "org.apache.thrift7.transport.TTransportException: Peer indicated failure: GSS initiate failed",
                "at org.apache.thrift7.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:199) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:277) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:145) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:141) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_60]",
                "at javax.security.auth.Subject.doAs(Subject.java:415) [?:1.7.0_60]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:140) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:103) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:72) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:106) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:82) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.ui.core$nimbus_summary.invoke(core.clj:584) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at backtype.storm.ui.core$fn__10334.invoke(core.clj:1009) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at compojure.core$make_route$fn__7476.invoke(core.clj:93) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at compojure.core$if_route$fn__7464.invoke(core.clj:39) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at compojure.core$if_method$fn__7457.invoke(core.clj:24) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at compojure.core$routing$fn__7482.invoke(core.clj:106) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]",
                "at clojure.core$some.invoke(core.clj:2515) [clojure-1.6.0.jar:?]"
            ],
            "StepsToReproduce": [
                "1. Attempt to connect to the Nimbus server using Kerberos authentication.",
                "2. Ensure that the GSSAPI configuration is set up correctly.",
                "3. Observe the logs for any GSS initiate failures."
            ],
            "ExpectedBehavior": "The application should successfully establish a connection to the Nimbus server using Kerberos SASL transport without throwing any exceptions.",
            "ObservedBehavior": "The application throws a TTransportException indicating that the GSS initiate failed, preventing a successful connection to the Nimbus server.",
            "AdditionalDetails": "[Provide additional details regarding the environment, configuration, or any other relevant information that may assist in diagnosing the issue.]"
        }
    },
    {
        "filename": "STORM-1596.json",
        "creation_time": "2016-03-02T23:42:56.000+0000",
        "bug_report": {
            "Title": "SaslException: GSS initiate failed due to BAD TGS SERVER NAME",
            "Description": "The application encounters a SaslException indicating that the GSS initiate has failed. The root cause appears to be related to Kerberos authentication, specifically a BAD TGS SERVER NAME error, which suggests that the ticket provided is not valid for the requested service.",
            "StackTrace": [
                "javax.security.sasl.SaslException: GSS initiate failed",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_40]",
                "at org.apache.thrift7.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:271) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:195) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:191) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_40]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_40]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:190) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:54) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:109) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.DRPCInvocationsClient.reconnectClient(DRPCInvocationsClient.java:57) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.ReturnResults.reconnectClient(ReturnResults.java:113) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.ReturnResults.execute(ReturnResults.java:103) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$fn__6377$tuple_action_fn__6379.invoke(executor.clj:689) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$mk_task_receiver$fn__6301.invoke(executor.clj:448) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.disruptor$clojure_handler$reify__6018.onEvent(disruptor.clj:40) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:437) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:416) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$fn__6377$fn__6390$fn__6441.invoke(executor.clj:801) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.util$async_loop$fn__742.invoke(util.clj:482) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]",
                "Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: The ticket isn't for us (35) - BAD TGS SERVER NAME)",
                "at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:770) ~[?:1.8.0_40]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]",
                "... 23 more",
                "Caused by: sun.security.krb5.KrbException: The ticket isn't for us (35) - BAD TGS SERVER NAME",
                "at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73) ~[?:1.8.0_40]",
                "at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]",
                "at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]",
                "at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]",
                "at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]",
                "at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]",
                "at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]",
                "... 23 more",
                "Caused by: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)",
                "at sun.security.krb5.internal.KDCRep.init(KDCRep.java:140) ~[?:1.8.0_40]",
                "at sun.security.krb5.internal.TGSRep.init(TGSRep.java:65) ~[?:1.8.0_40]",
                "at sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60) ~[?:1.8.0_40]",
                "at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55) ~[?:1.8.0_40]",
                "at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]",
                "at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]",
                "at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]",
                "at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]",
                "at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]",
                "at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]"
            ],
            "StepsToReproduce": [
                "Attempt to connect to a service using Kerberos authentication.",
                "Ensure that the service principal name (SPN) is correctly configured.",
                "Use a valid Kerberos ticket for authentication."
            ],
            "ExpectedBehavior": "The application should successfully authenticate using Kerberos and establish a connection without errors.",
            "ObservedBehavior": "The application throws a SaslException indicating that the GSS initiate failed due to a BAD TGS SERVER NAME error.",
            "AdditionalDetails": "This issue may be related to misconfiguration of the Kerberos service principal name or the Kerberos ticket being used. Ensure that the ticket is valid and corresponds to the correct service."
        }
    },
    {
        "filename": "STORM-2142.json",
        "creation_time": "2016-10-10T04:42:01.000+0000",
        "bug_report": {
            "Title": "RuntimeException due to null conversion in SQL function",
            "Description": "A RuntimeException is thrown when attempting to convert a null value to an integer in the SQL function. This occurs during the evaluation of a script in the DisruptorQueue's consumeBatchToCursor method.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:468) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]",
                "at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]",
                "at org.codehaus.janino.ScriptEvaluator.evaluate(ScriptEvaluator.java:982) ~[dep-janino-2.7.6-dcb5bd18-a5dd-4976-a967-0108dcf46df0.jar.1475903522000:2.7.6]",
                "Caused by: java.lang.RuntimeException: Cannot convert null to int",
                "at org.apache.calcite.runtime.SqlFunctions.cannotConvert(SqlFunctions.java:1023) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]",
                "at org.apache.calcite.runtime.SqlFunctions.toInt(SqlFunctions.java:1134) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]",
                "at SC.eval0(Unknown Source) ~[?:?]"
            ],
            "StepsToReproduce": [
                "1. Trigger the DisruptorQueue to process events.",
                "2. Ensure that one of the events being processed contains a null value that is expected to be converted to an integer.",
                "3. Observe the exception thrown during the processing."
            ],
            "ExpectedBehavior": "The system should handle null values gracefully without throwing a RuntimeException.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that a null value cannot be converted to an integer.",
            "AdditionalDetails": "The issue arises in the consumeBatchToCursor method where events are processed. If an event contains a null value, it leads to a failure in the SQL function that attempts to convert it to an integer."
        }
    },
    {
        "filename": "STORM-2400.json",
        "creation_time": "2017-03-08T04:32:34.000+0000",
        "bug_report": {
            "Title": "NoNodeException Thrown When Accessing Zookeeper Node",
            "Description": "The application encounters a NoNodeException when attempting to access a Zookeeper node that does not exist. This issue arises during the leader election process in a Storm application, specifically when trying to retrieve data from a leader latch node.",
            "StackTrace": [
                "org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock/_c_97c09eed-5bba-4ac8-a05f-abdc4e8e95cf-latch-0000000002",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:304)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:293)",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:290)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:281)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:42)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.participantForPath(LeaderSelector.java:375)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.getLeader(LeaderSelector.java:346)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:454)"
            ],
            "StepsToReproduce": [
                "Start the Storm application with leader election enabled.",
                "Ensure that the Zookeeper ensemble is running.",
                "Trigger the leader election process.",
                "Observe the logs for the NoNodeException related to the specified path."
            ],
            "ExpectedBehavior": "The application should successfully retrieve the leader information from the Zookeeper node without throwing an exception.",
            "ObservedBehavior": "The application throws a NoNodeException indicating that the specified Zookeeper node does not exist.",
            "AdditionalDetails": "This issue may occur if the Zookeeper node was deleted or never created due to a misconfiguration in the leader election process. Further investigation into the Zookeeper setup and the lifecycle of the leader latch node is recommended."
        }
    },
    {
        "filename": "STORM-3084.json",
        "creation_time": "2018-05-24T20:45:32.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Nimbus during Scheduler Assignment",
            "Description": "A NullPointerException is thrown in the Nimbus class while attempting to read supervisor details and compute new scheduler assignments. This leads to a RuntimeException that halts the process.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "... 2 more",
                "java.lang.RuntimeException: Halting process: Error while processing event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) ~[storm-client-2.0.0.y.jar:2.0.0.y]"
            ],
            "StepsToReproduce": [
                "Start the Storm Nimbus server.",
                "Trigger a topology that requires scheduling.",
                "Ensure that there are missing assignments or dead ports in the supervisor details."
            ],
            "ExpectedBehavior": "The Nimbus should successfully read all supervisor details and compute new scheduler assignments without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException occurs, leading to a RuntimeException that halts the process.",
            "AdditionalDetails": "The NullPointerException is likely caused by a missing or improperly initialized supervisor detail in the Nimbus class, specifically in the readAllSupervisorDetails method. This method is called during the computeNewSchedulerAssignments process, which is critical for managing topology assignments."
        }
    },
    {
        "filename": "STORM-3118.json",
        "creation_time": "2018-06-21T13:46:08.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException in Netty Channel Write Operation",
            "Description": "An IndexOutOfBoundsException occurs when attempting to write to a Netty channel. The exception indicates that the writer index exceeds the maximum capacity of the buffer, leading to a failure in the message encoding process.",
            "StackTrace": [
                "org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)",
                "at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:65) [storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:635) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:582) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:461) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) [shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]",
                "Caused by: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)",
                "at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.ensureWritable0(AbstractByteBuf.java:276) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.writeShort(AbstractByteBuf.java:966) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.messaging.netty.SaslMessageToken.write(SaslMessageToken.java:104) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.pacemaker.codec.ThriftEncoder.encodeNettySerializable(ThriftEncoder.java:44) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.pacemaker.codec.ThriftEncoder.encode(ThriftEncoder.java:77) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]",
                "... 26 more"
            ],
            "StepsToReproduce": [
                "Establish a connection using the KerberosSaslClientHandler.",
                "Attempt to send a SaslMessageToken with a payload that exceeds the buffer capacity."
            ],
            "ExpectedBehavior": "The message should be encoded and sent successfully without exceeding the buffer capacity.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown, indicating that the write operation exceeds the buffer's maximum capacity.",
            "AdditionalDetails": "The issue seems to stem from the SaslMessageToken's write method, where the payload length is not properly managed, leading to an attempt to write beyond the buffer's capacity."
        }
    },
    {
        "filename": "STORM-2158.json",
        "creation_time": "2016-10-20T12:56:58.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError in Thrift Server due to Heap Space Exhaustion",
            "Description": "The application encounters a java.lang.OutOfMemoryError indicating that the Java heap space has been exhausted. This issue arises during the handling of read operations in the Thrift server, specifically when attempting to allocate a new ByteBuffer.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[?:1.8.0_92-internal]",
                "at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) ~[?:1.8.0_92-internal]",
                "at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:371) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]",
                "at org.apache.thrift7.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:203) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]",
                "at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:207) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]",
                "at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:158) [storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]"
            ],
            "StepsToReproduce": [
                "Start the Thrift server with a high volume of incoming requests.",
                "Ensure that the server is configured with a limited heap size.",
                "Monitor the server's memory usage until it reaches the limit."
            ],
            "ExpectedBehavior": "The Thrift server should handle incoming requests without exhausting the Java heap space, allowing for normal operation and response to clients.",
            "ObservedBehavior": "The Thrift server throws an OutOfMemoryError, causing it to crash and become unresponsive to incoming requests.",
            "AdditionalDetails": "The issue may be related to the handling of large payloads or a high number of concurrent connections. Consider reviewing the server's memory allocation settings and optimizing the handling of ByteBuffers."
        }
    },
    {
        "filename": "STORM-2682.json",
        "creation_time": "2017-08-07T15:20:27.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Localizer.updateBlobs Method",
            "Description": "A NullPointerException occurs in the Localizer.updateBlobs method when attempting to retrieve resources from a ConcurrentHashMap. This issue arises during the update of blobs for a topology, leading to a RuntimeException that halts the process.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_121]",
                "at org.apache.storm.localizer.Localizer.updateBlobs(Localizer.java:332) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.updateBlobsForTopology(UpdateBlobs.java:99) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.run(UpdateBlobs.java:72) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:54) ~[storm-core-1.0.4.jar:1.0.4]",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:1750) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:63) ~[storm-core-1.0.4.jar:1.0.4]"
            ],
            "StepsToReproduce": [
                "1. Start the Storm supervisor with a topology that requires blob updates.",
                "2. Ensure that the blobstore map is not properly initialized or contains null values.",
                "3. Trigger the update of blobs for the topology."
            ],
            "ExpectedBehavior": "The updateBlobs method should successfully retrieve and update the local resources without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to access a resource in the ConcurrentHashMap, leading to a RuntimeException that halts the process.",
            "AdditionalDetails": "The issue likely stems from the blobstoreMap being null or improperly populated, which is accessed in the updateBlobsForTopology method. Proper checks should be implemented to ensure that the map is not null before attempting to retrieve resources."
        }
    },
    {
        "filename": "STORM-3103.json",
        "creation_time": "2018-06-13T18:23:11.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Nimbus during Supervisor Details Read",
            "Description": "A NullPointerException occurs in the Nimbus class when attempting to read supervisor details, leading to a RuntimeException that halts the process. This issue arises during the scheduling of topologies, specifically when the Nimbus attempts to compute new scheduler assignments.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "    at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "    at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "    at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]",
                "Caused by: java.lang.NullPointerException",
                "    at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "    at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "    at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "    at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "    at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y]",
                "... 2 more"
            ],
            "StepsToReproduce": [
                "Start the Storm Nimbus server.",
                "Attempt to submit a topology that requires scheduling.",
                "Observe the logs for any NullPointerException related to supervisor details."
            ],
            "ExpectedBehavior": "The Nimbus should successfully read all supervisor details and compute new scheduler assignments without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when Nimbus attempts to read supervisor details, leading to a RuntimeException that halts the process.",
            "AdditionalDetails": "The issue seems to stem from the method 'readAllSupervisorDetails' which is called during the computation of new scheduler assignments. It is likely that the method is trying to access a null reference, possibly due to missing or improperly initialized supervisor details."
        }
    }
]