[
    {
        "filename": "HIVE-10992.json",
        "creation_time": "2015-06-12T19:52:35.000+0000",
        "bug_report": {
            "Title": "Invalid Token Exception in Job Management",
            "Description": "A RuntimeException occurs when attempting to find child jobs due to an invalid token error. This issue arises in the WebHCatJTShim23 class when the application tries to retrieve child jobs using the YARN API, but the token cannot be found in the cache.",
            "StackTrace": [
                "java.lang.RuntimeException: Exception occurred while finding child jobs",
                " at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:204)",
                " at org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(WebHCatJTShim23.java:158)",
                " at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(LaunchMapper.java:156)",
                " at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(LaunchMapper.java:124)",
                " at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(LaunchMapper.java:261)",
                " at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)",
                " at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                " at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                " at java.security.AccessController.doPrivileged(Native Method)",
                " at javax.security.auth.Subject.doAs(Subject.java:415)",
                " at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                " at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache",
                " at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                " at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                " at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                " at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                " at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)",
                " at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)",
                " at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:250)",
                " at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                " at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                " at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)",
                " at java.lang.reflect.Method.invoke(Method.java:606)",
                " at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                " at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                " at com.sun.proxy.$Proxy26.getApplications(Unknown Source)",
                " at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:198)",
                " ... 11 more",
                "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache",
                " at org.apache.hadoop.ipc.Client.call(Client.java:1469)",
                " at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                " at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)",
                " at com.sun.proxy.$Proxy25.getApplications(Unknown Source)",
                " at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:247)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that requires child jobs to be managed.",
                "2. Ensure that the token used for authentication is invalid or expired.",
                "3. Attempt to kill or manage the child jobs through the WebHCatJTShim23 class."
            ],
            "ExpectedBehavior": "The system should successfully retrieve and manage child jobs without throwing an exception.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the token cannot be found in the cache, preventing the retrieval of child jobs.",
            "AdditionalDetails": "The issue seems to stem from the token management in the Hadoop security framework, specifically related to the SecretManager. The token's validity should be checked before attempting to retrieve child jobs."
        }
    },
    {
        "filename": "HIVE-16450.json",
        "creation_time": "2017-04-14T13:59:12.000+0000",
        "bug_report": {
            "Title": "JDOException Thrown During Query Execution in Hive Metastore",
            "Description": "A JDOException is thrown when executing a query to retrieve table column statistics in the Hive Metastore. This issue occurs when the method `getMTableColumnStatistics` is called, leading to a failure in obtaining the required statistics for a table.",
            "StackTrace": [
                "javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(ObjectStore.java:6546)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.access$1200(ObjectStore.java:171)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6606)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6595)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.GetHelper.run(ObjectStore.java:2633)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(ObjectStore.java:6594)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatistics(ObjectStore.java:6588)",
                "at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:103)",
                "at com.sun.proxy.$Proxy0.getTableColumnStatistics(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:787)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:247)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3809)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3779)",
                "at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy3.alter_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9617)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9601)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the method `alterTable` on the Hive Metastore with a valid table name and column names.",
                "Ensure that the table exists in the metastore.",
                "Observe the logs for any JDOException thrown during the execution."
            ],
            "ExpectedBehavior": "The method should successfully retrieve the column statistics for the specified table without throwing any exceptions.",
            "ObservedBehavior": "A JDOException is thrown indicating an error occurred while executing the query to retrieve the column statistics.",
            "AdditionalDetails": "The issue may be related to the underlying database connection or the state of the table in the metastore. Further investigation into the database logs and the state of the table may be required."
        }
    },
    {
        "filename": "HIVE-6389.json",
        "creation_time": "2014-02-07T01:33:32.000+0000",
        "bug_report": {
            "Title": "ClassCastException during Hive processing due to null values",
            "Description": "A ClassCastException occurs when the Hive process attempts to cast an Integer to a Text object while processing a row with null values. This issue arises in the MapOperator when handling rows that contain null fields, leading to a runtime error.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":null,\"mymap\":null,\"isnull\":null}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with a schema that includes fields that can be null.",
                "2. Insert a row into the table with null values for some fields, e.g., {\"id\":null,\"mymap\":null,\"isnull\":null}.",
                "3. Execute a Hive query that processes this row."
            ],
            "ExpectedBehavior": "The Hive process should handle null values gracefully without throwing a ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown when attempting to cast an Integer to a Text object, causing the Hive job to fail.",
            "AdditionalDetails": "The issue seems to stem from the WritableStringObjectInspector trying to handle null values incorrectly, leading to a ClassCastException when it encounters an Integer instead of the expected Text type."
        }
    },
    {
        "filename": "HIVE-2372.json",
        "creation_time": "2011-08-12T09:07:34.000+0000",
        "bug_report": {
            "Title": "Hive Runtime Error: Argument List Too Long in ScriptOperator",
            "Description": "A Hive Runtime Error occurs during the processing of a row in the ExecReducer, specifically when attempting to initialize the ScriptOperator. The error is caused by an IOException indicating that the argument list is too long when trying to execute a Perl script.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":129390185139228,\"reducesinkkey1\":\"00008AF10000000063CA6F\"},\"value\":{\"_col0\":\"00008AF10000000063CA6F\",\"_col1\":\"2011-07-27 22:48:52\",\"_col2\":129390185139228,\"_col3\":2006,\"_col4\":4100,\"_col5\":\"10017388=6\",\"_col6\":1063,\"_col7\":\"NULL\",\"_col8\":\"address.com\",\"_col9\":\"NULL\",\"_col10\":\"NULL\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator",
                "at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)",
                "at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)",
                "... 7 more",
                "Caused by: java.io.IOException: Cannot run program \"/usr/bin/perl\": java.io.IOException: error=7, Argument list too long",
                "at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)",
                "at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)",
                "... 15 more",
                "Caused by: java.io.IOException: java.io.IOException: error=7, Argument list too long",
                "at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)",
                "at java.lang.ProcessImpl.start(ProcessImpl.java:65)",
                "at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves a ScriptOperator with a large number of arguments.",
                "2. Ensure that the total length of the arguments exceeds the system's limit for argument lists."
            ],
            "ExpectedBehavior": "The ScriptOperator should initialize successfully and process the input rows without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating that the argument list is too long, preventing the ScriptOperator from initializing and processing the row.",
            "AdditionalDetails": "The issue arises in the `processOp` method of the ScriptOperator class, where the command arguments are constructed and passed to a ProcessBuilder. The error suggests that the combined length of the command and its arguments exceeds the system's limit."
        }
    },
    {
        "filename": "HIVE-2958.json",
        "creation_time": "2012-04-17T15:02:38.000+0000",
        "bug_report": {
            "Title": "ClassCastException during Hive processing due to incompatible LazyDioInteger type",
            "Description": "A ClassCastException is thrown when processing a row in Hive due to an attempt to cast a LazyDioInteger to a LazyInteger. This occurs in the GroupByOperator while processing aggregation operations, leading to a HiveRuntimeError.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:270)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:264)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)",
                "... 9 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(LazyIntObjectInspector.java:43)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:239)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory$ListKeyWrapper.java:150)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory$ListKeyWrapper.java:142)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.copyKey(KeyWrapperFactory$ListKeyWrapper.java:119)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:750)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:722)"
            ],
            "StepsToReproduce": [
                "Run a Hive query that involves aggregation on a dataset containing a column with LazyDioInteger type.",
                "Ensure that the dataset includes rows similar to {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}."
            ],
            "ExpectedBehavior": "The Hive query should process the rows without throwing a ClassCastException, and the aggregation should complete successfully.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating that a LazyDioInteger cannot be cast to a LazyInteger, resulting in a HiveRuntimeError.",
            "AdditionalDetails": "The issue seems to stem from the handling of different Lazy types in the Hive serialization/deserialization process. The GroupByOperator is attempting to process a key that is not of the expected type, leading to the casting error."
        }
    },
    {
        "filename": "HIVE-13392.json",
        "creation_time": "2016-03-30T22:32:50.000+0000",
        "bug_report": {
            "Title": "File Creation Failure Due to Lease Ownership Conflict",
            "Description": "An attempt to create a file in HDFS fails because the file lease is currently owned by another client. This results in an AlreadyBeingCreatedException, indicating that the file is in the process of being created by a different DFS client.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2451)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)"
            ],
            "StepsToReproduce": [
                "Attempt to create a file in HDFS while another client is already creating a file with the same name.",
                "Ensure that the first client does not complete the file creation before the second client attempts to create the same file."
            ],
            "ExpectedBehavior": "The file should be created successfully if no other client is currently creating a file with the same name.",
            "ObservedBehavior": "The file creation fails with an AlreadyBeingCreatedException, indicating that the file lease is owned by another client.",
            "AdditionalDetails": "This issue may occur in high-concurrency environments where multiple clients attempt to create files in the same directory simultaneously. Consider implementing lease recovery mechanisms or file creation retries to mitigate this issue."
        }
    },
    {
        "filename": "HIVE-11301.json",
        "creation_time": "2015-07-18T00:41:40.000+0000",
        "bug_report": {
            "Title": "TProtocolException: Required field 'colStats' is unset in AggrStats",
            "Description": "The application throws a TProtocolException indicating that the required field 'colStats' is unset when attempting to validate an AggrStats object. This occurs during the processing of a Thrift RPC call to retrieve aggregate statistics from the Hive Metastore.",
            "StackTrace": [
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "java.net.SocketException: Socket closed",
                "at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)",
                "at java.net.SocketOutputStream.write(SocketOutputStream.java:153)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)",
                "at java.io.FilterOutputStream.close(FilterOutputStream.java:158)",
                "at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)",
                "at org.apache.thrift.transport.TSocket.close(TSocket.java:196)",
                "at org.apache.hadoop.hive.thrift.TFilterTransport.close(TFilterTransport.java:52)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "org.apache.thrift.transport.TTransportException",
                "at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_aggr_stats_for(ThriftHiveMetastore.java:3029)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_aggr_stats_for(ThriftHiveMetastore.java:3016)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2067)",
                "at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1991)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3124)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:252)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:137)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:125)",
                "at org.apache.hadoop.hive.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:111)"
            ],
            "StepsToReproduce": [
                "Invoke the method getAggrColStatsFor with a valid database name, table name, and column names.",
                "Ensure that the underlying data does not have the 'colStats' field set in the AggrStats object."
            ],
            "ExpectedBehavior": "The method getAggrColStatsFor should return valid aggregate statistics without throwing an exception.",
            "ObservedBehavior": "The method throws a TProtocolException indicating that the required field 'colStats' is unset.",
            "AdditionalDetails": "The AggrStats class requires the 'colStats' field to be set for validation. The absence of this field leads to a failure in the validate method, which is called during the processing of the Thrift RPC response."
        }
    },
    {
        "filename": "HIVE-11028.json",
        "creation_time": "2015-06-16T23:03:38.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException in TezProcessor during initialization",
            "Description": "A RuntimeException is thrown in the TezProcessor class when attempting to initialize and run a processor. The root cause is an IndexOutOfBoundsException, indicating that an attempt was made to access an index in an ArrayList that is out of bounds. This occurs when the list is empty, leading to an attempt to access index 0.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.<init>(StandardStructObjectInspector.java:109)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:290)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:275)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:175)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:313)",
                "at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:71)",
                "at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:99)",
                "at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:362)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that involves a join operation.",
                "2. Ensure that the input data for the join operation is empty or not properly initialized.",
                "3. Observe the exception thrown during the execution."
            ],
            "ExpectedBehavior": "The TezProcessor should handle empty input gracefully without throwing an IndexOutOfBoundsException.",
            "ObservedBehavior": "A RuntimeException is thrown due to an IndexOutOfBoundsException when trying to access an element in an empty list.",
            "AdditionalDetails": "The issue seems to originate from the initialization of the StandardStructObjectInspector, which expects a non-empty list of struct field names. If the input data is empty, this leads to the exception."
        }
    },
    {
        "filename": "HIVE-14380.json",
        "creation_time": "2016-07-29T00:14:58.000+0000",
        "bug_report": {
            "Title": "HiveException: Wrong FS Error When Checking Path Encryption",
            "Description": "The system throws a HiveException indicating that it is unable to determine if a specified HDFS path is encrypted due to a mismatch in the expected filesystem. The error occurs when the path being checked is from a different HDFS cluster than expected.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to determine if hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table, expected: hdfs://bar.ygrid.yahoo.com:8020",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2204)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStrongestEncryptedTablePath(SemanticAnalyzer.java:2274)"
            ],
            "StepsToReproduce": [
                "Attempt to check the encryption status of a table located at 'hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table'.",
                "Ensure that the expected filesystem is set to 'hdfs://bar.ygrid.yahoo.com:8020'.",
                "Observe the exception thrown during the process."
            ],
            "ExpectedBehavior": "The system should successfully determine if the specified HDFS path is encrypted without throwing an exception.",
            "ObservedBehavior": "The system throws a HiveException indicating a wrong filesystem error due to a mismatch between the actual and expected HDFS paths.",
            "AdditionalDetails": "The issue arises in the 'isPathEncrypted' method when it attempts to check the encryption status of a path that belongs to a different HDFS cluster than the one expected. The method 'getStrongestEncryptedTablePath' calls 'isPathEncrypted', leading to the exception when it encounters the path from 'foo.ygrid.yahoo.com' instead of 'bar.ygrid.yahoo.com'."
        }
    },
    {
        "filename": "HIVE-7799.json",
        "creation_time": "2014-08-20T09:45:21.000+0000",
        "bug_report": {
            "Title": "NullPointerException in HiveKVResultCache.next() Method",
            "Description": "A NullPointerException is thrown when attempting to iterate over results in the HiveKVResultCache class. This issue occurs during the execution of a Spark job that involves shuffling data, specifically when accessing the next element in the result cache.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.next(HiveKVResultCache.java:113)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:124)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:82)",
                "at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)",
                "at scala.collection.Iterator$class.foreach(Iterator.scala:727)",
                "at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)",
                "at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:65)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "at org.apache.spark.scheduler.Task.run(Task.scala:54)",
                "at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Execute a Spark job that involves shuffling data using Hive.",
                "2. Ensure that the job attempts to access results from the HiveKVResultCache.",
                "3. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The HiveKVResultCache should return the next element without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when calling the next() method on the HiveKVResultCache, indicating that an expected object is null.",
            "AdditionalDetails": "The issue may be related to the state of the HiveKVResultCache or the data being processed. Further investigation is needed to determine the conditions under which the cache is null."
        }
    },
    {
        "filename": "HIVE-6537.json",
        "creation_time": "2014-03-03T18:57:44.000+0000",
        "bug_report": {
            "Title": "NullPointerException in HashTableLoader during MapJoin operation",
            "Description": "A NullPointerException is thrown in the HashTableLoader class when attempting to load a hash table during a MapJoin operation. This occurs specifically in the load method, which is called from the loadHashTable method. The issue arises when the input file changes and the hash table is not properly initialized or populated, leading to an attempt to fill an array with null values.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.NullPointerException",
                "at java.util.Arrays.fill(Arrays.java:2685)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:81)"
            ],
            "StepsToReproduce": [
                "1. Execute a MapJoin operation in Hive with input files that may change during processing.",
                "2. Ensure that the hash table is not properly initialized or populated before the operation.",
                "3. Observe the logs for a NullPointerException in the HashTableLoader."
            ],
            "ExpectedBehavior": "The hash table should be loaded successfully without throwing a NullPointerException, even if the input file changes.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that an attempt was made to fill an array with null values, leading to a failure in the MapJoin operation.",
            "AdditionalDetails": "The issue seems to stem from the loadHashTable method not properly handling cases where the input file changes and the hash table is not initialized correctly. The method should include checks to ensure that the hash table is ready for loading before attempting to fill it."
        }
    },
    {
        "filename": "HIVE-13691.json",
        "creation_time": "2016-05-04T23:40:03.000+0000",
        "bug_report": {
            "Title": "Timeout Exception in Hive Metastore during Partition Resolution",
            "Description": "A timeout exception occurs when attempting to retrieve table partitions in the Hive Metastore. This issue arises during the execution of the `getTable` method, which leads to a failure in the compaction process due to an inability to resolve the partition.",
            "StackTrace": [
                "Exception: MetaException(message:Timeout when executing method: getTable)",
                "at org.apache.hadoop.hive.metastore.Deadline.newMetaException(Deadline.java:187)",
                "at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:177)",
                "at org.apache.hadoop.hive.metastore.Deadline.checkTimeout(Deadline.java:160)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1839)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2255)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.access$300(ObjectStore.java:165)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2051)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2043)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2400)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal(ObjectStore.java:2043)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNames(ObjectStore.java:2037)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)",
                "at com.sun.proxy.$Proxy0.getPartitionsByNames(Unknown Source)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolvePartition(CompactorThread.java:111)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:129)",
                "Caused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable",
                "at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)"
            ],
            "StepsToReproduce": [
                "1. Initiate a compaction process in Hive.",
                "2. Ensure that the compaction process attempts to resolve a partition.",
                "3. Observe the logs for timeout exceptions related to the `getTable` method."
            ],
            "ExpectedBehavior": "The Hive Metastore should successfully retrieve the table partitions without timing out, allowing the compaction process to complete successfully.",
            "ObservedBehavior": "A timeout exception is thrown when executing the `getTable` method, leading to a failure in marking the compaction as completed due to the inability to find the required partition.",
            "AdditionalDetails": "The `check()` method in the `Deadline` class is responsible for monitoring the execution time of the `getTable` method. If the execution exceeds the specified timeout, a `DeadlineException` is thrown, which is then wrapped in a `MetaException`. The `markFailed` method in the `CompactionTxnHandler` class also indicates that no record with `CQ_ID=0` was found in the `COMPACTION_QUEUE`, which may be a consequence of the timeout issue."
        }
    },
    {
        "filename": "HIVE-17758.json",
        "creation_time": "2017-10-10T12:33:52.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException: Timeout Value is Negative in ObjectStore",
            "Description": "An IllegalArgumentException is thrown when a negative timeout value is passed to the Thread.sleep method within the ObjectStore class of the Hive Metastore. This occurs during the execution of the addNotificationEvent method, specifically when attempting to lock for an update.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: timeout value is negative",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:7407)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:7361)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7424)",
                "at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)"
            ],
            "StepsToReproduce": [
                "Invoke the addNotificationEvent method with a configuration that results in a negative sleepInterval value.",
                "Ensure that the method attempts to acquire a lock for update, which triggers the RetryingExecutor's run method."
            ],
            "ExpectedBehavior": "The system should handle the retry mechanism without throwing an IllegalArgumentException, even if the sleep interval is set to a low or zero value.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the timeout value is negative, causing the process to fail.",
            "AdditionalDetails": "The issue arises from the sleepInterval variable, which is likely derived from a configuration setting. It is essential to validate this value before passing it to Thread.sleep to prevent negative values."
        }
    },
    {
        "filename": "HIVE-14898.json",
        "creation_time": "2016-10-06T00:02:36.000+0000",
        "bug_report": {
            "Title": "HttpAuthenticationException due to Empty Authorization Header",
            "Description": "An HttpAuthenticationException is thrown when the authorization header received from the client is empty during Kerberos authentication in the ThriftHttpServlet class. This issue occurs in the doKerberosAuth method when attempting to retrieve the service ticket from the request.",
            "StackTrace": [
                "org.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:170)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:83)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)",
                "at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:952)",
                "at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:167)",
                "... 23 more",
                "Caused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(ThriftHttpServlet.java:311)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.access$100(ThriftHttpServlet.java:59)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:212)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)"
            ],
            "StepsToReproduce": [
                "Send a POST request to the ThriftHttpServlet without an Authorization header.",
                "Observe the server logs for the HttpAuthenticationException."
            ],
            "ExpectedBehavior": "The server should successfully authenticate the request if a valid Authorization header is provided.",
            "ObservedBehavior": "The server throws an HttpAuthenticationException indicating that the Authorization header received from the client is empty.",
            "AdditionalDetails": "The issue arises in the getAuthHeader method, which fails to retrieve a valid service ticket due to the absence of the Authorization header. This leads to a failure in establishing a Kerberos context."
        }
    },
    {
        "filename": "HIVE-5546.json",
        "creation_time": "2013-10-15T15:06:59.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError in Hadoop MapTask",
            "Description": "The application encounters a java.lang.OutOfMemoryError due to insufficient Java heap space while executing a MapTask. This error occurs during the initialization of the MapOutputBuffer, indicating that the system is unable to allocate enough memory for the task's output buffer.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "Run a Hadoop job that processes a large dataset.",
                "Ensure that the job requires a significant amount of memory for the MapTask.",
                "Monitor the memory usage during the execution of the job."
            ],
            "ExpectedBehavior": "The MapTask should execute successfully without running out of memory, processing the data as intended.",
            "ObservedBehavior": "The MapTask fails with a java.lang.OutOfMemoryError, indicating that the Java heap space is insufficient to handle the output buffer.",
            "AdditionalDetails": "Consider increasing the Java heap size allocated to the Hadoop MapTask by adjusting the configuration settings (e.g., mapreduce.map.memory.mb) to prevent this error in future executions."
        }
    },
    {
        "filename": "HIVE-7557.json",
        "creation_time": "2014-07-30T19:25:12.000+0000",
        "bug_report": {
            "Title": "ClassCastException in VectorExpressionWriter while processing vector batch",
            "Description": "A ClassCastException occurs when the system attempts to cast a DoubleColumnVector to a LongColumnVector during the processing of a vector batch in Hive. This issue arises in the VectorExpressionWriterFactory when writing values from a vectorized row batch.",
            "StackTrace": [
                "Error: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:394)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that processes a vectorized row batch containing both Double and Long column types.",
                "2. Ensure that the data being processed includes a DoubleColumnVector that is incorrectly expected to be a LongColumnVector.",
                "3. Observe the exception thrown during the execution."
            ],
            "ExpectedBehavior": "The system should correctly process the vectorized row batch without throwing a ClassCastException, handling the data types appropriately.",
            "ObservedBehavior": "A ClassCastException is thrown indicating that a DoubleColumnVector cannot be cast to a LongColumnVector, leading to a failure in processing the vector batch.",
            "AdditionalDetails": "The issue seems to stem from the way the vectorized row batch is being constructed or the data types being used. The method 'writeValue(long value)' in the VectorExpressionWriterFactory is expected to handle long values, but it encounters a double value instead, leading to the ClassCastException."
        }
    },
    {
        "filename": "HIVE-1712.json",
        "creation_time": "2010-10-14T17:17:44.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Hive.getTable() Method",
            "Description": "A NullPointerException occurs when attempting to retrieve a table using the Hive.getTable() method. The error is triggered when the method tries to put values into a Hashtable, which indicates that a required object is null.",
            "StackTrace": [
                "ERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException",
                "at java.util.Hashtable.put(Hashtable.java:394)",
                "at java.util.Hashtable.putAll(Hashtable.java:466)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)",
                "at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that attempts to retrieve a table using the Hive.getTable() method.",
                "2. Ensure that the table name provided is valid but the underlying metadata is not properly initialized or is null."
            ],
            "ExpectedBehavior": "The getTable() method should return the requested table object without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a required object is null during the execution of the getTable() method.",
            "AdditionalDetails": "The issue seems to stem from the getSchema() method in MetaStoreUtils, which is called by getTable(). If the schema retrieval fails due to a null reference, it leads to the NullPointerException when attempting to put values into a Hashtable."
        }
    },
    {
        "filename": "HIVE-12608.json",
        "creation_time": "2015-12-07T21:26:01.000+0000",
        "bug_report": {
            "Title": "RuntimeException: cannot find field c2 in [c1] during FetchOperator execution",
            "Description": "The application encounters a RuntimeException indicating that it cannot find the field 'c2' in the expected structure while processing a fetch operation in Hive. This issue arises during the execution of a query that involves fetching data from a Parquet file, which suggests a mismatch between the expected schema and the actual data schema.",
            "StackTrace": [
                "java.io.IOException: java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1029)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1003)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:139)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_type_promotion(TestCliDriver.java:123)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getStructFieldTypeInfo(HiveStructConverter.java:130)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getFieldTypeIgnoreCase(HiveStructConverter.java:103)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.init(HiveStructConverter.java:90)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:67)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:59)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:63)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:75)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter$ElementConverter.<init>(HiveCollectionConverter.java:141)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter.<init>(HiveCollectionConverter.java:52)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that fetches data from a Parquet file.",
                "Ensure that the expected schema includes a field 'c2' that is not present in the actual data."
            ],
            "ExpectedBehavior": "The fetch operation should successfully retrieve rows from the Parquet file without any exceptions, and all expected fields should be present in the data.",
            "ObservedBehavior": "The fetch operation fails with a RuntimeException indicating that the field 'c2' cannot be found in the expected structure.",
            "AdditionalDetails": "This issue may arise due to a schema mismatch between the Hive table definition and the actual data stored in the Parquet file. It is essential to verify that the schema defined in Hive matches the schema of the Parquet file being queried."
        }
    },
    {
        "filename": "HIVE-17774.json",
        "creation_time": "2017-10-11T20:02:01.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException during Job Commit in CompactorMR",
            "Description": "A FileNotFoundException is thrown when attempting to commit a job in the CompactorMR class. The error indicates that a temporary file expected to exist at a specific path does not exist, leading to a failure in the job commit process.",
            "StackTrace": [
                "java.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Initiate a job that involves the CompactorMR class.",
                "2. Ensure that the job attempts to commit after processing data.",
                "3. Observe the logs for any FileNotFoundException related to the temporary file path."
            ],
            "ExpectedBehavior": "The job should successfully commit, moving the contents from the temporary location to the final location without any exceptions.",
            "ObservedBehavior": "A FileNotFoundException is thrown indicating that the expected temporary file does not exist, causing the job commit to fail.",
            "AdditionalDetails": "The commitJob method in the CompactorOutputCommitter class attempts to list the contents of a temporary directory. If the directory does not exist, it results in a FileNotFoundException. This suggests that the temporary file creation process may have failed or the file was deleted prematurely."
        }
    },
    {
        "filename": "HIVE-14564.json",
        "creation_time": "2016-08-18T00:11:34.000+0000",
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException in Hive during row processing",
            "Description": "An ArrayIndexOutOfBoundsException occurs in the Hive processing pipeline when handling certain rows, leading to a HiveException. This issue arises during the execution of the MapOperator, specifically when processing rows that involve LazyBinaryStruct objects.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)",
                "... 9 more",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at java.lang.System.arraycopy(Native Method)",
                "at org.apache.hadoop.io.Text.set(Text.java:225)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(LazyBinaryString.java:48)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)",
                "at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:94)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(ReduceSinkOperator.java:550)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:377)"
            ],
            "StepsToReproduce": [
                "Run a Hive query that processes rows with LazyBinaryStruct objects.",
                "Ensure that the input data contains rows that may lead to an ArrayIndexOutOfBoundsException during processing."
            ],
            "ExpectedBehavior": "The Hive processing pipeline should handle all rows without throwing an ArrayIndexOutOfBoundsException, successfully processing each row and returning the expected results.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown during the processing of certain rows, resulting in a HiveException and halting the processing of the query.",
            "AdditionalDetails": "The issue seems to stem from the LazyBinaryStruct's handling of fields, particularly in the uncheckedGetField method, which may be trying to access an index that is out of bounds due to incorrect data or structure in the input."
        }
    },
    {
        "filename": "HIVE-3651.json",
        "creation_time": "2012-11-01T23:31:20.000+0000",
        "bug_report": {
            "Title": "Hive Job Fails Due to Missing Hashtable File",
            "Description": "A Hive job fails during execution because it attempts to access a hashtable file that does not exist in the specified directory. This results in a RuntimeException being thrown, indicating that the file could not be found.",
            "StackTrace": [
                "java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)",
                "Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:679)"
            ],
            "StepsToReproduce": [
                "Run a Hive job that involves a MapJoin operation.",
                "Ensure that the job attempts to create or access a hashtable file.",
                "Observe the job execution to see if it fails with a missing file error."
            ],
            "ExpectedBehavior": "The Hive job should successfully create and access the hashtable file without any errors.",
            "ObservedBehavior": "The Hive job fails with a RuntimeException indicating that the specified hashtable file does not exist.",
            "AdditionalDetails": "The issue may be related to the configuration of the scratch directory or the job's execution context. Further investigation is needed to ensure that the hashtable files are being created as expected."
        }
    },
    {
        "filename": "HIVE-5199.json",
        "creation_time": "2013-09-03T20:40:29.000+0000",
        "bug_report": {
            "Title": "ClassCastException in FetchOperator due to incompatible ObjectInspector",
            "Description": "A ClassCastException occurs in the FetchOperator when attempting to cast a ProtoMapObjectInspector to a SettableMapObjectInspector. This issue arises during the execution of the getNextRow() method, which is responsible for fetching the next row of data. The root cause is a mismatch in the expected and actual types of ObjectInspectors being used.",
            "StackTrace": [
                "java.io.IOException: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1412)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "Caused by: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:144)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:307)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:138)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:406)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that involves fetching data using the FetchOperator.",
                "Ensure that the data being fetched uses a ProtoMapObjectInspector.",
                "Observe the exception thrown during the execution."
            ],
            "ExpectedBehavior": "The FetchOperator should successfully fetch the next row of data without throwing a ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown indicating that ProtoMapObjectInspector cannot be cast to SettableMapObjectInspector, leading to a failure in fetching data.",
            "AdditionalDetails": "The issue seems to stem from the ObjectInspectorConverters.getConverter method, which is attempting to convert between incompatible ObjectInspector types. This indicates a potential misconfiguration or mismatch in the data types being used in the Hive query."
        }
    },
    {
        "filename": "HIVE-17368.json",
        "creation_time": "2017-08-22T01:27:32.000+0000",
        "bug_report": {
            "Title": "SaslException: GSS initiate failed due to missing Kerberos credentials",
            "Description": "The application encounters a SaslException indicating that the GSS initiate failed because no valid Kerberos credentials were provided. This issue arises during the connection to the Hive metastore when SASL authentication is enabled but the necessary Kerberos ticket-granting ticket (TGT) is not available.",
            "StackTrace": [
                "javax.security.sasl.SaslException: GSS initiate failed",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_121]",
                "at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:488) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:255) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_121]",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_121]",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3595) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3647) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3627) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]",
                "at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnTokenStore(DBTokenStore.java:157) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.DBTokenStore.addToken(DBTokenStore.java:74) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:142) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:56) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.security.token.Token.<init>(Token.java:59) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getDelegationToken(DelegationTokenSecretManager.java:109) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:123) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationToken(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationTokenWithService(HiveDelegationTokenManager.java:130) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.auth.HiveAuthFactory.getDelegationToken(HiveAuthFactory.java:261) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken(HiveSessionImplwithUGI.java:174) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at com.sun.proxy.$Proxy36.getDelegationToken(Unknown Source) [?:?]",
                "at org.apache.hive.service.cli.CLIService.getDelegationToken(CLIService.java:589) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.GetDelegationToken(ThriftCLIService.java:254) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1737) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1722) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:621) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [libthrift-0.9.3.jar:0.9.3]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]",
                "Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) ~[?:1.8.0_121]",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) ~[?:1.8.0_121]",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) ~[?:1.8.0_121]",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) ~[?:1.8.0_121]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) ~[?:1.8.0_121]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_121]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_121]",
                "... 65 more"
            ],
            "StepsToReproduce": [
                "1. Ensure that the Hive metastore is configured to use SASL authentication.",
                "2. Attempt to connect to the Hive metastore without a valid Kerberos ticket.",
                "3. Observe the exception thrown during the connection attempt."
            ],
            "ExpectedBehavior": "The application should successfully connect to the Hive metastore using SASL authentication if valid Kerberos credentials are provided.",
            "ObservedBehavior": "The application throws a SaslException indicating that the GSS initiate failed due to missing Kerberos credentials.",
            "AdditionalDetails": "Ensure that the Kerberos configuration is correct and that a valid TGT is available before attempting to connect to the Hive metastore."
        }
    },
    {
        "filename": "HIVE-4233.json",
        "creation_time": "2013-03-26T13:02:20.000+0000",
        "bug_report": {
            "Title": "RuntimeException when instantiating HiveMetaStoreClient due to invalid Kerberos ticket",
            "Description": "A RuntimeException is thrown when attempting to instantiate the HiveMetaStoreClient. The root cause is an IllegalStateException indicating that the Kerberos ticket is no longer valid. This issue arises during the process of creating a new instance of the HiveMetaStoreClient, which is essential for interacting with the Hive metastore.",
            "StackTrace": [
                "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1084)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:51)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:61)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2140)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2151)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(Hive.java:2275)",
                "at org.apache.hive.service.cli.CLIService.getDelegationTokenFromMetaStore(CLIService.java:358)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:127)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1073)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1058)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:565)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer$WorkerProcess.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedConstructorAccessor52.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at sun.reflect.Constructor.newInstance(Constructor.java:513)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1082)",
                "... 16 more",
                "Caused by: java.lang.IllegalStateException: This ticket is no longer valid",
                "at javax.security.auth.kerberos.KerberosTicket.toString(KerberosTicket.java:601)",
                "at java.lang.String.valueOf(String.java:2826)",
                "at java.lang.StringBuilder.append(StringBuilder.java:115)",
                "at sun.security.jgss.krb5.SubjectComber.findAux(SubjectComber.java:120)",
                "at sun.security.jgss.krb5.SubjectComber.find(SubjectComber.java:41)",
                "at sun.security.jgss.krb5.Krb5Util.getTicket(Krb5Util.java:130)",
                "at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:328)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getTgt(Krb5InitCredential.java:325)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:128)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)",
                "at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)",
                "at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)",
                "at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:277)"
            ],
            "StepsToReproduce": [
                "Attempt to create a new HiveMetaStoreClient instance.",
                "Ensure that the Kerberos ticket used for authentication is expired or invalid."
            ],
            "ExpectedBehavior": "The HiveMetaStoreClient should be instantiated successfully, allowing interaction with the Hive metastore.",
            "ObservedBehavior": "A RuntimeException is thrown, indicating that the HiveMetaStoreClient cannot be instantiated due to an invalid Kerberos ticket.",
            "AdditionalDetails": "The issue is likely related to the Kerberos authentication mechanism, which requires a valid ticket for successful instantiation of the HiveMetaStoreClient. Ensure that the Kerberos ticket is valid before attempting to create the client."
        }
    },
    {
        "filename": "HIVE-14303.json",
        "creation_time": "2016-07-21T03:16:20.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Hive Join Operator during Reduce Task Closure",
            "Description": "A NullPointerException is thrown in the Hive Join Operator when closing the ExecReducer, leading to a RuntimeException. This occurs during the execution of a MapReduce job, specifically when the endGroup method is called, which subsequently calls checkAndGenObject. The issue arises when the method attempts to access an object that is null, causing the process to fail.",
            "StackTrace": [
                "Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: null",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)",
                "at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)"
            ],
            "StepsToReproduce": [
                "1. Execute a MapReduce job that involves a Hive join operation.",
                "2. Ensure that the data being processed includes cases that may lead to null entries in the join conditions.",
                "3. Monitor the execution until the reduce phase is reached, particularly focusing on the closure of the ExecReducer."
            ],
            "ExpectedBehavior": "The ExecReducer should close without throwing any exceptions, successfully processing all rows and handling any null values appropriately.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the checkAndGenObject method, leading to a RuntimeException when closing the ExecReducer.",
            "AdditionalDetails": "The issue seems to stem from the checkAndGenObject method, which does not handle cases where the AbstractRowContainer may not contain any rows, leading to potential null dereferences. Further investigation into the conditions that lead to this state is required."
        }
    },
    {
        "filename": "HIVE-19248.json",
        "creation_time": "2018-04-19T17:45:21.000+0000",
        "bug_report": {
            "Title": "IOException during HDFS file copy due to checksum mismatch",
            "Description": "An IOException occurs when attempting to copy a file from one HDFS location to another. The error indicates a checksum mismatch between the source and target files, which is likely caused by differing block sizes. The operation fails, and the system suggests using the '-pb' option to preserve block sizes or '-skipCrc' to bypass checksum checks, although the latter may risk data corruption.",
            "StackTrace": [
                "java.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                " at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:299)",
                " at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:266)",
                " at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)",
                " at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                " at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                " at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                " at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)",
                " at java.security.AccessController.doPrivileged(Native Method)",
                " at javax.security.auth.Subject.doAs(Subject.java:422)",
                " at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                " at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)",
                "Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                " at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)",
                " at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:296)",
                " ... 10 more",
                "Caused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)",
                " at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:212)",
                " at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)",
                " at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)",
                " at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)"
            ],
            "StepsToReproduce": [
                "Attempt to copy a file from hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "Ensure that the source and target files have different block sizes."
            ],
            "ExpectedBehavior": "The file should be copied successfully from the source HDFS location to the target HDFS location without any errors.",
            "ObservedBehavior": "The file copy operation fails with an IOException due to a checksum mismatch, indicating that the source and target files differ in block size.",
            "AdditionalDetails": "Consider using the '-pb' option to preserve block sizes during the copy operation or the '-skipCrc' option to bypass checksum checks, keeping in mind the risks associated with data integrity."
        }
    },
    {
        "filename": "HIVE-7167.json",
        "creation_time": "2014-06-02T18:13:36.000+0000",
        "bug_report": {
            "Title": "Connection Refused Error When Starting Hive Session",
            "Description": "The application throws a RuntimeException when attempting to start a Hive session due to an inability to connect to the Hive MetaStore. The root cause appears to be a connection refusal error, indicating that the MetaStore service may not be running or is unreachable.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:347)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1413)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2444)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2456)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:341)",
                "... 7 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingMethodAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1411)",
                "... 12 more",
                "Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused: connect",
                "at org.apache.thrift.transport.TSocket.open(TSocket.java:185)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:336)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:214)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingMethodAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1411)",
                "... 12 more",
                "Caused by: java.net.ConnectException: Connection refused: connect",
                "at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method)",
                "at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)",
                "at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)",
                "at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)",
                "at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:157)",
                "at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)",
                "at java.net.Socket.connect(Socket.java:579)",
                "at org.apache.thrift.transport.TSocket.open(TSocket.java:180)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "Start the Hive CLI or application that initializes a Hive session.",
                "Ensure that the Hive MetaStore service is not running or is unreachable.",
                "Observe the exception thrown during the initialization process."
            ],
            "ExpectedBehavior": "The Hive session should start successfully, establishing a connection to the Hive MetaStore.",
            "ObservedBehavior": "The application throws a RuntimeException indicating that it is unable to instantiate the HiveMetaStoreClient due to a connection refusal error.",
            "AdditionalDetails": "Check if the Hive MetaStore service is running and accessible at the configured URIs. Ensure that network configurations allow connections to the MetaStore."
        }
    },
    {
        "filename": "HIVE-12360.json",
        "creation_time": "2015-11-06T18:04:00.000+0000",
        "bug_report": {
            "Title": "IOException due to IllegalArgumentException in FetchOperator",
            "Description": "An IOException is thrown when attempting to fetch rows from a Hive table, indicating that a seek operation is attempting to access an index that is outside the available data range. This issue arises in the FetchOperator's getNextRow method, which is responsible for retrieving the next row of data from the input source.",
            "StackTrace": [
                "java.io.IOException: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.seek(InStream.java:139)",
                "at org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.read(InStream.java:87)",
                "at java.io.InputStream.read(InputStream.java:102)",
                "at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)",
                "at com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7429)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7393)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7482)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7477)",
                "at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200)",
                "at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)",
                "at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:223)",
                "at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.parseFrom(OrcProto.java:7593)",
                "at org.apache.hadoop.hive.ql.io.orc.MetadataReader.readRowIndex(MetadataReader.java:88)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1166)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1151)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:750)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1235)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1117)",
                "at org.apache.hadoop.hive.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)",
                "at org.apache.hadoop.hive.exec.FetchOperator.getRecordReader(FetchOperator.java:324)",
                "at org.apache.hadoop.hive.exec.FetchOperator.getNextRow(FetchOperator.java:446)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that fetches data from a table.",
                "Ensure that the data being fetched has an index that is out of bounds.",
                "Observe the exception thrown during the fetch operation."
            ],
            "ExpectedBehavior": "The FetchOperator should successfully retrieve the next row of data without throwing an exception.",
            "ObservedBehavior": "An IOException is thrown indicating that a seek operation is attempting to access an index that is outside the available data range.",
            "AdditionalDetails": "The root cause appears to be related to the seek method in the InStream class, which is unable to find the requested index within the available data. This may indicate a problem with the data integrity or the way the data is being read."
        }
    },
    {
        "filename": "HIVE-13160.json",
        "creation_time": "2016-02-26T00:02:11.000+0000",
        "bug_report": {
            "Title": "RuntimeException during HiveServer2 Initialization",
            "Description": "The application encounters a RuntimeException when attempting to instantiate the SessionHiveMetaStoreClient, which leads to a NullPointerException during the shutdown process of HiveServer2. This issue appears to stem from a failure in the configuration or initialization of the Hive metastore client.",
            "StackTrace": [
                "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)",
                "java.lang.NullPointerException",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)",
                "at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:69)",
                "at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:545)"
            ],
            "StepsToReproduce": [
                "Start the HiveServer2 service.",
                "Ensure that the Hive metastore configuration is set up correctly.",
                "Observe the logs for any RuntimeExceptions during the initialization phase."
            ],
            "ExpectedBehavior": "The HiveServer2 should start successfully without throwing any exceptions, and the metastore client should be instantiated correctly.",
            "ObservedBehavior": "The HiveServer2 fails to start due to a RuntimeException when trying to instantiate the SessionHiveMetaStoreClient, followed by a NullPointerException during the shutdown process.",
            "AdditionalDetails": "The issue may be related to the configuration of the Hive metastore or the classpath not including the necessary dependencies for the SessionHiveMetaStoreClient. Further investigation into the configuration settings and the environment setup is recommended."
        }
    },
    {
        "filename": "HIVE-12008.json",
        "creation_time": "2015-10-01T19:26:46.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException during Map Operator Initialization",
            "Description": "An IndexOutOfBoundsException occurs when initializing the map operator in the Hadoop job configuration. This issue arises when the code attempts to access an index in an ArrayList that is out of bounds, leading to a runtime failure.",
            "StackTrace": [
                "Exception running child : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 9 more",
                "Caused by: java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)",
                "... 14 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 17 more",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:147)",
                "... 22 more",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)"
            ],
            "StepsToReproduce": [
                "1. Configure a Hadoop job with a Map operator.",
                "2. Ensure that the job configuration leads to a scenario where the ArrayList accessed in the ExecMapper class has fewer elements than expected.",
                "3. Execute the job."
            ],
            "ExpectedBehavior": "The map operator should initialize successfully without throwing an IndexOutOfBoundsException.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown during the initialization of the map operator, causing the job to fail.",
            "AdditionalDetails": "The issue seems to stem from the 'configure' method in the ExecMapper class, where it attempts to access an index in an ArrayList that is not available. The specific line causing the issue is not provided, but it is likely related to the handling of dummy operators or local work."
        }
    },
    {
        "filename": "HIVE-6205.json",
        "creation_time": "2014-01-15T07:34:15.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Hive Driver during Authorization Check",
            "Description": "A NullPointerException occurs in the Hive Driver's doAuthorization method when attempting to compile and run a query. This issue arises when the semantic analyzer object is null, leading to a failure in the authorization process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that requires authorization checks.",
                "2. Ensure that the semantic analyzer is not properly initialized or is null.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The Hive Driver should successfully perform authorization checks without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the doAuthorization method, indicating that the semantic analyzer object is null.",
            "AdditionalDetails": "The issue likely stems from the semantic analysis phase where the semantic analyzer is expected to be initialized but is not. This can occur if the query parsing fails or if there are issues with the configuration that prevent the semantic analyzer from being created."
        }
    },
    {
        "filename": "HIVE-15309.json",
        "creation_time": "2016-11-29T21:56:28.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException in OrcRawRecordMerger during Compaction",
            "Description": "A FileNotFoundException is thrown when attempting to access a file that does not exist in the specified HDFS path during the compaction process in Hive. This issue occurs in the OrcRawRecordMerger class when trying to read the last flush length of an ORC file.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/rrslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1831)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1744)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:373)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1496)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1396)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)",
                "at com.sun.proxy.$Proxy14.getBlockLocations(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:270)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:278)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:194)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:176)",
                "at com.sun.proxy.$Proxy15.getBlockLocations(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1211)",
                "at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)",
                "at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)",
                "at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)",
                "at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1536)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:330)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:326)",
                "at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:782)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:460)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1525)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:631)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:610)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "Trigger a compaction job in Hive that processes ORC files.",
                "Ensure that the specified delta directory contains a reference to a non-existent file.",
                "Monitor the logs for the FileNotFoundException."
            ],
            "ExpectedBehavior": "The compaction process should successfully read the ORC files and complete without throwing a FileNotFoundException.",
            "ObservedBehavior": "The compaction process fails with a FileNotFoundException indicating that the specified file does not exist in HDFS.",
            "AdditionalDetails": "The issue may be related to the management of delta directories and the lifecycle of files within HDFS. Further investigation is needed to ensure that files are correctly created and referenced during the compaction process."
        }
    },
    {
        "filename": "HIVE-10808.json",
        "creation_time": "2015-05-23T02:24:16.000+0000",
        "bug_report": {
            "Title": "ClassCastException during Map Operator Initialization in Hive",
            "Description": "A ClassCastException occurs when initializing the Map operator in Hive due to an incorrect type cast involving ObjectInspectors. This leads to a RuntimeException that prevents the job from configuring properly.",
            "StackTrace": [
                "java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 9 more",
                "Caused by: java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)",
                "... 14 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 17 more",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:157)",
                "... 22 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:334)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:352)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)",
                "... 22 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspectorUtils.java:1111)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1149)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:219)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:183)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:316)"
            ],
            "StepsToReproduce": [
                "1. Submit a Hive job that involves a Map operator.",
                "2. Ensure that the job configuration includes a NullStructSerDe.",
                "3. Observe the job execution to see if the ClassCastException occurs."
            ],
            "ExpectedBehavior": "The Map operator should initialize successfully without any ClassCastException, allowing the job to run as expected.",
            "ObservedBehavior": "A ClassCastException is thrown during the initialization of the Map operator, causing the job to fail with a RuntimeException.",
            "AdditionalDetails": "The issue seems to stem from the use of NullStructSerDe, which is not compatible with the expected PrimitiveObjectInspector type. This indicates a potential misconfiguration in the job setup or an issue with the data types being processed."
        }
    },
    {
        "filename": "HIVE-18429.json",
        "creation_time": "2018-01-10T20:45:15.000+0000",
        "bug_report": {
            "Title": "FileNotFoundException during Hive Compaction Job Commit",
            "Description": "A FileNotFoundException is thrown when attempting to commit a Hive compaction job due to a missing temporary file in HDFS. This issue occurs in the CompactorOutputCommitter class when it tries to list the status of a file that does not exist.",
            "StackTrace": [
                "java.io.FileNotFoundException: File hdfs://OTCHaaS/apps/hive/warehouse/momi.db/sensor_data/babyid=5911806ebf69640100004257/_tmp_b4c5a3f3-44e5-4d45-86af-5b773bf0fc96 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:923)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:114)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:985)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:981)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:992)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:785)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Initiate a Hive compaction job on the specified table.",
                "2. Ensure that the temporary file is not created or is deleted before the commit phase.",
                "3. Observe the logs for the FileNotFoundException during the commit process."
            ],
            "ExpectedBehavior": "The Hive compaction job should successfully commit without throwing a FileNotFoundException, indicating that all necessary files are present.",
            "ObservedBehavior": "The Hive compaction job fails with a FileNotFoundException, indicating that a required temporary file does not exist in HDFS.",
            "AdditionalDetails": "The issue may be related to the lifecycle of temporary files during the compaction process. Further investigation is needed to ensure that temporary files are created and managed correctly before the commit phase."
        }
    },
    {
        "filename": "HIVE-10776.json",
        "creation_time": "2015-05-21T00:56:28.000+0000",
        "bug_report": {
            "Title": "NullPointerException in SemanticAnalyzer during query compilation",
            "Description": "A NullPointerException occurs in the SemanticAnalyzer class when attempting to generate a reduce sink plan during the compilation of a Hive query. This issue arises specifically in the genReduceSinkPlan method, indicating that a required object is not being initialized properly before being accessed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:7257)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6100)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6271)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8972)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8863)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9708)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9601)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10037)",
                "        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:323)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)",
                "        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)",
                "        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "        at java.lang.reflect.Method.invoke(Method.java:606)",
                "        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that requires a reduce sink operation.",
                "2. Observe the logs for a NullPointerException in the SemanticAnalyzer."
            ],
            "ExpectedBehavior": "The query should compile successfully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the compilation of the query, specifically in the genReduceSinkPlan method of the SemanticAnalyzer.",
            "AdditionalDetails": "The issue may be related to uninitialized variables or objects in the genReduceSinkPlan method. Further investigation is needed to identify which specific object is null at the time of the exception."
        }
    },
    {
        "filename": "HIVE-6301.json",
        "creation_time": "2014-01-24T01:42:18.000+0000",
        "bug_report": {
            "Title": "IllegalStateException in UDFJson.evaluate due to No match found",
            "Description": "The UDFJson class's evaluate method throws an IllegalStateException when attempting to extract data from a JSON string using a specified path. This occurs when the regex matcher does not find a match for the provided path, leading to an attempt to access a group that does not exist.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String)  on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2",
                "Caused by: java.lang.IllegalStateException: No match found",
                "at java.util.regex.Matcher.group(Matcher.java:468)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)"
            ],
            "StepsToReproduce": [
                "Invoke the evaluate method of UDFJson with a JSON string and a path that does not match any keys in the JSON.",
                "For example, use a path like '$.nonExistentKey' on a JSON object that does not contain 'nonExistentKey'."
            ],
            "ExpectedBehavior": "The evaluate method should return null or an appropriate response when the path does not match any keys in the JSON object.",
            "ObservedBehavior": "The evaluate method throws an IllegalStateException due to an attempt to access a regex group that does not exist, resulting in a runtime error.",
            "AdditionalDetails": "The issue arises in the extract method where the regex matcher is used to find groups in the provided path. If the matcher does not find a match, calling group(1) results in an IllegalStateException. Proper error handling should be implemented to check if a match was found before accessing groups."
        }
    },
    {
        "filename": "HIVE-8295.json",
        "creation_time": "2014-09-29T21:16:32.000+0000",
        "bug_report": {
            "Title": "SQLSyntaxErrorException due to exceeding maximum number of expressions in a list",
            "Description": "A SQL query executed in the Hive Metastore is failing with a SQLSyntaxErrorException. The error indicates that the maximum number of expressions in a list has been exceeded, which is causing the query to fail. This issue arises when attempting to retrieve partitions using a SQL filter that includes too many partition IDs.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\", \"SDS\".\"SD_ID\", \"SDS\".\"CD_ID\", \"SERDES\".\"SERDE_ID\", \"PARTITIONS\".\"CREATE_TIME\", \"PARTITIONS\".\"LAST_ACCESS_TIME\", \"SDS\".\"INPUT_FORMAT\", \"SDS\".\"IS_COMPRESSED\", \"SDS\".\"IS_STOREDASSUBDIRECTORIES\", \"SDS\".\"LOCATION\", \"SDS\".\"NUM_BUCKETS\", \"SDS\".\"OUTPUT_FORMAT\", \"SERDES\".\"NAME\", \"SERDES\".\"SLIB\" from \"PARTITIONS\"  left outer join \"SDS\" on \"PARTITIONS\".\"SD_ID\" = \"SDS\".\"SD_ID\"   left outer join \"SERDES\" on \"SDS\".\"SERDE_ID\" = \"SERDES\".\"SERDE_ID\" where \"PART_ID\" in (136,140,143,147,152,156,160,163,167,171,174,180,185,191,196,198,203,208,212,217...) order by \"PART_NAME\" asc\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:422)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:331)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1920)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1914)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2213)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1914)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1887)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)",
                "at com.sun.proxy.$Proxy8.getPartitionsByExpr(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3800)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9366)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9350)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:206)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "StepsToReproduce": [
                "Execute a query to retrieve partitions with a list of PART_IDs exceeding 1000.",
                "Ensure that the query is routed through the Hive Metastore using direct SQL."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the requested partitions without exceeding the maximum number of expressions in a list.",
            "ObservedBehavior": "The query fails with a SQLSyntaxErrorException indicating that the maximum number of expressions in a list is exceeded.",
            "AdditionalDetails": "The issue is likely due to the way partition IDs are being passed to the SQL query. Consider implementing pagination or batching to handle large lists of partition IDs."
        }
    },
    {
        "filename": "HIVE-8915.json",
        "creation_time": "2014-11-19T19:40:17.000+0000",
        "bug_report": {
            "Title": "MetaException: Unable to connect to transaction database due to missing COMPACTION_QUEUE table",
            "Description": "The application encounters a MetaException when attempting to execute a query on the COMPACTION_QUEUE table, which does not exist in the database. This issue arises during the execution of the compactor cleaner's main loop, leading to a failure in the cleaning process.",
            "StackTrace": [
                "2014-11-19 01:44:57,654 ERROR compactor.Cleaner (Cleaner.java:run(143)) - Caught an exception in the main loop of compactor cleaner, MetaException(message:Unable to connect to transaction database",
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hive.COMPACTION_QUEUE' doesn't exist",
                "at sun.reflect.GeneratedConstructorAccessor20.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:386)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2569)",
                "at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1524)",
                "at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:266)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)"
            ],
            "StepsToReproduce": [
                "1. Start the compactor cleaner process.",
                "2. Ensure that the database is set up without the COMPACTION_QUEUE table.",
                "3. Observe the logs for the MetaException related to the missing table."
            ],
            "ExpectedBehavior": "The compactor cleaner should successfully connect to the transaction database and execute the necessary queries without encountering exceptions.",
            "ObservedBehavior": "The compactor cleaner fails to connect to the transaction database and throws a MetaException due to the absence of the COMPACTION_QUEUE table.",
            "AdditionalDetails": "The method 'findReadyToClean()' attempts to execute a SQL query on the COMPACTION_QUEUE table, which is expected to exist for the cleaning process to function correctly. The absence of this table indicates a potential issue with the database schema setup."
        }
    },
    {
        "filename": "HIVE-7249.json",
        "creation_time": "2014-06-18T00:09:42.000+0000",
        "bug_report": {
            "Title": "NoSuchLockException Thrown During Unlock Operation",
            "Description": "A NoSuchLockException is thrown when attempting to unlock a lock in the Hive metastore. This indicates that the lock with the specified ID does not exist, which may lead to inconsistencies in transaction management.",
            "StackTrace": [
                "ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - NoSuchLockException(message:No such lock: 1)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(TxnHandler.java:1407)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(TxnHandler.java:477)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.unlock(HiveMetaStore.java:4817)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy14.unlock(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.unlock(HiveMetaStoreClient.java:1598)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(DbLockManager.java:110)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close(DbLockManager.java:162)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct(DbTxnManager.java:300)",
                "at org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager(HiveTxnManagerImpl.java:39)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.closeTxnManager(DbTxnManager.java:43)",
                "at org.apache.hive.hcatalog.mapreduce.TransactionContext.cleanup(TransactionContext.java:327)",
                "at org.apache.hive.hcatalog.mapreduce.TransactionContext.onCommitJob(TransactionContext.java:142)",
                "at org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.commitJob(OutputCommitterContainer.java:61)",
                "at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(FileOutputCommitterContainer.java:251)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:537)"
            ],
            "StepsToReproduce": [
                "1. Start a transaction in Hive that acquires a lock.",
                "2. Attempt to unlock the lock after it has been released or if it was never acquired.",
                "3. Observe the exception thrown in the logs."
            ],
            "ExpectedBehavior": "The unlock operation should successfully release the lock if it exists, or handle the case where the lock does not exist gracefully without throwing an exception.",
            "ObservedBehavior": "A NoSuchLockException is thrown indicating that the lock with the specified ID does not exist, which may disrupt the transaction management process.",
            "AdditionalDetails": "The unlock method in the TxnHandler class attempts to unlock a lock using its ID. If the lock ID does not exist, it throws a NoSuchLockException, which is logged and rethrown as a LockException. This indicates a potential issue in the transaction lifecycle management where locks may not be properly tracked or released."
        }
    },
    {
        "filename": "HIVE-11540.json",
        "creation_time": "2015-08-12T23:12:18.000+0000",
        "bug_report": {
            "Title": "IOException during Compaction Job Execution",
            "Description": "An IOException is thrown when attempting to compact the weblogs in the Hive compactor, indicating that the job has failed. This issue occurs in the Worker class of the compactor module, specifically during the execution of the run method.",
            "StackTrace": [
                "2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12.  Marking clean to avoid repeated failures, java.io.IOException: Job failed!",
                "at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)",
                "2015-08-12 15:36:18,444 ERROR [upladevhwd04v.researchnow.com-18]: txn.CompactionTxnHandler (CompactionTxnHandler.java:markCleaned(327))"
            ],
            "StepsToReproduce": [
                "Trigger a compaction job on the weblogs table in Hive.",
                "Monitor the logs for any exceptions during the compaction process."
            ],
            "ExpectedBehavior": "The compaction job should complete successfully without throwing an IOException.",
            "ObservedBehavior": "The compaction job fails with an IOException, indicating that the job has failed.",
            "AdditionalDetails": "The run method in the Worker class calls the mr.run method, which is responsible for executing the MapReduce job. The failure could be due to various reasons such as configuration issues, resource limitations, or data inconsistencies."
        }
    },
    {
        "filename": "HIVE-15755.json",
        "creation_time": "2017-01-30T20:48:25.000+0000",
        "bug_report": {
            "Title": "NullPointerException in UpdateDeleteSemanticAnalyzer during Merge Operation",
            "Description": "A NullPointerException occurs in the UpdateDeleteSemanticAnalyzer class when attempting to compile a SQL statement involving a merge operation. The error arises specifically in the getPredicate method, which fails to handle a null reference appropriately.",
            "StackTrace": [
                "Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)",
                "        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)",
                "        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)",
                "        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)",
                "        at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)",
                "        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)",
                "        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)",
                "        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)",
                "        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)",
                "        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)",
                "        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)",
                "        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)",
                "        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)",
                "        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:298)",
                "        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:506)",
                "        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317)",
                "        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302)",
                "        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Execute a SQL statement that involves a MERGE operation using the UpdateDeleteSemanticAnalyzer.",
                "Ensure that the target table or columns referenced in the statement are not properly initialized or are null."
            ],
            "ExpectedBehavior": "The SQL statement should compile successfully without throwing a NullPointerException, and the merge operation should be processed correctly.",
            "ObservedBehavior": "A NullPointerException is thrown during the compilation of the SQL statement, specifically in the getPredicate method of the OnClauseAnalyzer class.",
            "AdditionalDetails": "The getPredicate method attempts to access a list of target columns using a potentially null reference (targetTableNameInSourceQuery). This indicates that the method does not handle cases where the target table name is not found or is null, leading to the exception."
        }
    },
    {
        "filename": "HIVE-9390.json",
        "creation_time": "2015-01-15T18:50:32.000+0000",
        "bug_report": {
            "Title": "Timeout Exception when Retrieving Open Transactions from Hive Metastore",
            "Description": "A timeout exception occurs when attempting to retrieve open transactions from the Hive Metastore. The error indicates that the JDBC connection could not be established within the expected time frame, leading to a failure in the transaction management process.",
            "StackTrace": [
                "org.apache.thrift.TException: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5324)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)",
                "at com.sun.proxy.$Proxy11.get_open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidTxns(HiveMetaStoreClient.java:1696)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)",
                "at com.sun.proxy.$Proxy12.getValidTxns(Unknown Source)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidTxns(DbTxnManager.java:289)",
                "at org.apache.hadoop.hive.ql.Driver.recordValidTxns(Driver.java:882)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:399)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)",
                "at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)",
                "at com.sun.proxy.$Proxy21.executeStatementAsync(Unknown Source)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:401)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.thrift.server.TServlet.doPost(TServlet.java:83)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:101)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)",
                "at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857)",
                "at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:850)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:196)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5322)"
            ],
            "StepsToReproduce": [
                "Attempt to execute a query that requires retrieving open transactions from the Hive Metastore.",
                "Ensure that the JDBC connection pool is configured with a timeout setting.",
                "Observe the logs for timeout exceptions related to JDBC connections."
            ],
            "ExpectedBehavior": "The system should successfully retrieve open transactions from the Hive Metastore without timing out.",
            "ObservedBehavior": "The system throws a timeout exception indicating that it was unable to get a JDBC connection from the pool.",
            "AdditionalDetails": "The issue may be related to the configuration of the JDBC connection pool or the database's responsiveness. The method 'getDbConn' in the 'TxnHandler' class is responsible for obtaining the database connection, and it throws a MetaException if the connection cannot be established."
        }
    },
    {
        "filename": "HIVE-7623.json",
        "creation_time": "2014-08-05T23:58:27.000+0000",
        "bug_report": {
            "Title": "InvalidOperationException when renaming Hive partition due to different file systems",
            "Description": "An InvalidOperationException is thrown when attempting to rename a partition in Hive, indicating that the new location is on a different file system than the old location. This operation is not supported, leading to a failure in the rename_partition method.",
            "StackTrace": [
                "2014-08-05 21:46:14,522 ERROR [pool-3-thread-1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - InvalidOperationException(message:table new location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=123 is on a different file system than the old location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=456. This operation is not supported)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(HiveAlterHandler.java:361)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2629)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2602)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:622)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy5.rename_partition(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9057)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9041)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)"
            ],
            "StepsToReproduce": [
                "Attempt to rename a Hive partition where the new location is on a different file system than the old location."
            ],
            "ExpectedBehavior": "The partition should be renamed successfully if the new location is valid and on the same file system.",
            "ObservedBehavior": "An InvalidOperationException is thrown indicating that the new location is on a different file system than the old location, preventing the rename operation.",
            "AdditionalDetails": "The issue arises in the HiveAlterHandler class, specifically in the alterPartition method, which is invoked during the rename_partition process. The error suggests a validation check that prevents renaming partitions across different file systems."
        }
    },
    {
        "filename": "HIVE-15997.json",
        "creation_time": "2017-02-21T16:49:46.000+0000",
        "bug_report": {
            "Title": "IOException during Scratch Directory Removal in Hive",
            "Description": "An IOException occurs when attempting to remove scratch directories in Hive, specifically due to a ClosedByInterruptException. This issue arises during the cleanup process when the Hive context is being released, leading to potential resource leaks and incomplete cleanup operations.",
            "StackTrace": [
                "2017-02-02 06:23:25,410 WARN hive.ql.Context: [HiveServer2-Background-Pool: Thread-61]: Error Removing Scratch: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: \"ychencdh511t-1.vpc.cloudera.com/172.26.11.50\"; destination host is: \"ychencdh511t-1.vpc.cloudera.com\":8020;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1476)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1409)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)",
                "at com.sun.proxy.$Proxy25.delete(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:535)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy26.delete(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:2059)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:675)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:671)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:671)",
                "at org.apache.hadoop.hive.ql.Context.removeScratchDir(Context.java:405)",
                "at org.apache.hadoop.hive.ql.Context.clear(Context.java:541)",
                "at org.apache.hadoop.hive.ql.Driver.releaseContext(Driver.java:2109)",
                "at org.apache.hadoop.hive.ql.Driver.closeInProcess(Driver.java:2150)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1472)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)",
                "at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:681)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client$Connection.java:615)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client$Connection.java:714)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2900(Client$Connection.java:376)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1525)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1448)",
                "... 35 more"
            ],
            "StepsToReproduce": [
                "1. Start a Hive session.",
                "2. Execute a query that requires the creation of scratch directories.",
                "3. Attempt to close the Hive session or context while the query is still running or has been interrupted."
            ],
            "ExpectedBehavior": "The scratch directories should be removed without any exceptions, ensuring proper cleanup of resources.",
            "ObservedBehavior": "An IOException is thrown indicating a failure to remove the scratch directory due to a ClosedByInterruptException, leading to incomplete cleanup.",
            "AdditionalDetails": "The issue seems to stem from the handling of thread interruptions during the cleanup process, particularly in the removeScratchDir() method. The method attempts to delete scratch directories but fails when the thread is interrupted, which may indicate a need for better handling of thread states during cleanup."
        }
    },
    {
        "filename": "HIVE-7009.json",
        "creation_time": "2014-05-02T20:50:24.000+0000",
        "bug_report": {
            "Title": "IOException: Invalid HDFS URI when accessing Hive JAR directory",
            "Description": "An IOException is thrown when attempting to access a Hive JAR directory due to an invalid HDFS URI. The error indicates that the provided URI is not recognized as a valid HDFS URI, which leads to failure in creating local resources for the Tez session.",
            "StackTrace": [
                "java.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:912)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "1. Configure Hive to use a non-HDFS URI (e.g., wasb://) for the HIVE_USER_INSTALL_DIR.",
                "2. Attempt to execute a Hive query that requires accessing the JAR directory.",
                "3. Observe the IOException indicating the invalid HDFS URI."
            ],
            "ExpectedBehavior": "The system should correctly identify the HDFS URI and allow access to the Hive JAR directory without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown stating that the provided URI is not a valid HDFS URI, preventing the execution of the Hive query.",
            "AdditionalDetails": "The method 'getDefaultDestDir(Configuration conf)' checks if the provided path is an instance of DistributedFileSystem. Since 'wasb://' is not a valid HDFS URI, it throws an IOException. The configuration should be updated to use a valid HDFS URI."
        }
    },
    {
        "filename": "HIVE-2031.json",
        "creation_time": "2011-03-08T11:38:53.000+0000",
        "bug_report": {
            "Title": "SemanticException: Partition not found '21Oct'",
            "Description": "A SemanticException is thrown when attempting to load data into a Hive table, indicating that the specified partition '21Oct' does not exist. This error occurs during the semantic analysis phase of the query execution.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)",
                "at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)",
                "at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that attempts to load data into a partitioned table with a partition specified as '21Oct'.",
                "2. Ensure that the partition '21Oct' does not exist in the Hive metastore.",
                "3. Observe the error thrown during the execution."
            ],
            "ExpectedBehavior": "The query should successfully load data into the specified partition if it exists, or provide a clear error message indicating the absence of the partition.",
            "ObservedBehavior": "The query fails with a SemanticException indicating that the partition '21Oct' was not found.",
            "AdditionalDetails": "The error occurs in the `analyzeInternal` method of the `LoadSemanticAnalyzer` class, specifically when initializing the `tableSpec` object. The method checks for the existence of the specified partition and throws an exception if it is not found."
        }
    },
    {
        "filename": "HIVE-4018.json",
        "creation_time": "2013-02-13T09:02:20.000+0000",
        "bug_report": {
            "Title": "EOFException in MapJoinOperator during Hash Table Loading",
            "Description": "An EOFException is thrown in the MapJoinOperator's loadHashTable method, indicating that the end of the input stream was reached unexpectedly while attempting to load the hash table. This issue may arise due to improper handling of input files or corrupted data.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:189)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:203)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1421)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:614)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:266)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:260)"
            ],
            "StepsToReproduce": [
                "Run a Hive query that involves a MapJoin operation.",
                "Ensure that the input files are either corrupted or improperly formatted.",
                "Observe the logs for the EOFException in the MapJoinOperator."
            ],
            "ExpectedBehavior": "The MapJoinOperator should successfully load the hash table from the input files without throwing an EOFException.",
            "ObservedBehavior": "An EOFException is thrown, indicating that the end of the input stream was reached unexpectedly while loading the hash table.",
            "AdditionalDetails": "The loadHashTable method attempts to load data from a temporary file URI. If the input file is corrupted or not properly formatted, it may lead to an EOFException. The method also checks if the input file has changed and attempts to clean up accordingly, which may not be handled correctly in this case."
        }
    },
    {
        "filename": "HIVE-11255.json",
        "creation_time": "2015-07-14T15:39:11.000+0000",
        "bug_report": {
            "Title": "JDOException Thrown When Executing Query in Hive Metastore",
            "Description": "A JDOException is thrown when attempting to execute a query to retrieve table objects by name in the Hive Metastore. This issue appears to be related to the handling of database connections and query execution within the metastore's object store.",
            "StackTrace": [
                "javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:945)",
                "at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy0.getTableObjectsByName(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1618)",
                "at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:106)",
                "at com.sun.proxy.$Proxy5.get_table_objects_by_name(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8172)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8156)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the method getTableObjectsByName with a valid database name and a list of table names.",
                "Ensure that the Hive Metastore is properly configured and running.",
                "Observe the logs for any JDOException being thrown."
            ],
            "ExpectedBehavior": "The method getTableObjectsByName should return a list of Table objects corresponding to the provided database and table names without throwing any exceptions.",
            "ObservedBehavior": "A JDOException is thrown indicating an issue with executing the query, preventing the retrieval of table objects.",
            "AdditionalDetails": "The issue may be related to the configuration of the JDO connection or the state of the database. Further investigation into the database connectivity and query execution logic in the ObjectStore class is recommended."
        }
    },
    {
        "filename": "HIVE-10151.json",
        "creation_time": "2015-03-31T00:17:22.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException in OrcInputFormat when processing delta files",
            "Description": "A RuntimeException occurs in the OrcInputFormat class when attempting to generate splits for a delta file that does not conform to the expected naming convention. The exception indicates that the delta file name does not start with 'base_', leading to an IllegalArgumentException.",
            "StackTrace": [
                "java.lang.RuntimeException: serious problem",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)",
                "    at org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(BucketizedHiveInputFormat.java:141)",
                "    at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:624)",
                "    at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:616)",
                "    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:492)",
                "    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)",
                "    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:415)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)",
                "    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)",
                "    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:415)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)",
                "    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:430)",
                "    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)",
                "    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)",
                "    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)",
                "    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "    at org.apache.hadoop.hive.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:225)",
                "    at org.apache.hadoop.hive.TestTxnCommands2.testDeleteIn2(TestTxnCommands2.java:148)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:606)",
                "    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)",
                "    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)",
                "    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)",
                "    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)",
                "    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)",
                "    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)",
                "    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)",
                "    at org.junit.rules.RunRules.evaluate(RunRules.java:20)",
                "    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)",
                "    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)",
                "    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)",
                "    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)",
                "    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)",
                "    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)",
                "    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)",
                "    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)",
                "    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)",
                "    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:254)",
                "    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:149)",
                "    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_",
                "    at java.util.concurrent.FutureTask.report(FutureTask.java:122)",
                "    at java.util.concurrent.FutureTask.get(FutureTask.java:188)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:998)",
                "... 56 more",
                "Caused by: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_",
                "    at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)",
                "    at org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(AcidUtils.java:172)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:655)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:620)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Attempt to process a delta file named 'delta_0000001_0000001' using the OrcInputFormat.",
                "2. Ensure that the file does not have a corresponding base file that starts with 'base_'.",
                "3. Observe the exception thrown during the split generation process."
            ],
            "ExpectedBehavior": "The OrcInputFormat should successfully generate splits for valid delta files that conform to the expected naming convention.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the delta file name does not start with 'base_', causing the job submission to fail.",
            "AdditionalDetails": "The issue arises from the AcidUtils class, which is responsible for parsing the base file names. The delta file naming convention must be adhered to for successful processing."
        }
    },
    {
        "filename": "HIVE-13546.json",
        "creation_time": "2016-04-19T07:43:42.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException in ExecReducer during Hive processing",
            "Description": "An IndexOutOfBoundsException occurs in the ExecReducer class while processing rows in Hive. The error is triggered when attempting to access an element in an ArrayList that is empty, leading to a failure in the reduce operation.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":null},\"value\":{\"_col0\":null,\"_col1\":5588,\"_col2\":170300,\"_col3\":null,\"_col4\":756,\"_col5\":91384,\"_col6\":16,\"_col7\":null,\"_col8\":855582,\"_col9\":28,\"_col10\":null,\"_col11\":48.83,\"_col12\":null,\"_col13\":0.0,\"_col14\":null,\"_col15\":899.64,\"_col16\":null,\"_col17\":6.14,\"_col18\":0.0,\"_col19\":null,\"_col20\":null,\"_col21\":null,\"_col22\":null}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:180)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:174)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:653)",
                "at java.util.ArrayList.get(ArrayList.java:429)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:151)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:131)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:1003)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:919)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:713)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)"
            ],
            "StepsToReproduce": [
                "Run a Hive job that involves dynamic partitioning with an empty list of dynamic partition column values.",
                "Ensure that the input data contains rows that may lead to null or empty values for the dynamic partition columns."
            ],
            "ExpectedBehavior": "The ExecReducer should process rows without throwing an IndexOutOfBoundsException, even if some dynamic partition column values are null or empty.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when the ExecReducer attempts to access an element in an empty ArrayList, causing the job to fail.",
            "AdditionalDetails": "The issue arises in the method 'getDynPartDirectory' where it calls 'FileUtils.makePartName' with an empty list, leading to the exception. The root cause is likely due to the handling of dynamic partition columns when they are not properly populated."
        }
    },
    {
        "filename": "HIVE-7049.json",
        "creation_time": "2014-05-12T21:46:48.000+0000",
        "bug_report": {
            "Title": "AvroRuntimeException: Not a union: 'string' during Avro Deserialization",
            "Description": "An AvroRuntimeException is thrown when attempting to deserialize a nullable union type in the AvroDeserializer class. The error indicates that the expected schema is not a union type, leading to a failure in the deserialization process.",
            "StackTrace": [
                "org.apache.avro.AvroRuntimeException: Not a union: \"string\"",
                "at org.apache.avro.Schema.getTypes(Schema.java:272)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)"
            ],
            "StepsToReproduce": [
                "1. Attempt to deserialize an Avro record with a schema that includes a nullable union type.",
                "2. Ensure that the schema provided does not match the expected union type format.",
                "3. Execute the deserialization process using the AvroDeserializer."
            ],
            "ExpectedBehavior": "The AvroDeserializer should successfully deserialize the nullable union type without throwing an exception.",
            "ObservedBehavior": "An AvroRuntimeException is thrown indicating that the provided schema is not a union type, specifically stating 'Not a union: \"string\"'.",
            "AdditionalDetails": "The issue likely arises from the schema definition used during deserialization. The method 'deserializeNullableUnion' expects a union type but receives a simple type (string) instead. This indicates a potential mismatch in the schema definition or the data being processed."
        }
    },
    {
        "filename": "HIVE-9755.json",
        "creation_time": "2015-02-23T20:50:43.000+0000",
        "bug_report": {
            "Title": "Hive Runtime Error: Mismatch in value for 'n' in GenericUDAFnGramEvaluator",
            "Description": "A runtime error occurs in the Hive execution engine when processing rows with the GenericUDAFnGrams function. The error indicates a mismatch in the value for 'n', which is expected to be constant but is found to be varying between '0' and '1'. This issue arises during the aggregation phase of the query execution.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":[\"0\",\"0\",\"0\",\"0\"]},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:258)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:506)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:447)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: GenericUDAFnGramEvaluator: mismatch in value for 'n', which usually is caused by a non-constant expression. Found '0' and '1'.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams$GenericUDAFnGramEvaluator.merge(GenericUDAFnGrams.java:242)",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:142)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:658)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:911)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:753)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:474)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:249)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that uses the GenericUDAFnGrams function with varying values for 'n'.",
                "Ensure that the input data contains rows that lead to a non-constant expression for 'n'.",
                "Observe the execution to trigger the Hive Runtime Error."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully without runtime errors, and the aggregation should handle the values of 'n' consistently.",
            "ObservedBehavior": "A Hive Runtime Error occurs indicating a mismatch in the value for 'n', causing the query execution to fail.",
            "AdditionalDetails": "The error is likely caused by the input data not conforming to the expected structure for the GenericUDAFnGrams function, leading to inconsistent values for 'n'. The merge method in the GenericUDAFnGramEvaluator checks for a constant value of 'n' and throws an exception if it detects a mismatch."
        }
    },
    {
        "filename": "HIVE-19130.json",
        "creation_time": "2018-04-09T10:18:33.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DDLTask.dropPartitions Method",
            "Description": "A NullPointerException occurs in the dropPartitions method of the DDLTask class when attempting to drop partitions from a Hive table. This issue arises when the method getPartitionsByExpr is called with a null value, leading to a failure in the execution of the drop operation.",
            "StackTrace": [
                "ERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4016)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3983)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:341)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:162)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1765)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1506)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1303)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1165)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)",
                "at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)"
            ],
            "StepsToReproduce": [
                "1. Execute a DROP TABLE or DROP PARTITION command on a Hive table.",
                "2. Ensure that the table has partitions defined.",
                "3. The command should attempt to drop partitions, leading to the NullPointerException."
            ],
            "ExpectedBehavior": "The partitions should be dropped successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the drop operation to fail.",
            "AdditionalDetails": "The issue seems to stem from the getPartitionsByExpr method being called with a null argument, which indicates that the partition specification may not be properly initialized or passed to the method."
        }
    },
    {
        "filename": "HIVE-13090.json",
        "creation_time": "2016-02-18T21:58:48.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ZooKeeperTokenStore while decoding delegation token",
            "Description": "A NullPointerException occurs in the ZooKeeperTokenStore class when attempting to decode a delegation token. This issue arises during the removal of expired tokens, specifically when the token bytes retrieved from ZooKeeper are null, leading to a failure in decoding the token information.",
            "StackTrace": [
                "org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.NullPointerException",
                "at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)",
                "at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop Hive service with ZooKeeper integration.",
                "2. Ensure that there are expired delegation tokens in the system.",
                "3. Trigger the removal of expired tokens, which invokes the removeExpiredTokens method."
            ],
            "ExpectedBehavior": "The system should successfully decode the delegation token information and remove expired tokens without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to decode the token information due to null token bytes, resulting in a failure to remove expired tokens.",
            "AdditionalDetails": "The issue likely stems from the zkGetData method returning null for the token bytes, which should be investigated to ensure that valid token data is being stored and retrieved from ZooKeeper."
        }
    },
    {
        "filename": "HIVE-5664.json",
        "creation_time": "2013-10-28T03:50:29.000+0000",
        "bug_report": {
            "Title": "HiveException: Database does not exist: db2",
            "Description": "An attempt to drop a database named 'db2' resulted in a HiveException indicating that the database does not exist. The underlying cause appears to be a NoSuchObjectException for a table associated with the database.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Database does not exist: db2",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3473)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1441)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1219)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1047)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:915)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:43)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:160)",
                "Caused by: NoSuchObjectException(message:db2.tab1_indx table not found)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1376)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:43)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)",
                "at com.sun.proxy.$Proxy7.get_table(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:890)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:660)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:652)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:546)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:43)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)",
                "at com.sun.proxy.$Proxy8.dropDatabase(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:284)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3470)"
            ],
            "StepsToReproduce": [
                "Attempt to drop a database using the command: DROP DATABASE db2;"
            ],
            "ExpectedBehavior": "The database 'db2' should be dropped successfully if it exists.",
            "ObservedBehavior": "An exception is thrown indicating that the database 'db2' does not exist.",
            "AdditionalDetails": "The error is caused by a missing table 'db2.tab1_indx', which is likely expected to be present in the database before it can be dropped. This suggests that the database may not have been created properly or has been deleted without proper cleanup."
        }
    },
    {
        "filename": "HIVE-15778.json",
        "creation_time": "2017-02-01T04:20:12.000+0000",
        "bug_report": {
            "Title": "NullPointerException in HiveMetaStore when dropping an index",
            "Description": "A NullPointerException occurs in the HiveMetaStore when attempting to drop an index by name. This issue arises during the execution of the drop_index_by_name method, which fails to handle a null value appropriately, leading to a failure in the DDLTask.",
            "StackTrace": [
                "ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.NullPointerException",
                "2017-01-31 16:27:29,421 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-5-thread-3]: MetaException(message:java.lang.NullPointerException)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5823)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rethrowException(HiveMetaStore.java:4892)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4403)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy16.drop_index_by_name(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10803)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10787)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)",
                "at org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropIndexMessage(JSONMessageFactory.java:159)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name_core(HiveMetaStore.java:4469)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4396)"
            ],
            "StepsToReproduce": [
                "Invoke the drop_index_by_name method in the HiveMetaStore with a valid database name, table name, and index name.",
                "Ensure that the index being dropped is associated with a null value in the JSONDropIndexMessage constructor."
            ],
            "ExpectedBehavior": "The index should be dropped successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the drop operation to fail.",
            "AdditionalDetails": "The issue seems to stem from the JSONDropIndexMessage constructor, which does not handle null values correctly. This could be due to missing validation checks before creating the message."
        }
    },
    {
        "filename": "HIVE-8386.json",
        "creation_time": "2014-10-07T22:30:12.000+0000",
        "bug_report": {
            "Title": "RuntimeException: Cannot find field 'givenName' in StructTypeInfo",
            "Description": "A RuntimeException is thrown when attempting to retrieve the field type information for 'givenName' from a StructTypeInfo object. The error indicates that the field 'givenName' (in lowercase form: 'givenname') cannot be found in the list of available fields.",
            "StackTrace": [
                "java.lang.RuntimeException: cannot find field givenName(lowercase form: givenname) in [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase]",
                "at org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(StructTypeInfo.java:109)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(HCatSchemaUtils.java:154)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(HCatSchemaUtils.java:165)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:127)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:115)",
                "at org.apache.hive.hcatalog.api.HCatTable.<init>(HCatTable.java:59)",
                "at org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(HCatClientHMSImpl.java:157)",
                "at org.apache.falcon.catalog.HiveCatalogService.tableExists(HiveCatalogService.java:143)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateStorageExists(FeedEntityParser.java:367)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateFeedStorage(FeedEntityParser.java:309)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:79)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:54)",
                "at org.apache.falcon.resource.AbstractEntityManager.validate(AbstractEntityManager.java:364)",
                "at org.apache.falcon.resource.AbstractEntityManager.submitInternal(AbstractEntityManager.java:331)",
                "at org.apache.falcon.resource.AbstractEntityManager.submit(AbstractEntityManager.java:153)",
                "at org.apache.falcon.resource.ConfigSyncService.submit(ConfigSyncService.java:44)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.falcon.resource.channel.IPCChannel.invoke(IPCChannel.java:48)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$1.doExecute(SchedulableEntityManagerProxy.java:118)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$EntityProxy.execute(SchedulableEntityManagerProxy.java:410)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody0(SchedulableEntityManagerProxy.java:120)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure1.run(SchedulableEntityManagerProxy.java:1)",
                "at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)",
                "at org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit(SchedulableEntityManagerProxy.java:107)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody12(SchedulableEntityManagerProxy.java:341)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure13.run(SchedulableEntityManagerProxy.java:1)",
                "at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)",
                "at org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule_aroundBody16(SchedulableEntityManagerProxy.java:341)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure17.run(SchedulableEntityManagerProxy.java:1)",
                "at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)",
                "at org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule(SchedulableEntityManagerProxy.java:335)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(HttpMethodRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(HttpMethodRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(HttpMethodRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at org.mortbay.jetty.servlet.WebComponent.service(WebComponent.java:416)",
                "at org.mortbay.jetty.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at org.mortbay.jetty.servlet.ServletContainer.service(ServletContainer.java:699)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at org.apache.falcon.security.BasicAuthFilter$2.doFilter(BasicAuthFilter.java:183)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:392)",
                "at org.apache.falcon.security.BasicAuthFilter.doFilter(BasicAuthFilter.java:221)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "Attempt to retrieve the field type information for 'givenName' from a StructTypeInfo object.",
                "Ensure that the StructTypeInfo object is initialized with the fields: [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase].",
                "Call the method getStructFieldTypeInfo with the argument 'givenName'."
            ],
            "ExpectedBehavior": "The method getStructFieldTypeInfo should return the TypeInfo for the field 'givenName'.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the field 'givenName' cannot be found.",
            "AdditionalDetails": "The issue may stem from case sensitivity in field names. The method getStructFieldTypeInfo converts the field name to lowercase before searching, which may not match the actual field names in the StructTypeInfo."
        }
    },
    {
        "filename": "HIVE-14714.json",
        "creation_time": "2016-09-07T15:46:07.000+0000",
        "bug_report": {
            "Title": "IOException: Stream closed in SparkClientImpl Redirector",
            "Description": "An IOException is thrown indicating that the stream has been closed while attempting to read from it in the SparkClientImpl's Redirector thread. This issue arises when the input stream is accessed after it has already been closed, leading to unexpected behavior in the application.",
            "StackTrace": [
                "java.io.IOException: Stream closed",
                "at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)",
                "at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)",
                "at java.io.BufferedInputStream.read(BufferedInputStream.java:334)",
                "at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)",
                "at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)",
                "at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)",
                "at java.io.InputStreamReader.read(InputStreamReader.java:184)",
                "at java.io.BufferedReader.fill(BufferedReader.java:154)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:317)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:382)",
                "at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a Spark job that utilizes the SparkClientImpl.",
                "2. Ensure that the input stream is closed before the Redirector thread attempts to read from it.",
                "3. Monitor the logs for IOException indicating that the stream is closed."
            ],
            "ExpectedBehavior": "The Redirector thread should be able to read lines from the input stream without encountering an IOException, even if the stream is closed at some point during execution.",
            "ObservedBehavior": "An IOException is thrown with the message 'Stream closed' when the Redirector thread attempts to read from the input stream after it has been closed.",
            "AdditionalDetails": "The issue may be related to the timing of when the input stream is closed versus when the Redirector thread attempts to read from it. Proper synchronization or checks should be implemented to ensure that the stream is open before attempting to read."
        }
    },
    {
        "filename": "HIVE-5428.json",
        "creation_time": "2013-10-02T20:46:10.000+0000",
        "bug_report": {
            "Title": "SQLSyntaxErrorException: Table/View 'DBS' does not exist during Hive MetaStore initialization",
            "Description": "The application encounters a JDODataStoreException when attempting to execute an SQL query to select from the 'DBS' table during the initialization of the Hive MetaStore. The root cause is a SQLSyntaxErrorException indicating that the 'DBS' table does not exist in the database.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"DB_ID\" from \"DBS\"\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:230)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:108)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:249)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:418)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:405)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:444)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:329)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:289)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4084)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1211)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2404)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2415)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:871)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:853)",
                "at org.apache.hadoop.hive.ql.QTestUtil.cleanUp(QTestUtil.java:534)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.<clinit>(TestCliDriver.java:44)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:374)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: Table/View 'DBS' does not exist.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement40.<init>(Unknown Source)"
            ],
            "StepsToReproduce": [
                "Initialize the Hive MetaStore.",
                "Attempt to drop a table or perform an operation that requires access to the 'DBS' table."
            ],
            "ExpectedBehavior": "The Hive MetaStore should initialize successfully and allow operations on the database.",
            "ObservedBehavior": "The initialization fails with a JDODataStoreException due to the non-existence of the 'DBS' table.",
            "AdditionalDetails": "The 'DBS' table is expected to be part of the Hive MetaStore schema. Ensure that the database is correctly set up and that the necessary tables are created before initializing the MetaStore."
        }
    },
    {
        "filename": "HIVE-12567.json",
        "creation_time": "2015-12-02T16:38:52.000+0000",
        "bug_report": {
            "Title": "LockException: Error communicating with the metastore due to consistent read failure",
            "Description": "A LockException is thrown when attempting to acquire locks in Hive, indicating an error in communication with the metastore. The underlying cause is a consistent read failure in the Oracle database, which results in a SQLException (ORA-08176). This issue occurs during the lock acquisition process, which is critical for transaction management in Hive.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:132)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)",
                "at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-08176: consistent read failure; rollback data not available",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)",
                "at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1059)",
                "at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:522)",
                "at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:257)",
                "at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:587)",
                "at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)",
                "at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:30)",
                "at oracle.jdbc.driver.T4CStatement.executeForDescribe(T4CStatement.java:762)",
                "at oracle.jdbc.driver.OracleStatement.executeMaybeDescribe(OracleStatement.java:925)",
                "at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1111)",
                "at oracle.jdbc.driver.OracleStatement.executeQuery(OracleStatement.java:1309)",
                "at oracle.jdbc.driver.OracleStatementWrapper.executeQuery(OracleStatementWrapper.java:422)",
                "at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getLockInfoFromLockId(TxnHandler.java:1951)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock(TxnHandler.java:1600)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:1576)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:480)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5586)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy8.lock(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.lock(HiveMetaStoreClient.java:1869)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy9.lock(Unknown Source)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:93)"
            ],
            "StepsToReproduce": [
                "Attempt to execute a Hive query that requires acquiring locks.",
                "Ensure that the underlying Oracle database is configured to allow transactions.",
                "Observe the logs for LockException and SQLException."
            ],
            "ExpectedBehavior": "The locks should be acquired successfully without any exceptions, allowing the transaction to proceed.",
            "ObservedBehavior": "A LockException is thrown indicating an error communicating with the metastore, caused by a consistent read failure in the Oracle database.",
            "AdditionalDetails": "The issue appears to be related to the transaction management in Hive, specifically when interacting with the Oracle database. The consistent read failure suggests that there may be issues with the database state or transaction isolation levels."
        }
    },
    {
        "filename": "HIVE-6984.json",
        "creation_time": "2014-04-28T23:08:43.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Hive Runtime while processing rows with null values",
            "Description": "A NullPointerException occurs in the Hive execution framework when processing rows that contain null values, specifically when the 'age' field is null. This leads to a failure in the gatherStats method of the TableScanOperator, which is invoked during the processing of rows in the ExecMapper.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with a schema that includes a nullable 'age' field.",
                "2. Insert a row into the table with a null value for the 'age' field, e.g., {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}.",
                "3. Execute a Hive query that processes this row."
            ],
            "ExpectedBehavior": "The Hive execution framework should handle null values gracefully and process the row without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown during the processing of the row, causing the Hive job to fail.",
            "AdditionalDetails": "The issue appears to stem from the gatherStats method in the TableScanOperator, which does not handle null values correctly when attempting to gather statistics for the row being processed."
        }
    },
    {
        "filename": "HIVE-10736.json",
        "creation_time": "2015-05-18T03:25:45.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException during HiveServer2 Shutdown",
            "Description": "A ConcurrentModificationException is thrown when attempting to stop the HiveServer2 instance, specifically during the shutdown process of the TezSessionPoolManager. This occurs when the session pool is being modified while it is being iterated over, leading to an inconsistent state.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)",
                "at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)"
            ],
            "StepsToReproduce": [
                "Start the HiveServer2 instance.",
                "Trigger the shutdown process of HiveServer2 while there are active sessions in the TezSessionPoolManager."
            ],
            "ExpectedBehavior": "The HiveServer2 instance should shut down gracefully without throwing any exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, indicating that the session pool was modified during iteration, causing the shutdown process to fail.",
            "AdditionalDetails": "The issue likely arises from concurrent access to the session pool in TezSessionPoolManager. Proper synchronization mechanisms should be implemented to prevent concurrent modifications during the shutdown process."
        }
    },
    {
        "filename": "HIVE-7710.json",
        "creation_time": "2014-08-13T10:46:32.000+0000",
        "bug_report": {
            "Title": "SQLIntegrityConstraintViolationException during Table Alteration",
            "Description": "An SQLIntegrityConstraintViolationException occurs when attempting to alter a table in the Hive metastore, indicating that the operation would result in a duplicate key value in a unique or primary key constraint. This issue arises during the execution of the alterTable method, specifically when trying to update the storage descriptor of a table.",
            "StackTrace": [
                "java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)",
                "at com.jolbox.bonecp.PreparedStatementHandle.executeUpdate(PreparedStatementHandle.java:205)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:399)",
                "at org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:439)",
                "at org.datanucleus.store.rdbms.request.UpdateRequest.execute(UpdateRequest.java:374)",
                "at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateTable(RDBMSPersistenceHandler.java:417)",
                "at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateObject(RDBMSPersistenceHandler.java:390)",
                "at org.datanucleus.state.JDOStateManager.flush(JDOStateManager.java:5027)",
                "at org.datanucleus.flush.FlushOrdered.execute(FlushOrdered.java:106)",
                "at org.datanucleus.ExecutionContextImpl.flushInternal(ExecutionContextImpl.java:4119)",
                "at org.datanucleus.ExecutionContextThreadedImpl.flushInternal(ExecutionContextThreadedImpl.java:450)",
                "at org.datanucleus.ExecutionContextImpl.markDirty(ExecutionContextImpl.java:3879)",
                "at org.datanucleus.ExecutionContextThreadedImpl.markDirty(ExecutionContextThreadedImpl.java:422)",
                "at org.datanucleus.state.JDOStateManager.postWriteField(JDOStateManager.java:4815)",
                "at org.datanucleus.state.JDOStateManager.replaceField(JDOStateManager.java:3356)",
                "at org.datanucleus.state.JDOStateManager.updateField(JDOStateManager.java:2018)",
                "at org.datanucleus.state.JDOStateManager.setStringField(JDOStateManager.java:1791)",
                "at org.apache.hadoop.hive.metastore.model.MStorageDescriptor.jdoSetlocation(MStorageDescriptor.java)",
                "at org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setLocation(MStorageDescriptor.java:88)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.copyMSD(ObjectStore.java:2699)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2572)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)",
                "at com.sun.proxy.$Proxy6.alterTable(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:205)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2771)",
                "at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy8.alter_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:293)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(SessionHiveMetaStoreClient.java:201)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:288)",
                "at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)",
                "at com.sun.proxy.$Proxy9.alter_table(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:404)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3542)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:318)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1538)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1305)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1118)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:833)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_table(TestCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at junit.framework.TestCase.runTest(TestCase.java:168)",
                "at junit.framework.TestCase.runBare(TestCase.java:134)",
                "at junit.framework.TestResult$1.protect(TestResult.java:110)",
                "at junit.framework.TestResult.runProtected(TestResult.java:128)",
                "at junit.framework.TestResult.run(TestResult.java:113)",
                "at junit.framework.TestCase.run(TestCase.java:124)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:243)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:238)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)"
            ],
            "StepsToReproduce": [
                "1. Attempt to alter a table in the Hive metastore.",
                "2. Ensure that the alteration would result in a duplicate key value in a unique or primary key constraint."
            ],
            "ExpectedBehavior": "The table alteration should succeed without any SQLIntegrityConstraintViolationException.",
            "ObservedBehavior": "An SQLIntegrityConstraintViolationException is thrown, indicating a duplicate key value would occur.",
            "AdditionalDetails": "The issue seems to stem from the copyMSD method in the ObjectStore class, which attempts to update the storage descriptor of a table. The method does not check for existing unique constraints before performing the update."
        }
    },
    {
        "filename": "HIVE-8735.json",
        "creation_time": "2014-11-04T22:20:02.000+0000",
        "bug_report": {
            "Title": "SQLDataException: Truncation Error on VARCHAR Length",
            "Description": "A truncation error occurs when attempting to shrink a VARCHAR field to a length of 255 characters. This issue arises during the execution of a SQL statement in the Hive JDBC Stats Publisher, specifically when publishing statistics for a file ID. The error indicates that the provided string exceeds the maximum allowed length for the VARCHAR field in the database.",
            "StackTrace": [
                "java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:147)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:144)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.executeWithRetry(Utilities.java:2910)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.publishStat(JDBCStatsPublisher.java:160)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats(FileSinkOperator.java:1153)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:992)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:205)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves publishing statistics using the JDBCStatsPublisher.",
                "2. Ensure that the file ID being published exceeds the VARCHAR length limit of 255 characters.",
                "3. Observe the SQLDataException being thrown during the execution."
            ],
            "ExpectedBehavior": "The statistics should be published successfully without any truncation errors, regardless of the length of the file ID.",
            "ObservedBehavior": "A SQLDataException is thrown indicating a truncation error when attempting to publish statistics with a file ID that exceeds the VARCHAR length limit.",
            "AdditionalDetails": "The issue seems to stem from the method 'publishStat(String fileID, Map<String, String> stats)' in the JDBCStatsPublisher class, where the fileID is being set in a PreparedStatement. The length of the fileID should be validated before attempting to execute the update to prevent this error."
        }
    },
    {
        "filename": "HIVE-13209.json",
        "creation_time": "2016-03-04T21:39:50.000+0000",
        "bug_report": {
            "Title": "Unauthorized Connection Error in HiveMetaStore",
            "Description": "The application encounters a MetaException indicating an unauthorized connection for the super-user when attempting to retrieve a delegation token from the HiveMetaStore. This issue arises during the invocation of the get_delegation_token method, which is part of the HiveMetaStore's HMSHandler.",
            "StackTrace": [
                "ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/<hostname@realm> from IP null)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Attempt to call the get_delegation_token method on the HiveMetaStore with an unauthorized super-user.",
                "Ensure that the connection settings do not allow for the super-user to authenticate properly."
            ],
            "ExpectedBehavior": "The get_delegation_token method should return a valid delegation token for the super-user without throwing an exception.",
            "ObservedBehavior": "The method throws a MetaException indicating an unauthorized connection for the super-user.",
            "AdditionalDetails": "The issue may be related to the configuration of the HiveMetaStore or the authentication mechanism being used. Ensure that the super-user has the correct permissions and that the connection settings are properly configured."
        }
    },
    {
        "filename": "HIVE-13065.json",
        "creation_time": "2016-02-16T21:11:31.000+0000",
        "bug_report": {
            "Title": "NullPointerException during Hive processing in ExecReducer",
            "Description": "A NullPointerException is thrown while processing rows in the Hive ExecReducer, specifically when handling a row with a null value in one of its fields. This issue arises during the serialization process in the HBaseSerDe class, leading to a Hive Runtime Error.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)",
                "... 14 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that processes rows containing null values in the '_col1' field.",
                "2. Ensure that the job involves the ExecReducer class and the HBaseSerDe serialization process."
            ],
            "ExpectedBehavior": "The Hive job should process all rows without throwing a NullPointerException, even if some fields contain null values.",
            "ObservedBehavior": "The Hive job fails with a NullPointerException when attempting to serialize a row that contains a null value in the '_col1' field.",
            "AdditionalDetails": "The issue appears to stem from the HBaseSerDe's handling of null values during serialization, specifically in the writePrimitiveUTF8 method. Further investigation into the handling of null values in the HBaseRowSerializer may be necessary."
        }
    },
    {
        "filename": "HIVE-11470.json",
        "creation_time": "2015-08-05T18:45:26.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DynamicPartitionFileRecordWriterContainer",
            "Description": "A NullPointerException is thrown during the execution of a MapReduce job when attempting to write records using the DynamicPartitionFileRecordWriterContainer. This occurs specifically in the getLocalFileWriter method, indicating that a required object is not initialized properly.",
            "StackTrace": [
                "java.io.IOException: java.lang.NullPointerException",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)",
                "at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)",
                "at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)",
                "at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)",
                "at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)",
                "at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)"
            ],
            "StepsToReproduce": [
                "Run a MapReduce job that involves dynamic partitioning using HCatalog.",
                "Ensure that the input data contains dynamic partition columns.",
                "Monitor the job execution to observe the NullPointerException."
            ],
            "ExpectedBehavior": "The MapReduce job should complete successfully, writing records to the appropriate dynamic partitions without throwing exceptions.",
            "ObservedBehavior": "The job fails with a NullPointerException, indicating that a required object in the getLocalFileWriter method is not initialized.",
            "AdditionalDetails": "The issue appears to stem from the getLocalFileWriter method in the DynamicPartitionFileRecordWriterContainer class, where it attempts to access a key in the baseDynamicWriters map that may not have been initialized due to missing dynamic partition values."
        }
    },
    {
        "filename": "HIVE-12476.json",
        "creation_time": "2015-11-20T03:30:18.000+0000",
        "bug_report": {
            "Title": "NullPointerException in TBinaryProtocol.writeString",
            "Description": "A NullPointerException occurs when attempting to write a string in the TBinaryProtocol class. This issue arises during the serialization process of Hive's SerDeInfo and StorageDescriptor classes, indicating that a null value is being passed where a non-null string is expected.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)",
                "at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)",
                "at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)",
                "at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the method that triggers the serialization of a SerDeInfo object.",
                "Ensure that one of the fields being serialized is null, particularly a string field."
            ],
            "ExpectedBehavior": "The serialization process should handle null values gracefully, either by skipping them or by providing a default value.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the serialization process to fail.",
            "AdditionalDetails": "The issue likely stems from the SerDeInfo or StorageDescriptor classes where a string field is not properly initialized before serialization. Further investigation is needed to identify which specific field is null."
        }
    },
    {
        "filename": "HIVE-10559.json",
        "creation_time": "2015-04-30T21:12:40.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException in RemoveDynamicPruningBySize",
            "Description": "An IndexOutOfBoundsException occurs when attempting to access an element from an empty ArrayList in the RemoveDynamicPruningBySize class. This issue arises during the optimization phase of query compilation in Hive, specifically when processing nodes that have no children.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process(RemoveDynamicPruningBySize.java:61)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:77)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:281)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:123)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10092)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9932)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1026)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1000)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:139)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_q85(TestMiniTezCliDriver.java:123)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that triggers the optimization phase.",
                "Ensure that the query involves nodes that do not have any children."
            ],
            "ExpectedBehavior": "The optimization process should handle nodes without children gracefully without throwing an exception.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when attempting to access the first element of an empty ArrayList.",
            "AdditionalDetails": "The issue occurs in the 'process' method of the RemoveDynamicPruningBySize class, specifically at line 61 where it attempts to access an element from an ArrayList that is expected to have children."
        }
    },
    {
        "filename": "HIVE-9721.json",
        "creation_time": "2015-02-19T06:56:17.000+0000",
        "bug_report": {
            "Title": "UnsupportedOperationException in Hive when accessing ACL status",
            "Description": "An UnsupportedOperationException is thrown when attempting to access the ACL status of a file in a RawLocalFileSystem. This occurs during the execution of a Hive query that involves creating a staging directory.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus",
                "at org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)",
                "at org.apache.hadoop.fs.FilterFileSystem.getAclStatus(FilterFileSystem.java:562)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:645)",
                "at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:524)",
                "at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)",
                "at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that requires creating a staging directory.",
                "2. Ensure that the file system being used is RawLocalFileSystem.",
                "3. Observe the exception thrown during the execution."
            ],
            "ExpectedBehavior": "The query should execute successfully, creating the necessary staging directories without throwing exceptions.",
            "ObservedBehavior": "An UnsupportedOperationException is thrown indicating that RawLocalFileSystem does not support the getAclStatus operation.",
            "AdditionalDetails": "The issue arises in the method getFullFileStatus in Hadoop23Shims, which is called during the mkdir operation in FileUtils. The RawLocalFileSystem does not implement ACLs, leading to this exception."
        }
    },
    {
        "filename": "HIVE-4216.json",
        "creation_time": "2013-03-21T20:53:24.000+0000",
        "bug_report": {
            "Title": "NullPointerException in Hive during Reduce Task Execution",
            "Description": "A NullPointerException occurs in the Hive execution framework while processing a reduce task. The error is triggered when attempting to create a Hive record writer, leading to a failure in processing the output of the reduce operation.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:268)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:448)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:399)",
                "at org.apache.hadoop.yarn.applications.distributedshell.YarnChild$2.run(YarnChild.java:157)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:152)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:237)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:477)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:525)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)",
                "... 7 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.TaskID$CharTaskTypeMaps.getRepresentingCharacter(TaskID.java:265)",
                "at org.apache.hadoop.mapreduce.TaskID.appendTo(TaskID.java:153)",
                "at org.apache.hadoop.mapreduce.TaskAttemptID.appendTo(TaskAttemptID.java:119)",
                "at org.apache.hadoop.mapreduce.TaskAttemptID.toString(TaskAttemptID.java:151)",
                "at java.lang.String.valueOf(String.java:2826)",
                "at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209)",
                "at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:69)",
                "at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.getRecordWriter(HFileOutputFormat.java:90)",
                "at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getHiveRecordWriter(HiveHFileOutputFormat.java:67)",
                "at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getHiveRecordWriter(HiveHFileOutputFormat.java:104)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves a reduce task.",
                "2. Ensure that the input data contains rows that trigger the reduce operation.",
                "3. Monitor the execution for any NullPointerExceptions during the reduce phase."
            ],
            "ExpectedBehavior": "The Hive reduce task should process the input rows and write the output without encountering any exceptions.",
            "ObservedBehavior": "The Hive reduce task fails with a NullPointerException, preventing successful completion of the job.",
            "AdditionalDetails": "The root cause appears to be related to the handling of task IDs in the Hadoop framework, specifically in the methods related to creating record writers and managing task attempts. Further investigation into the state of the task ID and its associated properties at the time of the exception may be necessary."
        }
    },
    {
        "filename": "HIVE-13836.json",
        "creation_time": "2016-05-24T22:37:59.000+0000",
        "bug_report": {
            "Title": "NucleusTransactionException: Invalid state when adding notification event",
            "Description": "An exception is thrown when attempting to add a notification event due to an invalid transaction state. The transaction manager indicates that a transaction has already started, leading to a NucleusTransactionException.",
            "StackTrace": [
                "org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started",
                "at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)",
                "at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)",
                "at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)",
                "at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:463)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7522)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)",
                "at com.sun.proxy.$Proxy10.addNotificationEvent(Unknown Source)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.enqueue(DbNotificationListener.java:261)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onCreateTable(DbNotificationListener.java:123)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1483)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1502)",
                "at sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy14.create_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_table_with_environment_context.getResult(ThriftHiveMetastore.java:9267)"
            ],
            "StepsToReproduce": [
                "1. Call the addNotificationEvent method in the ObjectStore class.",
                "2. Ensure that a transaction is already active before the call.",
                "3. Observe the exception thrown."
            ],
            "ExpectedBehavior": "The notification event should be added successfully without throwing an exception, even if a transaction is already active.",
            "ObservedBehavior": "A NucleusTransactionException is thrown indicating that the transaction has already started, preventing the addition of the notification event.",
            "AdditionalDetails": "The issue arises from the openTransaction method in the ObjectStore class, which does not handle the case where a transaction is already active. The openTransactionCalls counter is incremented, leading to an invalid state when trying to begin a new transaction."
        }
    },
    {
        "filename": "HIVE-9873.json",
        "creation_time": "2015-03-05T17:35:33.000+0000",
        "bug_report": {
            "Title": "IOException in DeprecatedParquetHiveInput due to size mismatch",
            "Description": "An IOException is thrown when the size of the object being read differs from the expected size. This occurs in the DeprecatedParquetHiveInput class when attempting to read records from a Parquet file. The error message indicates that the value size is 23 while the current object size is 29, leading to a failure in processing the records.",
            "StackTrace": [
                "Exception running child : java.io.IOException: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:105)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:224)",
                "... 11 more",
                "Caused by: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:199)",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:52)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)"
            ],
            "StepsToReproduce": [
                "Attempt to read records from a Parquet file using DeprecatedParquetHiveInput.",
                "Ensure that the records being read have a size mismatch (e.g., expected size 23 but actual size 29).",
                "Observe the IOException being thrown during the read operation."
            ],
            "ExpectedBehavior": "The records should be read successfully without any size mismatch errors.",
            "ObservedBehavior": "An IOException is thrown indicating a size mismatch between the expected and actual sizes of the records being read.",
            "AdditionalDetails": "The issue seems to stem from the method 'next(final Void key, final ArrayWritable value)' in the DeprecatedParquetHiveInput class, where it checks the size of the current value against the expected value. If they differ, an IOException is thrown with a detailed message."
        }
    },
    {
        "filename": "HIVE-13174.json",
        "creation_time": "2016-02-26T23:34:36.000+0000",
        "bug_report": {
            "Title": "HiveException: No vector argument type for type name binary",
            "Description": "A HiveException is thrown when attempting to vectorize an expression that contains a binary type. The error indicates that there is no vector argument type defined for the binary type, which leads to a failure in the vectorization process.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that involves a binary type in a vectorized context.",
                "Ensure that vectorization is enabled in the Hive configuration."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully without throwing an exception related to vectorization.",
            "ObservedBehavior": "A HiveException is thrown indicating that there is no vector argument type for the binary type, causing the query to fail.",
            "AdditionalDetails": "The issue arises in the method 'getConstantVectorExpression' where it attempts to retrieve a vector expression for a constant value of binary type, which is not supported."
        }
    },
    {
        "filename": "HIVE-5431.json",
        "creation_time": "2013-10-03T03:35:44.000+0000",
        "bug_report": {
            "Title": "Null Property Value Causes IOException in FetchOperator",
            "Description": "An IOException is thrown in the FetchOperator when attempting to fetch rows due to a null property value being set in the Hadoop configuration. This occurs when the method 'copyTableJobPropertiesToConf' is called with a TableDesc that contains null job properties.",
            "StackTrace": [
                "java.io.IOException: java.lang.IllegalArgumentException: Property value must not be null",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "Caused by: java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1826)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:380)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that involves fetching data from a table.",
                "2. Ensure that the table has job properties set to null.",
                "3. Observe the exception thrown during the fetch operation."
            ],
            "ExpectedBehavior": "The FetchOperator should successfully fetch rows without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating that a property value must not be null, leading to a failure in fetching rows.",
            "AdditionalDetails": "The issue arises in the 'copyTableJobPropertiesToConf' method where it attempts to set job properties from a TableDesc that may contain null values. Proper validation should be implemented to check for null values before setting properties."
        }
    },
    {
        "filename": "HIVE-13115.json",
        "creation_time": "2016-02-22T21:43:32.000+0000",
        "bug_report": {
            "Title": "MetaException: Unexpected null for one of the IDs in getPartitions",
            "Description": "A MetaException is thrown when attempting to retrieve partitions from the Hive metastore. The exception indicates that a null value was encountered for one of the IDs, which is unexpected for a non-view table. This issue arises during the execution of the getPartitions method, specifically when trying to access partitions via SQL filtering.",
            "StackTrace": [
                "MetaException(message:Unexpected null for one of the IDs, SD 6437, column null, serde 6437 for a non- view)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)",
                "at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the get_partitions method on a non-view table in the Hive metastore.",
                "Ensure that the table has partitions defined.",
                "Check the database and table names passed to the method."
            ],
            "ExpectedBehavior": "The get_partitions method should return a list of partitions for the specified table without throwing an exception.",
            "ObservedBehavior": "A MetaException is thrown indicating an unexpected null value for one of the IDs, preventing the retrieval of partitions.",
            "AdditionalDetails": "The issue may be related to the SQL filter applied in the getPartitionsViaSqlFilterInternal method. It is important to verify that the database and table names are correctly specified and that the partitions exist in the metastore."
        }
    },
    {
        "filename": "HIVE-4723.json",
        "creation_time": "2013-06-12T20:37:55.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DDLSemanticAnalyzer during ALTER TABLE operation",
            "Description": "A NullPointerException occurs in the DDLSemanticAnalyzer class when attempting to add table partition outputs during an ALTER TABLE operation. This issue arises when the method 'addTablePartsOutputs' is called with a null value for the partition specifications.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2912)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2877)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(DDLSemanticAnalyzer.java:2730)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:316)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:277)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "Execute an ALTER TABLE command that involves adding partitions.",
                "Ensure that the partition specifications provided are null or not properly initialized."
            ],
            "ExpectedBehavior": "The ALTER TABLE command should successfully add the specified partitions without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a null value was encountered when attempting to add table partition outputs.",
            "AdditionalDetails": "The issue appears to stem from the 'addTablePartsOutputs' method being called with a null 'partSpecs' argument. This method is responsible for handling the outputs related to table partitions, and it expects a valid list of partition specifications."
        }
    },
    {
        "filename": "HIVE-15686.json",
        "creation_time": "2017-01-20T22:29:36.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to Wrong FileSystem Path in Hive Metastore",
            "Description": "An IllegalArgumentException is thrown when attempting to drop partitions in the Hive Metastore due to a mismatch in the expected and actual HDFS file system paths. The operation expects a local HDFS path but receives a remote HDFS path instead.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://remote-cluster-nn1.myth.net:8020/dbs/mythdb/myth_table/dt=20170120, expected: hdfs://local-cluster-n1.myth.net:8020",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1985)",
                "at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1290)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.checkTrashPurgeCombination(HiveMetaStore.java:1746)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2974)",
                "at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy5.drop_partitions_req(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:10005)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:9989)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:767)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:763)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:763)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:285)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Attempt to drop partitions from a Hive table that is configured to use a remote HDFS path.",
                "2. Ensure that the Hive Metastore is configured to expect a local HDFS path.",
                "3. Execute the drop_partitions_req method."
            ],
            "ExpectedBehavior": "The drop_partitions_req method should successfully drop the specified partitions without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a mismatch in the expected and actual HDFS file system paths.",
            "AdditionalDetails": "The issue arises in the isPathEncrypted method when it attempts to check the encryption zone for the provided path, which is not recognized as belonging to the expected file system."
        }
    },
    {
        "filename": "HIVE-4975.json",
        "creation_time": "2013-08-01T16:21:38.000+0000",
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException in Hive ORC Struct Processing",
            "Description": "A runtime exception occurs when processing rows in Hive due to an ArrayIndexOutOfBoundsException. This happens specifically when trying to access a field in an OrcStruct that does not exist, leading to a failure in the MapOperator.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "... 8 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating d",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:80)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:654)",
                "... 9 more",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.evaluate(ExprNodeColumnEvaluator.java:98)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:76)"
            ],
            "StepsToReproduce": [
                "1. Load a dataset into Hive that uses ORC format.",
                "2. Execute a query that involves selecting fields from an OrcStruct.",
                "3. Ensure that the dataset has fewer fields than expected in the OrcStruct."
            ],
            "ExpectedBehavior": "The query should process the rows without throwing an ArrayIndexOutOfBoundsException, returning the expected results.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown when attempting to access a field in an OrcStruct that does not exist, causing the query to fail.",
            "AdditionalDetails": "The issue seems to stem from the getStructFieldData method in OrcStructInspector, which does not handle cases where the field index exceeds the available fields in the OrcStruct."
        }
    },
    {
        "filename": "HIVE-10538.json",
        "creation_time": "2015-04-29T20:06:38.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FileSinkOperator during Hive Reduce Task",
            "Description": "A NullPointerException is thrown in the FileSinkOperator while processing rows in a Hive reduce task. This occurs when the method findWriterOffset is called, leading to a failure in the reduce phase of the job.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"113\",\"_col1\":\"val_113\"}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)"
            ],
            "StepsToReproduce": [
                "Run a Hive job that involves a reduce task with a FileSinkOperator.",
                "Ensure that the input data contains rows that may lead to a null value being processed.",
                "Observe the logs for a NullPointerException during the reduce phase."
            ],
            "ExpectedBehavior": "The reduce task should process all rows without throwing a NullPointerException, and the output should be generated successfully.",
            "ObservedBehavior": "A NullPointerException is thrown in the FileSinkOperator, causing the reduce task to fail and preventing the output from being generated.",
            "AdditionalDetails": "The issue seems to stem from the findWriterOffset method, which may not be handling null values correctly. The method checks for a condition on multiFileSpray and attempts to evaluate partitioning expressions, which could lead to a null reference if any of the evaluated objects are null."
        }
    },
    {
        "filename": "HIVE-11902.json",
        "creation_time": "2015-09-21T16:12:37.000+0000",
        "bug_report": {
            "Title": "SQL Syntax Error in Transaction Abortion Process",
            "Description": "A SQL syntax error occurs when attempting to abort transactions in the Hive metastore. The error is triggered during the execution of a delete statement on the HIVE_LOCKS table, specifically when the generated SQL query is malformed due to an empty list of transaction IDs.",
            "StackTrace": [
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:360)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)",
                "at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)",
                "at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Trigger a transaction timeout in the Hive metastore.",
                "Ensure that the list of transaction IDs to abort is empty.",
                "Observe the logs for the SQL execution attempt."
            ],
            "ExpectedBehavior": "The system should handle the case of an empty transaction ID list gracefully, either by skipping the SQL execution or by generating a valid SQL statement.",
            "ObservedBehavior": "The system throws a MySQLSyntaxErrorException due to an invalid SQL statement generated with an empty list of transaction IDs, resulting in a syntax error near ')'.",
            "AdditionalDetails": "The issue arises in the 'abortTxns' method where the SQL statement is constructed. If 'txnids' is empty, the resulting SQL query becomes 'delete from HIVE_LOCKS where hl_txnid in ()', which is invalid syntax."
        }
    },
    {
        "filename": "HIVE-18918.json",
        "creation_time": "2018-03-09T00:47:55.000+0000",
        "bug_report": {
            "Title": "IOException during Compaction Job Launch in Hive",
            "Description": "An IOException is thrown when attempting to launch a compaction job in Hive. The error message indicates a major issue, but the specific cause is not detailed in the stack trace. This issue occurs within the CompactorMR class during the execution of the compaction process.",
            "StackTrace": [
                "java.io.IOException: Major",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob(CompactorMR.java:314)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:269)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:172)"
            ],
            "StepsToReproduce": [
                "1. Initiate a compaction job in Hive using the CompactorMR class.",
                "2. Ensure that the job configuration and parameters are set correctly.",
                "3. Observe the logs for any IOException being thrown during the job launch."
            ],
            "ExpectedBehavior": "The compaction job should launch successfully without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown with the message 'Major', preventing the compaction job from launching.",
            "AdditionalDetails": "The method 'launchCompactionJob' is responsible for initiating the compaction process. The IOException suggests that there may be an issue with the job configuration or the environment setup that needs to be investigated further."
        }
    },
    {
        "filename": "HIVE-8107.json",
        "creation_time": "2014-09-15T19:49:09.000+0000",
        "bug_report": {
            "Title": "SemanticException: Table not found during update/delete operation",
            "Description": "A SemanticException is thrown when attempting to parse an update or delete query due to a missing table. The error indicates that the specified table 'no_such_table' does not exist in the Hive metastore, leading to a failure in the query execution.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:97)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:66)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:217)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:406)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:302)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1051)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1121)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:988)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:978)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:441)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:457)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found no_such_table",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:978)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:128)"
            ],
            "StepsToReproduce": [
                "Attempt to execute an update or delete query on a non-existent table, e.g., 'UPDATE no_such_table SET column1 = value1;' or 'DELETE FROM no_such_table WHERE condition;'",
                "Observe the thrown SemanticException indicating the table was not found."
            ],
            "ExpectedBehavior": "The query should execute successfully if the specified table exists, or provide a meaningful error message if it does not.",
            "ObservedBehavior": "A SemanticException is thrown indicating that the table 'no_such_table' cannot be found, preventing the execution of the query.",
            "AdditionalDetails": "The method 'reparseAndSuperAnalyze' in the 'UpdateDeleteSemanticAnalyzer' class is responsible for rewriting the delete or update query into an insert statement. The failure occurs when it attempts to retrieve the table metadata using 'db.getTable(tableName[0], tableName[1])', which fails due to the non-existent table."
        }
    },
    {
        "filename": "HIVE-1326.json",
        "creation_time": "2010-04-25T20:50:54.000+0000",
        "bug_report": {
            "Title": "IOException: No space left on device during file write operation",
            "Description": "The application encounters an IOException indicating that there is no space left on the device when attempting to write data to a file. This issue arises during the execution of a Hive job that involves spilling data to a temporary file.",
            "StackTrace": [
                "org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:346)",
                "at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)",
                "at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)",
                "at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)",
                "at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)",
                "at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1013)",
                "at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)",
                "at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat$1.write(HiveSequenceFileOutputFormat.java:70)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.processOp(JoinOperator.java:118)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:456)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:158)",
                "Caused by: java.io.IOException: No space left on device",
                "at java.io.FileOutputStream.writeBytes(Native Method)",
                "at java.io.FileOutputStream.write(FileOutputStream.java:260)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves writing data to a temporary file.",
                "2. Ensure that the disk space on the device is limited or nearly full.",
                "3. Monitor the job execution to observe the IOException being thrown."
            ],
            "ExpectedBehavior": "The application should successfully write data to the temporary file without encountering any IOException related to disk space.",
            "ObservedBehavior": "The application throws an IOException indicating 'No space left on device' when attempting to write data to a file, causing the job to fail.",
            "AdditionalDetails": "The issue occurs in the 'spillBlock' method of the 'RowContainer' class, which is responsible for writing data to a temporary file. The failure to write is due to insufficient disk space, which can be resolved by ensuring adequate space is available before executing the job."
        }
    },
    {
        "filename": "HIVE-11369.json",
        "creation_time": "2015-07-24T16:28:47.000+0000",
        "bug_report": {
            "Title": "HiveSQLException during query execution",
            "Description": "A HiveSQLException is thrown when executing a SQL statement due to an execution error with return code 1 from the MapredLocalTask. This indicates that there was an issue processing the statement, which could be related to the query itself or the underlying data.",
            "StackTrace": [
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:146)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:173)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:379)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegating Method)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)",
                "at com.sun.proxy.$Proxy23.executeStatement(Unknown Source)",
                "at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:258)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Execute a SQL statement that triggers the MapredLocalTask.",
                "Ensure that the Hive environment is set up correctly.",
                "Monitor for any execution errors during the query processing."
            ],
            "ExpectedBehavior": "The SQL statement should execute successfully without throwing a HiveSQLException.",
            "ObservedBehavior": "A HiveSQLException is thrown indicating an execution error with return code 1 from the MapredLocalTask.",
            "AdditionalDetails": "The error may be related to the specific SQL statement being executed or the data being processed. Further investigation into the query and the data is required to identify the exact cause of the execution error."
        }
    },
    {
        "filename": "HIVE-9055.json",
        "creation_time": "2014-12-09T19:51:18.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException in GenTezWork Processing",
            "Description": "An IndexOutOfBoundsException occurs when processing a node in the GenTezWork class. The exception is thrown due to an attempt to access an index that is out of bounds in a LinkedList, specifically when the index is -1 while the size of the list is 1.",
            "StackTrace": [
                "java.lang.IndexOutOfBoundsException: Index: -1, Size: 1",
                "at java.util.LinkedList.checkElementIndex(LinkedList.java:553)",
                "at java.util.LinkedList.get(LinkedList.java:474)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:354)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:420)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1108)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1035)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:151)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:362)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:297)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez(TestMiniTezCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that involves a union operation using Tez as the execution engine.",
                "Ensure that the query involves a node that has children, but the processing logic attempts to access an invalid index."
            ],
            "ExpectedBehavior": "The GenTezWork class should process nodes without throwing an IndexOutOfBoundsException, correctly handling the indices of the LinkedList.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when attempting to access an index of -1 in a LinkedList, indicating a logic error in index management.",
            "AdditionalDetails": "The issue appears to stem from the method 'dispatchAndReturn' in the DefaultRuleDispatcher class, where the node's children are processed. The logic should ensure that the index being accessed is valid and within the bounds of the list."
        }
    },
    {
        "filename": "HIVE-13856.json",
        "creation_time": "2016-05-25T21:50:49.000+0000",
        "bug_report": {
            "Title": "SQLSyntaxErrorException: ORA-00933 - SQL command not properly ended",
            "Description": "The application encounters a SQLSyntaxErrorException indicating that a SQL command is not properly ended when attempting to open transactions in the Hive Metastore. This issue arises during the execution of SQL statements related to transaction management.",
            "StackTrace": [
                "java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)",
                "at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)",
                "at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)",
                "at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)",
                "at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)",
                "at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)",
                "at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)",
                "at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)",
                "at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)",
                "at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)",
                "at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)",
                "at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy15.open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the open_txns method in the Hive Metastore with a valid OpenTxnRequest.",
                "Ensure that the database connection is established with an Oracle database.",
                "Monitor the logs for SQL execution errors."
            ],
            "ExpectedBehavior": "The open_txns method should successfully execute the SQL commands to open transactions without any syntax errors.",
            "ObservedBehavior": "The method fails with a SQLSyntaxErrorException indicating that the SQL command is not properly ended, preventing the opening of transactions.",
            "AdditionalDetails": "The issue may stem from the SQL statements being constructed in the openTxns method, particularly the update and insert statements. The addForUpdateClause method is used to append 'for update' to the select statement, which may not be compatible with Oracle's SQL syntax in this context."
        }
    },
    {
        "filename": "HIVE-7374.json",
        "creation_time": "2014-07-09T16:02:15.000+0000",
        "bug_report": {
            "Title": "TProtocolException: Required field 'compacts' is unset in ShowCompactResponse",
            "Description": "A TProtocolException is thrown when the required field 'compacts' in the ShowCompactResponse struct is not set. This indicates that the response from the Thrift service is missing a critical field, leading to a failure in processing the response.",
            "StackTrace": [
                "org.apache.thrift.protocol.TProtocolException: Required field 'compacts' is unset! Struct:ShowCompactResponse(compacts:null)",
                "at org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate(ShowCompactResponse.java:310)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.run(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "Invoke the Thrift service method that returns a ShowCompactResponse.",
                "Ensure that the response does not set the 'compacts' field."
            ],
            "ExpectedBehavior": "The ShowCompactResponse should contain a valid 'compacts' field that is set, allowing for successful validation and processing of the response.",
            "ObservedBehavior": "The response from the Thrift service is missing the required 'compacts' field, resulting in a TProtocolException being thrown during validation.",
            "AdditionalDetails": "The validate method in ShowCompactResponse is responsible for checking the presence of required fields. The absence of 'compacts' indicates a potential issue in the service logic that generates this response."
        }
    },
    {
        "filename": "HIVE-12206.json",
        "creation_time": "2015-10-17T00:30:22.000+0000",
        "bug_report": {
            "Title": "ClassNotFoundException for UniqueNumberGenerator during Hive query execution",
            "Description": "A ClassNotFoundException is thrown when attempting to deserialize a Hive query plan that references the class 'com.aginity.amp.hive.udf.UniqueNumberGenerator'. This indicates that the class is not available in the classpath during the execution of the Hive query.",
            "StackTrace": [
                "org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "Serialization trace:",
                "genericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)",
                "colExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "parentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)",
                "at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138)",
                "at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:99)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:112)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)",
                "at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507)",
                "at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1081)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:970)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.cloneOperatorTree(Utilities.java:928)",
                "at org.apache.hadoop.hive.ql.parse.GenTezUtils.removeUnionOperators(GenTezUtils.java:228)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:373)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:205)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10193)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "Caused by: java.lang.ClassNotFoundException: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:358)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:270)",
                "at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that utilizes the UniqueNumberGenerator UDF.",
                "Ensure that the class com.aginity.amp.hive.udf.UniqueNumberGenerator is not included in the classpath."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully without throwing a ClassNotFoundException.",
            "ObservedBehavior": "A ClassNotFoundException is thrown indicating that the class com.aginity.amp.hive.udf.UniqueNumberGenerator cannot be found.",
            "AdditionalDetails": "Ensure that the UDF class is properly compiled and included in the Hive classpath before executing queries that reference it."
        }
    },
    {
        "filename": "HIVE-10098.json",
        "creation_time": "2015-03-26T17:12:20.000+0000",
        "bug_report": {
            "Title": "AuthenticationException during FetchOperator execution",
            "Description": "An UndeclaredThrowableException is thrown during the execution of the FetchOperator in Hive, caused by an AuthenticationException indicating that no valid Kerberos credentials were provided. This issue arises when attempting to add delegation tokens for accessing HDFS resources.",
            "StackTrace": [
                "java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:634)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:363)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:337)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:303)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:735)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.io.IOException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:826)",
                "at org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension.addDelegationTokens(KeyProviderDelegationTokenExtension.java:86)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2017)",
                "at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)",
                "at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)",
                "at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)",
                "at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)",
                "at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:413)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:559)",
                "... 9 more",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1655)",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:808)",
                "... 18 more",
                "Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:306)",
                "at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:196)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:127)"
            ],
            "StepsToReproduce": [
                "Run a Hive query that requires access to HDFS resources with Kerberos authentication enabled.",
                "Ensure that the Kerberos ticket is not available or has expired.",
                "Observe the exception thrown during the execution of the FetchOperator."
            ],
            "ExpectedBehavior": "The FetchOperator should successfully retrieve the next row of data without throwing an exception, provided that valid Kerberos credentials are available.",
            "ObservedBehavior": "An UndeclaredThrowableException is thrown, indicating that the FetchOperator cannot proceed due to missing Kerberos credentials, leading to an AuthenticationException.",
            "AdditionalDetails": "The issue is likely related to the configuration of Kerberos authentication in the Hadoop environment. Ensure that the Kerberos ticket is valid and accessible before executing Hive queries that require HDFS access."
        }
    },
    {
        "filename": "HIVE-7745.json",
        "creation_time": "2014-08-16T01:20:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException in GenMapRedUtils.createMoveTask",
            "Description": "A NullPointerException is thrown in the createMoveTask method of GenMapRedUtils when processing a file sink in a Hive query. This issue occurs during the task generation phase of query compilation, indicating that a required object is not being properly initialized or passed.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(GenMapRedUtils.java:1738)",
                "        at org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.processFileSink(GenSparkUtils.java:281)",
                "        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:187)",
                "        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:199)",
                "        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9508)",
                "        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:414)",
                "        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)",
                "        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1005)",
                "        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1070)",
                "        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)",
                "        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)",
                "        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "        at java.lang.reflect.Method.invoke(Method.java:597)",
                "        at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that involves a file sink operation.",
                "Ensure that the query is structured in a way that triggers the task generation phase.",
                "Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The query should compile successfully and generate the necessary tasks without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the task generation phase, preventing the query from compiling successfully.",
            "AdditionalDetails": "The issue likely stems from an uninitialized or null object being passed to the createMoveTask method. Further investigation is needed to identify which specific object is null and under what conditions."
        }
    },
    {
        "filename": "HIVE-11762.json",
        "creation_time": "2015-09-08T20:10:54.000+0000",
        "bug_report": {
            "Title": "NoSuchMethodError in Hadoop23Shims during HCatLoaderEncryption Test",
            "Description": "A NoSuchMethodError is thrown when attempting to call the setKeyProvider method on the DFSClient class in the Hadoop23Shims class. This issue occurs during the initialization of the encryption shim in the TestHCatLoaderEncryption class.",
            "StackTrace": [
                "java.lang.NoSuchMethodError: org.apache.hadoop.hdfs.DFSClient.setKeyProvider(Lorg/apache/hadoop/crypto/key/KeyProviderCryptoExtension;)V",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Hadoop23Shims.java:534)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.initEncryptionShim(TestHCatLoaderEncryption.java:252)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.setup(TestHCatLoaderEncryption.java:200)"
            ],
            "StepsToReproduce": [
                "Run the TestHCatLoaderEncryption test suite.",
                "Ensure that the Hadoop version being used is compatible with the expected method signatures.",
                "Observe the initialization of the encryption shim."
            ],
            "ExpectedBehavior": "The encryption shim should initialize without errors, and the setKeyProvider method should be successfully called on the DFSClient.",
            "ObservedBehavior": "A NoSuchMethodError is thrown indicating that the setKeyProvider method cannot be found, leading to a failure in the test setup.",
            "AdditionalDetails": "The issue may stem from an incompatible version of Hadoop being used, which does not include the setKeyProvider method in the DFSClient class. It is important to verify the Hadoop version and ensure it matches the expected API."
        }
    },
    {
        "filename": "HIVE-6990.json",
        "creation_time": "2014-04-30T04:24:25.000+0000",
        "bug_report": {
            "Title": "JDODataStoreException when executing SQL query for partition retrieval",
            "Description": "A JDODataStoreException is thrown when attempting to execute a SQL query to retrieve partitions from the Hive metastore. The error occurs during the execution of the query that joins multiple tables to filter partitions based on specific criteria.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select PARTITIONS.PART_ID from PARTITIONS  inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID   inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:98)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1806)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)",
                "at java.lang.reflect.Method.invoke(Method.java:619)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)",
                "at com.sun.proxy.$Proxy11.getPartitionsByFilter(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:3310)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)",
                "at java.lang.reflect.Method.invoke(Method.java:619)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)",
                "at com.sun.proxy.$Proxy12.get_partitions_by_filter(Unknown Source)"
            ],
            "StepsToReproduce": [
                "Invoke the method get_partitions_by_filter with valid parameters for dbName, tblName, and filter.",
                "Ensure that the database and table exist in the Hive metastore.",
                "Check if the filter criteria provided matches any existing partitions."
            ],
            "ExpectedBehavior": "The method should return a list of partitions that match the specified filter criteria without throwing an exception.",
            "ObservedBehavior": "A JDODataStoreException is thrown, indicating an error in executing the SQL query for retrieving partitions.",
            "AdditionalDetails": "The issue may be related to the SQL query structure or the parameters being passed to it. Further investigation into the database schema and the values being used in the filter is required."
        }
    },
    {
        "filename": "HIVE-7114.json",
        "creation_time": "2014-05-22T14:58:09.000+0000",
        "bug_report": {
            "Title": "Exception during Tez session opening in HiveServer2",
            "Description": "An exception is thrown when attempting to open a Tez session in HiveServer2. This occurs during the initialization of the HiveServer2 service, specifically when the TezSessionState is being opened. The root cause appears to be related to the configuration or state of the session.",
            "StackTrace": [
                "java.lang.Exception: Opening session",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)",
                "at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)",
                "at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)",
                "at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "Start the HiveServer2 service.",
                "Ensure that the configuration is set to use Tez as the execution engine.",
                "Attempt to open a session that requires a Tez session."
            ],
            "ExpectedBehavior": "The Tez session should open successfully without throwing an exception.",
            "ObservedBehavior": "An exception is thrown indicating an error while opening the Tez session, preventing the HiveServer2 from starting properly.",
            "AdditionalDetails": "The issue may be related to the configuration of the HiveServer2 or the state of the Tez session pool. Further investigation into the configuration settings and the state of the TezSessionPoolManager may be required."
        }
    },
    {
        "filename": "HIVE-11991.json",
        "creation_time": "2015-09-29T21:46:06.000+0000",
        "bug_report": {
            "Title": "ClassCastException when fetching results in Hive",
            "Description": "A ClassCastException occurs when attempting to fetch results from a Hive query. The exception indicates that a String object is being incorrectly cast to a Hadoop Text object, which leads to a failure in the fetch operation.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1621)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:221)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:153)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:364)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:299)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:832)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11(TestCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:85)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:572)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:564)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "... 27 more",
                "Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:225)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:492)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:445)",
                "at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:429)",
                "at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:50)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:71)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:40)",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that is expected to return results.",
                "Attempt to fetch the results using the FetchTask."
            ],
            "ExpectedBehavior": "The results should be fetched successfully without any exceptions.",
            "ObservedBehavior": "A ClassCastException occurs indicating that a String cannot be cast to a Hadoop Text object.",
            "AdditionalDetails": "The issue seems to stem from the serialization process where a String is being incorrectly handled as a Text object. This could be due to a mismatch in the expected data types during the fetch operation."
        }
    },
    {
        "filename": "HIVE-17900.json",
        "creation_time": "2017-10-25T17:12:42.000+0000",
        "bug_report": {
            "Title": "ParseException in Hive Analyze Statement",
            "Description": "A ParseException occurs when executing an analyze statement in Hive, indicating a syntax error near the input 'dates'. This prevents the successful gathering of statistics for the specified table.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1158)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1148)",
                "at org.apache.hadoop.hive.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:294)",
                "at org.apache.hadoop.hive.txn.compactor.CompactorMR.run(CompactorMR.java:265)",
                "at org.apache.hadoop.hive.txn.compactor.Worker.run(Worker.java:168)",
                "java.io.IOException: Could not update stats for table mobiusad.zces_img_data_small_pt/month=201608/dates=9 due to: (40000,FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement,42000line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement)"
            ],
            "StepsToReproduce": [
                "Execute an analyze statement on the table 'mobiusad.zces_img_data_small_pt' with the specified partition 'month=201608' and 'dates=9'."
            ],
            "ExpectedBehavior": "The analyze statement should successfully gather statistics for the specified table and partition without any syntax errors.",
            "ObservedBehavior": "A ParseException is thrown indicating a syntax error in the analyze statement, preventing the gathering of statistics.",
            "AdditionalDetails": "The error occurs specifically at line 1, character 70 of the analyze statement, where the parser expects a closing parenthesis but encounters the input 'dates'. This suggests a potential issue with the syntax of the analyze command being executed."
        }
    },
    {
        "filename": "HIVE-10816.json",
        "creation_time": "2015-05-25T08:08:37.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ExecDriver.handleSampling",
            "Description": "A NullPointerException occurs in the handleSampling method of the ExecDriver class when executing a Hive job. This issue arises when the context or MapWork object is not properly initialized before being passed to the handleSampling method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)",
                "        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)",
                "        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "        at java.lang.reflect.Method.invoke(Method.java:497)",
                "        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that requires sampling.",
                "2. Ensure that the DriverContext and MapWork objects are not properly initialized.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The handleSampling method should execute without throwing a NullPointerException, properly handling the context and MapWork objects.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that either the context or MapWork object is null when passed to the handleSampling method.",
            "AdditionalDetails": "The handleSampling method is called within the execute method of ExecDriver when the sampling type is greater than 0. If the context or MapWork is not initialized correctly, it leads to a NullPointerException. Proper checks should be implemented to ensure these objects are not null before invoking handleSampling."
        }
    },
    {
        "filename": "HIVE-13017.json",
        "creation_time": "2016-02-05T23:40:09.000+0000",
        "bug_report": {
            "Title": "Hive Query Execution Failure with Return Code 2",
            "Description": "A Hive query execution fails with an error indicating a return code of 2 from the MapredLocalTask. This issue occurs when attempting to execute a join query on multiple tables with specific age conditions.",
            "StackTrace": [
                "ERROR : Execution failed with exit status: 2",
                "ERROR : Obtaining error information",
                "ERROR : Task failed!",
                "Task ID: Stage-5",
                "ERROR : /var/log/hive/hiveServer2.log",
                "Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)",
                "Aborting command set because \"force\" is false and command failed: \"select registration from s10k s join v10k v on (s.name = v.name) join studentparttab30k p on (p.name = v.name) where s.age < 25 and v.age < 25 and p.age < 25;\"",
                "hiveServer2.log shows:",
                "2016-02-02 18:04:43,766 ERROR [HiveServer2-Background-Pool: Thread-517]: operation.Operation (SQLOperation.java:run(209)) - Error running hive query:",
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:156)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Execute the following Hive query: 'select registration from s10k s join v10k v on (s.name = v.name) join studentparttab30k p on (p.name = v.name) where s.age < 25 and v.age < 25 and p.age < 25;'",
                "Monitor the HiveServer2 logs for errors."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results based on the join conditions and age filters.",
            "ObservedBehavior": "The query fails with an execution error, returning code 2, indicating a problem during the processing of the statement.",
            "AdditionalDetails": "The error is likely related to the execution of the MapredLocalTask, which may indicate issues with the underlying data or configuration. Further investigation into the logs and the data being queried is recommended."
        }
    },
    {
        "filename": "HIVE-11303.json",
        "creation_time": "2015-07-18T01:31:02.000+0000",
        "bug_report": {
            "Title": "LimitExceededException: Too Many Counters in Tez Execution",
            "Description": "The application throws a LimitExceededException when the number of counters exceeds the maximum allowed limit of 1200 during the execution of a Tez task. This issue arises when the system attempts to increment the counter beyond the defined limit, leading to a failure in the task execution.",
            "StackTrace": [
                "org.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200",
                "at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)",
                "at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)",
                "at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)",
                "at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that generates a large number of counters (more than 1200).",
                "Monitor the execution of the query to observe the counter increments.",
                "The execution should fail with a LimitExceededException."
            ],
            "ExpectedBehavior": "The system should handle the counter increments without exceeding the maximum limit, allowing the task to complete successfully.",
            "ObservedBehavior": "The system throws a LimitExceededException indicating that the number of counters has exceeded the maximum limit of 1200, causing the task execution to fail.",
            "AdditionalDetails": "The issue is likely related to the configuration of counters in the Tez execution environment. It may require adjustments to the counter management or an increase in the maximum allowed counters."
        }
    },
    {
        "filename": "HIVE-5899.json",
        "creation_time": "2013-11-27T02:48:39.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ColumnStatisticsData.getStringStats()",
            "Description": "A NullPointerException occurs when attempting to retrieve string statistics from the ColumnStatisticsData class. This issue arises when the method getStringStats() is called, and the union field is not set correctly, leading to an attempt to access a null field.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:367)",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats(ColumnStatisticsData.java:444)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:414)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStatsForColumn(StatsUtils.java:369)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:465)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:102)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)",
                "at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:76)",
                "at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:136)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8913)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:341)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:994)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:905)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that involves retrieving column statistics.",
                "2. Ensure that the column statistics for the specified column are not set correctly.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The method getStringStats() should return the string statistics data without throwing an exception, even if the statistics are not set.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to access the string statistics due to a null field.",
            "AdditionalDetails": "The issue seems to stem from the getStringStats() method in ColumnStatisticsData, which does not handle the case where the union field is not set correctly. The method should ideally check if the field is set before attempting to access it."
        }
    },
    {
        "filename": "HIVE-11102.json",
        "creation_time": "2015-06-24T22:54:55.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException in OrcProto Type Subtypes Retrieval",
            "Description": "An IndexOutOfBoundsException occurs when attempting to access subtypes of an empty list in the OrcProto class. This issue arises when the method getColumnIndicesFromNames is called with an empty or invalid column name list, leading to an attempt to access the first element of an empty list.",
            "StackTrace": [
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:938)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:847)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:713)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Call the method getRawDataSizeOfColumns with an empty or invalid list of column names.",
                "2. Ensure that the footer object does not contain any types or subtypes."
            ],
            "ExpectedBehavior": "The method getRawDataSizeOfColumns should handle empty or invalid column names gracefully, returning a size of 0 or throwing a controlled exception.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when attempting to access the first element of an empty list in the getSubtypes method.",
            "AdditionalDetails": "The issue originates from the getColumnIndicesFromNames method, which does not validate the input list of column names before attempting to access the types list. It is recommended to add input validation to check if colNames is empty before proceeding with the logic."
        }
    },
    {
        "filename": "HIVE-9195.json",
        "creation_time": "2014-12-23T01:08:45.000+0000",
        "bug_report": {
            "Title": "UDFArgumentTypeException: Second Argument Must Be a Constant",
            "Description": "An exception is thrown when executing a Hive query that uses the GenericUDAFPercentileApprox function. The error indicates that the second argument passed to the function is not a constant, which violates the expected input types for this function.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:146)",
                "at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:1160)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:3794)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:4467)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5536)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8884)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9745)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9638)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10086)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:877)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23(TestCliDriver.java:120)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that uses the GenericUDAFPercentileApprox function with a non-constant second argument."
            ],
            "ExpectedBehavior": "The second argument to the GenericUDAFPercentileApprox function should be a constant value or a constant array of doubles.",
            "ObservedBehavior": "An UDFArgumentTypeException is thrown indicating that the second argument must be a constant, but a double was passed instead.",
            "AdditionalDetails": "The source code for the getEvaluator method in GenericUDAFPercentileApprox checks if the second parameter is a constant using ObjectInspectorUtils.isConstantObjectInspector. If it is not, it throws the UDFArgumentTypeException."
        }
    },
    {
        "filename": "HIVE-11285.json",
        "creation_time": "2015-07-16T20:46:22.000+0000",
        "bug_report": {
            "Title": "ClassCastException in Hive during MapReduce Execution",
            "Description": "A ClassCastException occurs in the Hive MapReduce job when processing a row with a key-value pair. The error indicates that an IntWritable object cannot be cast to an Integer, leading to a failure in the map task.",
            "StackTrace": [
                "ERROR:",
                "2015-07-15 13:39:04,333 WARN main org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "{\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "{\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)",
                "... 8 more",
                "Caused by: java.lang.RuntimeException: Map local work failed",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:569)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchNextGroup(SMBMapJoinOperator.java:429)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:260)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 9 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Integer",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(JavaIntObjectInspector.java:35)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:305)",
                "at org.apache.hadoop.hive.ql.exec.JoinUtil.computeValues(JoinUtil.java:193)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getFilteredValue(CommonJoinOperator.java:408)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:270)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:558)"
            ],
            "StepsToReproduce": [
                "Run a Hive MapReduce job that processes a dataset containing key-value pairs.",
                "Ensure that the dataset includes an IntWritable type for the key.",
                "Observe the logs for any ClassCastException during the execution."
            ],
            "ExpectedBehavior": "The Hive MapReduce job should process the key-value pairs without throwing any exceptions.",
            "ObservedBehavior": "A ClassCastException is thrown indicating that an IntWritable cannot be cast to an Integer, causing the job to fail.",
            "AdditionalDetails": "The error occurs in the method 'getPrimitiveWritableObject' of 'JavaIntObjectInspector', which suggests that there may be a mismatch in expected data types during the processing of rows in the MapReduce job."
        }
    },
    {
        "filename": "HIVE-10288.json",
        "creation_time": "2015-04-09T23:24:16.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ExprNodeGenericFuncDesc.newInstance",
            "Description": "A NullPointerException occurs in the method ExprNodeGenericFuncDesc.newInstance when processing an expression node in Hive. This issue arises during the semantic analysis phase of query compilation, specifically when generating expression nodes from the AST (Abstract Syntax Tree). The root cause appears to be a null reference being passed to the newInstance method, which is not handled properly.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:232)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1048)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1265)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:205)",
                "at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:149)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10383)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10338)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3815)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3594)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8864)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8819)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9663)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9556)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:9992)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:306)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10003)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:195)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that involves complex expressions or functions.",
                "2. Monitor the execution to reach the semantic analysis phase.",
                "3. Observe the NullPointerException being thrown."
            ],
            "ExpectedBehavior": "The query should compile successfully without throwing a NullPointerException during the semantic analysis phase.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a null reference was encountered in the ExprNodeGenericFuncDesc.newInstance method.",
            "AdditionalDetails": "The issue may be related to the handling of null values in the expression nodes being processed. Further investigation is needed to identify the specific conditions under which the null reference occurs."
        }
    },
    {
        "filename": "HIVE-8771.json",
        "creation_time": "2014-11-07T00:30:25.000+0000",
        "bug_report": {
            "Title": "IOException during file merge operation in Hive",
            "Description": "An IOException occurs when attempting to close the AbstractFileMergeOperator due to a file system issue where a destination path exists but is not a directory. This prevents the successful renaming of files during the merge operation.",
            "StackTrace": [
                "java.lang.Exception: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)",
                "Caused by: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:100)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:679)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:233)",
                "at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.closeOp(OrcFileMergeOperator.java:220)",
                "at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:98)",
                "... 10 more",
                "Caused by: java.io.FileNotFoundException: Destination exists and is not a directory: /home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0",
                "at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:423)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:267)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:257)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:339)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:507)",
                "at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)",
                "at org.apache.hadoop.fs.ProxyFileSystem.rename(ProxyFileSystem.java:177)",
                "at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.renameOrMoveFiles(Utilities.java:1589)",
                "at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:218)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves merging files using AbstractFileMergeOperator.",
                "2. Ensure that the destination path for the merge operation exists and is not a directory.",
                "3. Observe the exception thrown during the close operation of the merge."
            ],
            "ExpectedBehavior": "The merge operation should complete successfully, and files should be renamed or moved to the final destination without errors.",
            "ObservedBehavior": "An IOException is thrown indicating that the destination exists and is not a directory, preventing the merge operation from completing.",
            "AdditionalDetails": "The issue arises in the closeOp method of AbstractFileMergeOperator, specifically when attempting to rename the output path to the final path. The renameOrMoveFiles utility method is called, which fails due to the existing non-directory destination."
        }
    },
    {
        "filename": "HIVE-8008.json",
        "creation_time": "2014-09-05T23:00:11.000+0000",
        "bug_report": {
            "Title": "NullPointerException in FetchTask during data fetching",
            "Description": "A NullPointerException is thrown in the FetchTask class while attempting to fetch results from a Hive query. This occurs during the serialization of data, specifically when writing a primitive UTF-8 value. The issue seems to stem from an uninitialized or null object being passed to the serialization method.",
            "StackTrace": [
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)",
                "  at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "  at java.lang.reflect.Method.invoke(Method.java:606)",
                "  at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)",
                "  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)",
                "  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)",
                "  ... 12 more",
                "Caused by: java.lang.NullPointerException",
                "  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)",
                "  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "  at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)",
                "  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)",
                "  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that involves fetching data.",
                "Ensure that the data being fetched includes a null value or an uninitialized object.",
                "Observe the exception thrown during the fetch operation."
            ],
            "ExpectedBehavior": "The fetch operation should complete successfully, returning the expected results without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the fetch operation, leading to an IOException and preventing the successful retrieval of results.",
            "AdditionalDetails": "The issue appears to be related to the serialization process in the LazySimpleSerDe class, specifically when handling primitive UTF-8 values. It is likely that a null object is being passed to the serialization method, which needs to be handled appropriately."
        }
    },
    {
        "filename": "HIVE-6915.json",
        "creation_time": "2014-04-15T20:20:15.000+0000",
        "bug_report": {
            "Title": "SaslException: GSS initiate failed due to missing Kerberos credentials",
            "Description": "The application encounters a SaslException indicating that the GSS initiation failed because no valid credentials were provided. This issue arises during the setup of a SASL connection in the HBase client, specifically when attempting to obtain an authentication token for a job. The root cause is a failure to find any Kerberos ticket-granting ticket (TGT), which is necessary for authentication.",
            "StackTrace": [
                "javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)",
                "at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:918)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:915)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:915)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1065)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1032)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1474)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1684)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClientProtos.java:1737)",
                "at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:29288)",
                "at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1562)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:87)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:84)",
                "at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:121)",
                "at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:97)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)",
                "at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:67)",
                "at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:174)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:172)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)",
                "at org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:334)",
                "at org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:201)",
                "at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:415)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)",
                "at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)",
                "at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)",
                "at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)"
            ],
            "StepsToReproduce": [
                "Attempt to connect to HBase using SASL authentication without a valid Kerberos TGT.",
                "Ensure that the environment is configured to use Kerberos for authentication."
            ],
            "ExpectedBehavior": "The application should successfully establish a SASL connection using valid Kerberos credentials.",
            "ObservedBehavior": "The application fails to establish a SASL connection and throws a SaslException due to missing Kerberos credentials.",
            "AdditionalDetails": "Ensure that the Kerberos configuration is correct and that a valid TGT is obtained before attempting to connect to HBase."
        }
    },
    {
        "filename": "HIVE-12364.json",
        "creation_time": "2015-11-07T02:04:25.000+0000",
        "bug_report": {
            "Title": "HiveException: Unable to Move File Due to Incompatible InputFormat Class",
            "Description": "A HiveException is thrown when attempting to move a file from a source HDFS path to a destination path. The underlying cause is an IOException indicating that the mapreduce.job.inputformat.class is incompatible with the map compatibility mode. This issue arises during the execution of a MoveTask in Hive.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir",
                "at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)",
                "at org.apache.hadoop.hive.exec.MoveTask.moveFile(MoveTask.java:105)",
                "at org.apache.hadoop.hive.exec.MoveTask.execute(MoveTask.java:222)",
                "at org.apache.hadoop.hive.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "Caused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatibility mode.",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1156)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2647)",
                "... 21 more",
                "Caused by: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatibility mode.",
                "at org.apache.hadoop.mapreduce.Job.ensureNotSet(Job.java:1194)",
                "at org.apache.hadoop.mapreduce.Job.setUseNewAPI(Job.java:1229)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1283)",
                "at org.apache.hadoop.tools.DistCp.createAndSubmitJob(DistCp.java:183)",
                "at org.apache.hadoop.tools.DistCp.execute(DistCp.java:153)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1153)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that involves moving files from one HDFS location to another.",
                "2. Ensure that the input format class specified in the job configuration is incompatible with the map compatibility mode."
            ],
            "ExpectedBehavior": "The file should be successfully moved from the source HDFS path to the destination path without any exceptions.",
            "ObservedBehavior": "A HiveException is thrown indicating that the file cannot be moved due to an incompatible input format class.",
            "AdditionalDetails": "The issue seems to stem from the configuration of the mapreduce.job.inputformat.class, which is not compatible with the current map compatibility mode. This could be due to a misconfiguration in the Hive or Hadoop settings."
        }
    },
    {
        "filename": "HIVE-8766.json",
        "creation_time": "2014-11-06T22:08:43.000+0000",
        "bug_report": {
            "Title": "NucleusDataStoreException: Size request failed during table retrieval",
            "Description": "A NucleusDataStoreException occurs when attempting to retrieve the size of skewed values from the Hive metastore. The error indicates that the SQL Server connection was terminated unexpectedly, leading to a failure in executing the SQL query to count the skewed values.",
            "StackTrace": [
                "org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5183)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1738)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1699)",
                "at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:101)",
                "at com.sun.proxy.$Proxy11.get_table(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:112)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)",
                "at com.sun.proxy.$Proxy12.getTable(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1060)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1015)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerde(DDLSemanticAnalyzer.java:1356)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:299)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:396)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:666)",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.size(ElementContainerStore.java:429)",
                "at org.datanucleus.store.types.backed.List.size(List.java:581)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToSkewedValues(ObjectStore.java:1190)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1168)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToTable(ObjectStore.java:1035)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:893)",
                "at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)",
                "at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:638)"
            ],
            "StepsToReproduce": [
                "Attempt to retrieve a table from the Hive metastore that has skewed values.",
                "Ensure that the SQL Server connection is active and properly configured.",
                "Execute a query that involves counting skewed values."
            ],
            "ExpectedBehavior": "The table should be retrieved successfully without any exceptions, and the count of skewed values should be returned correctly.",
            "ObservedBehavior": "A NucleusDataStoreException is thrown indicating that the size request failed due to an SSL peer shutdown error.",
            "AdditionalDetails": "The root cause of the issue appears to be a connection problem with the SQL Server, specifically an SSL termination issue. This may require checking the SQL Server configuration and network stability."
        }
    },
    {
        "filename": "HIVE-5857.json",
        "creation_time": "2013-11-20T01:12:54.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ExecReducer.configure method",
            "Description": "A NullPointerException is thrown in the configure method of the ExecReducer class when attempting to configure the reducer with a JobConf object. This issue occurs during the initialization of the reducer, specifically when trying to access properties of the keyTableDesc or valueTableDesc, which may not be properly initialized.",
            "StackTrace": [
                "java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 7 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)",
                "... 12 more"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop job that uses the ExecReducer class.",
                "2. Ensure that the JobConf object passed to the configure method is missing necessary configurations.",
                "3. Execute the job and observe the stack trace."
            ],
            "ExpectedBehavior": "The ExecReducer should configure itself without throwing a NullPointerException, successfully initializing the reducer with the provided JobConf.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the configure method in the ExecReducer class, indicating that an expected object is null.",
            "AdditionalDetails": "The issue may stem from the keyTableDesc or valueTableDesc being null or improperly initialized before they are accessed in the configure method. Further investigation into how these descriptors are set up in the ReduceWork object is needed."
        }
    },
    {
        "filename": "HIVE-1547.json",
        "creation_time": "2010-08-17T02:09:54.000+0000",
        "bug_report": {
            "Title": "NullPointerException in DDLTask.unarchive Method",
            "Description": "A NullPointerException is thrown in the DDLTask.unarchive method when attempting to execute a Hive command. This issue occurs during the execution of a task, indicating that an expected object is null.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)",
                "        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)",
                "        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)",
                "        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)",
                "        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)",
                "        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)",
                "        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)",
                "        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "        at java.lang.reflect.Method.invoke(Method.java:597)",
                "        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive command that triggers the DDLTask.",
                "2. Observe the logs for a NullPointerException in the DDLTask.unarchive method."
            ],
            "ExpectedBehavior": "The DDLTask should execute without throwing a NullPointerException, successfully processing the Hive command.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that an object expected by the unarchive method is null, leading to a failure in executing the Hive command.",
            "AdditionalDetails": "The unarchive method is likely trying to access a property or method on a null object. Further investigation is needed to determine which object is null and under what conditions this occurs."
        }
    },
    {
        "filename": "HIVE-6113.json",
        "creation_time": "2013-12-27T07:07:00.000+0000",
        "bug_report": {
            "Title": "HiveMetaStoreClient Instantiation Failure Due to Duplicate Database Entry",
            "Description": "The application encounters a runtime exception when attempting to instantiate the HiveMetaStoreClient, which is caused by a duplicate entry for the default database in the metastore. This prevents the application from switching databases or executing tasks that require database access.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1143)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:260)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:217)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:507)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:875)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:769)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1217)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2372)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2383)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1139)",
                "... 20 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1210)",
                "... 25 more",
                "Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore",
                "NestedThrowables:",
                "java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:165)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:358)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:404)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)",
                "at $Proxy9.createDatabase(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:422)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:441)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:326)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:286)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4060)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:121)",
                "... 30 more",
                "Caused by: java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'",
                "at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2028)",
                "at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1451)",
                "at com.jolbox.bonecp.StatementHandle.executeBatch(StatementHandle.java:469)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:372)",
                "at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:628)",
                "at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:596)",
                "at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:683)",
                "at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:86)",
                "at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:199)",
                "at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:263)",
                "at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:98)",
                "... 46 more",
                "Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:386)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3609)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3541)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2002)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2163)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)",
                "at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)",
                "at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2427)"
            ],
            "StepsToReproduce": [
                "Attempt to switch to a database that does not exist or is in the process of being created.",
                "Ensure that the default database is already present in the metastore.",
                "Run a command that triggers the creation of the default database again."
            ],
            "ExpectedBehavior": "The application should successfully instantiate the HiveMetaStoreClient and allow database operations without errors.",
            "ObservedBehavior": "The application throws a HiveException indicating that it is unable to instantiate the HiveMetaStoreClient due to a duplicate entry for the default database.",
            "AdditionalDetails": "The issue arises from the createDatabase method in the ObjectStore class, which attempts to create a database with the name 'default' that already exists in the metastore. This indicates a potential race condition or improper handling of database creation logic."
        }
    },
    {
        "filename": "HIVE-9570.json",
        "creation_time": "2015-02-03T23:30:09.000+0000",
        "bug_report": {
            "Title": "NullPointerException in SparkCompiler.setInputFormat",
            "Description": "A NullPointerException occurs in the SparkCompiler class when attempting to set the input format for a Spark task. This issue arises during the compilation of a Hive query, specifically when the query involves Spark tasks. The exception indicates that a null reference is being accessed, which suggests that the input task or its associated work may not be properly initialized.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:274)",
                "  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:253)",
                "  at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:222)",
                "  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10231)",
                "  at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "  at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "  at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1160)",
                "  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1039)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "  at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1019)",
                "  at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:993)",
                "  at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:136)",
                "  at org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_view(TestSparkCliDriver.java:120)",
                "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "  at java.lang.reflect.Method.invoke(Method.java:606)",
                "  at junit.framework.TestCase.runTest(TestCase.java:176)",
                "  at junit.framework.TestCase.runBare(TestCase.java:141)",
                "  at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "  at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "  at junit.framework.TestResult.run(TestResult.java:125)",
                "  at junit.framework.TestCase.run(TestCase.java:129)",
                "  at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "  at junit.framework.TestSuite.run(TestSuite.java:250)",
                "  at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that involves Spark tasks.",
                "Ensure that the query is structured in a way that requires the SparkCompiler to set the input format.",
                "Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The input format for the Spark task should be set without any exceptions, allowing the query to compile and execute successfully.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a null reference was accessed in the SparkCompiler's setInputFormat method.",
            "AdditionalDetails": "The issue likely stems from the input task or its associated work not being properly initialized before being passed to the setInputFormat method. Further investigation is needed to determine the exact conditions under which this occurs."
        }
    },
    {
        "filename": "HIVE-1678.json",
        "creation_time": "2010-10-01T05:41:21.000+0000",
        "bug_report": {
            "Title": "NullPointerException in MapJoinOperator during row processing",
            "Description": "A NullPointerException is thrown in the MapJoinOperator's processOp method when attempting to process a row. This issue arises when the row being processed is null, leading to a failure in the evaluation of expressions.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that involves a MapJoin operation.",
                "2. Ensure that the input data contains null rows or that the deserialization process results in a null row.",
                "3. Observe the logs for a NullPointerException in the MapJoinOperator."
            ],
            "ExpectedBehavior": "The MapJoinOperator should handle null rows gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when processing a null row, causing the query to fail.",
            "AdditionalDetails": "The issue likely originates from the 'processOp' method in the MapJoinOperator, where the evaluation of expressions is attempted on a null row. The method does not currently check for null values before proceeding with evaluations."
        }
    },
    {
        "filename": "HIVE-11820.json",
        "creation_time": "2015-09-14T22:20:28.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException when using Skip CRC option in DistCp",
            "Description": "An IllegalArgumentException is thrown when attempting to set the Skip CRC option in the DistCpOptions without the appropriate update options being set. This occurs in the runDistCp method of the Hadoop23Shims class.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Skip CRC is valid only with update options",
                "at org.apache.hadoop.tools.DistCpOptions.validate(DistCpOptions.java:556)",
                "at org.apache.hadoop.tools.DistCpOptions.setSkipCRC(DistCpOptions.java:311)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1147)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Call the runDistCp method with a source and destination path, and set the Skip CRC option to true without setting the update options."
            ],
            "ExpectedBehavior": "The DistCp operation should execute successfully without throwing an exception when the Skip CRC option is set correctly.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that the Skip CRC option is valid only with update options.",
            "AdditionalDetails": "The issue arises from the DistCpOptions.validate method, which checks if the Skip CRC option is set without the necessary update options. The runDistCp method in Hadoop23Shims sets the Skip CRC option without ensuring that the update options are also set."
        }
    },
    {
        "filename": "HIVE-17274.json",
        "creation_time": "2017-08-08T22:20:33.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to Relative Path in Absolute URI",
            "Description": "An IllegalArgumentException is thrown when attempting to create a Path object with a relative path in an absolute URI. This occurs during the process of writing data to a temporary file in the Hive framework, specifically when the RowContainer attempts to spill data to a temporary file.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:205)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:171)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:93)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:94)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:404)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926)",
                "at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1137)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:530)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(Utilities.java:1643)",
                "at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.getHiveRecordWriter(HiveSequenceFileOutputFormat.java:64)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:243)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter(RowContainer.java:538)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:299)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.copyToDFSDirecory(RowContainer.java:407)",
                "at org.apache.hadoop.hive.ql.exec.SkewJoinHandler.endGroup(SkewJoinHandler.java:185)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:249)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:195)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)",
                "Caused by: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at java.net.URI.checkPath(URI.java:1823)",
                "at java.net.URI.<init>(URI.java:745)",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:202)"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves writing data using the RowContainer.",
                "2. Ensure that the job attempts to spill data to a temporary file.",
                "3. Observe the logs for the IllegalArgumentException."
            ],
            "ExpectedBehavior": "The system should successfully create a Path object for the temporary file without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown due to a relative path being used in an absolute URI when creating a Path object.",
            "AdditionalDetails": "The issue seems to stem from the construction of the temporary file name in the RowContainer class, specifically in the setupWriter() method. The temporary file name generated may not be properly formatted, leading to the relative path error."
        }
    },
    {
        "filename": "HIVE-12522.json",
        "creation_time": "2015-11-25T19:48:44.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to Wrong FileSystem Path",
            "Description": "An IllegalArgumentException is thrown when the application attempts to access a file in a different filesystem than expected. The application expects a path in the HDFS filesystem but receives a path in the WASB filesystem instead.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Wrong FS: wasb://chaoyiteztest@chaoyiteztest.blob.core.windows.net/hive/scratch/chaoyitest/c888f405-3c98-46b1-bf39-e57f067dfe4c/hive_2015-11-13_10-16-10_216_8161037519951665173-1/_tmp.-ext-10000, expected: hdfs://headnodehost:9000",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1136)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1423)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:579)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1083)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:156)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1606)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1367)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1179)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:733)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Configure the application to use HDFS as the default filesystem.",
                "Attempt to access a file located in a WASB (Windows Azure Storage Blob) path.",
                "Observe the IllegalArgumentException thrown due to the filesystem mismatch."
            ],
            "ExpectedBehavior": "The application should successfully access files in the configured filesystem without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a mismatch between the expected HDFS filesystem and the provided WASB path.",
            "AdditionalDetails": "The issue arises in the method 'createVertex' within the 'DagUtils' class when it attempts to check the existence of a file in the filesystem. The application is likely misconfigured to point to the wrong filesystem."
        }
    },
    {
        "filename": "HIVE-16845.json",
        "creation_time": "2017-06-07T17:07:55.000+0000",
        "bug_report": {
            "Title": "NullPointerException in ConditionalTask Execution",
            "Description": "A NullPointerException occurs during the execution of a ConditionalTask in Hive, leading to a HiveSQLException. This issue arises when the system attempts to generate actual tasks without proper initialization of required objects.",
            "StackTrace": [
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)",
                "at org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1201)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)"
            ],
            "StepsToReproduce": [
                "Execute a query that involves a ConditionalTask in Hive.",
                "Ensure that the necessary configurations and inputs for the ConditionalTask are set.",
                "Observe the logs for the NullPointerException and HiveSQLException."
            ],
            "ExpectedBehavior": "The ConditionalTask should execute successfully without throwing a NullPointerException, and the query should return the expected results.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the ConditionalTask, resulting in a HiveSQLException and failure of the query execution.",
            "AdditionalDetails": "The issue seems to stem from the method 'generateActualTasks' in the 'ConditionalResolverMergeFiles' class, which may not be properly handling null values or uninitialized objects."
        }
    },
    {
        "filename": "HIVE-9655.json",
        "creation_time": "2015-02-11T20:58:13.000+0000",
        "bug_report": {
            "Title": "Hive Runtime Error: Cannot Find Field _col2",
            "Description": "A runtime error occurs in the Hive processing pipeline when attempting to access a non-existent field '_col2' in the input data. This issue arises during the execution of the MapOperator, specifically when processing a row that is expected to contain this field.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"c1\":1,\"c2\":\"one\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 10 more",
                "Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)",
                "at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)"
            ],
            "StepsToReproduce": [
                "Run a Hive query that processes a row with the structure {\"c1\":1,\"c2\":\"one\"}.",
                "Ensure that the query attempts to access a field named '_col2' which does not exist in the input data."
            ],
            "ExpectedBehavior": "The Hive query should process the input row without errors, accessing only the fields that exist in the data.",
            "ObservedBehavior": "A HiveException is thrown indicating that the field '_col2' cannot be found in the input data, leading to a runtime error.",
            "AdditionalDetails": "The error suggests that the query or the schema expects a field '_col2' which is not present in the input data. This could be due to a mismatch between the expected schema and the actual data being processed."
        }
    },
    {
        "filename": "HIVE-11441.json",
        "creation_time": "2015-08-03T17:42:24.000+0000",
        "bug_report": {
            "Title": "Connection Refused Error When Fetching Table in Hive",
            "Description": "A SemanticException is thrown when attempting to fetch the table 'testloc' in Hive due to a connection refusal error. This indicates that the Hive service is unable to connect to the specified host and port, which is critical for fetching table metadata.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1323)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(DDLSemanticAnalyzer.java:1452)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:295)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1069)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1131)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1072)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)"
            ],
            "StepsToReproduce": [
                "1. Attempt to execute a Hive command that fetches the table 'testloc'.",
                "2. Ensure that the Hive service is running on the expected host and port.",
                "3. Observe the error message indicating a connection refusal."
            ],
            "ExpectedBehavior": "The table 'testloc' should be fetched successfully without any connection errors.",
            "ObservedBehavior": "A SemanticException is thrown indicating that the connection to the Hive service was refused, preventing the fetching of the table.",
            "AdditionalDetails": "The error suggests that the Hive service may not be running or is not accessible at the specified host and port (hdpsecb02.secb.hwxsup.com:8020). Further investigation into the service status and network configuration is required."
        }
    },
    {
        "filename": "HIVE-10801.json",
        "creation_time": "2015-05-22T19:43:23.000+0000",
        "bug_report": {
            "Title": "NullPointerException when dropping a table in Hive",
            "Description": "A NullPointerException occurs when attempting to drop a table using the HiveMetaStore. The error is triggered during the execution of the drop_table_with_environment_context method, specifically when checking if the path is encrypted.",
            "StackTrace": [
                "2015-05-21 11:53:06,134 ERROR [HiveServer2-Background-Pool: Thread-197]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(155)) - MetaException(message:java.lang.NullPointerException)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy8.dropTable(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)",
                "... 40 more"
            ],
            "StepsToReproduce": [
                "Attempt to drop a table using the HiveMetaStoreClient.",
                "Ensure that the table being dropped has an associated HDFS path.",
                "Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The table should be dropped successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that a required object is null during the drop operation.",
            "AdditionalDetails": "The issue seems to stem from the isPathEncrypted method in Hadoop23Shims, which is called during the drop_table_core method. This suggests that the path being checked for encryption may not be properly initialized."
        }
    },
    {
        "filename": "HIVE-9141.json",
        "creation_time": "2014-12-17T07:23:05.000+0000",
        "bug_report": {
            "Title": "ClassCastException in GenTezWork Processing",
            "Description": "A ClassCastException occurs when attempting to cast an instance of MapWork to ReduceWork in the GenTezWork class. This issue arises during the processing of the query plan in Hive, specifically when generating the task tree for Tez execution.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:361)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez2(TestMiniTezCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "StepsToReproduce": [
                "Execute a Hive query that involves a UNION operation with Tez as the execution engine.",
                "Ensure that the query plan includes both MapWork and ReduceWork tasks.",
                "Observe the execution to trigger the ClassCastException."
            ],
            "ExpectedBehavior": "The query should execute successfully, generating the appropriate task tree for both Map and Reduce phases without any casting errors.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating that a MapWork instance cannot be cast to ReduceWork, leading to a failure in query execution.",
            "AdditionalDetails": "The issue seems to stem from the way the GenTezWork class processes the query plan. The method 'process' in GenTezWork is expected to handle both MapWork and ReduceWork, but it appears that the logic does not correctly differentiate between the two types of work, leading to the casting error."
        }
    },
    {
        "filename": "HIVE-10010.json",
        "creation_time": "2015-03-18T17:48:46.000+0000",
        "bug_report": {
            "Title": "NullPointerException in StorageDescriptor Initialization",
            "Description": "A NullPointerException occurs when attempting to initialize a StorageDescriptor object within the Hive metastore. This issue arises during the execution of an alter table command, specifically when copying a table's metadata.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)",
                "    at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)",
                "    at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)",
                "    at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)",
                "    at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)",
                "    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)",
                "    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)",
                "    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)",
                "    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "    at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)"
            ],
            "StepsToReproduce": [
                "1. Execute an alter table command on a Hive table that has a StorageDescriptor with null values.",
                "2. Observe the exception thrown during the execution."
            ],
            "ExpectedBehavior": "The alter table command should execute successfully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the initialization of a StorageDescriptor, causing the alter table command to fail.",
            "AdditionalDetails": "The issue likely stems from the fact that the StorageDescriptor being copied may not have been properly initialized or may contain null fields that are not handled correctly in the constructor."
        }
    },
    {
        "filename": "HIVE-7763.json",
        "creation_time": "2014-08-18T09:35:09.000+0000",
        "bug_report": {
            "Title": "Map Operator Initialization Failure Due to Inconsistent Configuration and Input Path",
            "Description": "A RuntimeException occurs during the initialization of the Map operator in Hive when the configuration and input path are inconsistent. This issue arises in the SparkMapRecordHandler class, specifically in the init method, which is responsible for setting up the Map operator's children based on the provided configuration.",
            "StackTrace": [
                "java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:127)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:52)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:30)",
                "at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)",
                "at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)",
                "at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "at org.apache.spark.scheduler.Task.run(Task.java:54)",
                "at org.apache.spark.executor.Executor$TaskRunner.run(Executor.java:199)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:404)",
                "at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:93)",
                "... 16 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent"
            ],
            "StepsToReproduce": [
                "1. Configure a Hive job with an input path that does not match the expected configuration.",
                "2. Execute the job using Spark as the execution engine.",
                "3. Observe the exception thrown during the initialization of the Map operator."
            ],
            "ExpectedBehavior": "The Map operator should initialize successfully without throwing any exceptions, provided that the configuration and input path are consistent.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the Map operator initialization failed due to inconsistent configuration and input path.",
            "AdditionalDetails": "The issue is likely caused by the logic in the setChildren method of the MapOperator class, which checks for the consistency of the input path against the configuration. If no matching alias is found for the input path, a HiveException is thrown, leading to the RuntimeException observed in the stack trace."
        }
    },
    {
        "filename": "HIVE-12083.json",
        "creation_time": "2015-10-09T22:45:47.000+0000",
        "bug_report": {
            "Title": "TProtocolException: Required field 'colStats' is unset in AggrStats",
            "Description": "A TProtocolException is thrown when attempting to validate an AggrStats object that has a required field 'colStats' unset. This indicates that the object is not being populated correctly before being processed, leading to a failure in the Thrift protocol handling.",
            "StackTrace": [
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Invoke the ThriftHiveMetastore method that retrieves AggrStats.",
                "Ensure that the AggrStats object is created without setting the 'colStats' field.",
                "Attempt to validate or process the AggrStats object."
            ],
            "ExpectedBehavior": "The AggrStats object should be properly populated with all required fields, including 'colStats', before being processed, preventing any exceptions during validation.",
            "ObservedBehavior": "A TProtocolException is thrown indicating that the required field 'colStats' is unset, leading to a failure in processing the AggrStats object.",
            "AdditionalDetails": "The issue likely stems from the logic that populates the AggrStats object. It is crucial to ensure that all required fields are set before the object is validated or processed."
        }
    },
    {
        "filename": "HIVE-14784.json",
        "creation_time": "2016-09-17T02:00:28.000+0000",
        "bug_report": {
            "Title": "IOException when creating operation log file in Hive",
            "Description": "An IOException is thrown when the Hive service attempts to create an operation log file. The error indicates that the specified directory does not exist, leading to failure in logging operations.",
            "StackTrace": [
                "java.io.IOException: No such file or directory",
                "at java.io.UnixFileSystem.createFileExclusively(Native Method)",
                "at java.io.File.createNewFile(File.java:1012)",
                "at org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)",
                "at org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:385)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService$Processor$ExecuteStatement.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService$Processor$ExecuteStatement.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer$WorkerProcess.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Hive service.",
                "2. Execute a statement that requires logging.",
                "3. Ensure that the directory specified for operation logs does not exist.",
                "4. Observe the IOException thrown during the log file creation."
            ],
            "ExpectedBehavior": "The operation log file should be created successfully without any exceptions.",
            "ObservedBehavior": "An IOException is thrown indicating that the specified file or directory does not exist, preventing the creation of the operation log file.",
            "AdditionalDetails": "The issue arises in the `createOperationLog()` method of the `Operation` class, specifically when attempting to create a new file in a directory that is not present. The method checks if the log file exists and attempts to create it, but fails due to the missing directory."
        }
    }
]