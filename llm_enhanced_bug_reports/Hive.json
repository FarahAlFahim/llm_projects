[
    {
        "filename": "HIVE-10992.json",
        "creation_time": "2015-06-12T19:52:35.000+0000",
        "bug_report": {
            "BugID": "HIVE-10992",
            "Title": "WebHCat Should Not Create Delegation Tokens When Kerberos is Not Enabled",
            "Description": "The `TempletonControllerJob.run()` method incorrectly creates delegation tokens even when Kerberos is not enabled. This can lead to issues with long-running jobs submitted via WebHCat, as the tokens may be prematurely canceled, resulting in errors when attempting to find child jobs.",
            "StackTrace": [
                "2015-05-25 20:49:38,026 WARN [main] org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache",
                "2015-05-25 20:49:38,058 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child :",
                "java.lang.RuntimeException: Exception occurred while finding child jobs",
                " at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:204)",
                " at org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(WebHCatJTShim23.java:158)",
                " at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(LaunchMapper.java:156)",
                " at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(LaunchMapper.java:124)",
                " at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(LaunchMapper.java:261)",
                " at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)",
                " at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                " at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                " at java.security.AccessController.doPrivileged(Native Method)",
                " at javax.security.auth.Subject.doAs(Subject.java:415)",
                " at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                " at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.security.token.SecretManager$InvalidToken: token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache",
                " at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                " at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                " at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                " at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                " at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)",
                " at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:104)",
                " at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:250)",
                " at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                " at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                " at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)",
                " at java.lang.reflect.Method.invoke(Method.java:606)",
                " at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                " at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                " at com.sun.proxy.$Proxy26.getApplications(Unknown Source)",
                " at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:198) ... 11 more",
                "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache",
                " at org.apache.hadoop.ipc.Client.call(Client.java:1469)",
                " at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                " at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)",
                " at com.sun.proxy.$Proxy25.getApplications(Unknown Source)",
                " at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplications(ApplicationClientProtocolPBClientImpl.java:247) ... 19 more"
            ],
            "StepsToReproduce": [
                "Submit a long-running job via WebHCat without Kerberos enabled.",
                "Monitor the job for token creation and cancellation.",
                "Observe the errors related to token invalidation."
            ],
            "ExpectedBehavior": "The system should not create delegation tokens when Kerberos is not enabled, preventing any token-related errors during job execution.",
            "ObservedBehavior": "The system creates delegation tokens even when Kerberos is not enabled, leading to errors when the tokens are canceled prematurely.",
            "Resolution": "Fixed in version 1.2.1"
        }
    },
    {
        "filename": "HIVE-16450.json",
        "creation_time": "2017-04-14T13:59:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-16450",
            "Title": "Metastore Operations Not Retried for JDOException or NucleusException",
            "Description": "In the RetryingHMSHandler class, operations are expected to retry when the cause of a MetaException is either JDOException or NucleusException. However, in the ObjectStore, many instances throw a new MetaException without the cause, leading to missed retry opportunities. This issue was observed with a specific JDOException that should have triggered a retry but was ignored.",
            "StackTrace": [
                "2017-04-04 17:28:21,602 ERROR metastore.ObjectStore (ObjectStore.java:getMTableColumnStatistics(6555)) - Error retrieving statistics via jdo",
                "javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(ObjectStore.java:6546)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.access$1200(ObjectStore.java:171)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6606)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6595)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2633)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(ObjectStore.java:6594)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatistics(ObjectStore.java:6588)",
                "at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:103)",
                "at com.sun.proxy.$Proxy0.getTableColumnStatistics(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:787)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:247)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3809)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3779)",
                "at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy3.alter_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9617)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9601)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Trigger a metastore operation that results in a JDOException or NucleusException.",
                "Observe the behavior of the system when the exception occurs."
            ],
            "ExpectedBehavior": "The system should retry the metastore operation when a JDOException or NucleusException is encountered.",
            "ObservedBehavior": "The system does not retry the operation, leading to job failures.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6389.json",
        "creation_time": "2014-02-07T01:33:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-6389",
            "Title": "Null Map Lookups Fail in LazyBinaryColumnarSerDe RCFile Tables",
            "Description": "When querying RCFile tables that use LazyBinaryColumnarSerDe, lookups into map-columns fail if the value of the column is null. This results in a ClassCastException, causing the query to fail.",
            "StackTrace": [
                "2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":null,\"mymap\":null,\"isnull\":null}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)",
                "... 10 more"
            ],
            "StepsToReproduce": [
                "Create an RCFile table using LazyBinaryColumnarSerDe.",
                "Insert a row with a map-column where the map contains null values.",
                "Execute the query: select mymap['1024'] from mytable;"
            ],
            "ExpectedBehavior": "The query should return null when looking up a key in a map-column that contains null values.",
            "ObservedBehavior": "The query fails with a ClassCastException when attempting to look up a key in a map-column that contains null values.",
            "Resolution": "A patch is on the way to ensure that the LazyBinaryMapOI returns nulls if either the map or the lookup-key is null."
        }
    },
    {
        "filename": "HIVE-2372.json",
        "creation_time": "2011-08-12T09:07:34.000+0000",
        "bug_report": {
            "BugID": "HIVE-2372",
            "Title": "IOException: Argument list too long during reducer execution",
            "Description": "Executing a large query with a Perl reducer fails due to an IOException caused by exceeding the environment variable size limit in Linux. The issue arises when the mapred.input.dir variable becomes excessively large, leading to the error: 'java.io.IOException: error=7, Argument list too long'. This halts all work related to the query processing.",
            "StackTrace": [
                "2011-08-11 04:58:29,865 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: Executing [/usr/bin/perl, <reducer.pl>, <my_argument>]",
                "2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: tablename=null",
                "2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: partname=null",
                "2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: alias=null",
                "2011-08-11 04:58:29,935 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":129390185139228,\"reducesinkkey1\":\"00008AF10000000063CA6F\"},\"value\":{\"_col0\":\"00008AF10000000000063CA6F\",\"_col1\":\"2011-07-27 22:48:52\",\"_col2\":129390185139228,\"_col3\":2006,\"_col4\":4100,\"_col5\":\"10017388=6\",\"_col6\":1063,\"_col7\":\"NULL\",\"_col8\":\"address.com\",\"_col9\":\"NULL\",\"_col10\":\"NULL\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator",
                "at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)",
                "at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)",
                "... 7 more",
                "Caused by: java.io.IOException: Cannot run program \"/usr/bin/perl\": java.io.IOException: error=7, Argument list too long",
                "at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)",
                "at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)",
                "... 15 more",
                "Caused by: java.io.IOException: java.io.IOException: error=7, Argument list too long",
                "at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)",
                "at java.lang.ProcessImpl.start(ProcessImpl.java:65)",
                "at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)",
                "... 16 more"
            ],
            "StepsToReproduce": [
                "Execute a large query on a table with multiple 2-level partitions using a Perl reducer.",
                "Monitor the execution logs for any errors related to the reducer."
            ],
            "ExpectedBehavior": "The reducer should execute successfully without exceeding the environment variable limits.",
            "ObservedBehavior": "The reducer fails with an IOException indicating that the argument list is too long, halting the query processing.",
            "Resolution": "The issue was resolved by modifying the handling of environment variables in the ScriptOperator to avoid exceeding the limits."
        }
    },
    {
        "filename": "HIVE-2958.json",
        "creation_time": "2012-04-17T15:02:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-2958",
            "Title": "ClassCastException during GROUP BY operation in Hive with HBase",
            "Description": "When executing a GROUP BY query on an HBase-backed Hive table, a ClassCastException occurs, preventing the query from completing successfully. This issue is related to the handling of data types in Hive when interacting with HBase.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:548)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:143)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:270)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:264)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)",
                "... 9 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyIntObjectInspector.copyObject(LazyIntObjectInspector.java:43)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:239)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:150)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.deepCopyElements(KeyWrapperFactory.java:142)",
                "at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.copyKey(KeyWrapperFactory.java:119)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:750)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:722)",
                "... 18 more"
            ],
            "StepsToReproduce": [
                "Create an external table in Hive backed by HBase with the following schema: id int, scientific_name string, data_resource_id int.",
                "Insert sample data into the HBase table.",
                "Execute the following query: SELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id."
            ],
            "ExpectedBehavior": "The query should return the count of records grouped by data_resource_id without any errors.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating a type mismatch during processing.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-13392.json",
        "creation_time": "2016-03-30T22:32:50.000+0000",
        "bug_report": {
            "BugID": "HIVE-13392",
            "Title": "Speculative Execution Causes File Lease Issues in ACID Compactor",
            "Description": "The ACID Compactor in Hive is not handling speculative execution properly, leading to file lease conflicts. This occurs when speculative execution is enabled, causing multiple tasks to attempt to create the same file, resulting in an AlreadyBeingCreatedException.",
            "StackTrace": [
                "2016-02-08 22:56:38,256 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2451)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:397)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2151)"
            ],
            "StepsToReproduce": [
                "Enable speculative execution for mappers and reducers in the JobConf options.",
                "Run a job that utilizes the ACID Compactor.",
                "Observe the logs for warnings related to file lease issues."
            ],
            "ExpectedBehavior": "The ACID Compactor should handle speculative execution without causing file lease conflicts.",
            "ObservedBehavior": "The ACID Compactor throws an AlreadyBeingCreatedException due to multiple tasks attempting to create the same file.",
            "Resolution": "Short term: Disable speculative execution for this job. Longer term: Consider implementing a mechanism for each task to write to a directory with a unique UUID."
        }
    },
    {
        "filename": "HIVE-11301.json",
        "creation_time": "2015-07-18T00:41:40.000+0000",
        "bug_report": {
            "BugID": "HIVE-11301",
            "Title": "Thrift Metastore Disconnects When Retrieving Stats",
            "Description": "When attempting to retrieve aggregate statistics from the metastore, a Thrift error occurs, leading to a disconnect. This issue manifests as a long hang in the CLI while retrying the connection.",
            "StackTrace": [
                "2015-07-17 20:32:27,795 ERROR [pool-3-thread-150]: server.TThreadPoolServer (TThreadPoolServer.java:run(294)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-07-17 20:32:27,796 WARN  [pool-3-thread-150]: transport.TIOStreamTransport (TIOStreamTransport.java:close(112)) - Error closing output stream.",
                "java.net.SocketException: Socket closed",
                "at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)",
                "at java.net.SocketOutputStream.write(SocketOutputStream.java:153)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)",
                "at java.io.FilterOutputStream.close(FilterOutputStream.java:158)",
                "at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)",
                "at org.apache.thrift.transport.TSocket.close(TSocket.java:196)",
                "at org.apache.hadoop.hive.thrift.TFilterTransport.close(TFilterTransport.java:52)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-07-17 20:32:27,796 WARN  [main()]: metastore.RetryingMetaStoreClient (RetryingMetaStoreClient.java:invoke(187)) - MetaStoreClient lost connection. Attempting to reconnect.",
                "org.apache.thrift.transport.TTransportException",
                "at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_aggr_stats_for(ThriftHiveMetastore.java:3029)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_aggr_stats_for(ThriftHiveMetastore.java:3016)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2067)",
                "at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1991)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3124)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:252)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:137)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:125)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:111)"
            ],
            "StepsToReproduce": [
                "1. Connect to the Hive metastore.",
                "2. Attempt to retrieve aggregate statistics for a table with all partitions pruned.",
                "3. Observe the error and subsequent disconnect."
            ],
            "ExpectedBehavior": "The aggregate statistics should be retrieved without errors or disconnects.",
            "ObservedBehavior": "A Thrift error occurs, leading to a disconnect and the CLI hangs while retrying the connection.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11028.json",
        "creation_time": "2015-06-16T23:03:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-11028",
            "Title": "IndexOutOfBoundsException during self join in Tez",
            "Description": "When executing a self join on the table `tez_self_join1` and joining it with `tez_self_join2`, an IndexOutOfBoundsException occurs, causing the query to fail.",
            "StackTrace": [
                "2015-06-16 15:41:55,759 ERROR [main]: ql.Driver (SessionState.java:printError(979)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask.",
                "Vertex failed, vertexName=Reducer 3, vertexId=vertex_1434494327112_0002_4_04, diagnostics=[Task failed, taskId=task_1434494327112_0002_4_04_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.<init>(StandardStructObjectInspector.java:109)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:290)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:275)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:175)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:313)",
                "at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:71)",
                "at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:99)",
                "at org.apache.hadoop.hive.exec.Operator.initialize(Operator.java:362)",
                "at org.apache.hadoop.hive.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)",
                "at org.apache.hadoop.hive.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)",
                "... 13 more"
            ],
            "StepsToReproduce": [
                "Create table `tez_self_join1` with columns (id1, id2, id3).",
                "Insert values into `tez_self_join1`.",
                "Create table `tez_self_join2` with column (id1).",
                "Insert values into `tez_self_join2`.",
                "Execute the following query: `explain select s.id2, s.id3 from (select self1.id1, self1.id2, self1.id3 from tez_self_join1 self1 join tez_self_join1 self2 on self1.id2=self2.id3) s join tez_self_join2 on s.id1=tez_self_join2.id1 where s.id2='ab';`"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors.",
            "ObservedBehavior": "The query fails with an IndexOutOfBoundsException, indicating an issue with the join operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14380.json",
        "creation_time": "2016-07-29T00:14:58.000+0000",
        "bug_report": {
            "BugID": "HIVE-14380",
            "Title": "IAException when querying tables with remote HDFS paths due to encryption checks",
            "Description": "When querying tables that have their locations set to remote HDFS paths, an IAException is thrown indicating an inability to determine if the path is encrypted. This issue arises from the way the FileSystem instance is created in the SessionState class, which does not correctly fetch the FileSystem instance corresponding to the path being checked.",
            "StackTrace": [
                "2016-07-26 01:16:27,471 ERROR parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1867)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to determine if hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table, expected: hdfs://bar.ygrid.yahoo.com:8020",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2204)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStrongestEncryptedTablePath(SemanticAnalyzer.java:2274)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive table with a location pointing to a remote HDFS path.",
                "2. Attempt to query the table.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing an IAException related to encryption checks.",
            "ObservedBehavior": "An IAException is thrown indicating that the system is unable to determine if the specified HDFS path is encrypted.",
            "Resolution": "A fix is forthcoming. The code in SessionState needs to be modified to correctly fetch the FileSystem instance corresponding to the path being checked."
        }
    },
    {
        "filename": "HIVE-7799.json",
        "creation_time": "2014-08-20T09:45:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-7799",
            "Title": "NullPointerException in Hive on Spark during transformation",
            "Description": "A NullPointerException occurs in the Hive on Spark implementation when executing a transformation query. The issue seems to stem from the misuse of RowContainer, which is not allowed to be written to after a row has been read from it.",
            "StackTrace": [
                "2014-08-20 01:14:36,594 ERROR executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.next(HiveKVResultCache.java:113)",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:124)",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:82)",
                "    at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)",
                "    at scala.collection.Iterator$class.foreach(Iterator.scala:727)",
                "    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)",
                "    at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.java:65)",
                "    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "    at org.apache.spark.scheduler.Task.run(Task.scala:54)",
                "    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.java:199)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "    at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive on Spark environment.",
                "2. Execute the transformation query 'transform_ppr1.q'.",
                "3. Observe the logs for any exceptions."
            ],
            "ExpectedBehavior": "The transformation query should execute successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the transformation query.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6537.json",
        "creation_time": "2014-03-03T18:57:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-6537",
            "Title": "NullPointerException in HashTableLoader during MapJoin execution",
            "Description": "A NullPointerException occurs when loading a hashtable for MapJoin directly, leading to a failure in processing. The error seems to stem from uninitialized tables in the HashTableLoader class.",
            "StackTrace": [
                "2014-02-20 23:33:15,743 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.NullPointerException",
                "at java.util.Arrays.fill(Arrays.java:2685)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:81)",
                "... 15 more"
            ],
            "StepsToReproduce": [
                "1. Execute a MapJoin operation in Hive.",
                "2. Monitor the logs for any NullPointerException errors.",
                "3. Observe the stack trace for the error location."
            ],
            "ExpectedBehavior": "The MapJoin operation should complete successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the tables used in the operation are not initialized properly.",
            "Resolution": "The issue has been fixed in version 0.13.0, released on 2014-04-21."
        }
    },
    {
        "filename": "HIVE-13691.json",
        "creation_time": "2016-05-04T23:40:03.000+0000",
        "bug_report": {
            "BugID": "HIVE-13691",
            "Title": "Compactor Fails to Record Entry in Completed Compaction Queue",
            "Description": "When the compactor fails, it calls `CompactionTxnHandler.markedCleaned()`, but it should not do so if there is no record with `CQ_ID=0` found in `COMPACTION_QUEUE`. This leads to a situation where a failed compaction is not properly recorded, which can cause confusion and issues in tracking compaction status.",
            "StackTrace": [
                "2016-04-29 18:49:31,594 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(141)) - Caught exception while trying to determine if we should compact id:0,dbname:default,tableName:service_logs_v2,par\\ntName:ds=2016-04-21,state:^@,type:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking clean to avoid repeated failures, MetaException(message:Timeout when executing method: getTable)",
                "at org.apache.hadoop.hive.metastore.Deadline.newMetaException(Deadline.java:187)",
                "at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:177)",
                "at org.apache.hadoop.hive.metastore.Deadline.checkTimeout(Deadline.java:160)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToParts(ObjectStore.java:1839)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2255)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.access$300(ObjectStore.java:165)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2051)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getJdoResult(ObjectStore.java:2043)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2400)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal(ObjectStore.java:2043)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNames(ObjectStore.java:2037)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)",
                "at com.sun.proxy.$Proxy0.getPartitionsByNames(Unknown Source)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.resolvePartition(CompactorThread.java:111)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:129)",
                "Caused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable",
                "at org.apache.hadoop.hive.metastore.Deadline.check(Deadline.java:174)",
                "... 16 more",
                "2016-04-29 18:49:31,595 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(154)) - Initiator loop caught unexpected exception this time through the loop: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed(CompactionTxnHandler.java:861)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Initiator.run(Initiator.java:144)"
            ],
            "StepsToReproduce": [
                "1. Trigger a compaction on a table that has no entries in the COMPACTION_QUEUE.",
                "2. Monitor the logs for any errors related to compaction.",
                "3. Observe the behavior when the compactor fails."
            ],
            "ExpectedBehavior": "The compactor should not call `CompactionTxnHandler.markedCleaned()` if there is no record with `CQ_ID=0` in `COMPACTION_QUEUE`. A failed compaction should be recorded in the `completed_compaction_queue`.",
            "ObservedBehavior": "The compactor calls `CompactionTxnHandler.markedCleaned()` even when there is no record with `CQ_ID=0`, leading to a lack of proper logging for failed compactions.",
            "Resolution": "[Provide additional details about the resolution]"
        }
    },
    {
        "filename": "HIVE-17758.json",
        "creation_time": "2017-10-10T12:33:52.000+0000",
        "bug_report": {
            "BugID": "HIVE-17758",
            "Title": "Negative Timeout Value in HiveConf Causes IllegalArgumentException",
            "Description": "The introduction of retry logic in HIVE-16886 has led to a situation where the default value of the timeout is not properly loaded into the relevant field. This results in the client code using a default value of -1, which triggers an IllegalArgumentException when attempting to sleep for a negative duration.",
            "StackTrace": [
                "2017-10-10 11:22:37,638 ERROR [load-dynamic-partitions-12]: metastore.ObjectStore (ObjectStore.java:addNotificationEvent(7444)) - could not get lock for update",
                "java.lang.IllegalArgumentException: timeout value is negative",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:7407)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:7361)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7424)",
                "at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)"
            ],
            "StepsToReproduce": [
                "1. Configure Hive with a retry interval using the NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL.defaultLongVal setting.",
                "2. Set the value to -1.",
                "3. Trigger an operation that requires a lock update in the ObjectStore."
            ],
            "ExpectedBehavior": "The system should handle the retry logic without throwing an exception, and the timeout value should be non-negative.",
            "ObservedBehavior": "An IllegalArgumentException is thrown due to a negative timeout value, causing the operation to fail.",
            "Resolution": "A fix has been implemented to ensure that the default value is properly loaded and validated before use."
        }
    },
    {
        "filename": "HIVE-14898.json",
        "creation_time": "2016-10-06T00:02:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-14898",
            "Title": "HS2 Logs Callstack for Empty Auth Header Error",
            "Description": "When the authentication header is not sent by the client, HiveServer2 (HS2) logs an error with a call stack, which is unnecessary since a 401 response is expected in this scenario. This behavior leads to cluttered logs and does not provide useful information.",
            "StackTrace": [
                "2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(169)) - Failed to authenticate with hive/_HOST kerberos principal",
                "2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(104)) - Error: ",
                "org.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:170)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:83)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)",
                "at org.eclipse.jetty.servlet.ScopedHandler.handle(ScopedHandler.java:117)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)",
                "at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:952)",
                "at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:167)",
                "... 23 more",
                "Caused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(ThriftHttpServlet.java:311)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.access$100(ThriftHttpServlet.java:59)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:212)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "... 24 more"
            ],
            "StepsToReproduce": [
                "1. Ensure that the client (Knox) does not send the authentication header.",
                "2. Attempt to access a resource that requires authentication.",
                "3. Observe the logs generated by HiveServer2."
            ],
            "ExpectedBehavior": "HS2 should not log a call stack for the empty authentication header error; it should simply return a 401 response to the client.",
            "ObservedBehavior": "HS2 logs an error message along with a full call stack, which is unnecessary and clutters the logs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5546.json",
        "creation_time": "2013-10-15T15:06:59.000+0000",
        "bug_report": {
            "BugID": "HIVE-5546",
            "Title": "OutOfMemoryError in OrcInputFormat when includedColumnIds is empty",
            "Description": "A change in ORCInputFormat made by HIVE-4113 was reverted by HIVE-5391. The current implementation incorrectly assumes that all columns need to be read when includedColumnIds is an empty string, leading to an OutOfMemoryError.",
            "StackTrace": [
                "2013-10-15 10:49:49,996 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with ORC file format.",
                "2. Execute a query that results in an empty includedColumnIds list.",
                "3. Observe the system behavior."
            ],
            "ExpectedBehavior": "The system should handle the empty includedColumnIds gracefully without throwing an OutOfMemoryError.",
            "ObservedBehavior": "The system throws a java.lang.OutOfMemoryError when includedColumnIds is an empty list.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7557.json",
        "creation_time": "2014-07-30T19:25:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-7557",
            "Title": "Vectorization Failure in dynpart_sort_opt_vectorization.q under Tez",
            "Description": "The query 'dynpart_sort_opt_vectorization.q' fails when the reduce operation is vectorized, leading to a runtime exception. This issue is related to HIVE-7029.",
            "StackTrace": [
                "Container released by application, AttemptID:attempt_1406747677386_0003_2_00_000000_2",
                "Error: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:394)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)",
                "Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:394)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.serialize(VectorizedOrcSerde.java:75)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcSerde.serializeVector(OrcSerde.java:148)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.processOp(VectorFileSinkOperator.java:79)",
                "at org.apache.hadoop.hive.exec.Operator.forward(Operator.java:800)",
                "at org.apache.hadoop.hive.exec.vector.VectorExtractOperator.processOp(VectorExtractOperator.java:99)",
                "at org.apache.hadoop.hive.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:470)"
            ],
            "StepsToReproduce": [
                "1. Disable vectorization for the query 'dynpart_sort_opt_vectorization.q'.",
                "2. Execute the query with vectorization enabled.",
                "3. Observe the runtime exception in the logs."
            ],
            "ExpectedBehavior": "The query should execute successfully without any runtime exceptions when vectorization is enabled.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating a type mismatch between DoubleColumnVector and LongColumnVector.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-1712.json",
        "creation_time": "2010-10-14T17:17:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-1712",
            "Title": "NullPointerException when migrating metadata from Derby to MySQL",
            "Description": "During the migration of metadata from Derby to MySQL, a NullPointerException is thrown when executing a Hive query that previously worked in Derby. This issue occurs after exporting Derby data to CSV and loading it into MySQL.",
            "StackTrace": [
                "2010-10-16 08:57:30,140 ERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException",
                "at java.util.Hashtable.put(Hashtable.java:394)",
                "at java.util.Hashtable.putAll(Hashtable.java:466)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)",
                "at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "Export data from Derby to CSV.",
                "Load the exported CSV data into MySQL.",
                "Run a Hive query that accesses the migrated data."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the Hive query.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12608.json",
        "creation_time": "2015-12-07T21:26:01.000+0000",
        "bug_report": {
            "BugID": "HIVE-12608",
            "Title": "Parquet Schema Evolution Fails When Dropping Column from array<struct<>>",
            "Description": "When attempting to drop a column from an array of structs in a Parquet table, an exception is thrown indicating that the field cannot be found. This issue occurs during schema evolution operations.",
            "StackTrace": [
                "2015-12-07 11:47:28,503 ERROR [main]: CliDriver (SessionState.java:printError(921)) - Failed with exception java.io.IOException:java.lang.RuntimeException: cannot find field c2 in [c1]",
                "java.io.IOException: java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1029)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1003)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:139)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_type_promotion(TestCliDriver.java:123)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getStructFieldTypeInfo(HiveStructConverter.java:130)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getFieldTypeIgnoreCase(HiveStructConverter.java:103)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.init(HiveStructConverter.java:90)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:67)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.<init>(HiveStructConverter.java:59)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:63)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveGroupConverter.getConverterFromDescription(HiveGroupConverter.java:75)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter$ElementConverter.<init>(HiveCollectionConverter.java:141)",
                "at org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter.<init>(HiveCollectionConverter.java:52)"
            ],
            "StepsToReproduce": [
                "Create a table with an array of structs using the following SQL: CREATE TABLE arrays_of_struct_to_map (locations1 array<struct<c1:int,c2:int>>, locations2 array<struct<f1:int,f2:int,f3:int>>) STORED AS PARQUET;",
                "Insert data into the table: INSERT INTO TABLE arrays_of_struct_to_map select array(named_struct('c1',1,'c2',2)), array(named_struct('f1',77,'f2',88,'f3',99)) FROM parquet_type_promotion LIMIT 1;",
                "Select data from the table: SELECT * FROM arrays_of_struct_to_map;",
                "Attempt to drop a column from the array of structs: ALTER TABLE arrays_of_struct_to_map REPLACE COLUMNS (locations1 array<struct<c1:int>>, locations2 array<struct<f2:int>>);",
                "Select data again: SELECT * FROM arrays_of_struct_to_map;"
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing an exception, and the resulting table should reflect the dropped columns.",
            "ObservedBehavior": "An exception is thrown indicating that the field 'c2' cannot be found in the resulting structure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-17774.json",
        "creation_time": "2017-10-11T20:02:01.000+0000",
        "bug_report": {
            "BugID": "HIVE-17774",
            "Title": "Compaction Job Fails When No Splits Are Available",
            "Description": "The compaction job fails when there are no splits available, leading to a FileNotFoundException. This issue occurs during the execution of a minor compaction job.",
            "StackTrace": [
                "java.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit a minor compaction job with no available splits.",
                "2. Monitor the job execution."
            ],
            "ExpectedBehavior": "The compaction job should handle the case of zero splits gracefully without failing.",
            "ObservedBehavior": "The compaction job fails with a FileNotFoundException due to the absence of splits.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14564.json",
        "creation_time": "2016-08-18T00:11:34.000+0000",
        "bug_report": {
            "BugID": "HIVE-14564",
            "Title": "ArrayIndexOutOfBoundsException due to Column Pruning in SelectOperator",
            "Description": "The Column Pruning feature generates out-of-order columns in the SelectOperator, leading to an ArrayIndexOutOfBoundsException during processing. This issue arises when the serialization and deserialization of data do not match due to different column orders used in previous MapReduce jobs.",
            "StackTrace": [
                "2016-07-26 21:49:24,390 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)",
                "... 9 more",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at java.lang.System.arraycopy(Native Method)",
                "at org.apache.hadoop.io.Text.set(Text.java:225)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(LazyBinaryString.java:48)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)",
                "at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:94)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(ReduceSinkOperator.java:550)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:377)",
                "... 13 more"
            ],
            "StepsToReproduce": [
                "1. Execute a MapReduce job that uses LazyBinarySerDe for serialization.",
                "2. Run a subsequent MapReduce job that attempts to deserialize the output of the first job using a different column order.",
                "3. Observe the logs for ArrayIndexOutOfBoundsException."
            ],
            "ExpectedBehavior": "The SelectOperator should process rows without throwing an ArrayIndexOutOfBoundsException, regardless of the column order used in previous jobs.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown during the processing of rows due to mismatched serialization and deserialization column orders.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-3651.json",
        "creation_time": "2012-11-01T23:31:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-3651",
            "Title": "BucketMapJoin Test Failure with Hadoop 0.23",
            "Description": "The Hive log shows an error in the MapReduce job related to the BucketMapJoin test case. The job fails due to a missing file in the specified directory.",
            "StackTrace": [
                "2012-11-01 15:51:20,253 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(479)) - job_local_0001",
                "java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)",
                "Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:679)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop 0.23 environment.",
                "2. Run the BucketMapJoin test case located in the Hive test suite.",
                "3. Observe the logs for any errors related to missing files."
            ],
            "ExpectedBehavior": "The BucketMapJoin test case should complete successfully without any errors.",
            "ObservedBehavior": "The test case fails with a warning indicating that a required file is missing, leading to a HiveException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5199.json",
        "creation_time": "2013-09-03T20:40:29.000+0000",
        "bug_report": {
            "BugID": "HIVE-5199",
            "Title": "ClassCastException in FetchOperator due to incompatible ObjectInspector types",
            "Description": "The issue occurs because of changes introduced in HIVE-3833. When using a partitioned table with different custom SerDes for the partition and the table, a ClassCastException is thrown. Specifically, the table-level SerDe (customSerDe1) has a settable data type, while the partition-level SerDe (customSerDe2) has a non-settable data type. The current implementation fails to convert nested complex data types that extend nonSettableObjectInspector to a settableObjectInspector type, leading to a ClassCastException when attempting to typecast the non-settable object inspector to a settable one.",
            "StackTrace": [
                "2013-08-28 17:57:25,307 ERROR CliDriver (SessionState.java:printError(432)) - Failed with exception java.io.IOException:java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "java.io.IOException: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1412)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:160)",
                "Caused by: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:144)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:307)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:138)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:406)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)"
            ],
            "StepsToReproduce": [
                "Create a partitioned table with different custom SerDes for the partition and the table.",
                "Ensure the table-level SerDe has a settable data type and the partition-level SerDe has a non-settable data type.",
                "Execute a query that involves fetching data from the partitioned table."
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing a ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown when attempting to fetch data from the partitioned table.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-17368.json",
        "creation_time": "2017-08-22T01:27:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-17368",
            "Title": "DBTokenStore Fails to Connect in Kerberos Enabled Remote HMS Environment",
            "Description": "In setups where HMS is running as a remote process secured using Kerberos, and when DBTokenStore is configured as the token store, the HS2 Thrift API call GetDelegationToken fails with an exception. HS2 cannot invoke HMS APIs needed to add/remove/renew tokens from the DB because the user issuing GetDelegationToken may not be Kerberos enabled. For example, when Oozie submits a job on behalf of user 'Joe', it uses Oozie's principal to create a proxy UGI with Hive, which can establish a transport authenticated using Kerberos. However, when Oozie issues a GetDelegationToken with 'Joe' as the owner and 'oozie' as the renewer, the API call fails to instantiate a HMSClient due to the inability to establish transport using Kerberos, as user 'Joe' is not Kerberos enabled.",
            "StackTrace": [
                "2017-08-21T18:07:19,644 ERROR [HiveServer2-Handler-Pool: Thread-61] transport.TSaslTransport: SASL negotiation failure",
                "javax.security.sasl.SaslException: GSS initiate failed",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_121]",
                "at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:488) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:255) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_121]",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_121]",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1699) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) [hive-metastore-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3595) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3647) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3627) [hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]",
                "at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnTokenStore(DBTokenStore.java:157) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.DBTokenStore.addToken(DBTokenStore.java:74) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:142) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.createPassword(TokenStoreDelegationTokenSecretManager.java:56) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.security.token.Token.<init>(Token.java:59) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.getDelegationToken(DelegationTokenSecretManager.java:109) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:123) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager$1.run(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationToken(HiveDelegationTokenManager.java:119) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.getDelegationTokenWithService(HiveDelegationTokenManager.java:130) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.auth.HiveAuthFactory.getDelegationToken(HiveAuthFactory.java:261) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionImplwithUGI.getDelegationToken(HiveSessionImplwithUGI.java:174) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]",
                "at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_121]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_121]",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) [hadoop-common-2.7.2.jar:?]",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at com.sun.proxy.$Proxy36.getDelegationToken(Unknown Source) [?:?]",
                "at org.apache.hive.service.cli.CLIService.getDelegationToken(CLIService.java:589) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.GetDelegationToken(ThriftCLIService.java:254) [hive-service-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1737) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetDelegationToken.getResult(TCLIService.java:1722) [hive-service-rpc-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [libthrift-0.9.3.jar:0.9.3]",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:621) [hive-shims-common-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [libthrift-0.9.3.jar:0.9.3]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_121]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_121]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]",
                "Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) ~[?:1.8.0_121]",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) ~[?:1.8.0_121]",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) ~[?:1.8.0_121]",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) ~[?:1.8.0_121]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) ~[?:1.8.0_121]",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_121]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_121]",
                "... 65 more"
            ],
            "StepsToReproduce": [
                "1. Set up HMS as a remote process secured with Kerberos.",
                "2. Configure DBTokenStore as the token store.",
                "3. Submit a job using Oozie on behalf of a user who is not Kerberos enabled.",
                "4. Observe the logs for the GetDelegationToken API call."
            ],
            "ExpectedBehavior": "HS2 should successfully invoke HMS APIs to add/remove/renew tokens without any exceptions.",
            "ObservedBehavior": "HS2 fails to invoke HMS APIs, resulting in a SASL negotiation failure and a GSSException due to invalid credentials.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-4233.json",
        "creation_time": "2013-03-26T13:02:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-4233",
            "Title": "Kerberos Authentication Failure After 7 Days of HiveServer2 Running",
            "Description": "When HiveServer2 has been running for more than 7 days, attempts to connect using the Beeline shell result in failures due to Kerberos authentication issues. The logs indicate a failure to instantiate the HiveMetaStoreClient, which is likely caused by an expired Kerberos ticket.",
            "StackTrace": [
                "2013-03-26 11:55:20,932 ERROR hive.ql.metadata.Hive: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1084)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:51)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:61)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2140)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2151)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(Hive.java:2275)",
                "at org.apache.hive.service.cli.CLIService.getDelegationTokenFromMetaStore(CLIService.java:358)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:127)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1073)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1058)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:565)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedConstructorAccessor52.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1082)",
                "... 16 more",
                "Caused by: java.lang.IllegalStateException: This ticket is no longer valid",
                "at javax.security.auth.kerberos.KerberosTicket.toString(KerberosTicket.java:601)",
                "at java.lang.String.valueOf(String.java:2826)",
                "at java.lang.StringBuilder.append(StringBuilder.java:115)",
                "at sun.security.jgss.krb5.SubjectComber.findAux(SubjectComber.java:120)",
                "at sun.security.jgss.krb5.SubjectComber.find(SubjectComber.java:41)",
                "at sun.security.jgss.krb5.Krb5Util.getTicket(Krb5Util.java:130)",
                "at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:328)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.kerberos.KerberosTicket.getTgt(KerberosTicket.java:325)",
                "at javax.security.auth.kerberos.KerberosTicket.getInstance(KerberosTicket.java:128)",
                "at javax.security.auth.kerberos.KerberosTicket.getMechanismContext(KerberosTicket.java:172)",
                "at javax.security.auth.kerberos.KerberosTicket.initSecContext(KerberosTicket.java:195)",
                "at javax.security.auth.kerberos.KerberosTicket.evaluateChallenge(KerberosTicket.java:175)",
                "at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)",
                "at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)",
                "at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:277)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:163)",
                "... 20 more"
            ],
            "StepsToReproduce": [
                "Start HiveServer2 and let it run for more than 7 days.",
                "Use the Beeline shell to connect to the HiveServer2 instance.",
                "Attempt to perform any operation."
            ],
            "ExpectedBehavior": "The Beeline shell should connect successfully and allow operations to be performed without authentication errors.",
            "ObservedBehavior": "The connection fails with a Kerberos authentication error, indicating that the ticket is no longer valid.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-14303.json",
        "creation_time": "2016-07-21T03:16:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-14303",
            "Title": "NPE in CommonJoinOperator.checkAndGenObject when ExecReducer.close is called multiple times",
            "Description": "The method CommonJoinOperator.checkAndGenObject should return directly after CommonJoinOperator.closeOp is called to avoid a NullPointerException (NPE) when ExecReducer.close is invoked multiple times. The NPE obscures the original exception that occurs during the first call to ExecReducer.close.",
            "StackTrace": [
                "Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: null",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)",
                "at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)",
                "... 8 more"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive job that involves a CommonJoinOperator.",
                "2. Ensure that the ExecReducer.close method is called multiple times during the job execution.",
                "3. Observe the logs for any NullPointerException."
            ],
            "ExpectedBehavior": "The CommonJoinOperator.checkAndGenObject method should handle multiple calls to ExecReducer.close without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when ExecReducer.close is called multiple times, obscuring the original exception.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-19248.json",
        "creation_time": "2018-04-19T17:45:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-19248",
            "Title": "REPL LOAD Fails to Copy Files from Source CM Path Without Error Notification",
            "Description": "Hive replication uses Hadoop distcp to copy files from the primary to the replica warehouse. If the HDFS block size differs across clusters, it causes file copy failures. The REPL LOAD command returns success even if distcp jobs fail, and CopyUtils.doCopyRetry does not throw an error if the copy fails after maximum attempts. This leads to confusion and potential data integrity issues.",
            "StackTrace": [
                "2018-04-09 14:32:06,690 ERROR [main] org.apache.hadoop.tools.mapred.CopyMapper: Failure in copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "java.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:299)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:266)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)",
                "Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:296)",
                "... 10 more",
                "Caused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:212)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)",
                "at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "1. Set up two Hadoop clusters with different HDFS block sizes.",
                "2. Execute the REPL LOAD command to copy files from the primary cluster to the replica cluster.",
                "3. Observe the output and check for any errors."
            ],
            "ExpectedBehavior": "The REPL LOAD command should fail and notify the user if the file copy fails due to differing HDFS block sizes.",
            "ObservedBehavior": "The REPL LOAD command returns success even if the distcp jobs fail, leading to confusion.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7167.json",
        "creation_time": "2014-06-02T18:13:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-7167",
            "Title": "Hive Metastore Fails to Start with SQLServerException",
            "Description": "When hiveserver2 uses an embedded metastore and hiveserver uses a remote metastore, an exception occurs if both are started simultaneously. The metastore service status is running, but launching the Hive CLI results in a connection error.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:347)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1413)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2444)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2456)",
                "at org.apache.hadoop.hive.session.SessionState.start(SessionState.java:341)",
                "... 7 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1411)",
                "... 12 more",
                "Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused: connect",
                "at org.apache.thrift.transport.TSocket.open(TSocket.java:185)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:336)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:214)",
                "... 17 more",
                "Caused by: java.net.ConnectException: Connection refused: connect",
                "at java.net.TwoStacksPlainSocketImpl.socketConnect(Native Method)",
                "at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)",
                "at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)",
                "at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)",
                "at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:157)",
                "at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)",
                "at java.net.Socket.connect(Socket.java:579)",
                "at org.apache.thrift.transport.TSocket.open(TSocket.java:180)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "Start hiveserver2 with an embedded metastore.",
                "Start hiveserver with a remote metastore.",
                "Launch the Hive CLI using the command: hive.cmd"
            ],
            "ExpectedBehavior": "The Hive CLI should connect to the metastore without errors.",
            "ObservedBehavior": "The Hive CLI fails to connect to the metastore, resulting in a SQLServerException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-12360.json",
        "creation_time": "2015-11-06T18:04:00.000+0000",
        "bug_report": {
            "BugID": "HIVE-12360",
            "Title": "IOException on Predicate Pushdown with Uncompressed ORC Files",
            "Description": "Reading from an ORC file fails in HDP-2.3.2 when pushing down predicates, resulting in an IOException. This issue is similar to HIVE-9471, but occurs despite the fact that HDP-2.3.2 claims to incorporate fixes from HIVE-9471 and HIVE-10303. The failure only occurs when 'hive.optimize.index.filter' is set to true and the ORC file is uncompressed.",
            "StackTrace": [
                "2015-11-06 09:48:11,873 ERROR [main]: CliDriver (SessionState.java:printError(960)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "java.io.IOException: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "at org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.seek(InStream.java:139)",
                "at org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.read(InStream.java:87)",
                "at java.io.InputStream.read(InputStream.java:102)",
                "at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)",
                "at com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7429)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7393)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7482)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7477)",
                "at org.apache.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200)",
                "at org.apache.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)",
                "at org.apache.protobuf.AbstractParser.parseFrom(AbstractParser.java:223)",
                "at org.apache.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.parseFrom(OrcProto.java:7593)",
                "at org.apache.hadoop.hive.ql.io.orc.MetadataReader.readRowIndex(MetadataReader.java:88)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1166)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1151)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:750)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)",
                "at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1235)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1117)",
                "at org.apache.hadoop.hive.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)",
                "at org.apache.hadoop.hive.exec.FetchOperator.getRecordReader(FetchOperator.java:324)",
                "at org.apache.hadoop.hive.exec.FetchOperator.getNextRow(FetchOperator.java:446)",
                "... 15 more"
            ],
            "StepsToReproduce": [
                "Set up an environment with Oracle Linux 6.4 and HDP 2.3.2.0-2950.",
                "Create an uncompressed ORC file.",
                "Enable the 'hive.optimize.index.filter' setting.",
                "Attempt to read from the ORC file using a predicate pushdown."
            ],
            "ExpectedBehavior": "The ORC file should be read successfully without any exceptions.",
            "ObservedBehavior": "An IOException occurs indicating that the seek index is outside of the data.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13160.json",
        "creation_time": "2016-02-26T00:02:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-13160",
            "Title": "HS2 Fails to Load UDFs on Startup When HMS is Unavailable",
            "Description": "When HiveServer2 (HS2) starts up and the Hive Metastore (HMS) is not ready, HS2 fails to load User Defined Functions (UDFs). This results in HS2 being in a servicing state without any available functions, which is not the desired behavior. The functions should either be initialized when HS2 starts or loaded when each Hive session is created, potentially using a cached function list for performance.",
            "StackTrace": [
                "2016-02-18 14:58:57,994 WARN  hive.ql.metadata.Hive: [main]: Failed to register all functions.",
                "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)",
                "at org.apache.hive.service.server.HiveServer2.access$400(HiveServer2.java:69)",
                "at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:545)"
            ],
            "StepsToReproduce": [
                "Start HiveServer2 (HS2) while the Hive Metastore (HMS) is not available.",
                "Attempt to use any User Defined Functions (UDFs) in HS2."
            ],
            "ExpectedBehavior": "HS2 should either wait for the HMS to be ready before starting or load the UDFs when a Hive session is created.",
            "ObservedBehavior": "HS2 starts in a servicing state without any available functions, leading to a failure in function registration.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12008.json",
        "creation_time": "2015-10-01T19:26:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-12008",
            "Title": "Hive Queries Fail with count(*) on View Columns",
            "Description": "Executing a count(*) on a view that utilizes get_json_object() UDF along with lateral views and unions results in a runtime exception. This issue does not occur in version 1.1 of Hive.",
            "StackTrace": [
                "2015-10-27 17:51:33,742 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 9 more",
                "Caused by: java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)",
                "... 14 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 17 more",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:147)",
                "... 22 more",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)"
            ],
            "StepsToReproduce": [
                "Create a view that includes get_json_object() UDF and lateral views.",
                "Execute a query using count(*) on the view.",
                "Observe the runtime exception in the logs."
            ],
            "ExpectedBehavior": "The count(*) query should execute successfully and return the correct count of records.",
            "ObservedBehavior": "The count(*) query fails with a runtime exception related to object configuration.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-6205.json",
        "creation_time": "2014-01-15T07:34:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-6205",
            "Title": "NullPointerException when altering table partition column in Hive",
            "Description": "When executing the command to alter a table's partition column, a NullPointerException is thrown during the authorization process. This issue appears to be related to the handling of authorization in the Hive Driver.",
            "StackTrace": [
                "2014-01-15 15:53:40,364 ERROR ql.Driver (SessionState.java:printError(457)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "StepsToReproduce": [
                "1. Open Hive CLI.",
                "2. Execute the command: alter table alter_coltype partition column (dt int);"
            ],
            "ExpectedBehavior": "The command should successfully alter the partition column without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the command, indicating an issue with authorization.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-15309.json",
        "creation_time": "2016-11-29T21:56:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-15309",
            "Title": "FileNotFoundException in OrcAcidUtils.getLastFlushLength() due to missing file check",
            "Description": "The method OrcAcidUtils.getLastFlushLength() does not check for file existence before attempting to access it, leading to unnecessary and confusing logging of FileNotFoundException. This issue can cause confusion for users and may lead to misinterpretation of the system's state.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/r\\rslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1831)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1744)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:693)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:373)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2313)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2309)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2307)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1552)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1496)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1396)",
                "at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1236)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1211)",
                "at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:309)",
                "at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:274)",
                "at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:266)",
                "at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1536)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:330)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:326)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:326)",
                "at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:782)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:460)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:1525)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:631)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:610)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "Invoke the method OrcAcidUtils.getLastFlushLength() without the required file present.",
                "Observe the logs for the FileNotFoundException."
            ],
            "ExpectedBehavior": "The method should check for the existence of the file before attempting to access it, preventing unnecessary logging of exceptions.",
            "ObservedBehavior": "The method logs a FileNotFoundException when the file does not exist, leading to confusion.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-10808.json",
        "creation_time": "2015-05-23T02:24:16.000+0000",
        "bug_report": {
            "BugID": "HIVE-10808",
            "Title": "Inner Join on Null Throws ClassCastException",
            "Description": "Executing an inner join on a table with null values results in a ClassCastException. This issue occurs when the query attempts to cast a NullStructSerDe object to a PrimitiveObjectInspector.",
            "StackTrace": [
                "java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 9 more",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:157)",
                "... 22 more",
                "Caused by: org.apache.hadoop.hive.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:334)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:352)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)",
                "... 22 more"
            ],
            "StepsToReproduce": [
                "Execute the following SQL query:",
                "SELECT a.col1, a.col2, a.col3, a.col4 FROM tab1 a INNER JOIN (SELECT max(x) as x FROM tab1 WHERE x < 20130327) r ON a.x = r.x WHERE a.col1 = 'F' AND a.col3 IN ('A', 'S', 'G');"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException due to an attempt to cast a NullStructSerDe object to a PrimitiveObjectInspector.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-18429.json",
        "creation_time": "2018-01-10T20:45:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-18429",
            "Title": "Compaction Fails to Produce Output When Deltas are Empty",
            "Description": "When starting with empty delta_8_8 and delta_9_9, the compaction process fails to create the expected temporary location, leading to a failure in the commit job. This issue prevents further compaction unless new deltas with data are created. If the number of empty deltas exceeds the configured maximum, compaction cannot proceed at all.",
            "StackTrace": [
                "2017-12-27 17:19:28,850 ERROR CommitterEvent Processor #1 org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job",
                "java.io.FileNotFoundException: File hdfs://OTCHaaS/apps/hive/warehouse/momi.db/sensor_data/babyid=5911806ebf69640100004257/_tmp_b4c5a3f3-44e5-4d45-86af-5b773bf0fc96 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:923)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:114)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:985)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:981)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:992)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:785)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start with empty delta_8_8 and delta_9_9.",
                "2. Run the compaction process.",
                "3. Observe the logs for errors related to temporary location creation."
            ],
            "ExpectedBehavior": "The compaction process should produce a delta_8_9 even if it is empty, allowing further compaction to proceed.",
            "ObservedBehavior": "The compaction process fails to create the temporary location, resulting in a FileNotFoundException during the commit job.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-10776.json",
        "creation_time": "2015-05-21T00:56:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-10776",
            "Title": "NullPointerException on Insert Queries with Bucketed Tables in Hive",
            "Description": "When executing insert queries with select * on bucketed tables in Hive, a NullPointerException is thrown, causing the operation to fail.",
            "StackTrace": [
                "2015-05-15 19:29:01,278 ERROR [main]: ql.Driver (SessionState.java:printError(957)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:7257)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6100)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6271)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8972)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8863)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9708)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9601)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10037)",
                "    at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:323)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)",
                "    at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)",
                "    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:606)",
                "    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Set hive.support.concurrency=true;",
                "Set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;",
                "Set hive.enforce.bucketing=true;",
                "Drop table if exists studenttab10k;",
                "Create table studenttab10k (age int, name varchar(50), gpa decimal(3,2));",
                "Insert into studenttab10k values(1,'foo', 1.1), (2,'bar', 2.3), (3,'baz', 3.1);",
                "Drop table if exists student_acid;",
                "Create table student_acid (age int, name varchar(50), gpa decimal(3,2), grade int) clustered by (age) into 2 buckets stored as orc tblproperties ('transactional'='true');",
                "Insert into student_acid(name, age, gpa) select * from studenttab10k;"
            ],
            "ExpectedBehavior": "The insert operation should complete successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the insert operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6301.json",
        "creation_time": "2014-01-24T01:42:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-6301",
            "Title": "IllegalStateException: No match found in UDFJson.evaluate() method",
            "Description": "The UDFJson class in Hive throws an IllegalStateException when the method evaluate() is called without a prior match check on the regex matcher. This issue occurs when using the get_json_object function in certain queries, leading to execution failures.",
            "StackTrace": [
                "2014-01-23 11:08:19,869 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String) on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2",
                "Caused by: java.lang.IllegalStateException: No match found",
                "at java.util.regex.Matcher.group(Matcher.java:468)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)",
                "... 24 more"
            ],
            "StepsToReproduce": [
                "1. Execute a Hive query that uses the get_json_object function with a malformed JSON string.",
                "2. Observe the execution failure and the thrown exception."
            ],
            "ExpectedBehavior": "The UDFJson.evaluate() method should handle cases where no match is found without throwing an IllegalStateException.",
            "ObservedBehavior": "The UDFJson.evaluate() method throws an IllegalStateException when no match is found, causing the query to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8295.json",
        "creation_time": "2014-09-29T21:16:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-8295",
            "Title": "Direct SQL Fails When Fetching Over 1000 Partition IDs in Hive Metastore",
            "Description": "When attempting to fetch partition objects in the Hive Metastore using direct SQL, an error occurs if the number of partition IDs exceeds 1000. This results in a failure of the SQL query, which falls back to ORM. The issue is specific to Oracle databases due to a limitation on the maximum number of expressions in a list.",
            "StackTrace": [
                "2014-09-29 19:30:02,942 DEBUG [pool-1-thread-1] metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(604)) - Direct SQL query in 122.085893ms + 13.048901ms, the query is [select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\"  inner join \"TBLS\" on \"PARTITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\"     and \"TBLS\".\"TBL_NAME\" = ?   inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\"      and \"DBS\".\"NAME\" = ? inner join \"PARTITION_KEY_VALS\" \"FILTER2\" on \"FILTER2\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER2\".\"INTEGER_IDX\" = 2 where (\"FILTER2\".\"PART_KEY_VAL\" = ?)]",
                "2014-09-29 19:30:02,949 ERROR [pool-1-thread-1] metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2248)) - Direct SQL failed, falling back to ORM",
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\", \"SDS\".\"SD_ID\", \"SDS\".\"CD_ID\", \"SERDES\".\"SERDE_ID\", \"PARTITIONS\".\"CREATE_TIME\", \"PARTITIONS\".\"LAST_ACCESS_TIME\", \"SDS\".\"INPUT_FORMAT\", \"SDS\".\"IS_COMPRESSED\", \"SDS\".\"IS_STOREDASSUBDIRECTORIES\", \"SDS\".\"LOCATION\", \"SDS\".\"NUM_BUCKETS\", \"SDS\".\"OUTPUT_FORMAT\", \"SERDES\".\"NAME\", \"SERDES\".\"SLIB\" from \"PARTITIONS\"  left outer join \"SDS\" on \"PARTITIONS\".\"SD_ID\" = \"SDS\".\"SD_ID\"   left outer join \"SERDES\" on \"SDS\".\"SERDE_ID\" = \"SERDES\".\"SERDE_ID\" where \"PART_ID\" in (136,140,143,147,152,156,160,163,167,171,174,180,185,191,196,198,203,208,212,217...) order by \"PART_NAME\" asc\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:422)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:331)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:211)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1920)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$3.getSqlResult(ObjectStore.java:1914)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2213)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal(ObjectStore.java:1914)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExpr(ObjectStore.java:1887)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)",
                "at com.sun.proxy.$Proxy8.getPartitionsByExpr(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_expr(HiveMetaStore.java:3800)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9366)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_expr.getResult(ThriftHiveMetastore.java:9350)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1637)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:206)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table with more than 1000 partitions.",
                "2. Attempt to fetch partition objects using direct SQL.",
                "3. Observe the error in the logs indicating SQL failure."
            ],
            "ExpectedBehavior": "The system should retrieve partition objects without error, regardless of the number of partitions.",
            "ObservedBehavior": "The system fails to execute the SQL query when the number of partition IDs exceeds 1000, resulting in a fallback to ORM.",
            "Resolution": "Implement batch retrieval of partition objects in direct SQL to avoid exceeding the maximum expression limit in Oracle."
        }
    },
    {
        "filename": "HIVE-8915.json",
        "creation_time": "2014-11-19T19:40:17.000+0000",
        "bug_report": {
            "BugID": "HIVE-8915",
            "Title": "Endless Log File Growth Due to Missing COMPACTION_QUEUE Table",
            "Description": "When starting the metastore without the required database tables (specifically the COMPACTION_QUEUE table), an endless loop of errors occurs, causing the log file to grow excessively. This issue needs to be addressed to prevent log file overflow and improve error handling.",
            "StackTrace": [
                "2014-11-19 01:44:57,654 ERROR compactor.Cleaner (Cleaner.java:run(143)) - Caught an exception in the main loop of compactor cleaner, MetaException(message:Unable to connect to transaction database",
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hive.COMPACTION_QUEUE' doesn't exist",
                "at sun.reflect.GeneratedConstructorAccessor20.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:386)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2569)",
                "at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1524)",
                "at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:266)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:291)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)"
            ],
            "StepsToReproduce": [
                "1. Set up a fresh instance of Hive in a virtual machine.",
                "2. Ensure that the database tables specified by hive-txn-schema-0.14.0.mysql.sql are not created.",
                "3. Start the Hive metastore."
            ],
            "ExpectedBehavior": "The metastore should start without errors, or provide a clear error message indicating the missing tables.",
            "ObservedBehavior": "An endless loop of errors is logged, causing the log file to grow to 1.7GB in 5 minutes, with repeated error messages.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7249.json",
        "creation_time": "2014-06-18T00:09:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-7249",
            "Title": "Exception in HiveTxnManager.closeTxnManager() after commitTxn()",
            "Description": "When calling closeTxnManager() after commitTxn(), an exception is thrown indicating that the lock is not found. This suggests that the TxnMgr does not recognize that the commit has released the locks.",
            "StackTrace": [
                "2014-06-17 15:54:40,804 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - NoSuchLockException(message:No such lock: 1)",
                "        at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(TxnHandler.java:1407)",
                "        at org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(TxnHandler.java:477)",
                "        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.unlock(HiveMetaStore.java:4817)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "        at java.lang.reflect.Method.invoke(Method.java:597)",
                "        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "        at com.sun.proxy.$Proxy14.unlock(Unknown Source)",
                "        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.unlock(HiveMetaStoreClient.java:1598)",
                "        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(DbLockManager.java:110)",
                "        at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close(DbLockManager.java:162)",
                "        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct(DbTxnManager.java:300)",
                "        at org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager(HiveTxnManagerImpl.java:39)",
                "        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.closeTxnManager(DbTxnManager.java:43)",
                "        at org.apache.hive.hcatalog.mapreduce.TransactionContext.cleanup(TransactionContext.java:327)",
                "        at org.apache.hive.hcatalog.mapreduce.TransactionContext.onCommitJob(TransactionContext.java:142)",
                "        at org.apache.hive.hcatalog.mapreduce.OutputCommitterContainer.commitJob(OutputCommitterContainer.java:61)",
                "        at org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.commitJob(FileOutputCommitterContainer.java:251)",
                "        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:537)"
            ],
            "StepsToReproduce": [
                "1. Call openTxn() to start a transaction.",
                "2. Call acquireLocks() with a query like 'INSERT INTO T PARTITION(p) SELECT * FROM T'.",
                "3. Call commitTxn() to commit the transaction.",
                "4. Call closeTxnManager() after the commit."
            ],
            "ExpectedBehavior": "The transaction manager should successfully close the transaction without throwing an exception.",
            "ObservedBehavior": "An exception is thrown indicating 'No such lock: 1', suggesting that the transaction manager does not recognize that the commit has released the locks.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11540.json",
        "creation_time": "2015-08-12T23:12:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-11540",
            "Title": "OutOfMemoryError during Compaction due to Excessive Delta Files",
            "Description": "The application encounters an OutOfMemoryError while attempting to compact delta files in Hive. This issue arises when streaming a high volume of weblogs to Kafka and subsequently to Flume using a Hive sink. The compaction process fails to keep up with the load, leading to memory exhaustion.",
            "StackTrace": [
                "2015-08-12 15:05:01,197 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Direct buffer memory",
                "2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12. Marking clean to avoid repeated failures, java.io.IOException: Job failed!",
                "at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:186)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:169)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:166)"
            ],
            "StepsToReproduce": [
                "1. Stream weblogs to Kafka.",
                "2. Use Flume 1.6 with a Hive sink to process the streamed data.",
                "3. Configure 5 compactors to run at various intervals (30m/5m/5s).",
                "4. Monitor the compaction process and observe memory usage."
            ],
            "ExpectedBehavior": "The compaction process should complete successfully without running out of memory, even with a high volume of delta files.",
            "ObservedBehavior": "The compaction process fails with an OutOfMemoryError, indicating that the system cannot handle the number of delta files generated.",
            "Resolution": "[Provide additional details on the resolution or workaround if available]"
        }
    },
    {
        "filename": "HIVE-15755.json",
        "creation_time": "2017-01-30T20:48:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-15755",
            "Title": "NullPointerException on Invalid Table Name in Merge Statement",
            "Description": "Encountered a NullPointerException when using an invalid table name in the ON clause of a MERGE statement in Hive. The error occurs specifically when the table name does not exist, leading to a failure in compiling the statement.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)",
                "    at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)",
                "    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "    at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)",
                "    at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)",
                "    at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:226)",
                "    at org.apache.hive.service.cli.operation.Operation.run(Operation.java:276)",
                "    at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:468)",
                "    at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:456)",
                "    at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:298)",
                "    at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:506)",
                "    at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1317)",
                "    at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1302)",
                "    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "    at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Create a source table with the command: create table src (col1 int, col2 int);",
                "Create a target table with the command: create table trgt (tcol1 int, tcol2 int);",
                "Insert a row into the source table: insert into src values (1, 232);",
                "Execute a merge statement with an invalid table name: merge into trgt using (select * from src) sub on sub.col1 = *invalidtablename.tcol1* when not matched then insert values (sub.col1, sub.col2);"
            ],
            "ExpectedBehavior": "The merge statement should either handle the invalid table name gracefully or provide a clear error message indicating the issue without causing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException when an invalid table name is specified in the ON clause of the merge statement.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-9390.json",
        "creation_time": "2015-01-15T18:50:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-9390",
            "Title": "Timeout Error When Accessing JDBC Connection in TxnHandler",
            "Description": "The system encounters a timeout error when attempting to retrieve a JDBC connection from the pool, leading to a failure in transaction management. This issue affects the ability to open transactions in the Hive metastore.",
            "StackTrace": [
                "2015-01-13 16:09:21,148 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(141)) - org.apache.thrift.TException: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5324)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)",
                "at com.sun.proxy.$Proxy11.get_open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidTxns(HiveMetaStoreClient.java:1696)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)",
                "at com.sun.proxy.$Proxy12.getValidTxns(Unknown Source)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidTxns(DbTxnManager.java:289)",
                "at org.apache.hadoop.hive.ql.Driver.recordValidTxns(Driver.java:882)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:399)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)",
                "at sun.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)",
                "at com.sun.proxy.$Proxy21.executeStatementAsync(Unknown Source)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:401)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.thrift.server.TServlet.doPost(TServlet.java:83)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:101)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)",
                "at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857)",
                "at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:850)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:196)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5322)",
                "... 66 more"
            ],
            "StepsToReproduce": [
                "1. Start the Hive metastore service.",
                "2. Attempt to open a transaction using the HiveMetaStoreClient.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The transaction should open successfully without any timeout errors.",
            "ObservedBehavior": "A timeout error occurs when trying to retrieve a JDBC connection, preventing the transaction from opening.",
            "Resolution": "The issue has been fixed in the latest release."
        }
    },
    {
        "filename": "HIVE-7623.json",
        "creation_time": "2014-08-05T23:58:27.000+0000",
        "bug_report": {
            "BugID": "HIVE-7623",
            "Title": "Hive Partition Rename Fails When Filesystem Cache is Disabled",
            "Description": "When attempting to rename partitions in Hive, an InvalidOperationException is thrown if the filesystem cache is disabled. This issue is similar to HIVE-3815. A workaround is to set `fs.hdfs.impl.disable.cache=false` and `fs.file.impl.disable.cache=false`.",
            "StackTrace": [
                "2014-08-05 21:46:14,522 ERROR [pool-3-thread-1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - InvalidOperationException(message:table new location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=123 is on a different file system than the old location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=456. This operation is not supported)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(HiveAlterHandler.java:361)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2629)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2602)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:622)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy5.rename_partition(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9057)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9041)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)"
            ],
            "StepsToReproduce": [
                "1. Disable filesystem cache by setting `fs.hdfs.impl.disable.cache=true` and `fs.file.impl.disable.cache=true`.",
                "2. Attempt to rename a partition in Hive using the `ALTER TABLE` command.",
                "3. Observe the error message thrown."
            ],
            "ExpectedBehavior": "The partition should be renamed successfully without any errors.",
            "ObservedBehavior": "An InvalidOperationException is thrown indicating that the new location is on a different file system than the old location.",
            "Resolution": "Fixed in version 0.14.0"
        }
    },
    {
        "filename": "HIVE-15997.json",
        "creation_time": "2017-02-21T16:49:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-15997",
            "Title": "Resource Leaks Occur When Query is Cancelled",
            "Description": "There may be resource leaks when a query is cancelled. The logs indicate possible file and lock leaks, which could lead to performance degradation and resource exhaustion.",
            "StackTrace": [
                "2017-02-02 06:23:25,410 WARN hive.ql.Context: [HiveServer2-Background-Pool: Thread-61]: Error Removing Scratch: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: \"ychencdh511t-1.vpc.cloudera.com/172.26.11.50\"; destination host is: \"ychencdh511t-1.vpc.cloudera.com\":8020;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1476)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1409)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)",
                "at com.sun.proxy.$Proxy25.delete(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:535)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy26.delete(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:2059)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:675)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:671)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:671)",
                "at org.apache.hadoop.hive.ql.Context.removeScratchDir(Context.java:405)",
                "at org.apache.hadoop.hive.ql.Context.clear(Context.java:541)",
                "at org.apache.hadoop.hive.ql.Driver.releaseContext(Driver.java:2109)",
                "at org.apache.hadoop.hive.ql.Driver.closeInProcess(Driver.java:2150)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1472)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)",
                "at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:681)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:192)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:714)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1525)",
                "... 35 more",
                "2017-02-02 12:26:52,706 INFO org.apache.hive.service.cli.operation.OperationManager: [HiveServer2-Background-Pool: Thread-23]: Operation is timed out, operation=OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=2af82100-94cf-4f26-abaa-c4b57c57b23c], state=CANCELED",
                "2017-02-02 06:21:05,054 ERROR ZooKeeperHiveLockManager: [HiveServer2-Background-Pool: Thread-61]: Failed to release ZooKeeper lock: java.lang.InterruptedException",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Object.wait(Object.java:503)",
                "at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)",
                "at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:871)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)",
                "at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive(ZooKeeperHiveLockManager.java:488)",
                "at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockWithRetry(ZooKeeperHiveLockManager.java:466)",
                "at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlock(ZooKeeperHiveLockManager.java:454)",
                "at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.releaseLocks(ZooKeeperHiveLockManager.java:236)",
                "at org.apache.hadoop.hive.ql.Driver.releaseLocksAndCommitOrRollback(Driver.java:1175)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1432)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Execute a query in Hive.",
                "Cancel the query before it completes.",
                "Check the logs for any warnings or errors related to resource leaks."
            ],
            "ExpectedBehavior": "No resource leaks should occur when a query is cancelled.",
            "ObservedBehavior": "Resource leaks are observed in the logs, indicating potential file and lock leaks.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7009.json",
        "creation_time": "2014-05-02T20:50:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-7009",
            "Title": "HIVE_USER_INSTALL_DIR Cannot Be Set to Non-HDFS Filesystem",
            "Description": "In `hive/ql/exec/tez/DagUtils.java`, the user path enforced from `HIVE_USER_INSTALL_DIR` must be HDFS. This restriction prevents running Hive+Tez jobs on non-HDFS filesystems, such as WASB. The relevant code checks if the filesystem is an instance of `DistributedFileSystem`, throwing an exception if it is not. This leads to failures when jobs are executed with `defaultFs` configured to WASB.",
            "StackTrace": [
                "2014-05-01 00:21:39,847 ERROR exec.Task (TezTask.java:execute(192)) - Failed to execute tez graph.",
                "java.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:912)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "Set the `defaultFs` configuration to a non-HDFS filesystem (e.g., WASB).",
                "Attempt to run a Hive+Tez job that utilizes the `HIVE_USER_INSTALL_DIR` variable.",
                "Observe the error message indicating that the URI is not a valid HDFS URI."
            ],
            "ExpectedBehavior": "The Hive+Tez job should execute successfully regardless of the filesystem type specified in `HIVE_USER_INSTALL_DIR`.",
            "ObservedBehavior": "The job fails with an IOException stating that the provided URI is not an HDFS URI.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-2031.json",
        "creation_time": "2011-03-08T11:38:53.000+0000",
        "bug_report": {
            "BugID": "HIVE-2031",
            "Title": "SemanticException when loading data into partitioned table with missing partition",
            "Description": "Loading data into a partitioned table with two partitions by specifying only one partition in the load statement fails, resulting in a SemanticException. The exception message does not provide sufficient information for debugging.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)",
                "at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)",
                "at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Create a partitioned table with at least two partitions.",
                "2. Attempt to load data into the table by specifying only one of the partitions in the load statement.",
                "3. Observe the exception thrown."
            ],
            "ExpectedBehavior": "The data should load successfully into the specified partition without throwing an exception.",
            "ObservedBehavior": "The operation fails with a SemanticException indicating that the specified partition was not found.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-4018.json",
        "creation_time": "2013-02-13T09:02:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-4018",
            "Title": "MapJoin Fails with Distributed Cache Error",
            "Description": "When executing a star join query after the fix for HIVE-3784, the query fails with a Distributed Cache error. The error message indicates an EOFException during the loading of the hash table.",
            "StackTrace": [
                "2013-02-13 08:36:04,584 ERROR org.apache.hadoop.hive.ql.exec.MapJoinOperator: Load Distributed Cache Error",
                "2013-02-13 08:36:04,585 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:189)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:203)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1421)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:614)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:266)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:260)"
            ],
            "StepsToReproduce": [
                "1. Execute a star join query in Hive after applying the fix for HIVE-3784.",
                "2. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The star join query should execute successfully without any errors.",
            "ObservedBehavior": "The query fails with a Distributed Cache error and an EOFException is thrown.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11255.json",
        "creation_time": "2015-07-14T15:39:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-11255",
            "Title": "Batch Retrieval of Table Objects in HiveMetaStore to Avoid SQL Syntax Error",
            "Description": "The `get_table_objects_by_name()` function in `HiveMetaStore.java` currently attempts to pass all tables of a single database to `ObjectStore` for retrieval. This results in a `java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000` error when using an Oracle database. The function should be modified to break the table list into multiple sublists, similar to the drop database operation.",
            "StackTrace": [
                "2015-06-29 13:36:00,093 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: Retrying HMSHandler after 1000 ms (attempt 1 of 1) with error: javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:945)",
                "at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy0.getTableObjectsByName(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1618)",
                "at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:106)",
                "at com.sun.proxy.$Proxy5.get_table_objects_by_name(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8172)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8156)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "StepsToReproduce": [
                "1. Connect to an Oracle database with more than 1000 tables in a single schema.",
                "2. Call the `get_table_objects_by_name()` function in `HiveMetaStore.java` with the name of the database.",
                "3. Observe the error message returned."
            ],
            "ExpectedBehavior": "The function should retrieve table objects without exceeding the maximum number of expressions in a list, ideally by batching the requests.",
            "ObservedBehavior": "The function throws a `java.sql.SQLSyntaxErrorException` due to exceeding the maximum number of expressions allowed in a single SQL query.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10151.json",
        "creation_time": "2015-03-31T00:17:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-10151",
            "Title": "Insert into Acid Table Fails When Both Source and Target Are Bucketed",
            "Description": "The issue occurs when attempting to insert data from one Acid table to another Acid table that are both bucketed in the same way. The operation fails due to an error in the ORC input format processing.",
            "StackTrace": [
                "2015-04-29 13:57:35,807 ERROR [main]: exec.Task (SessionState.java:printError(956)) - Job Submission failed with exception 'java.lang.RuntimeException(serious problem)'",
                "java.lang.RuntimeException: serious problem",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)",
                "    at org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(BucketizedHiveInputFormat.java:141)",
                "    at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:624)",
                "    at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:616)",
                "    at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:492)",
                "    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)",
                "    at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:415)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "    at org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)",
                "    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)",
                "    at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:415)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)",
                "    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:430)",
                "    at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:137)",
                "    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)",
                "    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)",
                "    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "    at org.apache.hadoop.hive.ql.TestTxnCommands2.runStatementOnDriver(TestTxnCommands2.java:225)",
                "    at org.apache.hadoop.hive.ql.TestTxnCommands2.testDeleteIn2(TestTxnCommands2.java:148)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:606)",
                "    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)",
                "    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)",
                "    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)",
                "    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)",
                "    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)",
                "    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)",
                "    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)",
                "    at org.junit.rules.RunRules.evaluate(RunRules.java:20)",
                "    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)",
                "    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)",
                "    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)",
                "    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)",
                "    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)",
                "    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)",
                "    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)",
                "    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)",
                "    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)",
                "    at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:254)",
                "    at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:149)",
                "    at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_",
                "    at java.util.concurrent.FutureTask.report(FutureTask.java:122)",
                "    at java.util.concurrent.FutureTask.get(FutureTask.java:188)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:998)",
                "    ... 56 more",
                "Caused by: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_",
                "    at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)",
                "    at org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(AcidUtils.java:172)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:655)",
                "    at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$FileGenerator.call(OrcInputFormat.java:620)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Create an Acid table 'acidTbl' with the following SQL: CREATE TABLE acidTbl(a int, b int) CLUSTERED BY (a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')",
                "Create another Acid table 'acidTblPart' with the following SQL: CREATE TABLE acidTblPart(a int, b int) PARTITIONED BY (p string) CLUSTERED BY (a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')",
                "Insert data into 'acidTblPart': INSERT INTO acidTblPart PARTITION(p=1) (a,b) VALUES(1,2)",
                "Attempt to insert data from 'acidTblPart' into 'acidTbl': INSERT INTO acidTbl(a,b) SELECT a,b FROM acidTblPart WHERE p = 1"
            ],
            "ExpectedBehavior": "The data should be successfully inserted from 'acidTblPart' into 'acidTbl' without any errors.",
            "ObservedBehavior": "The operation fails with a RuntimeException indicating a serious problem related to ORC input format processing.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13546.json",
        "creation_time": "2016-04-19T07:43:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-13546",
            "Title": "Hive Runtime Error During Dynamic Partitioning Insert",
            "Description": "The following SQL fails when attempting to insert data into a dynamically partitioned table in Hive. The error log indicates a runtime exception related to processing a row with null values.",
            "StackTrace": [
                "2016-04-19 15:15:35,252 FATAL [main] ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":null},\"value\":{\"_col0\":null,\"_col1\":5588,\"_col2\":170300,\"_col3\":null,\"_col4\":756,\"_col5\":91384,\"_col6\":16,\"_col7\":null,\"_col8\":855582,\"_col9\":28,\"_col10\":null,\"_col11\":48.83,\"_col12\":null,\"_col13\":0.0,\"_col14\":null,\"_col15\":899.64,\"_col16\":null,\"_col17\":6.14,\"_col18\":0.0,\"_col19\":null,\"_col20\":null,\"_col21\":null,\"_col22\":null}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:180)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1732)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:174)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:653)",
                "at java.util.ArrayList.get(ArrayList.java:429)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:151)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:131)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:1003)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:919)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:713)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)",
                "... 7 more"
            ],
            "StepsToReproduce": [
                "Set the following Hive configurations:",
                "set hive.map.aggr=true;",
                "set mapreduce.reduce.speculative=false;",
                "set hive.auto.convert.join=true;",
                "set hive.optimize.reducededuplication = false;",
                "set hive.optimize.reducededuplication.min.reducer=1;",
                "set hive.optimize.mapjoin.mapreduce=true;",
                "set hive.stats.autogather=true;",
                "set mapred.reduce.parallel.copies=30;",
                "set mapred.job.shuffle.input.buffer.percent=0.5;",
                "set mapred.job.reduce.input.buffer.percent=0.2;",
                "set mapred.map.child.java.opts=-server -Xmx2800m -Djava.net.preferIPv4Stack=true;",
                "set mapred.reduce.child.java.opts=-server -Xmx3800m -Djava.net.preferIPv4Stack=true;",
                "set mapreduce.map.memory.mb=3072;",
                "set mapreduce.reduce.memory.mb=4096;",
                "set hive.enforce.bucketing=true;",
                "set hive.enforce.sorting=true;",
                "set hive.exec.dynamic.partition.mode=nonstrict;",
                "set hive.exec.max.dynamic.partitions.pernode=100000;",
                "set hive.exec.max.dynamic.partitions=100000;",
                "set hive.exec.max.created.files=1000000;",
                "set hive.exec.parallel=true;",
                "set hive.exec.reducers.max=2000;",
                "set hive.stats.autogather=true;",
                "set hive.optimize.sort.dynamic.partition=true;",
                "set mapred.job.reduce.input.buffer.percent=0.0;",
                "set mapreduce.input.fileinputformat.split.minsizee=240000000;",
                "set mapreduce.input.fileinputformat.split.minsize.per.node=240000000;",
                "set mapreduce.input.fileinputformat.split.minsize.per.rack=240000000;",
                "use tpcds_bin_partitioned_orc_4;",
                "Execute the following SQL query:",
                "insert overwrite table store_sales partition (ss_sold_date_sk) select ... from tpcds_text_4.store_sales ss;"
            ],
            "ExpectedBehavior": "The SQL query should execute successfully and insert data into the dynamically partitioned table without errors.",
            "ObservedBehavior": "The SQL query fails with a Hive Runtime Error indicating an IndexOutOfBoundsException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7049.json",
        "creation_time": "2014-05-12T21:46:48.000+0000",
        "bug_report": {
            "BugID": "HIVE-7049",
            "Title": "Deserialization Failure for AVRO Data with Mismatched Schemas",
            "Description": "The issue occurs when the file schema and record schema are not compatible, specifically when the record schema is nullable but the file schema is not. This leads to an AvroRuntimeException during deserialization.",
            "StackTrace": [
                "org.apache.avro.AvroRuntimeException: Not a union: \"string\"",
                "at org.apache.avro.Schema.getTypes(Schema.java:272)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)"
            ],
            "StepsToReproduce": [
                "1. Create a record schema that includes nullable types, e.g., [\"null\", \"string\"].",
                "2. Create a file schema that does not include nullable types, e.g., \"string\".",
                "3. Attempt to deserialize the AVRO data using the mismatched schemas."
            ],
            "ExpectedBehavior": "The deserialization process should handle nullable types correctly without throwing an exception.",
            "ObservedBehavior": "An AvroRuntimeException is thrown indicating that the file schema is not a union type.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9755.json",
        "creation_time": "2015-02-23T20:50:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-9755",
            "Title": "Hive UDAF 'ngram' Fails with Null Values in Mapper Results",
            "Description": "The Hive built-in 'ngram' UDAF fails when a mapper returns no matches, specifically when any result has a value equal to null. This leads to a runtime error during processing.",
            "StackTrace": [
                "2015-01-08 09:15:00,262 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":[\"0\",\"0\",\"0\",\"0\"]},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:258)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:506)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:447)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: GenericUDAFnGramEvaluator: mismatch in value for 'n', which usually is caused by a non-constant expression. Found '0' and '1'.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams$GenericUDAFnGramEvaluator.merge(GenericUDAFnGrams.java:242)",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:142)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:658)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:911)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:753)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:474)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:249)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive table named 'ngramtest' with columns 'col1' (int) and 'col3' (string).",
                "2. Insert data into 'ngramtest' where 'col1' is 0 and 'col3' contains null values.",
                "3. Execute the query: SELECT explode(ngrams(sentences(lower(t.col3)), 3, 10)) as x FROM (SELECT col3 FROM ngramtest WHERE col1=0) t;"
            ],
            "ExpectedBehavior": "The query should return an empty result set or handle null values gracefully without throwing an error.",
            "ObservedBehavior": "The query throws a Hive Runtime Error when processing rows with null values.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-19130.json",
        "creation_time": "2018-04-09T10:18:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-19130",
            "Title": "NullPointerException during REPL LOAD on Drop Partition Event",
            "Description": "During incremental replication, if the events batch is split as follows, the REPL LOAD on the second batch throws a NullPointerException (NPE).\n\nBatch-1: CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION(t1.p1)\n\nBatch-2: DROP_TABLE(t1) -> CREATE_TABLE(t1) -> ADD_PARTITION(t1.p1) -> DROP_PARTITION(t1.p1)",
            "StackTrace": [
                "2018-04-05 16:20:36,531 ERROR [HiveServer2-Background-Pool: Thread-107044]: metadata.Hive (Hive.java:getTable(1219)) - Table catalog_sales_new not found: new5_tpcds_real_bin_partitioned_orc_1000.catalog_sales_new table not found",
                "2018-04-05 16:20:36,538 ERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4016)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3983)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:341)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:162)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:176)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1506)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1303)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1165)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)",
                "at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)",
                "... 23 more"
            ],
            "StepsToReproduce": [
                "1. Create a table t1.",
                "2. Add a partition to t1.",
                "3. Drop the partition from t1.",
                "4. Drop the table t1.",
                "5. Create the table t1 again.",
                "6. Add a partition to t1.",
                "7. Drop the partition from t1."
            ],
            "ExpectedBehavior": "The REPL LOAD should complete successfully without throwing a NullPointerException.",
            "ObservedBehavior": "The REPL LOAD throws a NullPointerException when processing the second batch of events.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13090.json",
        "creation_time": "2016-02-18T21:58:48.000+0000",
        "bug_report": {
            "BugID": "HIVE-13090",
            "Title": "Hive Metastore Crashes with NullPointerException in ZooKeeperTokenStore",
            "Description": "The Hive metastore crashes due to a NullPointerException when attempting to retrieve a delegation token from ZooKeeper. This issue occurs during the shutdown process of the metastore.",
            "StackTrace": [
                "ERROR [Thread[Thread-6,5,main]]: thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(331)) - ExpiredTokenRemover thread received unexpected exception. org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.NullPointerException",
                "at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)",
                "at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)",
                "... 3 more"
            ],
            "StepsToReproduce": [
                "1. Start the Hive metastore service.",
                "2. Ensure that there are no delegation tokens available in ZooKeeper.",
                "3. Trigger a shutdown of the Hive metastore service."
            ],
            "ExpectedBehavior": "The Hive metastore should shut down gracefully without throwing any exceptions.",
            "ObservedBehavior": "The Hive metastore crashes with a NullPointerException during the shutdown process when no delegation token is found in ZooKeeper.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5664.json",
        "creation_time": "2013-10-28T03:50:29.000+0000",
        "bug_report": {
            "BugID": "HIVE-5664",
            "Title": "Drop Cascade Database Fails When Database Contains Tables with Indexes",
            "Description": "When attempting to drop a database that contains tables with indexes, the operation fails with an error indicating that the database does not exist. This issue occurs even when the database was created successfully prior to the drop operation.",
            "StackTrace": [
                "2013-10-27 20:46:16,629 ERROR exec.DDLTask (DDLTask.java:execute(434)) - org.apache.hadoop.hive.ql.metadata.HiveException: Database does not exist: db2",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3473)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)",
                "at org.apache.hadoop.hive.exec.TaskRunner.runSequential(TaskRunner.java:65)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1441)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1219)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1047)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:915)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:160)",
                "Caused by: NoSuchObjectException(message:db2.tab1_indx table not found)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1376)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)",
                "at com.sun.proxy.$Proxy7.get_table(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:890)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:660)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:652)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase(HiveMetaStoreClient.java:546)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)",
                "at com.sun.proxy.$Proxy8.dropDatabase(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropDatabase(Hive.java:284)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase(DDLTask.java:3470)",
                "... 18 more"
            ],
            "StepsToReproduce": [
                "1. Create a new database using the command: CREATE DATABASE db2;",
                "2. Use the new database: USE db2;",
                "3. Create a table with an index: CREATE TABLE tab1 (id int, name string); CREATE INDEX idx1 ON TABLE tab1(id) as 'COMPACT' with DEFERRED REBUILD IN TABLE tab1_indx;",
                "4. Attempt to drop the database with the command: DROP DATABASE db2 CASCADE;"
            ],
            "ExpectedBehavior": "The database should be dropped successfully, including all associated tables and indexes.",
            "ObservedBehavior": "The drop operation fails with an error indicating that the database does not exist, despite having been created successfully.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-15778.json",
        "creation_time": "2017-02-01T04:20:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-15778",
            "Title": "NullPointerException when dropping a non-existent index using DbNotificationListener",
            "Description": "Executing a DROP INDEX operation on a non-existent index results in a NullPointerException (NPE). This occurs when the HiveMetaStore attempts to handle the drop index event, leading to an unhandled exception in the DbNotificationListener.",
            "StackTrace": [
                "2017-01-31 16:27:29,421 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-5-thread-3]: MetaException(message:java.lang.NullPointerException)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5823)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rethrowException(HiveMetaStore.java:4892)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4403)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy16.drop_index_by_name(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10803)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10787)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)",
                "at org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropIndexMessage(JSONMessageFactory.java:159)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name_core(HiveMetaStore.java:4469)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4396)",
                "... 20 more"
            ],
            "StepsToReproduce": [
                "Connect to Hive using JDBC.",
                "Execute the command: DROP INDEX IF EXISTS vamsee1 ON sample_07."
            ],
            "ExpectedBehavior": "The command should execute without errors, indicating that the index does not exist.",
            "ObservedBehavior": "The command fails with a NullPointerException, indicating an internal error in the HiveMetaStore.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-8386.json",
        "creation_time": "2014-10-07T22:30:12.000+0000",
        "bug_report": {
            "BugID": "HIVE-8386",
            "Title": "HCatalog API Call Fails Due to Case Sensitivity in Struct Column Fields",
            "Description": "When using the HCatalog API to verify the target table schema, an error occurs indicating that a field cannot be found due to case sensitivity. The error message suggests that the field 'givenName' (in lowercase form: 'givenname') is not recognized, even though it exists in the schema.",
            "StackTrace": [
                "2014-10-07 00:30:23,255 ERROR - [1972803970@qtp-1214921164-3:gfoetl:POST//entities/submitAndSchedule/feed a0c221e3-efa8-4235-a403-b1047f23ec05] ~ Failure reason (FalconWebException:40)",
                "java.lang.RuntimeException: cannot find field givenName(lowercase form: givenname) in [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase]",
                "at org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(StructTypeInfo.java:109)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(HCatSchemaUtils.java:154)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(HCatSchemaUtils.java:165)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:127)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:115)",
                "at org.apache.hive.hcatalog.api.HCatTable.<init>(HCatTable.java:59)",
                "at org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(HCatClientHMSImpl.java:157)",
                "at org.apache.falcon.catalog.HiveCatalogService.tableExists(HiveCatalogService.java:143)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateStorageExists(FeedEntityParser.java:367)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateFeedStorage(FeedEntityParser.java:309)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:79)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:54)",
                "at org.apache.falcon.resource.AbstractEntityManager.validate(AbstractEntityManager.java:364)",
                "at org.apache.falcon.resource.AbstractEntityManager.submitInternal(AbstractEntityManager.java:331)",
                "at org.apache.falcon.resource.AbstractEntityManager.submit(AbstractEntityManager.java:153)",
                "at org.apache.falcon.resource.ConfigSyncService.submit(ConfigSyncService.java:44)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.falcon.resource.channel.IPCChannel.invoke(IPCChannel.java:48)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$1.doExecute(SchedulableEntityManagerProxy.java:118)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$EntityProxy.execute(SchedulableEntityManagerProxy.java:410)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody0(SchedulableEntityManagerProxy.java:120)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure1.run(SchedulableEntityManagerProxy.java:1)",
                "at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)",
                "at org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit(SchedulableEntityManagerProxy.java:107)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submit_aroundBody12(SchedulableEntityManagerProxy.java:341)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure13.run(SchedulableEntityManagerProxy.java:1)",
                "at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)",
                "at org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule_aroundBody16(SchedulableEntityManagerProxy.java:341)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy$AjcClosure17.run(SchedulableEntityManagerProxy.java:1)",
                "at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)",
                "at org.apache.falcon.aspect.AbstractFalconAspect.logAround(AbstractFalconAspect.java:50)",
                "at org.apache.falcon.resource.proxy.SchedulableEntityManagerProxy.submitAndSchedule(SchedulableEntityManagerProxy.java:335)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:699)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at org.apache.falcon.security.BasicAuthFilter$2.doFilter(BasicAuthFilter.java:183)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:392)",
                "at org.apache.falcon.security.BasicAuthFilter.doFilter(BasicAuthFilter.java:221)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "1. Use the HCatalog API to verify the target table schema.",
                "2. Attempt to access the field 'givenName' in lowercase form.",
                "3. Observe the error message indicating that the field cannot be found."
            ],
            "ExpectedBehavior": "The API should recognize the field 'givenName' regardless of case sensitivity.",
            "ObservedBehavior": "The API throws a RuntimeException indicating that it cannot find the field 'givenName' when accessed in lowercase.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-14714.json",
        "creation_time": "2016-09-07T15:46:07.000+0000",
        "bug_report": {
            "BugID": "HIVE-14714",
            "Title": "IOException: Stream closed when shutting down HiveServer2 session",
            "Description": "After executing a Hive command with Spark, finishing the Beeline session or switching the execution engine causes an IOException. The issue occurs when using Ctrl-D to finish the session, as well as when using commands like '!quit' or 'set hive.execution.engine=mr;'.",
            "StackTrace": [
                "java.io.IOException: Stream closed",
                "at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)",
                "at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)",
                "at java.io.BufferedInputStream.read(BufferedInputStream.java:334)",
                "at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)",
                "at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)",
                "at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)",
                "at java.io.InputStreamReader.read(InputStreamReader.java:184)",
                "at java.io.BufferedReader.fill(BufferedReader.java:154)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:317)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:382)",
                "at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a Beeline session connected to HiveServer2.",
                "2. Execute a Hive command using Spark.",
                "3. Attempt to finish the session using Ctrl-D.",
                "4. Alternatively, try using '!quit' or 'set hive.execution.engine=mr;'."
            ],
            "ExpectedBehavior": "The Beeline session should terminate without any IOException.",
            "ObservedBehavior": "An IOException is thrown with the message 'Stream closed' when attempting to finish the session.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5428.json",
        "creation_time": "2013-10-02T20:46:10.000+0000",
        "bug_report": {
            "BugID": "HIVE-5428",
            "Title": "Direct SQL Check Fails During Tests",
            "Description": "An exception occurs during the execution of tests related to the Hive metastore. The issue seems to be related to the initialization order of components, leading to a failure in executing a SQL query against the metastore database.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"DB_ID\" from \"DBS\"\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:230)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:108)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:249)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:220)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:71)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:418)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:405)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:444)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:329)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:289)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4084)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:126)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1211)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2404)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2415)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:871)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:853)",
                "at org.apache.hadoop.hive.ql.QTestUtil.cleanUp(QTestUtil.java:534)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.<clinit>(TestCliDriver.java:44)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:374)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1060)",
                "at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:911)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: Table/View 'DBS' does not exist.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement40.<init>(Unknown Source)"
            ],
            "StepsToReproduce": [
                "Run the command: ant test -Dtestcase=TestCliDriver -Dqfile=udf_case.q -Dtest.silent=false",
                "Check the top of the logs for exceptions."
            ],
            "ExpectedBehavior": "The tests should execute successfully without any SQL exceptions.",
            "ObservedBehavior": "An SQL exception occurs indicating that the table/view 'DBS' does not exist.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-12567.json",
        "creation_time": "2015-12-02T16:38:52.000+0000",
        "bug_report": {
            "BugID": "HIVE-12567",
            "Title": "Lock Acquisition Failure in Hive Metastore",
            "Description": "The system fails to acquire locks due to communication issues with the metastore, resulting in a LockException. This impacts transaction management and can lead to inconsistent states in the application.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:132)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:227)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.acquireLocks(DbTxnManager.java:92)",
                "at org.apache.hadoop.hive.ql.Driver.acquireLocksAndOpenTxn(Driver.java:1029)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1226)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1100)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1095)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the HiveServer2 service.",
                "2. Attempt to execute a transaction that requires acquiring locks.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The transaction should acquire the necessary locks without errors.",
            "ObservedBehavior": "The transaction fails with a LockException indicating an error in acquiring locks due to communication issues with the metastore.",
            "Resolution": "The issue was resolved by enhancing the TxnHandler retry logic to handle ORA-08176 errors."
        }
    },
    {
        "filename": "HIVE-6984.json",
        "creation_time": "2014-04-28T23:08:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-6984",
            "Title": "NullPointerException when analyzing partitioned table with NULL values in Hive",
            "Description": "When analyzing a partitioned table in Hive that contains NULL values for the partition column, a NullPointerException (NPE) occurs, preventing the operation from completing successfully.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)",
                "... 9 more"
            ],
            "StepsToReproduce": [
                "Create a Hive table named 'test2' with the following data:",
                "6666666666666666666\tNULL",
                "5555555555555555555\tNULL",
                "tom\t15",
                "john\tNULL",
                "mayr\t40",
                "Create another table 'test3' partitioned by 'age'.",
                "Run the following query: 'from test2 insert overwrite table test3 partition(age) select test2.name, test2.age;'",
                "Execute 'analyze table test3 partition(age) compute statistics;'"
            ],
            "ExpectedBehavior": "The statistics for the partitioned table should be computed successfully without any errors.",
            "ObservedBehavior": "A NullPointerException occurs during the analysis of the partitioned table, preventing the operation from completing.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-10736.json",
        "creation_time": "2015-05-18T03:25:45.000+0000",
        "bug_report": {
            "BugID": "HIVE-10736",
            "Title": "ConcurrentModificationException during HiveServer2 Shutdown",
            "Description": "The shutdown process of HiveServer2 throws concurrent modification exceptions and fails to clean up the application masters per queue. This issue can lead to resource leaks and instability in the HiveServer2 service.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)",
                "at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)"
            ],
            "StepsToReproduce": [
                "Start HiveServer2 with cached Tez application masters.",
                "Trigger the shutdown process of HiveServer2."
            ],
            "ExpectedBehavior": "HiveServer2 should shut down cleanly without throwing exceptions and should properly clean up all resources, including application masters.",
            "ObservedBehavior": "During the shutdown process, a ConcurrentModificationException is thrown, indicating that the application masters are not being cleaned up properly.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-7710.json",
        "creation_time": "2014-08-13T10:46:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-7710",
            "Title": "Rename Table Across Databases Fails Due to Duplicate Key Constraint",
            "Description": "When attempting to rename a table from one database to another, the operation fails if the target database already contains a table with the same name. This results in a SQLIntegrityConstraintViolationException due to a duplicate key value in a unique constraint.",
            "StackTrace": [
                "2014-08-13 03:32:40,512 ERROR Datastore.Persist (Log4JLogger.java:error(115)) - Update of object \"org.apache.hadoop.hive.metastore.model.MTable@729c5167\" using statement \"UPDATE TBLS SET TBL_NAME=? WHERE TBL_ID=?\" failed : java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)",
                "at com.jolbox.bonecp.PreparedStatementHandle.executeUpdate(PreparedStatementHandle.java:205)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:399)",
                "at org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:439)",
                "at org.datanucleus.store.rdbms.request.UpdateRequest.execute(UpdateRequest.java:374)",
                "at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateTable(RDBMSPersistenceHandler.java:417)",
                "at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.updateObject(RDBMSPersistenceHandler.java:390)",
                "at org.datanucleus.state.JDOStateManager.flush(JDOStateManager.java:5027)",
                "at org.datanucleus.flush.FlushOrdered.execute(FlushOrdered.java:106)",
                "at org.datanucleus.ExecutionContextImpl.flushInternal(ExecutionContextImpl.java:4119)",
                "at org.datanucleus.ExecutionContextThreadedImpl.flushInternal(ExecutionContextThreadedImpl.java:450)",
                "at org.datanucleus.ExecutionContextImpl.markDirty(ExecutionContextImpl.java:3879)",
                "at org.datanucleus.ExecutionContextThreadedImpl.markDirty(ExecutionContextImpl.java:422)",
                "at org.datanucleus.state.JDOStateManager.postWriteField(JDOStateManager.java:4815)",
                "at org.datanucleus.state.JDOStateManager.replaceField(JDOStateManager.java:3356)",
                "at org.datanucleus.state.JDOStateManager.updateField(JDOStateManager.java:2018)",
                "at org.datanucleus.state.JDOStateManager.setStringField(JDOStateManager.java:1791)",
                "at org.apache.hadoop.hive.metastore.model.MStorageDescriptor.jdoSetlocation(MStorageDescriptor.java)",
                "at org.apache.hadoop.hive.metastore.model.MStorageDescriptor.setLocation(MStorageDescriptor.java:88)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.copyMSD(ObjectStore.java:2699)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2572)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)",
                "at com.sun.proxy.$Proxy6.alterTable(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:205)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:2771)",
                "at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy8.alter_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:293)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.alter_table(SessionHiveMetaStoreClient.java:201)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_table(HiveMetaStoreClient.java:288)",
                "at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)",
                "at com.sun.proxy.$Proxy9.alter_table(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.alterTable(Hive.java:404)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3542)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:318)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:161)",
                "at org.apache.hadoop.hive.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1538)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1305)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1118)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:833)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_alter_rename_table(TestCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at junit.framework.TestCase.runTest(TestCase.java:168)",
                "at junit.framework.TestCase.runBare(TestCase.java:134)",
                "at junit.framework.TestResult$1.protect(TestResult.java:110)",
                "at junit.framework.TestResult.runProtected(TestResult.java:128)",
                "at junit.framework.TestResult.run(TestResult.java:113)",
                "at junit.framework.TestCase.run(TestCase.java:124)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:243)",
                "at junit.framework.TestSuite.run(TestSuite.java:238)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)"
            ],
            "StepsToReproduce": [
                "Ensure that there is a table named 't2' in database 'd1'.",
                "Execute the following SQL command: 'ALTER TABLE d1.t1 RENAME TO d2.t2;'."
            ],
            "ExpectedBehavior": "The table 't1' should be renamed to 't2' in database 'd2' without any errors.",
            "ObservedBehavior": "The operation fails with a SQLIntegrityConstraintViolationException due to a duplicate key value in the unique constraint 'UNIQUETABLE'.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-8735.json",
        "creation_time": "2014-11-04T22:20:02.000+0000",
        "bug_report": {
            "BugID": "HIVE-8735",
            "Title": "Statistics Update Fails Due to Long Path Truncation Error",
            "Description": "An error occurs during the publishing of statistics when the path exceeds the maximum length for a VARCHAR field in the database. This results in a truncation error, preventing the statistics from being updated successfully.",
            "StackTrace": [
                "2014-11-04 01:34:38,610 ERROR jdbc.JDBCStatsPublisher (JDBCStatsPublisher.java:publishStat(198)) - Error during publishing statistics.",
                "java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:147)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:144)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.executeWithRetry(Utilities.java:2910)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.publishStat(JDBCStatsPublisher.java:160)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats(FileSinkOperator.java:1153)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:992)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:205)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: java.sql.SQLException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)",
                "... 31 more",
                "Caused by: ERROR 22001: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.",
                "at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)",
                "at org.apache.derby.iapi.types.SQLChar.hasNonBlankChars(Unknown Source)",
                "at org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)",
                "at org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)",
                "at org.apache.derby.iapi.types.DataTypeDescriptor.normalize(Unknown Source)",
                "at org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeColumn(Unknown Source)",
                "at org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeRow(Unknown Source)",
                "at org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(Unknown Source)",
                "at org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(Unknown Source)",
                "at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)",
                "at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)",
                "at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)",
                "... 25 more"
            ],
            "StepsToReproduce": [
                "1. Attempt to publish statistics with a path longer than 255 characters.",
                "2. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "Statistics should be published successfully without any truncation errors.",
            "ObservedBehavior": "An SQLDataException is thrown indicating a truncation error due to the path length exceeding the VARCHAR limit.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13209.json",
        "creation_time": "2016-03-04T21:39:50.000+0000",
        "bug_report": {
            "BugID": "HIVE-13209",
            "Title": "Metastore get_delegation_token fails with null IP address",
            "Description": "After changes in HIVE-13169, the metastore's get_delegation_token method fails due to a null IP address, resulting in an unauthorized connection error.",
            "StackTrace": [
                "2016-03-03 07:45:31,055 ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/<hostname@realm> from IP null)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Apply changes from HIVE-13169.",
                "2. Attempt to call the get_delegation_token method on the metastore.",
                "3. Observe the error message regarding the null IP address."
            ],
            "ExpectedBehavior": "The get_delegation_token method should successfully return a delegation token without errors.",
            "ObservedBehavior": "The get_delegation_token method fails with an error indicating an unauthorized connection due to a null IP address.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13065.json",
        "creation_time": "2016-02-16T21:11:31.000+0000",
        "bug_report": {
            "BugID": "HIVE-13065",
            "Title": "NullPointerException when inserting NULL values in map type column of HBase backed Hive table",
            "Description": "Hive throws a NullPointerException (NPE) when writing data to an HBase backed table under specific conditions. The issue occurs when there is a map type column that contains NULL values in its entries.",
            "StackTrace": [
                "2016-02-15 02:26:33,225 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)",
                "... 14 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)",
                "at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)",
                "... 15 more"
            ],
            "StepsToReproduce": [
                "1) Create a HBase backed Hive table with a map type column:",
                "   ```sql",
                "   create table hbase_test (id bigint, data map<string, string>)",
                "   stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'",
                "   with serdeproperties ('hbase.columns.mapping' = ':key,cf:map_col')",
                "   tblproperties ('hbase.table.name' = 'hive_test');",
                "   ```",
                "2) Insert data into the above table with a NULL value in the map:",
                "   ```sql",
                "   insert overwrite table hbase_test select 1 as id, map('abcd', null) as data from src limit 1;",
                "   ```"
            ],
            "ExpectedBehavior": "The data should be inserted into the HBase table without any errors, even if the map type column contains NULL values.",
            "ObservedBehavior": "The mapreduce job for the insert query fails with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11470.json",
        "creation_time": "2015-08-05T18:45:26.000+0000",
        "bug_report": {
            "BugID": "HIVE-11470",
            "Title": "NullPointerException in DynamicPartitionFileRecordWriterContainer when using null partition keys",
            "Description": "When partitioning data using HCatStorer, a NullPointerException (NPE) occurs if the dynamic partition key is null. This issue arises from an assumption made in the DynamicPartitionFileRecordWriterContainer when fetching a local file-writer instance.",
            "StackTrace": [
                "2015-07-30 23:59:59,627 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.lang.NullPointerException",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)",
                "at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)",
                "at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)",
                "at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)",
                "at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)",
                "at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer$Context.java:105)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with HCatalog.",
                "2. Attempt to partition data using HCatStorer with a dynamic partition key set to null.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The system should handle null partition keys gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to partition data with a null dynamic partition key.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-12476.json",
        "creation_time": "2015-11-20T03:30:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-12476",
            "Title": "NullPointerException in Metastore on Oracle with Direct SQL",
            "Description": "A NullPointerException occurs in the Hive Metastore when using Direct SQL mode on Oracle. This issue appears to be related to the handling of Partition/StorageDescriptorSerDe parameters, similar to HIVE-8485.",
            "StackTrace": [
                "2015-11-19 18:08:33,841 ERROR [pool-5-thread-2]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.",
                "java.lang.NullPointerException",
                "    at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)",
                "    at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)",
                "    at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)",
                "    at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)",
                "    at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)",
                "    at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)",
                "    at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)",
                "    at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)",
                "    at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)",
                "    at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)",
                "    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)",
                "    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)",
                "    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)",
                "    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)",
                "    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "    at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)",
                "    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up Hive Metastore with Oracle as the backend.",
                "2. Enable Direct SQL mode in the configuration.",
                "3. Attempt to perform an operation that involves Partition/StorageDescriptorSerDe parameters."
            ],
            "ExpectedBehavior": "The operation should complete successfully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the processing of the message, leading to a failure in the operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10559.json",
        "creation_time": "2015-04-30T21:12:40.000+0000",
        "bug_report": {
            "BugID": "HIVE-10559",
            "Title": "IndexOutOfBoundsException in RemoveDynamicPruningBySize",
            "Description": "An IndexOutOfBoundsException occurs when running the script attached to this bug report. The error indicates that an attempt was made to access an index in an empty list.",
            "StackTrace": [
                "2015-04-29 10:34:36,390 ERROR [main]: ql.Driver (SessionState.java:printError(956)) - FAILED: IndexOutOfBoundsException Index: 0, Size: 0",
                "java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process(RemoveDynamicPruningBySize.java:61)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:77)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:281)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:123)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10092)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9932)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1026)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1000)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:139)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_q85(TestMiniTezCliDriver.java:123)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)"
            ],
            "StepsToReproduce": [
                "Run the attached script.",
                "Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The script should execute without throwing an IndexOutOfBoundsException.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown indicating an attempt to access an index in an empty list.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9721.json",
        "creation_time": "2015-02-19T06:56:17.000+0000",
        "bug_report": {
            "BugID": "HIVE-9721",
            "Title": "NullPointerException in Hadoop23Shims.setFullFileStatus due to ACL settings",
            "Description": "The method Hadoop23Shims.setFullFileStatus is throwing a NullPointerException when attempting to set the full file status on a RawLocalFileSystem, which does not support ACLs. This occurs despite the configuration indicating that ACLs are enabled.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus",
                "at org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)",
                "at org.apache.hadoop.fs.FilterFileSystem.getAclStatus(FilterFileSystem.java:562)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:645)",
                "at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:524)",
                "at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)",
                "at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Set up a Hive environment with ACLs enabled.",
                "Attempt to create a staging directory using the Hive context.",
                "Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The staging directory should be created without any exceptions, regardless of the file system's ACL support.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to set the full file status on a RawLocalFileSystem.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-4216.json",
        "creation_time": "2013-03-21T20:53:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-4216",
            "Title": "TestHBaseMinimrCliDriver Fails with NPE in Reducer Phase after Upgrading to HBase 0.94.5 and Hadoop 23",
            "Description": "After upgrading to Hadoop 23 and HBase 0.94.5, the TestHBaseMinimrCliDriver fails during execution. The test hangs indefinitely after the reducer phase of the second query fails, despite the configuration settings being applied correctly. The error indicates a NullPointerException occurring in the Hive execution process.",
            "StackTrace": [
                "13-03-20 16:26:48,942 FATAL [IPC Server handler 17 on 55996] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1363821864968_0003_r_000002_0 - exited : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:268)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:448)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:399)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:157)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1212)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:152)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:237)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:477)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:525)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)",
                "... 7 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.TaskID$CharTaskTypeMaps.getRepresentingCharacter(TaskID.java:265)",
                "at org.apache.hadoop.mapreduce.TaskID.appendTo(TaskID.java:153)",
                "at org.apache.hadoop.mapreduce.TaskAttemptID.appendTo(TaskAttemptID.java:119)",
                "at org.apache.hadoop.mapreduce.TaskAttemptID.toString(TaskAttemptID.java:151)",
                "at java.lang.String.valueOf(String.java:282)",
                "at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209)",
                "at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:69)",
                "at org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.getRecordWriter(HFileOutputFormat.java:90)",
                "at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getFileWriter(HiveHFileOutputFormat.java:67)",
                "at org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.getHiveRecordWriter(HiveHFileOutputFormat.java:104)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:246)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:234)",
                "... 14 more"
            ],
            "StepsToReproduce": [
                "Upgrade to Hadoop 23 and HBase 0.94.5.",
                "Update 'hbase_bulk.m' with the following properties:",
                "set mapreduce.totalorderpartitioner.naturalorder=false;",
                "set mapreduce.totalorderpartitioner.path=/tmp/hbpartition.lst;",
                "Run the TestHBaseMinimrCliDriver."
            ],
            "ExpectedBehavior": "The TestHBaseMinimrCliDriver should complete successfully without errors.",
            "ObservedBehavior": "The test fails during the reducer phase with a NullPointerException, causing the process to hang indefinitely.",
            "Resolution": "[Provide additional details about the resolution if available]"
        }
    },
    {
        "filename": "HIVE-13836.json",
        "creation_time": "2016-05-24T22:37:59.000+0000",
        "bug_report": {
            "BugID": "HIVE-13836",
            "Title": "Invalid State Error in Hive Metastore During Concurrent DDL Operations",
            "Description": "When using the pyhs2 Python client to create tables and partitions in Hive, an error occurs when multiple connections are made to run DDL queries concurrently. The error message indicates that a transaction has already started, leading to a failure in the operation.",
            "StackTrace": [
                "2016-05-04 17:49:26,226 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-4-thread-194]: HMSHandler Fatal error: Invalid state. Transaction has already started",
                "org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started",
                "at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)",
                "at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)",
                "at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)",
                "at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:463)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7522)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)",
                "at com.sun.proxy.$Proxy10.addNotificationEvent(Unknown Source)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.enqueue(DbNotificationListener.java:261)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onCreateTable(DbNotificationListener.java:123)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1483)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1502)",
                "at sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy14.create_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_table_with_environment_context.getResult(ThriftHiveMetastore.java:9267)"
            ],
            "StepsToReproduce": [
                "1. Use the pyhs2 Python client to connect to Hive.",
                "2. Create multiple threads (e.g., 8) to run DDL queries concurrently.",
                "3. Attempt to create tables or partitions in Hive."
            ],
            "ExpectedBehavior": "The DDL operations should complete successfully without any errors, even when executed concurrently.",
            "ObservedBehavior": "An error occurs stating 'Invalid state. Transaction has already started' when multiple DDL operations are attempted concurrently.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-9873.json",
        "creation_time": "2015-03-05T17:35:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-9873",
            "Title": "IOException in projectionPusher.pushProjectionsAndFilters due to incorrect metadata",
            "Description": "An IOException is thrown when changing column information in projectionPusher.pushProjectionsAndFilters. The error indicates a mismatch in object sizes, leading to an empty read schema and null records during joins.",
            "StackTrace": [
                "2015-02-26 15:56:40,275 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:105)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:224)",
                "... 11 more",
                "Caused by: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:199)",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:52)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)",
                "... 15 more"
            ],
            "StepsToReproduce": [
                "Change the column information in the Hive table.",
                "Call projectionPusher.pushProjectionsAndFilters.",
                "Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The column information should be updated without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating a size mismatch in the object.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-13174.json",
        "creation_time": "2016-02-26T23:34:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-13174",
            "Title": "Excessive Vectorizer Stack Traces in Logs for Binary Columns",
            "Description": "When querying a table with a binary column, the logs are flooded with stack traces related to the Vectorizer. This behavior is undesirable and should be addressed by either changing the log level to debug or suppressing the stack trace output.",
            "StackTrace": [
                "2015-10-12 12:34:23,922 INFO  [main]: physical.Vectorizer (Vectorizer.java:validateExprNodeDesc(1249)) - Failed to vectorize",
                "org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)",
                "at org.apache.hadoop.hive.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "at org.apache.hadoop.hive.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)",
                "at org.apache.hadoop.hive.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)",
                "at org.apache.hadoop.hive.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)",
                "at org.apache.hadoop.hive.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)",
                "at org.apache.hadoop.hive.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)",
                "at org.apache.hadoop.hive.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)",
                "at org.apache.hadoop.hive.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)",
                "at org.apache.hadoop.hive.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)",
                "at org.apache.hadoop.hive.parse.TaskCompiler.compile(TaskCompiler.java:227)",
                "at org.apache.hadoop.hive.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)",
                "at org.apache.hadoop.hive.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)",
                "at org.apache.hadoop.hive.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Create a table with a binary column.",
                "Run a query against the table using Hive.",
                "Check the hs2/client logs for stack traces."
            ],
            "ExpectedBehavior": "The logs should not contain excessive stack traces related to the Vectorizer when querying binary columns.",
            "ObservedBehavior": "The logs are filled with stack traces indicating failures to vectorize binary columns.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-5431.json",
        "creation_time": "2013-10-03T03:35:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-5431",
            "Title": "IllegalArgumentException on Read Operations Due to Missing Property Value",
            "Description": "The recent changes with HIVE-4331 introduced a new key \"hive.passthrough.storagehandler.of\", whose value is set only on storage handler writes, but will not be set on reads. This leads to failures when attempting to read data without a preceding write operation, resulting in an IllegalArgumentException.",
            "StackTrace": [
                "2013-09-30 16:20:01,989 ERROR CliDriver (SessionState.java:printError(419)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Property value must not be null",
                "java.io.IOException: java.lang.IllegalArgumentException: Property value must not be null",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1826)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:380)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)",
                "... 17 more"
            ],
            "StepsToReproduce": [
                "Create a Hive script (.q) that only reads data from an HBase table without any preceding write operations.",
                "Execute the script."
            ],
            "ExpectedBehavior": "The script should execute successfully and return the requested data from the HBase table.",
            "ObservedBehavior": "The script fails with an IllegalArgumentException indicating that the property value must not be null.",
            "Resolution": "Fixed in version 0.12.0"
        }
    },
    {
        "filename": "HIVE-13115.json",
        "creation_time": "2016-02-22T21:43:32.000+0000",
        "bug_report": {
            "BugID": "HIVE-13115",
            "Title": "MetaStore Direct SQL getPartitions Call Fails When Column Schemas Are Null",
            "Description": "The MetaStore throws a NullPointerException (NPE) when attempting to retrieve partitions using Direct SQL if the column schemas for a partition are not set. This issue occurs consistently for every getPartitions call, which results in a fallback to ORM. The root cause is identified as a null column descriptor ID in the SQL query, which is not handled properly in the Direct SQL path.",
            "StackTrace": [
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)",
                "at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Use the MetaStoreClient API to add a new partition without setting column level schemas.",
                "2. Call the getPartitions method on the MetaStore.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The getPartitions call should return the partitions without throwing an exception, regardless of whether column level schemas are set.",
            "ObservedBehavior": "The getPartitions call fails with a NullPointerException when column schemas are null, causing a fallback to ORM.",
            "Resolution": "The issue can be resolved by either enforcing stricter checks in the MetaStoreClient API to prevent creating partitions without column schemas or by making the Direct SQL and ORM code paths consistent in handling null column descriptors."
        }
    },
    {
        "filename": "HIVE-4723.json",
        "creation_time": "2013-06-12T20:37:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-4723",
            "Title": "NullPointerException in DDLSemanticAnalyzer when archiving non-partitioned table",
            "Description": "When attempting to archive a partition on a non-partitioned table, a NullPointerException is thrown due to inadequate error handling in the DDLSemanticAnalyzer. The error message generated is unclear and does not provide sufficient information for debugging.",
            "StackTrace": [
                "2013-06-09 16:36:12,628 ERROR parse.DDLSemanticAnalyzer (DDLSemanticAnalyzer.java:addTablePartsOutputs(2899)) - Got HiveException during obtaining list of partitions",
                "2013-06-09 16:36:12,628 ERROR ql.Driver (SessionState.java:printError(383)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2912)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2877)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(DDLSemanticAnalyzer.java:2730)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:316)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:277)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "1. Attempt to archive a partition on a non-partitioned table using the Hive CLI.",
                "2. Observe the error message generated."
            ],
            "ExpectedBehavior": "The system should provide a clear error message indicating that the table is not partitioned and cannot be archived.",
            "ObservedBehavior": "A NullPointerException is thrown, and the error message is unclear, making it difficult to understand the issue.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-15686.json",
        "creation_time": "2017-01-20T22:29:36.000+0000",
        "bug_report": {
            "BugID": "HIVE-15686",
            "Title": "Remote HDFS Partitions Fail Encryption-Zone Checks",
            "Description": "This issue relates to HIVE-13243, which aimed to fix encryption-zone checks for external tables. However, the implementation fails for partitions with remote HDFS paths, resulting in an error when attempting to process messages related to these partitions.",
            "StackTrace": [
                "2015-12-09 19:26:14,997 ERROR [pool-4-thread-1476] server.TThreadPoolServer (TThreadPoolServer.java:run_aroundBody0(305)) - Error occurred during processing of message.",
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://remote-cluster-nn1.myth.net:8020/dbs/mythdb/myth_table/dt=20170120, expected: hdfs://local-cluster-n1.myth.net:8020",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1985)",
                "at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1290)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.checkTrashPurgeCombination(HiveMetaStore.java:1746)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2974)",
                "at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy5.drop_partitions_req(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:10005)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:9989)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:767)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:763)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:763)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:285)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive environment with remote HDFS paths.",
                "2. Create a partitioned table in Hive.",
                "3. Attempt to drop a partition that has a remote HDFS path."
            ],
            "ExpectedBehavior": "The system should successfully check the encryption zone and drop the specified partition without errors.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a mismatch in the expected and actual file system paths.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-4975.json",
        "creation_time": "2013-08-01T16:21:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-4975",
            "Title": "ORC File Read Failure After Adding New Column",
            "Description": "An exception occurs when attempting to read an ORC file after adding a new column to the table. The issue arises specifically when executing a HiveQL query to select the newly added column.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating d",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:80)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:654)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.evaluate(ExprNodeColumnEvaluator.java:98)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:76)",
                "2013-08-01 23:34:22,883 INFO org.apache.hadoop.mapred.Task: Running cleanup for the task"
            ],
            "StepsToReproduce": [
                "Create a table with three columns: a (string), b (string), c (string).",
                "Execute the command: ALTER TABLE table ADD COLUMNS (d string).",
                "Run the HiveQL query: SELECT d FROM table."
            ],
            "ExpectedBehavior": "The query should return the newly added column 'd' without any exceptions.",
            "ObservedBehavior": "An exception is thrown indicating a Hive Runtime Error while processing the row due to an ArrayIndexOutOfBoundsException.",
            "Resolution": "Fixed in version 0.13.0"
        }
    },
    {
        "filename": "HIVE-10538.json",
        "creation_time": "2015-04-29T20:06:38.000+0000",
        "bug_report": {
            "BugID": "HIVE-10538",
            "Title": "Null Pointer Exception in FileSinkOperator with Bucketed Tables and MultiFileSpray Enabled",
            "Description": "A Null Pointer Exception occurs in the FileSinkOperator when using bucketed tables and the distribute by clause with multiFileSpray enabled. This issue arises during the execution of a specific Hive query.",
            "StackTrace": [
                "2015-04-29 12:54:12,841 FATAL [pool-110-thread-1]: ExecReducer (ExecReducer.java:reduce(255)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"113\",\"_col1\":\"val_113\"}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)",
                "... 8 more"
            ],
            "StepsToReproduce": [
                "Set hive.enforce.bucketing = true.",
                "Set hive.exec.reducers.max = 20.",
                "Create table bucket_a with the following command: create table bucket_a(key int, value_a string) clustered by (key) into 256 buckets;",
                "Create table bucket_b with the following command: create table bucket_b(key int, value_b string) clustered by (key) into 256 buckets;",
                "Create table bucket_ab with the following command: create table bucket_ab(key int, value_a string, value_b string) clustered by (key) into 256 buckets;",
                "Insert data into bucket_a and bucket_b.",
                "Execute the following query: insert overwrite table bucket_ab select a.key, a.value_a, b.value_b from bucket_a a join bucket_b b on (a.key = b.key) distribute by key;"
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing a Null Pointer Exception.",
            "ObservedBehavior": "A Null Pointer Exception is thrown during the execution of the query, causing it to fail.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-11902.json",
        "creation_time": "2015-09-21T16:12:37.000+0000",
        "bug_report": {
            "BugID": "HIVE-11902",
            "Title": "DeadTxnReaper Throws MySQLSyntaxErrorException on Empty Transaction List",
            "Description": "When cleaning leftover transactions, the DeadTxnReaper code throws a MySQLSyntaxErrorException due to an empty transaction ID list, resulting in an invalid SQL query.",
            "StackTrace": [
                "2015-09-21 05:23:38,148 WARN  [DeadTxnReaper-0]: txn.TxnHandler (TxnHandler.java:performTimeOuts(1876)) - Aborting timedout transactions failed due to You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1(SQLState=42000,ErrorCode=1064)",
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:360)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)",
                "at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)",
                "at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the DeadTxnReaper process.",
                "2. Ensure there are no active transactions.",
                "3. Wait for the DeadTxnReaper to attempt to clean up timed-out transactions."
            ],
            "ExpectedBehavior": "The DeadTxnReaper should handle empty transaction lists gracefully without throwing an exception.",
            "ObservedBehavior": "The DeadTxnReaper throws a MySQLSyntaxErrorException due to an invalid SQL query generated when the transaction ID list is empty.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-18918.json",
        "creation_time": "2018-03-09T00:47:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-18918",
            "Title": "Inadequate Error Message in CompactorMR.launchCompactionJob() During Major Compaction",
            "Description": "The error message produced during a major compaction in the CompactorMR class is not informative enough, making it difficult to diagnose issues. The current implementation throws a generic IOException without providing specific details about the failure.",
            "StackTrace": [
                "2018-02-28 00:59:16,416 ERROR [gdpr1-61]: compactor.Worker (Worker.java:run(191)) - Caught exception while trying to compact id:38602, dbname:audit, tableName:COMP_ENTRY_AF_A, partName:partition_dt=2017-04-11, state:^@, type:MAJOR, properties:null, runAs:null, tooManyAborts:false, highestTxnId:0. Marking failed to avoid repeated failures, java.io.IOException: Major",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob(CompactorMR.java:314)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:269)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:172)"
            ],
            "StepsToReproduce": [
                "1. Initiate a major compaction on a Hive table.",
                "2. Monitor the logs for any errors during the compaction process.",
                "3. Observe the error message generated when the compaction fails."
            ],
            "ExpectedBehavior": "The error message should provide detailed information about the failure, including the job ID, table name, and specific reasons for the failure.",
            "ObservedBehavior": "The error message is generic and does not provide useful information to diagnose the issue, only stating that the major compaction failed.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-8107.json",
        "creation_time": "2014-09-15T19:49:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-8107",
            "Title": "Inadequate Error Message for Non-Existent Table in Update/Delete Queries",
            "Description": "When executing an update query on a non-existent table, the error message returned is unclear and does not prioritize the relevant information. Instead of clearly stating that the table does not exist, the error is buried within a stack trace, making it difficult for users to understand the issue.",
            "StackTrace": [
                "2014-09-12 19:45:00,138 ERROR [main]: ql.Driver (SessionState.java:printError(824)) - FAILED: SemanticException [Error 10290]: Encountered parse error while parsing rewritten update or delete query",
                "org.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:97)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:66)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:217)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:406)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:302)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1051)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1121)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:988)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:978)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:441)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:457)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found no_such_table",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:978)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:128)",
                "... 24 more"
            ],
            "StepsToReproduce": [
                "Execute the following SQL command: 'update no_such_table set x = 3;'",
                "Observe the error message returned."
            ],
            "ExpectedBehavior": "The system should return a clear error message indicating that the specified table does not exist, rather than a lengthy stack trace.",
            "ObservedBehavior": "The system returns a complex error message that includes a stack trace, making it difficult to identify the root cause of the issue.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-1326.json",
        "creation_time": "2010-04-25T20:50:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-1326",
            "Title": "RowContainer uses hard-coded '/tmp/' path for temporary files",
            "Description": "In our production Hadoop environment, the '/tmp/' directory is limited in size, leading to issues when a query uses the RowContainer class, which fills up the '/tmp/' partition. The RowContainer class is incorrectly placing temporary files in the '/tmp/' path instead of utilizing the configured Hadoop temporary path. A patch has been attached to resolve this issue.",
            "StackTrace": [
                "2010-04-25 12:05:28,155 FATAL ExecReducer: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)",
                "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)",
                "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)",
                "\tat java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:346)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunk(FSOutputSummer.java:150)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:132)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:121)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:112)",
                "\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:86)",
                "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)",
                "\tat java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1013)",
                "\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)",
                "\tat org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat$1.write(HiveSequenceFileOutputFormat.java:70)",
                "\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)",
                "\tat org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)",
                "\tat org.apache.hadoop.hive.ql.exec.JoinOperator.processOp(JoinOperator.java:118)",
                "\tat org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:456)",
                "\tat org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:244)",
                "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)",
                "\tat org.apache.hadoop.mapred.Child.main(Child.java:158)",
                "Caused by: java.io.IOException: No space left on device",
                "\tat java.io.FileOutputStream.writeBytes(Native Method)",
                "\tat java.io.FileOutputStream.write(FileOutputStream.java:260)",
                "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:197)",
                "\tat ... 22 more"
            ],
            "StepsToReproduce": [
                "Set up a Hadoop environment with limited space in the '/tmp/' directory.",
                "Run a query that utilizes the RowContainer class.",
                "Monitor the '/tmp/' directory for temporary files created by the RowContainer."
            ],
            "ExpectedBehavior": "The RowContainer should use the configured Hadoop temporary path for storing temporary files, preventing any issues related to insufficient space in the '/tmp/' directory.",
            "ObservedBehavior": "The RowContainer creates temporary files in the '/tmp/' directory, leading to a 'No space left on device' error when the partition fills up.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-11369.json",
        "creation_time": "2015-07-24T16:28:47.000+0000",
        "bug_report": {
            "BugID": "HIVE-11369",
            "Title": "Map Joins Fail in HiveServer2 with JMX Remote Options Enabled",
            "Description": "When the `hive.auto.convert.join` property is set to true, map joins work correctly in the CLI. However, when starting HiveServer2 with JMX remote options enabled, map joins fail. The issue occurs specifically when the following JMX options are included in `hive-env.sh`:\n\n```\n-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.port=8009\n```\n\nRemoving these options allows the map joins to function correctly. The logs indicate an execution failure with exit status 1.",
            "StackTrace": [
                "2015-07-24 17:19:28,499 ERROR [HiveServer2-Handler-Pool: Thread-22]: exec.Task (SessionState.java:printError(921)) - Execution failed with exit status: 1",
                "2015-07-24 17:19:28,600 WARN  [HiveServer2-Handler-Pool: Thread-22]: thrift.ThriftCLIService (ThriftCLIService.java:ExecuteStatement(496)) - Error executing statement:\norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:146)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:173)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:379)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)",
                "at com.sun.proxy.$Proxy23.executeStatement(Unknown Source)"
            ],
            "StepsToReproduce": [
                "Set the property `hive.auto.convert.join` to true.",
                "Add the following JMX options to `hive-env.sh`:\n-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.port=8009",
                "Start HiveServer2.",
                "Attempt to execute a map join query."
            ],
            "ExpectedBehavior": "The map join query should execute successfully without errors.",
            "ObservedBehavior": "The map join query fails with an execution error and exit status 1 when JMX options are enabled.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-9055.json",
        "creation_time": "2014-12-09T19:51:18.000+0000",
        "bug_report": {
            "BugID": "HIVE-9055",
            "Title": "IndexOutOfBoundsException when using Tez with UNION ALL and GROUP BY in Hive",
            "Description": "Executing a Hive query that combines UNION ALL and GROUP BY with the Tez execution engine results in an IndexOutOfBoundsException. This issue does not occur when using the MapReduce execution engine.",
            "StackTrace": [
                "2014-12-09 11:38:48,316 ERROR ql.Driver (SessionState.java:printError(834)) - FAILED: IndexOutOfBoundsException Index: -1, Size: 1",
                "java.lang.IndexOutOfBoundsException: Index: -1, Size: 1",
                "at java.util.LinkedList.checkElementIndex(LinkedList.java:553)",
                "at java.util.LinkedList.get(LinkedList.java:474)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:354)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:420)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1108)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1035)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:151)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:362)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:297)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver:136)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez(TestMiniTezCliDriver:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "StepsToReproduce": [
                "Set the Hive execution engine to Tez: `set hive.execution.engine=tez;`",
                "Execute the following query: `select key from (select key from src union all select key from src) tab group by key union all select key from src;`"
            ],
            "ExpectedBehavior": "The query should execute successfully without any errors.",
            "ObservedBehavior": "The query fails with an IndexOutOfBoundsException.",
            "Resolution": "The issue has been fixed in version 1.1.0."
        }
    },
    {
        "filename": "HIVE-13856.json",
        "creation_time": "2016-05-25T21:50:49.000+0000",
        "bug_report": {
            "BugID": "HIVE-13856",
            "Title": "SQL Syntax Error When Fetching Transaction Batches in Hive Metastore with Oracle DB",
            "Description": "When attempting to fetch transaction batches during ACID streaming against the Hive Metastore using an Oracle database, a SQL syntax error occurs. The error message indicates that the SQL command is not properly ended, which is likely due to the way multiple rows are being inserted into the database.",
            "StackTrace": [
                "2016-05-25 00:43:49,682 INFO  [pool-4-thread-5]: txn.TxnHandler (TxnHandler.java:checkRetryable(1585)) - Non-retryable error: ORA-00933: SQL command not properly ended (SQLState=42000, ErrorCode=933)",
                "2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)",
                "at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)",
                "at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)",
                "at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)",
                "at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)",
                "at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)",
                "at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)",
                "at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)",
                "at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)",
                "at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)",
                "at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)",
                "at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy15.open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer$WorkerProcess.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Set up Hive Metastore with Oracle DB.",
                "Attempt to fetch transaction batches using ACID streaming.",
                "Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The transaction batches should be fetched successfully without any SQL syntax errors.",
            "ObservedBehavior": "An SQL syntax error occurs, indicating that the SQL command is not properly ended.",
            "Resolution": "The issue is likely due to the way multiple rows are being inserted into the Oracle database. Consider using individual inserts or the INSERT ALL syntax as suggested in the documentation."
        }
    },
    {
        "filename": "HIVE-7374.json",
        "creation_time": "2014-07-09T16:02:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-7374",
            "Title": "SHOW COMPACTIONS Fails with Remote Metastore When No Compactions Exist",
            "Description": "When executing the 'show compactions' command in the CLI with a remote metastore and no existing compactions, an error is returned. This issue occurs due to a missing required field in the response from the Thrift API.",
            "StackTrace": [
                "2014-07-09 17:54:10,537 ERROR [pool-3-thread-20]: server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'compacts' is unset! Struct:ShowCompactResponse(compacts:null)",
                "at org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate(ShowCompactResponse.java:310)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Ensure a remote metastore is configured.",
                "2. Ensure there are no existing compactions.",
                "3. Execute the command 'show compactions' in the CLI."
            ],
            "ExpectedBehavior": "The command should return an empty list or a message indicating no compactions exist.",
            "ObservedBehavior": "The command returns an error: 'FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.thrift.transport.TTransportException'.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12206.json",
        "creation_time": "2015-10-17T00:30:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-12206",
            "Title": "ClassNotFoundException during query compilation with Tez and Union query using GenericUDFs",
            "Description": "A ClassNotFoundException is thrown when executing a query that uses a user-defined function (UDF) in Hive with the Tez execution engine. The error occurs during the compilation of the query involving a UNION operation.",
            "StackTrace": [
                "2015-10-16 17:00:55,557 ERROR ql.Driver (SessionState.java:printError(963)) - FAILED: KryoException Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "Serialization trace:",
                "genericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)",
                "colExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "parentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)",
                "org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "Serialization trace:",
                "genericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)",
                "colExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "parentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)",
                "Caused by: java.lang.ClassNotFoundException: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:358)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:270)",
                "at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136)",
                "... 64 more"
            ],
            "StepsToReproduce": [
                "1. Execute the following query without UDF:",
                "   EXPLAIN SELECT * FROM (SELECT key + key FROM src LIMIT 1) a UNION ALL SELECT * FROM (SELECT key + key FROM src LIMIT 1) b;",
                "2. Add the UDF jar using the command: ADD JAR /tmp/udf-2.2.0-snapshot.jar;",
                "3. Create the temporary function: CREATE TEMPORARY FUNCTION myudf AS 'com.aginity.amp.hive.udf.UniqueNumberGenerator';",
                "4. Execute the following query with the UDF:",
                "   EXPLAIN SELECT myudf() FROM (SELECT key FROM src LIMIT 1) a UNION ALL SELECT myudf() FROM (SELECT key FROM src LIMIT 1) a;"
            ],
            "ExpectedBehavior": "The query should compile successfully and return the expected execution plan without any errors.",
            "ObservedBehavior": "A ClassNotFoundException is thrown indicating that the UDF class 'com.aginity.amp.hive.udf.UniqueNumberGenerator' cannot be found.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-10098.json",
        "creation_time": "2015-03-26T17:12:20.000+0000",
        "bug_report": {
            "BugID": "HIVE-10098",
            "Title": "MapJoin Fails with UndeclaredThrowableException in KMS Encrypted Cluster",
            "Description": "When executing a Hive query via Beeline that performs a MapJoin on a KMS encrypted cluster, the operation fails with a java.lang.reflect.UndeclaredThrowableException. This issue arises after enabling KMS on a Kerberos secured cluster.",
            "StackTrace": [
                "2015-03-18 08:49:19,048 WARN [main]: security.UserGroupInformation (UserGroupInformation.java:doAs(1645)) - PriviledgedActionException as:hive (auth:KERBEROS) cause:org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "2015-03-18 08:49:19,050 ERROR [main]: mr.MapredLocalTask (MapredLocalTask.java:executeFromChildJVM(314)) - Hive Runtime Error: Map local work failed",
                "java.io.IOException: java.io.IOException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:634)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:363)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:337)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:303)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:735)",
                "Caused by: java.io.IOException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:826)",
                "Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)"
            ],
            "StepsToReproduce": [
                "Enable KMS on a Kerberos secured cluster.",
                "Create two tables: a small table and a large table.",
                "Load data into both tables.",
                "Execute the following Hive query: SELECT COUNT(*) FROM jsmall small JOIN jbig1 big ON (small.code = big.code);"
            ],
            "ExpectedBehavior": "The Hive query should execute successfully and return the correct count of joined records.",
            "ObservedBehavior": "The Hive query fails with a java.lang.reflect.UndeclaredThrowableException due to authentication issues with KMS.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7745.json",
        "creation_time": "2014-08-16T01:20:06.000+0000",
        "bug_report": {
            "BugID": "HIVE-7745",
            "Title": "NullPointerException when hive.optimize.union.remove, hive.merge.mapfiles, and hive.merge.mapredfiles are enabled",
            "Description": "When the configuration options hive.optimize.union.remove, hive.merge.mapfiles, and hive.merge.mapredfiles are enabled, executing specific queries results in a NullPointerException. Disabling these options allows the queries to execute without errors.",
            "StackTrace": [
                "2014-08-16 01:32:26,849 ERROR [main]: ql.Driver (SessionState.java:printError(681)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(GenMapRedUtils.java:1738)",
                "    at org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.processFileSink(GenSparkUtils.java:281)",
                "    at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:187)",
                "    at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:199)",
                "    at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9508)",
                "    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "    at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:414)",
                "    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)",
                "    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1005)",
                "    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1070)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)",
                "    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "    at java.lang.reflect.Method.invoke(Method.java:597)",
                "    at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "StepsToReproduce": [
                "Set the following Hive configuration options to true: hive.optimize.union.remove, hive.merge.mapfiles, hive.merge.mapredfiles.",
                "Execute the following queries:",
                "create table inputTbl1(key string, val string) stored as textfile;",
                "create table outputTbl1(key string, values bigint) stored as rcfile;",
                "load data local inpath '../../data/files/T1.txt' into table inputTbl1;",
                "explain insert overwrite table outputTbl1 SELECT * FROM (select key, count(1) as values from inputTbl1 group by key union all select * FROM (SELECT key, 1 as values from inputTbl1 UNION ALL SELECT key, 2 as values from inputTbl1) a) b;"
            ],
            "ExpectedBehavior": "The queries should execute successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the queries when the specified configuration options are enabled.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11762.json",
        "creation_time": "2015-09-08T20:10:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-11762",
            "Title": "TestHCatLoaderEncryption Fails with NoSuchMethodError in Hadoop 2.7",
            "Description": "When running TestHCatLoaderEncryption with -Dhadoop23.version=2.7.0, a NoSuchMethodError occurs during the setup phase. This issue arises due to a change in the method signature of DFSClient.setKeyProvider() between Hadoop versions 2.6 and 2.7.",
            "StackTrace": [
                "java.lang.NoSuchMethodError: org.apache.hadoop.hdfs.DFSClient.setKeyProvider(Lorg/apache/hadoop/crypto/key/KeyProviderCryptoExtension;)V",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Hadoop23Shims.java:534)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.initEncryptionShim(TestHCatLoaderEncryption.java:252)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.setup(TestHCatLoaderEncryption.java:200)"
            ],
            "StepsToReproduce": [
                "1. Set up the environment with Hadoop version 2.7.0.",
                "2. Run the command: `-Dhadoop23.version=2.7.0`.",
                "3. Execute the TestHCatLoaderEncryption test."
            ],
            "ExpectedBehavior": "The TestHCatLoaderEncryption test should run successfully without any errors.",
            "ObservedBehavior": "The test fails with a NoSuchMethodError indicating that the method setKeyProvider() cannot be found.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-6990.json",
        "creation_time": "2014-04-30T04:24:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-6990",
            "Title": "Direct SQL Fails with Non-Default Schema in Hive",
            "Description": "An error occurs when executing SQL queries in Hive with an explicit schema setting that differs from the default. The error message indicates a failure in direct SQL execution, leading to a fallback to ORM.",
            "StackTrace": [
                "2014-04-23 17:30:23,331 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(1756)) - Direct SQL failed, falling back to ORM",
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select PARTITIONS.PART_ID from PARTITIONS inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:98)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1806)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:94)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)",
                "at java.lang.reflect.Method.invoke(Method.java:619)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)",
                "at com.sun.proxy.$Proxy11.getPartitionsByFilter(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:3310)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:103)",
                "at com.sun.proxy.$Proxy12.get_partitions_by_filter(Unknown Source)"
            ],
            "StepsToReproduce": [
                "Set the following properties in hive-site.xml:",
                "<property>",
                "  <name>javax.jdo.mapping.Schema</name>",
                "  <value>HIVE</value>",
                "</property>",
                "<property>",
                "  <name>javax.jdo.option.ConnectionUserName</name>",
                "  <value>user1</value>",
                "</property>",
                "Execute the following Hive queries:",
                "hive> create table mytbl ( key int, value string);",
                "hive> load data local inpath 'examples/files/kv1.txt' overwrite into table mytbl;",
                "hive> select * from mytbl;",
                "hive> create view myview partitioned on (value) as select key, value from mytbl where key=98;",
                "hive> alter view myview add partition (value='val_98');",
                "hive> alter view myview add partition (value='val_xyz');",
                "hive> alter view myview drop partition (value='val_xyz');"
            ],
            "ExpectedBehavior": "The SQL queries should execute successfully without falling back to ORM, and the expected results should be returned.",
            "ObservedBehavior": "An error occurs indicating that direct SQL execution failed, resulting in a fallback to ORM.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7114.json",
        "creation_time": "2014-05-22T14:58:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-7114",
            "Title": "Extra Tez Session Launched During HiveServer2 Startup",
            "Description": "When starting the HiveServer2, an extra Tez Application Master (AM) is launched unexpectedly. This behavior may lead to resource inefficiencies and potential conflicts in session management.",
            "StackTrace": [
                "2014-05-09 23:11:22,261 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:addAdminUsers(588)) - No user is added in admin role, since config is empty",
                "java.lang.Exception: Opening session",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)",
                "at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)",
                "at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)",
                "at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "Start the HiveServer2 service.",
                "Monitor the logs for any Tez session initialization messages."
            ],
            "ExpectedBehavior": "Only one Tez Application Master should be launched during the HiveServer2 startup.",
            "ObservedBehavior": "An extra Tez Application Master is launched, leading to potential resource conflicts.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11991.json",
        "creation_time": "2015-09-29T21:46:06.000+0000",
        "bug_report": {
            "BugID": "HIVE-11991",
            "Title": "ClassCastException in groupby11.q on branch-1.0",
            "Description": "Running the query 'groupby11.q' on the 'branch-1.0' branch results in a ClassCastException, indicating that a String cannot be cast to org.apache.hadoop.io.Text. This issue occurs during the execution of the FetchTask.",
            "StackTrace": [
                "2015-09-29 14:27:51,676 ERROR CliDriver (SessionState.java:printError(833)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1621)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:221)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:153)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:364)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:299)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:832)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11(TestCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:85)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:572)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:564)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "... 27 more",
                "Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:225)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:492)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:445)",
                "at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.doSerialize(LazySimpleSerDe.java:429)",
                "at org.apache.hadoop.hive.serde2.AbstractEncodingAwareSerDe.serialize(AbstractEncodingAwareSerDe.java:50)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:71)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:40)",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)",
                "... 34 more"
            ],
            "StepsToReproduce": [
                "1. Checkout the 'branch-1.0' branch.",
                "2. Run the query 'groupby11.q'."
            ],
            "ExpectedBehavior": "The query 'groupby11.q' should execute successfully without any exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that a String cannot be cast to org.apache.hadoop.io.Text.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-17900.json",
        "creation_time": "2017-10-25T17:12:42.000+0000",
        "bug_report": {
            "BugID": "HIVE-17900",
            "Title": "Malformed SQL Generation in Compactor for Multiple Partition Columns",
            "Description": "When analyzing statistics on columns triggered by the Compactor, the system generates malformed SQL statements when there is more than one partition column. This results in a ParseException.",
            "StackTrace": [
                "2017-10-16 09:01:51,255 ERROR [haddl0007.mycenterpointenergy.com-51]: ql.Driver (SessionState.java:printError(993)) - FAILED: ParseException line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement",
                "org.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1158)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1148)",
                "at org.apache.hadoop.hive.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:294)",
                "at org.apache.hadoop.hive.txn.compactor.CompactorMR.run(CompactorMR.java:265)",
                "at org.apache.hadoop.hive.txn.compactor.Worker.run(Worker.java:168)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hive table with multiple partition columns.",
                "2. Trigger the Compactor to analyze statistics on the table.",
                "3. Observe the generated SQL statements in the logs."
            ],
            "ExpectedBehavior": "The Compactor should generate valid SQL statements for analyzing statistics on tables with multiple partition columns.",
            "ObservedBehavior": "The Compactor generates malformed SQL, resulting in a ParseException.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-10816.json",
        "creation_time": "2015-05-25T08:08:37.000+0000",
        "bug_report": {
            "BugID": "HIVE-10816",
            "Title": "NullPointerException in ExecDriver::handleSampling when hive.exec.submitviachild is true",
            "Description": "When the configuration property `hive.exec.submitviachild` is set to true, the parallel order by operation fails with a NullPointerException (NPE) and falls back to single-reducer mode.",
            "StackTrace": [
                "2015-05-25 08:41:04,446 ERROR [main]: mr.ExecDriver (ExecDriver.java:execute(386)) - Sampling error",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)",
                "    at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:497)",
                "    at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "    at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Set the configuration property `hive.exec.submitviachild` to true.",
                "Execute a query that involves parallel order by."
            ],
            "ExpectedBehavior": "The query should execute successfully in parallel without falling back to single-reducer mode.",
            "ObservedBehavior": "The query fails with a NullPointerException and falls back to single-reducer mode.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-13017.json",
        "creation_time": "2016-02-05T23:40:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-13017",
            "Title": "Hive query execution fails with exit status 2 when using temporary tables in secured cluster",
            "Description": "When executing a Hive query that involves temporary tables while using Azure Filesystem as the default file system, the query fails with an exit status of 2. This issue occurs specifically when the query attempts to join multiple tables and filter based on age criteria.",
            "StackTrace": [
                "ERROR : Execution failed with exit status: 2",
                "ERROR : Obtaining error information",
                "ERROR : Task failed! Task ID: Stage-5",
                "ERROR : /var/log/hive/hiveServer2.log",
                "ERROR : Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask (state=08S01,code=2)",
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask"
            ],
            "StepsToReproduce": [
                "Set Azure Filesystem as the default file system.",
                "Create a temporary table using the following command: create temporary table s10k stored as orc as select * from studenttab10k;",
                "Create another temporary table: create temporary table v10k as select * from votertab10k;",
                "Execute the following query: select registration from s10k s join v10k v on (s.name = v.name) join studentparttab30k p on (p.name = v.name) where s.age < 25 and v.age < 25 and p.age < 25;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors.",
            "ObservedBehavior": "The query fails with an exit status of 2, indicating an execution error. The logs show that the task failed and provides an error message related to the MapredLocalTask.",
            "Resolution": "[Provide additional details about the resolution or fix applied]"
        }
    },
    {
        "filename": "HIVE-11303.json",
        "creation_time": "2015-07-18T01:31:02.000+0000",
        "bug_report": {
            "BugID": "HIVE-11303",
            "Title": "LimitExceededException in Tez Client When Executing Large DAG Queries",
            "Description": "The Tez client throws a LimitExceededException when executing a Directed Acyclic Graph (DAG) that exceeds the maximum allowed counters. This issue occurs during the execution of large queries, leading to failure in processing.",
            "StackTrace": [
                "2015-07-17 18:18:11,830 INFO  [main]: counters.Limits (Limits.java:ensureInitialized(59)) - Counter limits initialized with parameters:  GROUP_NAME_MAX=256, MAX_GROUPS=500, COUNTER_NAME_MAX=64, MAX_COUNTERS=1200",
                "2015-07-17 18:18:11,841 ERROR [main]: exec.Task (TezTask.java:execute(189)) - Failed to execute tez graph.",
                "org.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200",
                "        at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)",
                "        at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)",
                "        at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)",
                "        at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)",
                "        at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)",
                "        at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)",
                "        at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)",
                "        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)",
                "        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)",
                "        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)",
                "        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)",
                "        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)",
                "        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "        at java.lang.reflect.Method.invoke(Method.java:497)",
                "        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "1. Set up a Tez environment with the necessary configurations.",
                "2. Create a large query that is expected to generate more than 1200 counters.",
                "3. Execute the query using the Tez client.",
                "4. Observe the logs for any LimitExceededException errors."
            ],
            "ExpectedBehavior": "The Tez client should execute the DAG without exceeding the maximum counter limits, or provide a clear error message indicating the limit.",
            "ObservedBehavior": "The Tez client fails to execute the DAG and throws a LimitExceededException due to exceeding the maximum allowed counters.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-5899.json",
        "creation_time": "2013-11-27T02:48:39.000+0000",
        "bug_report": {
            "BugID": "HIVE-5899",
            "Title": "NullPointerException during Explain Extended with Char/Varchar Columns",
            "Description": "When running 'analyze table' for columns with char/varchar types and subsequently executing 'explain extended', a NullPointerException occurs. This happens when Hive attempts to annotate the operator tree with statistics.",
            "StackTrace": [
                "2013-11-26 01:53:06,682 ERROR ql.Driver (SessionState.java:printError(440)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:367)",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats(ColumnStatisticsData.java:444)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:414)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStatsForColumn(StatsUtils.java:369)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:465)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:102)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)",
                "at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:76)",
                "at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:136)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8913)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:341)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:994)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:905)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "1. Create a table with char/varchar columns in Hive.",
                "2. Run the command 'analyze table <table_name> compute statistics'.",
                "3. Execute 'explain extended <query>' on the table."
            ],
            "ExpectedBehavior": "The explain command should execute successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the explain command.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11102.json",
        "creation_time": "2015-06-24T22:54:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-11102",
            "Title": "IndexOutOfBoundsException in OrcReaderImpl when estimating ACID data file size",
            "Description": "The ORC reader implementation does not correctly estimate the size of ACID data files, leading to an IndexOutOfBoundsException. This issue occurs when attempting to access subtypes of an empty list.",
            "StackTrace": [
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:938)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:847)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:713)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Create an ACID table in Hive.",
                "2. Insert data into the table.",
                "3. Attempt to read the data using the ORC format.",
                "4. Observe the error in the logs."
            ],
            "ExpectedBehavior": "The ORC reader should correctly estimate the size of ACID data files without throwing an exception.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when accessing subtypes of an empty list.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-9195.json",
        "creation_time": "2014-12-23T01:08:45.000+0000",
        "bug_report": {
            "BugID": "HIVE-9195",
            "Title": "CBO Changes Constant Expression to Column Expression in TestCliDriver",
            "Description": "While creating a test case for HIVE-8613, it was discovered that the Cost-Based Optimizer (CBO) changes constant expressions to column expressions, leading to an argument type exception in TestCliDriver. The issue occurs specifically in test mode when executing a query that uses the `percentile_approx` function with a constant argument. The error message indicates that a constant was expected, but a double was passed instead.",
            "StackTrace": [
                "2014-12-22 17:03:31,433 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:analyzeInternal(10102)) - CBO failed, skipping CBO.",
                "org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:146)",
                "at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:1160)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:3794)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:4467)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5536)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8884)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9745)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9638)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10086)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:877)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23(TestCliDriver.java:120)"
            ],
            "StepsToReproduce": [
                "Create a table with the following SQL command: CREATE TABLE bucket (key double, value string) CLUSTERED BY (key) SORTED BY (key DESC) INTO 4 BUCKETS STORED AS TEXTFILE;",
                "Load data into the table using the following commands:",
                "load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket;",
                "load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket;",
                "load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket;",
                "load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket;",
                "Execute the following query: select percentile_approx(case when key < 100 then cast('NaN' as double) else key end, 0.5) from bucket;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected percentile value without any errors.",
            "ObservedBehavior": "The query fails with an argument type exception indicating that a constant was expected, but a double was passed instead.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11285.json",
        "creation_time": "2015-07-16T20:46:22.000+0000",
        "bug_report": {
            "BugID": "HIVE-11285",
            "Title": "ClassCastException in FetchOperator during SMBJoin due to ObjectInspector",
            "Description": "The issue occurs when executing a query that involves a join operation on partitioned tables. A ClassCastException is thrown, indicating that an IntWritable cannot be cast to an Integer. This happens during the processing of rows in the FetchOperator of the SMBJoin.",
            "StackTrace": [
                "2015-07-15 13:39:04,333 WARN main org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)",
                "... 8 more",
                "Caused by: java.lang.RuntimeException: Map local work failed",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:569)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchNextGroup(SMBMapJoinOperator.java:429)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:260)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 9 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Integer",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(JavaIntObjectInspector.java:35)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:305)",
                "at org.apache.hadoop.hive.ql.exec.JoinUtil.computeValues(JoinUtil.java:193)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getFilteredValue(CommonJoinOperator.java:408)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:270)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:558)",
                "... 17 more"
            ],
            "StepsToReproduce": [
                "Create a file named 'data.out' with the following content:",
                "1|One",
                "2|Two",
                "Execute the following Hive commands:",
                "CREATE TABLE data_table (key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';",
                "LOAD DATA LOCAL INPATH '${system:user.dir}/data.out' INTO TABLE data_table;",
                "CREATE TABLE smb_table (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS ORC;",
                "CREATE TABLE smb_table_part (key INT, value STRING) PARTITIONED BY (p1 DECIMAL) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS ORC;",
                "INSERT OVERWRITE TABLE smb_table SELECT * FROM data_table;",
                "INSERT OVERWRITE TABLE smb_table_part PARTITION (p1) SELECT key, value, 100 as p1 FROM data_table;",
                "Set Hive execution properties as needed.",
                "Run the following query to trigger the error:",
                "SELECT s1.key, s2.p1 FROM smb_table s1 INNER JOIN smb_table_part s2 ON s1.key = s2.key ORDER BY s1.key;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results from the join operation without any exceptions.",
            "ObservedBehavior": "A ClassCastException is thrown during the execution of the join query, indicating a type mismatch in the processing of rows.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-10288.json",
        "creation_time": "2015-04-09T23:24:16.000+0000",
        "bug_report": {
            "BugID": "HIVE-10288",
            "Title": "NullPointerException when calling permanent UDFs after exiting Hive CLI",
            "Description": "After creating a permanent UDF and exiting the Hive CLI, attempting to call the UDF results in a NullPointerException. This issue does not occur if the UDF is called immediately after registration, nor does it occur in the apache-hive-1.0.0 release.",
            "StackTrace": [
                "15-04-13 17:04:54,435 ERROR org.apache.hadoop.hive.ql.Driver (SessionState.java:printError(958)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:232)",
                "\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1048)",
                "\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1265)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "\tat org.apache.hadoop.hive.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:205)",
                "\tat org.apache.hadoop.hive.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:149)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10383)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10338)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3815)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3594)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8864)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8819)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9663)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9556)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:9992)",
                "\tat org.apache.hadoop.hive.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:306)",
                "\tat org.apache.hadoop.hive.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10003)",
                "\tat org.apache.hadoop.hive.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:195)",
                "\tat org.apache.hadoop.hive.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "\tat org.apache.hadoop.hive.Driver.compile(Driver.java:424)",
                "\tat org.apache.hadoop.hive.Driver.compile(Driver.java:308)",
                "\tat org.apache.hadoop.hive.Driver.compileInternal(Driver.java:1122)",
                "\tat org.apache.hadoop.hive.Driver.runInternal(Driver.java:1170)",
                "\tat org.apache.hadoop.hive.Driver.run(Driver.java:1059)",
                "\tat org.apache.hadoop.hive.Driver.run(Driver.java:1049)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "\tat java.lang.reflect.Method.invoke(Method.java:483)",
                "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "StepsToReproduce": [
                "1. Pull the latest trunk and build the Hive binary.",
                "2. Create a permanent UDF in the Hive CLI.",
                "3. Exit the Hive CLI.",
                "4. Reopen the Hive CLI.",
                "5. Attempt to call the permanent UDF."
            ],
            "ExpectedBehavior": "The permanent UDF should be callable without any exceptions after reopening the Hive CLI.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to call the permanent UDF after exiting and reopening the Hive CLI.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-8771.json",
        "creation_time": "2014-11-07T00:30:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-8771",
            "Title": "AbstractFileMergeOperator Fails to Move Incompatible Files on CentOS",
            "Description": "The AbstractFileMergeOperator is designed to move incompatible files to a final destination. However, it fails when the destination path is a file instead of a directory, leading to an IOException. This issue occurs specifically when running the orc_merge_incompat2.q test under CentOS.",
            "StackTrace": [
                "2014-11-05 02:38:56,588 DEBUG fs.FileSystem (RawLocalFileSystem.java:rename(337)) - Falling through to a copy of file:/home/prasanth/hive/itests/qtest/target/warehouse/orc_merge5a/st=80.0/000000_0 to file:/home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0/000000_0",
                "2014-11-05 02:38:56,589 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.",
                "2014-11-05 02:38:56,590 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local1144733438_0036",
                "java.lang.Exception: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)",
                "Caused by: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:100)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:679)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:233)",
                "at org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.closeOp(OrcFileMergeOperator.java:220)",
                "at org.apache.hadoop.hive.ql.io.merge.MergeFileMapper.close(MergeFileMapper.java:98)",
                "... 10 more",
                "Caused by: java.io.FileNotFoundException: Destination exists and is not a directory: /home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0",
                "at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:423)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:267)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:257)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:784)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:339)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:507)",
                "at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)",
                "at org.apache.hadoop.fs.ProxyFileSystem.rename(ProxyFileSystem.java:177)",
                "at org.apache.hadoop.fs.FilterFileSystem.rename(FilterFileSystem.java:214)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.renameOrMoveFiles(Utilities.java:1589)",
                "at org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp(AbstractFileMergeOperator.java:218)",
                "... 12 more"
            ],
            "StepsToReproduce": [
                "Run the orc_merge_incompat2.q test on a CentOS environment.",
                "Ensure that the destination path for merging is a file instead of a directory.",
                "Observe the resulting IOException."
            ],
            "ExpectedBehavior": "The AbstractFileMergeOperator should successfully move incompatible files to the specified directory without errors.",
            "ObservedBehavior": "The operator fails with an IOException indicating that the destination exists and is not a directory.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-8008.json",
        "creation_time": "2014-09-05T23:00:11.000+0000",
        "bug_report": {
            "BugID": "HIVE-8008",
            "Title": "NullPointerException when querying decimal values in Hive",
            "Description": "When querying a table with a decimal value of 9999999999.5, Hive crashes with a NullPointerException (NPE). This issue occurs specifically when the queried decimal value is null.",
            "StackTrace": [
                "2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)",
                "... 12 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "Create a table named 'dec_test' with a decimal column.",
                "Insert a row with a decimal value of 9999999999.5 into the table.",
                "Execute the query 'SELECT * FROM dec_test;'"
            ],
            "ExpectedBehavior": "The query should return the row with the decimal value without any errors.",
            "ObservedBehavior": "The query crashes Hive with a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-6915.json",
        "creation_time": "2014-04-15T20:20:15.000+0000",
        "bug_report": {
            "BugID": "HIVE-6915",
            "Title": "Hive HBase Queries Fail on Secure Tez Cluster Due to SASL Authentication Error",
            "Description": "Hive queries reading and writing to HBase are currently failing with a SASL authentication error in a secure Tez cluster. The error indicates that the client is missing or has invalid credentials, suggesting that 'kinit' should be performed. The issue appears in the Tez application logs, and the queries work fine when changing the execution engine to MapReduce (MR).",
            "StackTrace": [
                "2014-04-14 13:47:05,644 FATAL [InputInitializer [Map 1] #0] org.apache.hadoop.ipc.RpcClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.",
                "javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)",
                "at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:918)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:915)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:915)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1065)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1032)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1474)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1684)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1737)",
                "at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:29288)",
                "at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1562)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:87)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:84)",
                "at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:121)",
                "at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:97)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)",
                "at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:67)",
                "at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:174)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:172)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)",
                "at org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:334)",
                "at org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:201)",
                "at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:415)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)",
                "at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)",
                "at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)",
                "at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner$InputInitializerCallable.java:154)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner$InputInitializerCallable.java:146)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner$InputInitializerCallable.java:146)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner$InputInitializerCallable.java:114)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)",
                "... 55 more"
            ],
            "StepsToReproduce": [
                "1. Set up a secure Tez cluster with Kerberos authentication.",
                "2. Execute Hive queries that read from and write to HBase.",
                "3. Observe the application logs for authentication errors."
            ],
            "ExpectedBehavior": "Hive queries should execute successfully without authentication errors.",
            "ObservedBehavior": "Hive queries fail with a SASL authentication error indicating missing or invalid credentials.",
            "Resolution": "A fix for this issue has been implemented and is included in version 0.14.0."
        }
    },
    {
        "filename": "HIVE-12364.json",
        "creation_time": "2015-11-07T02:04:25.000+0000",
        "bug_report": {
            "BugID": "HIVE-12364",
            "Title": "Distcp Job Fails When Executed Under Tez Framework",
            "Description": "The issue occurs when executing a Distcp job under the Tez framework, resulting in a failure during the move task. The Hive client log indicates that the job fails due to an inability to move the source file to the destination, with an underlying exception related to the compatibility of the input format class.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Unable to move source hdfs://hdpsecehdfs/tmp/testindir/.hive-staging_hive_2015-11-05_15-59-44_557_1429894387987411483-1/-ext-10000 to destination /tmp/testindir",
                "at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)",
                "at org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(MoveTask.java:105)",
                "at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:222)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "Caused by: java.io.IOException: Cannot execute DistCp process: java.io.IOException: mapreduce.job.inputformat.class is incompatible with map compatibility mode.",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1156)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2647)"
            ],
            "StepsToReproduce": [
                "Set the Hive configuration parameter: set hive.exec.copyfile.maxsize=40000;",
                "Execute the following query: insert overwrite into '/tmp/testinser' select * from customer;",
                "Observe the failure during the move task."
            ],
            "ExpectedBehavior": "The Distcp job should successfully move the files from the source to the destination without errors.",
            "ObservedBehavior": "The Distcp job fails with an error indicating an inability to move the source file to the destination due to compatibility issues.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-8766.json",
        "creation_time": "2014-11-06T22:08:43.000+0000",
        "bug_report": {
            "BugID": "HIVE-8766",
            "Title": "HMS Retrying Mechanism Fails on NucleusException During Metastore Operations",
            "Description": "When performing Metastore operations on a heavily loaded Metastore Database (SQL Server), NucleusExceptions may occur due to connection timeouts. This issue arises when the SQL Server is configured to terminate connections after a specified timeout. The current HMS retry mechanism does not handle these exceptions properly, leading to failed Hive queries. A proposed fix is to allow retries when encountering a NucleusException.",
            "StackTrace": [
                "2014-11-04 06:40:03,208 ERROR bonecp.ConnectionHandle (ConnectionHandle.java:markPossiblyBroken(388)) - Database access problem. Killing off this connection and all remaining connections in the connection pool. SQL State = 08S01",
                "2014-11-04 06:40:03,213 ERROR DataNucleus.Transaction (Log4JLogger.java:error(115)) - Operation rollback failed on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16, error code UNKNOWN and transaction: [DataNucleus Transaction, ID=Xid=   \ufffd, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16]]",
                "2014-11-04 06:40:03,217 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(139)) - MetaException(message:org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5183)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1738)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1699)",
                "at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:101)",
                "at com.sun.proxy.$Proxy11.get_table(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:1091)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:112)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)",
                "at com.sun.proxy.$Proxy12.getTable(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1060)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1015)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableSerde(DDLSemanticAnalyzer.java:1356)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:299)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1061)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:256)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:363)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:396)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:666)",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.size(ElementContainerStore.java:429)",
                "at org.datanucleus.store.types.backed.List.size(List.java:581)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToSkewedValues(ObjectStore.java:1190)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1168)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToStorageDescriptor(ObjectStore.java:1178)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.convertToTable(ObjectStore.java:1035)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTable(ObjectStore.java:893)",
                "at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)",
                "at com.sun.proxy.$Proxy10.getTable(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_core(HiveMetaStore.java:1727)",
                "... 41 more",
                "Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: SSL peer shut down incorrectly",
                "at com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1352)",
                "at com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1339)",
                "at com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:1694)",
                "at com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:3734)",
                "at com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:5062)",
                "at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:388)",
                "at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:340)",
                "at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615)",
                "at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1400)",
                "at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:179)",
                "at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:154)",
                "at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:283)",
                "at com.jolbox.bonecp.PreparedStatementHandle.executeQuery(PreparedStatementHandle.java:174)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeQuery(ParamLoggingPreparedStatement.java:381)",
                "at org.datanucleus.store.rdbms.SQLController.executeStatementQuery(SQLController.java:504)",
                "at org.datanucleus.store.rdbms.scostore.ElementContainerStore.getSize(ElementContainerStore.java:638)"
            ],
            "StepsToReproduce": [
                "1. Configure SQL Server to timeout connections after a specified duration.",
                "2. Execute a Hive query that interacts with the Metastore while the SQL Server is under heavy load.",
                "3. Observe the resulting NucleusException in the logs."
            ],
            "ExpectedBehavior": "The HMS retry mechanism should allow retries when a NucleusException occurs due to a timeout or connection reset.",
            "ObservedBehavior": "The Hive query fails with a NucleusException, and the HMS retry mechanism does not attempt to retry the operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-5857.json",
        "creation_time": "2013-11-20T01:12:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-5857",
            "Title": "NullPointerException in ExecReducer when running reduce tasks in uber mode on YARN",
            "Description": "A Hive query fails when it tries to run a reduce task in uber mode in YARN. The NullPointerException is thrown in the ExecReducer.configure method because the plan file (reduce.xml) for a reduce task is not found. The Utilities.getBaseWork method is expected to return a BaseWork object, but it returns NULL due to FileNotFoundException. This occurs because the configuration is changed to local mode before running a reduce task, while map tasks run successfully in YARN mode.",
            "StackTrace": [
                "2013-11-20 00:50:56,862 INFO [uber-SubtaskRunner] org.apache.hadoop.hive.ql.exec.Utilities: No plan file found: hdfs://namenode.c.lon.spotify.net:54310/var/tmp/kawaa/hive_2013-11-20_00-50-43_888_3938384086824086680-2/-mr-10003/e3caacf6-15d6-4987-b186-d2906791b5b0/reduce.xml",
                "2013-11-20 00:50:56,862 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:427)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.runSubtask(LocalContainerLauncher.java:340)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$SubtaskRunner.run(LocalContainerLauncher.java:225)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 7 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)",
                "... 12 more"
            ],
            "StepsToReproduce": [
                "1. Submit a Hive query that requires a reduce task in uber mode.",
                "2. Monitor the execution of the query in YARN.",
                "3. Observe the logs for NullPointerException in ExecReducer."
            ],
            "ExpectedBehavior": "The reduce task should execute successfully without throwing a NullPointerException.",
            "ObservedBehavior": "The reduce task fails with a NullPointerException due to the absence of the plan file (reduce.xml).",
            "Resolution": "A fix has been implemented to check for the plan file in a different location when running reduce tasks in uber mode."
        }
    },
    {
        "filename": "HIVE-1547.json",
        "creation_time": "2010-08-17T02:09:54.000+0000",
        "bug_report": {
            "BugID": "HIVE-1547",
            "Title": "Null Pointer Exception during Partition Unarchiving",
            "Description": "Unarchiving a partition throws a Null Pointer Exception (NPE) in the Hive Query Processor. This issue appears to be specific to the Distributed File System (DFS) as local file system unit tests do not reproduce the error.",
            "StackTrace": [
                "2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)",
                "    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)",
                "    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)",
                "    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)",
                "    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)",
                "    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)",
                "    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)",
                "    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)",
                "    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "    at java.lang.reflect.Method.invoke(Method.java:597)",
                "    at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "StepsToReproduce": [
                "1. Attempt to unarchive a partition in the Hive Query Processor using DFS.",
                "2. Observe the system logs for any errors."
            ],
            "ExpectedBehavior": "The partition should be unarchived successfully without any exceptions.",
            "ObservedBehavior": "A Null Pointer Exception is thrown, preventing the unarchiving operation from completing.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-6113.json",
        "creation_time": "2013-12-27T07:07:00.000+0000",
        "bug_report": {
            "BugID": "HIVE-6113",
            "Title": "Error Instantiating HiveMetaStoreClient When Executing SQL Command",
            "Description": "When executing the SQL command `use fdm; desc formatted fdm.tableName;` in Python, an error is thrown indicating that the `HiveMetaStoreClient` cannot be instantiated. However, the command succeeds when retried immediately after the failure.",
            "StackTrace": [
                "2013-12-25 03:01:32,290 ERROR exec.DDLTask (DDLTask.java:execute(435)) - org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1143)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1128)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3479)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:237)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:260)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:217)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:507)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:875)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:769)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1217)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2372)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2383)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1139)",
                "... 20 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1210)",
                "... 25 more",
                "Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore",
                "NestedThrowables:",
                "java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:165)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.commitTransaction(ObjectStore.java:358)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:404)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hive.metastore.RetryingRawStore.invoke(RetryingRawStore.java:124)",
                "at $Proxy9.createDatabase(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:422)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:441)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:326)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:286)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4060)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:121)",
                "... 30 more",
                "Caused by: java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'",
                "at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2028)",
                "at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1451)",
                "at com.jolbox.bonecp.StatementHandle.executeBatch(StatementHandle.java:469)",
                "at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:372)",
                "at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:628)",
                "at org.datanucleus.store.rdbms.SQLController.processStatementsForConnection(SQLController.java:596)",
                "at org.datanucleus.store.rdbms.SQLController$1.transactionFlushed(SQLController.java:683)",
                "at org.datanucleus.store.connection.AbstractManagedConnection.transactionFlushed(AbstractManagedConnection.java:86)",
                "at org.datanucleus.store.connection.ConnectionManagerImpl$2.transactionFlushed(ConnectionManagerImpl.java:454)",
                "at org.datanucleus.TransactionImpl.flush(TransactionImpl.java:199)",
                "at org.datanucleus.TransactionImpl.commit(TransactionImpl.java:263)",
                "at org.datanucleus.api.jdo.JDOTransaction.commit(JDOTransaction.java:98)",
                "... 46 more",
                "Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:386)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3609)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3541)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2002)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2163)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2624)",
                "at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2127)",
                "at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2427)",
                "at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1980)",
                "... 57 more"
            ],
            "StepsToReproduce": [
                "1. Execute the SQL command `use fdm; desc formatted fdm.tableName;` in a Python environment.",
                "2. Observe the error thrown regarding the instantiation of `HiveMetaStoreClient`.",
                "3. Retry the same command immediately after the failure."
            ],
            "ExpectedBehavior": "The SQL command should execute successfully without errors.",
            "ObservedBehavior": "The command fails with an error about the `HiveMetaStoreClient` instantiation, but succeeds when retried immediately.",
            "Resolution": "The issue was resolved in Hive version 2.0.0."
        }
    },
    {
        "filename": "HIVE-9570.json",
        "creation_time": "2015-02-03T23:30:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-9570",
            "Title": "NullPointerException in union_view.q on Spark Branch",
            "Description": "The query 'union_view.q' fails with a NullPointerException when executed on the Spark branch of Hive. This issue needs to be investigated to identify the root cause and implement a fix.",
            "StackTrace": [
                "2015-02-03 15:27:05,723 ERROR [main]: ql.Driver (SessionState.java:printError(861)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:274)",
                "  at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:253)",
                "  at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:222)",
                "  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10231)",
                "  at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "  at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "  at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1160)",
                "  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1039)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "  at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1019)",
                "  at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:993)",
                "  at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:136)",
                "  at org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_view(TestSparkCliDriver.java:120)",
                "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "  at java.lang.reflect.Method.invoke(Method.java:606)",
                "  at junit.framework.TestCase.runTest(TestCase.java:176)",
                "  at junit.framework.TestCase.runBare(TestCase.java:141)",
                "  at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "  at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "  at junit.framework.TestResult.run(TestResult.java:125)",
                "  at junit.framework.TestCase.run(TestCase.java:129)",
                "  at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "  at junit.framework.TestSuite.run(TestSuite.java:250)",
                "  at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)"
            ],
            "StepsToReproduce": [
                "1. Checkout the Spark branch of Hive.",
                "2. Execute the query 'union_view.q'.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The query 'union_view.q' should execute successfully without throwing any exceptions.",
            "ObservedBehavior": "The query 'union_view.q' fails with a NullPointerException.",
            "Resolution": "Investigate the cause of the NullPointerException and implement a fix."
        }
    },
    {
        "filename": "HIVE-1678.json",
        "creation_time": "2010-10-01T05:41:21.000+0000",
        "bug_report": {
            "BugID": "HIVE-1678",
            "Title": "NullPointerException in MapJoin with Group By",
            "Description": "The query with two map joins and a group by fails with a NullPointerException, indicating a potential issue in the MapJoinOperator processing logic.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)"
            ],
            "StepsToReproduce": [
                "1. Create a Hive query that includes two map joins.",
                "2. Include a group by clause in the query.",
                "3. Execute the query."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any exceptions.",
            "ObservedBehavior": "The query fails with a NullPointerException, preventing successful execution.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-11820.json",
        "creation_time": "2015-09-14T22:20:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-11820",
            "Title": "IllegalArgumentException when exporting tables larger than 32MB",
            "Description": "When attempting to export tables with a size greater than 32MB, an IllegalArgumentException is thrown indicating that 'Skip CRC is valid only with update options'. This issue was encountered while testing a patch related to HIVE-11607.",
            "StackTrace": [
                "2015-09-14 21:44:16,817 ERROR [main]: exec.Task (SessionState.java:printError(960)) - Failed with exception Skip CRC is valid only with update options",
                "java.lang.IllegalArgumentException: Skip CRC is valid only with update options",
                "at org.apache.hadoop.tools.DistCpOptions.validate(DistCpOptions.java:556)",
                "at org.apache.hadoop.tools.DistCpOptions.setSkipCRC(DistCpOptions.java:311)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1147)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "1. Apply the patch related to HIVE-11607.",
                "2. Attempt to export a table larger than 32MB.",
                "3. Observe the error message thrown during the export process."
            ],
            "ExpectedBehavior": "The table should be exported successfully without any exceptions.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that 'Skip CRC is valid only with update options'.",
            "Resolution": "A possible resolution is to reverse the order of the following two lines in the patch: options.setSkipCRC(true); and options.setSyncFolder(true);"
        }
    },
    {
        "filename": "HIVE-17274.json",
        "creation_time": "2017-08-08T22:20:33.000+0000",
        "bug_report": {
            "BugID": "HIVE-17274",
            "Title": "Exception Thrown When Spilling RowContainer with Timestamp Column",
            "Description": "When using a timestamp column as a join key in Hive, an exception is thrown due to invalid path names containing ':' characters. This issue arises during the creation of a filename for the RowContainer, leading to a java.net.URISyntaxException.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:205)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:171)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:93)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:94)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:404)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926)",
                "at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1137)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:530)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(Utilities.java:1643)",
                "at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.getHiveRecordWriter(HiveSequenceFileOutputFormat.java:64)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:243)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter(RowContainer.java:538)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:299)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.copyToDFSDirecory(RowContainer.java:407)",
                "at org.apache.hadoop.hive.ql.exec.SkewJoinHandler.endGroup(SkewJoinHandler.java:185)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:249)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:195)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)",
                "Caused by: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at java.net.URI.checkPath(URI.java:1823)",
                "at java.net.URI.<init>(URI.java:745)",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:202)",
                "... 26 more"
            ],
            "StepsToReproduce": [
                "Create a Hive table with a timestamp column.",
                "Perform a join operation using the timestamp column as a key.",
                "Attempt to spill the RowContainer during the join operation."
            ],
            "ExpectedBehavior": "The RowContainer should spill without throwing an exception, and the output files should be created with valid path names.",
            "ObservedBehavior": "An IllegalArgumentException is thrown due to a URISyntaxException caused by invalid path names containing ':' characters.",
            "Resolution": "Fixed in version 3.0.0"
        }
    },
    {
        "filename": "HIVE-12522.json",
        "creation_time": "2015-11-25T19:48:44.000+0000",
        "bug_report": {
            "BugID": "HIVE-12522",
            "Title": "IllegalArgumentException: Wrong FS error during Tez merge files with different filesystems",
            "Description": "When the configuration 'hive.merge.tezfiles' is set to true, and the warehouse directory and scratch directory are located on different filesystems, an IllegalArgumentException is thrown indicating a wrong filesystem. This issue does not occur when both directories are on the same filesystem.",
            "StackTrace": [
                "2015-11-13 10:22:10,617 ERROR exec.Task (TezTask.java:execute(184)) - Failed to execute tez graph.",
                "java.lang.IllegalArgumentException: Wrong FS: wasb://chaoyiteztest@chaoyiteztest.blob.core.windows.net/hive/scratch/chaoyitest/c888f405-3c98-46b1-bf39-e57f067dfe4c/hive_2015-11-13_10-16-10_216_8161037519951665173-1/_tmp.-ext-10000, expected: hdfs://headnodehost:9000",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1136)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1423)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:579)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1083)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:156)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1606)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1367)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1179)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:733)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "StepsToReproduce": [
                "Set the configuration 'hive.merge.tezfiles' to true.",
                "Ensure that the warehouse directory and scratch directory are on different filesystems.",
                "Run a Tez job that requires merging files."
            ],
            "ExpectedBehavior": "The Tez job should execute successfully without throwing an IllegalArgumentException.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating a wrong filesystem.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-16845.json",
        "creation_time": "2017-06-07T17:07:55.000+0000",
        "bug_report": {
            "BugID": "HIVE-16845",
            "Title": "NullPointerException when inserting into partitioned S3 table",
            "Description": "When attempting to insert data into a partitioned table on S3, a NullPointerException (NPE) occurs. This issue arises during the execution of the INSERT OVERWRITE statement with dynamic partitions.",
            "StackTrace": [
                "2017-05-08 21:32:50,607 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-184028]: Error running hive query:",
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null",
                "at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)",
                "at org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "Create a partitioned table on S3 with the following SQL command:",
                "CREATE EXTERNAL TABLE s3table(user_id string COMMENT '', event_name string COMMENT '') PARTITIONED BY (reported_date string, product_id int) LOCATION 's3a://<bucket name>';",
                "Create a temporary table with the following SQL command:",
                "create table tmp_table (id string, name string, date string, pid int) row format delimited fields terminated by '\\t' lines terminated by '\\n' stored as textfile;",
                "Load the following rows into the temporary table:",
                "u1\tvalue1\t2017-04-10\t10000",
                "u2\tvalue2\t2017-04-10\t10000",
                "u3\tvalue3\t2017-04-10\t10001",
                "Set the following parameters:",
                "-- hive.exec.dynamic.partition.mode=nonstrict",
                "-- mapreduce.input.fileinputformat.split.maxsize=10",
                "-- hive.blobstore.optimizations.enabled=true",
                "-- hive.blobstore.use.blobstore.as.scratchdir=false",
                "-- hive.merge.mapfiles=true",
                "Insert the rows from the temporary table into the S3 table with the following SQL command:",
                "INSERT OVERWRITE TABLE s3table PARTITION (reported_date, product_id) SELECT t.id as user_id, t.name as event_name, t.date as reported_date, t.pid as product_id FROM tmp_table t;"
            ],
            "ExpectedBehavior": "The rows from the temporary table should be successfully inserted into the partitioned S3 table without any errors.",
            "ObservedBehavior": "A NullPointerException occurs during the execution of the INSERT OVERWRITE statement.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-9655.json",
        "creation_time": "2015-02-11T20:58:13.000+0000",
        "bug_report": {
            "BugID": "HIVE-9655",
            "Title": "Dynamic Partition Insertion Error in Hive",
            "Description": "When attempting to insert data from one table to a dynamically partitioned table in Hive, an error occurs indicating that a field cannot be found. This issue arises specifically when using the 'distribute by' clause with the same column structure.",
            "StackTrace": [
                "2015-02-11 12:50:52,756 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(178)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"c1\":1,\"c2\":\"one\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 10 more",
                "Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)",
                "at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)",
                "... 16 more"
            ],
            "StepsToReproduce": [
                "Create table t1 with columns c1 and c2.",
                "Create table t2 with columns c1 and c2, partitioned by p1.",
                "Load data into table t1 from a local path.",
                "Set dynamic partition mode to nonstrict.",
                "Execute the insert statement: insert overwrite table t2 partition(p1) select *, c1 as p1 from t1 distribute by p1."
            ],
            "ExpectedBehavior": "The data from table t1 should be successfully inserted into table t2, partitioned by p1 without any errors.",
            "ObservedBehavior": "The query fails with a Hive Runtime Error indicating that it cannot find field _col2.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-11441.json",
        "creation_time": "2015-08-03T17:42:24.000+0000",
        "bug_report": {
            "BugID": "HIVE-11441",
            "Title": "DDL Operations Fail When Table Location is Incorrect",
            "Description": "When a user sets an incorrect location for a Hive table, the system does not provide an appropriate error message or allow the user to correct the mistake. Instead, it throws a connection error when attempting to perform DDL operations on the table.",
            "StackTrace": [
                "2015-07-30 12:19:43,573 DEBUG [main]: transport.TSaslTransport (TSaslTransport.java:readFrame(429)) - CLIENT: reading data length: 293",
                "2015-07-30 12:19:43,720 ERROR [main]: ql.Driver (SessionState.java:printError(833)) - FAILED: SemanticException Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "org.apache.hadoop.hive.ql.parse.SemanticException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1323)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(DDLSemanticAnalyzer.java:1452)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:295)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1069)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1131)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1072)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1019)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1316)",
                "... 23 more"
            ],
            "StepsToReproduce": [
                "Create a table with an incorrect location: create table testwrongloc(id int);",
                "Attempt to set the location to an invalid HDFS path: alter table testwrongloc set location 'hdfs://a-valid-hostname/tmp/testwrongloc';",
                "Observe the error message when trying to perform DDL operations on the table."
            ],
            "ExpectedBehavior": "Hive should throw an error indicating that the specified HDFS path is invalid and allow the user to correct it.",
            "ObservedBehavior": "Hive throws a connection error stating that the host 'a-valid-hostname' is not reachable, preventing any DDL operations.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10801.json",
        "creation_time": "2015-05-22T19:43:23.000+0000",
        "bug_report": {
            "BugID": "HIVE-10801",
            "Title": "NullPointerException when dropping a view in Hive",
            "Description": "When attempting to drop a view in Hive, a NullPointerException is thrown due to a missing encryption key provider URI. This issue occurs in the HiveMetaStore when the table path is null during the check for encryption.",
            "StackTrace": [
                "2015-05-21 11:53:06,126 ERROR [HiveServer2-Background-Pool: Thread-197]: hdfs.KeyProviderCache (KeyProviderCache.java:createKeyProviderURI(87)) - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!",
                "2015-05-21 11:53:06,134 ERROR [HiveServer2-Background-Pool: Thread-197]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(155)) - MetaException(message:java.lang.NullPointerException)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5379)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy7.drop_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context(HiveMetaStoreClient.java:2056)",
                "at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.drop_table_with_environment_context(SessionHiveMetaStoreClient.java:118)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:968)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropTable(HiveMetaStoreClient.java:904)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy8.dropTable(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:1035)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.dropTable(Hive.java:972)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTable(DDLTask.java:3836)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropTableOrPartitions(DDLTask.java:3692)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:331)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1650)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1409)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1192)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:154)",
                "at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:71)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:206)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:218)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1723)",
                "... 40 more"
            ],
            "StepsToReproduce": [
                "1. Attempt to drop a view in Hive using the command: DROP VIEW <view_name>;",
                "2. Check the Hive logs for errors."
            ],
            "ExpectedBehavior": "The view should be dropped successfully without any errors.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the table path is null.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-9141.json",
        "creation_time": "2014-12-17T07:23:05.000+0000",
        "bug_report": {
            "BugID": "HIVE-9141",
            "Title": "ClassCastException in HiveOnTez when using UNION ALL with GROUP BY",
            "Description": "Executing a Hive query that combines UNION ALL and GROUP BY operations results in a ClassCastException. This issue occurs when using the Tez execution engine.",
            "StackTrace": [
                "2014-12-16 23:19:13,593 ERROR ql.Driver (SessionState.java:printError(834)) - FAILED: ClassCastException org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "java.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:361)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez2(TestMiniTezCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "StepsToReproduce": [
                "Set the Hive execution engine to Tez: `set hive.execution.engine=tez;`",
                "Execute the following query:\nSELECT key, value FROM\n  (\n  \tSELECT key, value FROM src\n\n    UNION ALL\n\n  \tSELECT key, key as value FROM \n  \t\n  \t\t(  \n  \t\t    SELECT distinct key FROM (\n\n      \t\tSELECT key, value FROM\n      \t\t(SELECT key, value FROM src\n        \t\tUNION ALL\n      \t\tSELECT key, value FROM src\n      \t\t)t1 \n      \t\tgroup by  key, value\n      \t\t)t2\n        )t3 \n      \n   )t4\n   group by  key, value;",
                "Observe the error message generated."
            ],
            "ExpectedBehavior": "The query should execute successfully without any errors.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that MapWork cannot be cast to ReduceWork.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-10010.json",
        "creation_time": "2015-03-18T17:48:46.000+0000",
        "bug_report": {
            "BugID": "HIVE-10010",
            "Title": "NullPointerException during ALTER TABLE operation in Hive Metastore",
            "Description": "When performing an ALTER TABLE operation, a NullPointerException is thrown, indicating a failure in the metastore's handling of storage descriptors.",
            "StackTrace": [
                "2015-03-18 10:45:54,189 ERROR [main]: exec.DDLTask (DDLTask.java:failed(512)) - java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)",
                "at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)",
                "at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)",
                "at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)"
            ],
            "StepsToReproduce": [
                "1. Connect to the Hive Metastore.",
                "2. Execute an ALTER TABLE command on a specific table.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The ALTER TABLE operation should complete successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the operation to fail.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HIVE-7763.json",
        "creation_time": "2014-08-18T09:35:09.000+0000",
        "bug_report": {
            "BugID": "HIVE-7763",
            "Title": "RuntimeException during TABLESAMPLE query on empty bucket table in Hive on Spark",
            "Description": "When executing a TABLESAMPLE query on an empty bucket table, a RuntimeException is thrown indicating that the map operator initialization failed due to inconsistent configuration and input path.",
            "StackTrace": [
                "2014-08-18 16:23:15,213 ERROR [Executor task launch worker-0]: executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)",
                "java.lang.RuntimeException: Map operator initialization failed",
                "    at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:127)",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:52)",
                "    at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:30)",
                "    at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "    at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)",
                "    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)",
                "    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)",
                "    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "    at org.apache.spark.scheduler.Task.run(Task.scala:54)",
                "    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "    at java.lang.Thread.run(Thread.java:722)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent",
                "    at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:404)",
                "    at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:93)",
                "    ... 16 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent",
                "    at org.apache.hadoop.hive.ql.exec"
            ],
            "StepsToReproduce": [
                "1. Create an empty bucket table in Hive.",
                "2. Execute a TABLESAMPLE query on the empty bucket table.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The TABLESAMPLE query should execute without errors, returning an empty result set.",
            "ObservedBehavior": "A RuntimeException is thrown indicating that the map operator initialization failed due to inconsistent configuration and input path.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HIVE-12083.json",
        "creation_time": "2015-10-09T22:45:47.000+0000",
        "bug_report": {
            "BugID": "HIVE-12083",
            "Title": "Thrift Error Occurs When partNames or colNames Are Empty in HIVE-10965",
            "Description": "In the fix for HIVE-10965, a short-circuit path returns an empty AggrStats object if either partNames or colNames is empty. This violates thrift requirements for AggrStats, leading to a TProtocolException when the required field 'colStats' is unset. The issue arises when the client does not have a guard for partNames being empty, which can occur if the client is from an older version before the patch.",
            "StackTrace": [
                "2015-10-08 00:00:25,413 ERROR server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:536)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Ensure you are using a client version prior to the patch for HIVE-10965.",
                "2. Call the method 'aggrColStatsForPartitions' with empty partNames or colNames.",
                "3. Observe the error in the logs."
            ],
            "ExpectedBehavior": "The method should handle empty partNames or colNames gracefully without throwing a Thrift error.",
            "ObservedBehavior": "A Thrift error occurs indicating that the required field 'colStats' is unset.",
            "Resolution": "Fixed in HIVE-10965 by adding a guard for partNames being empty."
        }
    },
    {
        "filename": "HIVE-14784.json",
        "creation_time": "2016-09-17T02:00:28.000+0000",
        "bug_report": {
            "BugID": "HIVE-14784",
            "Title": "Operation Logs Disabled When Parent Directory is Deleted",
            "Description": "Operation logging is automatically disabled for queries if the parent directory (named after the Hive session ID) is deleted. This can occur if the operation log directory is set to a temporary location (e.g., /tmp) that is purged by the operating system at configured intervals. This leads to warnings and errors when attempting to retrieve operation logs after the session has ended.",
            "StackTrace": [
                "2016-09-15 15:09:16,723 WARN org.apache.hive.service.cli.operation.Operation: Unable to create operation log file: /tmp/hive/operation_logs/b8809985-6b38-47ec-a49b-6158a67cd9fc/d35414f7-2418-426c-8489-c6f643ca4599",
                "java.io.IOException: No such file or directory",
                "\tat java.io.UnixFileSystem.createFileExclusively(Native Method)",
                "\tat java.io.File.createNewFile(File.java:1012)",
                "\tat org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)",
                "\tat org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)",
                "\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)",
                "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:385)",
                "\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)",
                "\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "\tat java.lang.Thread.run(Thread.java:745)",
                "WARN org.apache.hive.service.cli.thrift.ThriftCLIService: Error fetching results: ",
                "org.apache.hive.service.cli.HiveSQLException: Couldn't find log associated with operation handle: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=d35414f7-2418-426c-8489-c6f643ca4599]",
                "\tat org.apache.hive.service.cli.operation.OperationManager.getOperationLogRowSet(OperationManager.java:259)",
                "\tat org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:701)",
                "\tat org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:451)",
                "\tat org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:676)",
                "\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1553)",
                "\tat org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1538)",
                "\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "\tat org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Set the operation log directory to a temporary location (e.g., /tmp).",
                "Run a query that creates a Hive session.",
                "Delete the parent directory of the operation logs before the session ends.",
                "Attempt to retrieve operation logs after the session has ended."
            ],
            "ExpectedBehavior": "The operation logs should be created and accessible for the duration of the Hive session, regardless of the parent directory's state.",
            "ObservedBehavior": "Operation logs are not created, and warnings/errors are generated when attempting to access logs after the session has ended.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    }
]