[
    {
        "filename": "HADOOP-6989.json",
        "creation_time": "2010-10-04T23:55:16.000+0000",
        "bug_report": {
            "BugID": "HADOOP-6989",
            "Title": "TestSetFile Fails Due to Missing Key Class or Comparator Option",
            "Description": "The TestSetFile test case fails with an IllegalArgumentException indicating that the key class or comparator option must be set. This issue occurs during the execution of the test suite for org.apache.hadoop.io.TestSetFile.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: key class or comparator option must be set",
                "at org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:247)",
                "at org.apache.hadoop.io.SetFile$Writer.<init>(SetFile.java:60)",
                "at org.apache.hadoop.io.TestSetFile.writeTest(TestSetFile.java:73)",
                "at org.apache.hadoop.io.TestSetFile.testSetFile(TestSetFile.java:45)"
            ],
            "StepsToReproduce": [
                "Run the test suite for org.apache.hadoop.io.TestSetFile.",
                "Observe the output for any errors related to key class or comparator options."
            ],
            "ExpectedBehavior": "The TestSetFile test case should pass without any errors.",
            "ObservedBehavior": "The test case fails with an IllegalArgumentException indicating that the key class or comparator option must be set.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10823.json",
        "creation_time": "2014-07-15T07:20:00.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10823",
            "Title": "Flaky Test in TestReloadingX509TrustManager",
            "Description": "The unit test TestReloadingX509TrustManager is flaky, resulting in inconsistent test results. The test fails with an assertion error indicating an expected value of 2 but received 1.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: expected:<2> but was:<1>",
                "\tat junit.framework.Assert.fail(Assert.java:50)",
                "\tat junit.framework.Assert.failNotEquals(Assert.java:287)",
                "\tat junit.framework.Assert.assertEquals(Assert.java:67)",
                "\tat junit.framework.Assert.assertEquals(Assert.java:199)",
                "\tat junit.framework.Assert.assertEquals(Assert.java:205)",
                "\tat org.apache.hadoop.security.ssl.TestReloadingX509TrustManager.testReload(TestReloadingX509TrustManager.java:112)",
                "Standard Output:",
                "2014-07-06 06:12:21,170 WARN  ssl.ReloadingX509TrustManager (ReloadingX509TrustManager.java:run(197)) - Could not load truststore (keep using existing one) : java.io.EOFException",
                "java.io.EOFException",
                "\tat java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "\tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:628)",
                "\tat sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38)",
                "\tat java.security.KeyStore.load(KeyStore.java:1185)",
                "\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:166)",
                "\tat org.apache.hadoop.security.ssl.ReloadingX509TrustManager.run(ReloadingX509TrustManager.java:195)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the unit tests for the Hadoop security module.",
                "Observe the results of the TestReloadingX509TrustManager test."
            ],
            "ExpectedBehavior": "The test should pass consistently without any assertion errors.",
            "ObservedBehavior": "The test fails intermittently with an assertion error indicating expected:<2> but was:<1>.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-9125.json",
        "creation_time": "2012-12-10T02:07:52.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9125",
            "Title": "LdapGroupsMapping CommunicationException after Idle Period",
            "Description": "The LdapGroupsMapping component throws a CommunicationException after a period of inactivity. This occurs when no calls are made to the group mapping provider during idle time.",
            "StackTrace": [
                "2012-12-07 02:20:59,738 WARN org.apache.hadoop.security.LdapGroupsMapping: Exception trying to get groups for user aduser2",
                "javax.naming.CommunicationException: connection closed [Root exception is java.io.IOException: connection closed]; remaining name 'CN=Users,DC=EXAMPLE,DC=COM'",
                "at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1983)",
                "at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1827)",
                "at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1752)",
                "at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769)",
                "at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:394)",
                "at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:376)",
                "at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358)",
                "at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getGroups(LdapGroupsMapping.java:187)",
                "at org.apache.hadoop.security.CompositeGroupsMapping.getGroups(CompositeGroupsMapping.java:97)",
                "at org.apache.hadoop.security.Groups.doGetGroups(Groups.java:103)",
                "at org.apache.hadoop.security.Groups.getGroups(Groups.java:70)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1035)",
                "at org.apache.hadoop.hbase.security.User.getGroupNames(User.java:90)",
                "at org.apache.hadoop.hbase.security.access.TableAuthManager.authorize(TableAuthManager.java:355)",
                "at org.apache.hadoop.hbase.security.access.AccessController.requirePermission(AccessController.java:379)",
                "at org.apache.hadoop.hbase.security.access.AccessController.getUserPermissions(AccessController.java:1051)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hbase.regionserver.HRegion.exec(HRegion.java:4914)",
                "at org.apache.hadoop.hbase.regionserver.HRegionServer.execCoprocessor(HRegionServer.java:3546)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:372)",
                "at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1399)",
                "Caused by: java.io.IOException: connection closed",
                "at com.sun.jndi.ldap.LdapClient.ensureOpen(LdapClient.java:1558)",
                "at com.sun.jndi.ldap.LdapClient.search(LdapClient.java:503)",
                "at com.sun.jndi.ldap.LdapCtx.doSearch(LdapCtx.java:1965)",
                "... 28 more",
                "2012-12-07 02:20:59,739 WARN org.apache.hadoop.security.UserGroupInformation: No groups available for user aduser2"
            ],
            "StepsToReproduce": [
                "1. Ensure the LdapGroupsMapping is configured correctly.",
                "2. Allow the system to remain idle for a period of time without any calls to the group mapping provider.",
                "3. Attempt to retrieve groups for a user (e.g., aduser2)."
            ],
            "ExpectedBehavior": "The system should successfully retrieve groups for the user without throwing an exception.",
            "ObservedBehavior": "A CommunicationException is thrown indicating that the connection was closed after a period of inactivity.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-10252.json",
        "creation_time": "2014-01-22T16:43:27.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10252",
            "Title": "HttpServer Fails to Start When Hostname is Not Specified",
            "Description": "The HttpServer fails to start if the hostname is not specified in the configuration, leading to an IllegalArgumentException. This issue was introduced in HADOOP-8362, which added a check to ensure configuration values are not null.",
            "StackTrace": [
                "2014-01-22 08:43:05,969 FATAL [M:0;localhost:48573] master.HMaster(2187): Unhandled exception. Starting shutdown.",
                "java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:958)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:940)",
                "at org.apache.hadoop.http.HttpServer.initializeWebServer(HttpServer.java:510)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:470)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:458)",
                "at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:412)",
                "at org.apache.hadoop.hbase.util.InfoServer.<init>(InfoServer.java:59)",
                "at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:584)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the HttpServer configuration does not specify a hostname.",
                "2. Attempt to start the HttpServer.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The HttpServer should start successfully without any errors.",
            "ObservedBehavior": "The HttpServer fails to start and logs an IllegalArgumentException indicating that the property value must not be null.",
            "Resolution": "Fixed in version 2.3.0"
        }
    },
    {
        "filename": "HADOOP-12239.json",
        "creation_time": "2015-07-15T18:06:14.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12239",
            "Title": "StorageException: No Lease ID Specified When Updating Folder Last Modified Time in WASB",
            "Description": "This issue is similar to HADOOP-11523 and HADOOP-12089, observed in a customer's HBase cluster logs. The error occurs when attempting to update the last modified time of a folder in the Azure WASB filesystem, resulting in a StorageException due to a missing lease ID.",
            "StackTrace": [
                "2015-07-09 13:38:57,388 INFO org.apache.hadoop.hbase.master.SplitLogManager: dead splitlog workers [workernode3.xxx.b6.internal.cloudapp.net,60020,1436448555180]",
                "2015-07-09 13:38:57,466 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN",
                "java.io.IOException: failed log splitting for workernode12.xxx.b6.internal.cloudapp.net,60020,1436448556374, will retry",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.resubmit(ServerShutdownHandler.java:343)",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:211)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.IOException: Unable to write RenamePending file for folder rename from hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374 to hbase/WALs/workernode12.xxx.b6.internal.cloudapp.net,60020,1436448566374-splitting",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:258)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.prepareAtomicFolderRename(NativeAzureFileSystem.java:2110)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1998)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:325)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:412)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:390)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:288)",
                "at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:204)",
                "... 4 more",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2598)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2609)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1366)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem$FolderRenamePending.writeFile(NativeAzureFileSystem.java:255)",
                "... 11 more",
                "Caused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:89)",
                "at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)",
                "at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:182)",
                "at com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2892)",
                "at org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2593)",
                "... 19 more"
            ],
            "StepsToReproduce": [
                "1. Set up an HBase cluster on Windows Azure.",
                "2. Trigger a server shutdown event on one of the nodes.",
                "3. Monitor the logs for any errors related to log splitting."
            ],
            "ExpectedBehavior": "The system should successfully update the last modified time of the folder without throwing an exception.",
            "ObservedBehavior": "The system throws a StorageException indicating that there is a lease on the blob and no lease ID was specified in the request.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-11878.json",
        "creation_time": "2015-04-27T08:06:22.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11878",
            "Title": "NullPointerException in DeletionService when deleting log files",
            "Description": "A NullPointerException occurs in the DeletionService when it attempts to delete log files, leading to failure in the job execution. The log indicates that the absolute path is null, which causes the exception.",
            "StackTrace": [
                "2015-04-27 14:56:17,113 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : null",
                "2015-04-27 14:56:17,113 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "    at org.apache.hadoop.fs.FileContext.delete(FileContext.java:761)",
                "    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:457)",
                "    at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)",
                "    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Trigger a job that requires log file deletion.",
                "2. Ensure that the log file path is null.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The DeletionService should handle null paths gracefully and log an appropriate error message without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the DeletionService attempts to delete a log file with a null path.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-14949.json",
        "creation_time": "2017-10-13T23:44:09.000+0000",
        "bug_report": {
            "BugID": "HADOOP-14949",
            "Title": "Intermittent Failure in TestKMS#testACLs",
            "Description": "The test TestKMS#testACLs has been observed to fail intermittently with an AssertionError indicating that re-encryption of the encrypted key should not have been possible.",
            "StackTrace": [
                "java.lang.AssertionError: Should not have been able to reencryptEncryptedKey",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1616)",
                "\tat org.apache.hadoop.crypto.key.kms.server.TestKMS$11$15.run(TestKMS.java:1608)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:415)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.doAs(TestKMS.java:313)",
                "\tat org.apache.hadoop.crypto.key.kms.server.TestKMS.access$100(TestKMS.java:97)"
            ],
            "StepsToReproduce": [
                "Run the test suite for the KMS module.",
                "Observe the results for the TestKMS#testACLs test case."
            ],
            "ExpectedBehavior": "The test should pass without any assertion errors.",
            "ObservedBehavior": "The test fails intermittently with an AssertionError indicating that re-encryption of the encrypted key should not have been possible.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10540.json",
        "creation_time": "2014-04-11T05:14:07.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10540",
            "Title": "DataNode Upgrade Fails on Windows Due to Hardlink Exception",
            "Description": "When upgrading Hadoop from version 1.x to 2.4 on a Windows environment, the DataNode fails to start, resulting in a hard link exception. This issue occurs during the upgrade process after uninstalling the previous version and installing the new one.",
            "StackTrace": [
                "java.io.IOException: Usage: hardlink create [LINKNAME] [FILENAME] |Incorrect command line arguments.",
                "\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:479)",
                "\tat org.apache.hadoop.fs.HardLink.createHardLinkMult(HardLink.java:416)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkBlocks(DataStorage.java:816)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.linkAllBlocks(DataStorage.java:759)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doUpgrade(DataStorage.java:566)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:486)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:225)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:249)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:929)",
                "\tat org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:900)",
                "\tat org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:274)",
                "\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:220)",
                "\tat org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:815)",
                "\tat java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "Install Hadoop 1.x",
                "Run command: hadoop dfsadmin -safemode enter",
                "Run command: hadoop dfsadmin -saveNamespace",
                "Run command: hadoop namenode -finalize",
                "Stop all services",
                "Uninstall Hadoop 1.x",
                "Install Hadoop 2.4",
                "Start namenode with -upgrade option",
                "Attempt to start datanode and observe the logs for errors"
            ],
            "ExpectedBehavior": "The DataNode should start successfully after the upgrade without any exceptions.",
            "ObservedBehavior": "The DataNode fails to start, and the logs indicate a hard link exception.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-7629.json",
        "creation_time": "2011-09-09T17:45:14.000+0000",
        "bug_report": {
            "BugID": "HADOOP-7629",
            "Title": "RPC Failure Due to Immutable FsPermission in setPermission",
            "Description": "The change introduced in MAPREDUCE-2289 causes a runtime exception when using an immutable FsPermission in RPC calls. This results in the inability to read call parameters, leading to a failure in the system.",
            "StackTrace": [
                "2011-09-08 16:31:45,187 WARN org.apache.hadoop.ipc.Server: Unable to read call parameters for client 127.0.0.1",
                "java.lang.RuntimeException: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)",
                "    at org.apache.hadoop.io.WritableFactories.newInstance(WritableFactories.java:53)",
                "    at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:236)",
                "    at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:104)",
                "    at org.apache.hadoop.ipc.Server$Connection.processData(Server.java:1337)",
                "    at org.apache.hadoop.ipc.Server$Connection.processOneRpc(Server.java:1315)",
                "    at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1215)",
                "    at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:566)",
                "    at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:363)",
                "Caused by: java.lang.NoSuchMethodException: org.apache.hadoop.fs.permission.FsPermission$2.<init>()",
                "    at java.lang.Class.getConstructor0(Class.java:2706)",
                "    at java.lang.Class.getDeclaredConstructor(Class.java:1985)",
                "    at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:109)",
                "    ... 8 more"
            ],
            "StepsToReproduce": [
                "1. Implement the change from MAPREDUCE-2289 in the codebase.",
                "2. Attempt to call setPermission with an immutable FsPermission.",
                "3. Observe the logs for any warnings or errors."
            ],
            "ExpectedBehavior": "The setPermission method should execute without throwing an exception, allowing the RPC call to complete successfully.",
            "ObservedBehavior": "The RPC call fails with a RuntimeException indicating that the constructor for FsPermission$2 cannot be found.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-15060.json",
        "creation_time": "2017-11-22T00:18:55.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15060",
            "Title": "Flaky Test in TestShellBasedUnixGroupsMapping: Expected Command Timeout Log Not Found",
            "Description": "The test 'testFiniteGroupResolutionTime' in the 'TestShellBasedUnixGroupsMapping' class is failing intermittently. The test expects a log message indicating a command timeout, but instead, it logs a warning about an unknown user.",
            "StackTrace": [
                "[ERROR] testFiniteGroupResolutionTime(org.apache.hadoop.security.TestShellBasedUnixGroupsMapping)  Time elapsed: 61.975 s  <<< FAILURE!",
                "java.lang.AssertionError:",
                "Expected the logs to carry a message about command timeout but was: 2017-11-22 00:10:57,523 WARN  security.ShellBasedUnixGroupsMapping (ShellBasedUnixGroupsMapping.java:getUnixGroups(181)) - unable to return groups for user foobarnonexistinguser",
                "PartialGroupNameException The user name 'foobarnonexistinguser' is not found.",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:275)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:178)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:97)",
                "at org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:278)"
            ],
            "StepsToReproduce": [
                "Run the test suite for the Hadoop security module.",
                "Observe the output of the 'testFiniteGroupResolutionTime' test."
            ],
            "ExpectedBehavior": "The test should log a message indicating a command timeout when the user is not found.",
            "ObservedBehavior": "The test logs a warning about an unknown user instead of the expected command timeout message.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10937.json",
        "creation_time": "2014-08-04T21:22:55.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10937",
            "Title": "Null Pointer Exception when using touchz command in Hadoop",
            "Description": "Executing the 'touchz' command on a file results in a Null Pointer Exception due to an issue with the decryption of the Encrypted Key (EEK). This bug occurs when the version name is not set correctly before the decryption process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.decryptEncryptedKey(KMSClientProvider.java:652)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:342)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1319)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1364)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1352)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:391)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:384)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:384)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:328)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:906)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:887)"
            ],
            "StepsToReproduce": [
                "Open a terminal on the Hadoop node.",
                "Run the command: hdfs dfs -touchz /enc3/touchFile"
            ],
            "ExpectedBehavior": "The command should successfully create an empty file without any exceptions.",
            "ObservedBehavior": "The command fails with a Null Pointer Exception, indicating an internal error.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-9103.json",
        "creation_time": "2012-04-20T01:07:25.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9103",
            "Title": "UTF8 Class Fails to Decode Unicode Characters Outside Basic Multilingual Plane",
            "Description": "The UTF8 class in HDFS does not properly decode Unicode characters that are outside the basic multilingual plane, leading to issues with file names containing such characters. This bug was identified during the saveFSImage method execution in FSImage.java.",
            "StackTrace": [
                "2012-03-28 00:48:42,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/????@???????????????",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Create a file with a name that includes Unicode characters outside the basic multilingual plane.",
                "2. Attempt to save the FSImage using the saveFSImage method in FSImage.java.",
                "3. Observe the error in the logs indicating a failure to find the lease for the non-existent file."
            ],
            "ExpectedBehavior": "The FSImage should save without errors, and all file names, including those with Unicode characters, should be handled correctly.",
            "ObservedBehavior": "An IOException is thrown indicating a lease for a non-existent file, which is caused by improper handling of Unicode characters in file names.",
            "Resolution": "The issue has been fixed by ensuring consistent use of UTF8 encoding in the saveImage method and related functions."
        }
    },
    {
        "filename": "HADOOP-11151.json",
        "creation_time": "2014-09-29T08:18:04.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11151",
            "Title": "Authentication Failure When Copying Files to Encryption Zone After KMS Restart",
            "Description": "After enabling CFS and KMS services in the cluster, the system initially allows file operations in the encryption zone. However, after a period of time (approximately one day), attempts to copy files into the encryption zone fail with a 403 Forbidden error. The logs indicate issues with authentication, specifically that anonymous requests are disallowed.",
            "StackTrace": [
                "java.util.concurrent.ExecutionException: java.io.IOException: HTTP status [403], message [Forbidden]",
                "org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:184)",
                "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler.authenticate(DelegationTokenAuthenticationHandler.java:331)",
                "org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507)",
                "org.apache.hadoop.crypto.key.kms.server.KMSAuthenticationFilter.doFilter(KMSAuthenticationFilter.java:129)",
                "org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)",
                "org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)",
                "org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)",
                "org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)",
                "org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)",
                "org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)",
                "org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)",
                "org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)",
                "org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)",
                "org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)",
                "org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)",
                "java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Enable CFS and KMS services in the Hadoop cluster.",
                "2. Copy a file into the encryption zone and confirm it succeeds.",
                "3. Wait for approximately one day.",
                "4. Attempt to copy another file into the encryption zone."
            ],
            "ExpectedBehavior": "Files should be able to be copied into the encryption zone without any authentication errors.",
            "ObservedBehavior": "After a day, attempts to copy files into the encryption zone result in a 403 Forbidden error due to authentication issues.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-8031.json",
        "creation_time": "2012-02-07T20:22:24.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8031",
            "Title": "Configuration class fails to find embedded .jar resources; should use URL.openStream()",
            "Description": "While running a Hadoop client within RHQ (monitoring software) using its classloader, the application fails to start due to the Configuration class not being able to locate the embedded .jar resources. The error indicates that 'core-site.xml' cannot be found, leading to a failure in starting the component for the NameNode resource.",
            "StackTrace": [
                "org.rhq.core.clientapi.agent.PluginContainerException: Failed to start component for resource Resource[id=16290, type=NameNode, key=NameNode:/usr/lib/hadoop-0.20, name=NameNode, parent=vg61l01ad-hadoop002.apple.com].",
                "Caused by: java.lang.RuntimeException: core-site.xml not found",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1308)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1228)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1169)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:438)"
            ],
            "StepsToReproduce": [
                "1. Run a Hadoop client within RHQ using its classloader.",
                "2. Monitor the logs for errors related to resource loading."
            ],
            "ExpectedBehavior": "The Hadoop client should start successfully without errors related to missing configuration files.",
            "ObservedBehavior": "The application fails to start, and the logs indicate that 'core-site.xml' cannot be found.",
            "Resolution": "A patch has been submitted for approval that modifies the resource loading mechanism to use URL.openStream() instead of url.toString()."
        }
    },
    {
        "filename": "HADOOP-15411.json",
        "creation_time": "2018-04-24T23:18:39.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15411",
            "Title": "Node Manager Startup Failure Due to ConcurrentModificationException",
            "Description": "The Node Manager fails to start with a YarnRuntimeException caused by a ConcurrentModificationException during the initialization of the HTTP server.",
            "StackTrace": [
                "2018-04-19 13:08:30,638 ERROR nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(921)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: NMWebapps failed to start.",
                " at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:117)",
                " at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                " at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)",
                " at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                " at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:919)",
                " at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)",
                "Caused by: org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:377)",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:424)",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:420)",
                " at org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer.serviceStart(WebServer.java:112)",
                " ... 5 more",
                "Caused by: java.io.IOException: java.util.ConcurrentModificationException",
                " at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:532)",
                " at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:117)",
                " at org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:421)",
                " at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:333)",
                " ... 8 more",
                "Caused by: java.util.ConcurrentModificationException",
                " at java.util.Hashtable$Enumerator.next(Hashtable.java:1383)",
                " at org.apache.hadoop.conf.Configuration.iterator(Configuration.java:2853)",
                " at org.apache.hadoop.security.AuthenticationFilterInitializer.getFilterConfigMap(AuthenticationFilterInitializer.java:73)",
                " at org.apache.hadoop.http.HttpServer2.getFilterProperties(HttpServer2.java:647)",
                " at org.apache.hadoop.http.HttpServer2.constructSecretProvider(HttpServer2.java:637)",
                " at org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:525)",
                " ... 11 more",
                "2018-04-19 13:08:30,639 INFO timeline.HadoopTimelineMetricsSink (AbstractTimelineMetricsSink.java:getCurrentCollectorHost(291)) - No live collector to send metrics to. Metrics to be sent will be discarded. This message will be skipped for the next 20 times."
            ],
            "StepsToReproduce": [
                "Start the Node Manager service.",
                "Monitor the logs for any errors during startup."
            ],
            "ExpectedBehavior": "The Node Manager should start successfully without any errors.",
            "ObservedBehavior": "The Node Manager fails to start and logs a YarnRuntimeException due to a ConcurrentModificationException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-15850.json",
        "creation_time": "2018-10-13T14:24:29.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15850",
            "Title": "CopyCommitter#concatFileChunks Fails to Handle Inconsistent Sequence Files",
            "Description": "During the execution of the test case TestIncrementalBackupWithBulkLoad, an IOException is thrown due to inconsistent sequence files when using DistCp. The issue arises when two bulk loaded HFiles are processed, leading to a mismatch in the expected file lengths.",
            "StackTrace": [
                "java.io.IOException: Inconsistent sequence file: current chunk file org.apache.hadoop.tools.CopyListingFileStatus@bb8826ee{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/a7599081e835440eb7bf0dd3ef4fd7a5_SeqId_205_ length = 5100 aclEntries  = null, xAttrs = null} doesn't match prior entry org.apache.hadoop.tools.CopyListingFileStatus@243d544d{hdfs://localhost:42796/user/hbase/test-data/160aeab5-6bca-9f87-465e-2517a0c43119/data/default/test-1539439707496/96b5a3613d52f4df1ba87a1cef20684c/f/394e6d39a9b94b148b9089c4fb967aad_SeqId_205_ length = 5142 aclEntries = null, xAttrs = null}",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.concatFileChunks(CopyCommitter.java:276)",
                "at org.apache.hadoop.tools.mapred.CopyCommitter.commitJob(CopyCommitter.java:100)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:567)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with version 3.1.1.",
                "2. Run the test case TestIncrementalBackupWithBulkLoad.",
                "3. Observe the logs for the execution of BackupDistCp.",
                "4. Check for any IOException related to inconsistent sequence files."
            ],
            "ExpectedBehavior": "The DistCp process should successfully handle multiple bulk loaded HFiles without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating an inconsistency in the sequence files being processed.",
            "Resolution": "A fix has been implemented to ensure that CopyCommitter#concatFileChunks correctly checks for the number of blocks per chunk."
        }
    },
    {
        "filename": "HADOOP-11693.json",
        "creation_time": "2015-03-05T23:19:13.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11693",
            "Title": "Throttling of Azure Storage FileSystem Rename Operations Affects HBase WAL Archiving",
            "Description": "One of our customers' production HBase clusters was periodically throttled by Azure storage when archiving old WALs. This caused HMaster to abort the region server and attempt to restart it. However, due to continued throttling, the distributed log splitting also failed, leading to the hbase:meta table being offline and the entire cluster entering a bad state.",
            "StackTrace": [
                "2015-03-01 18:36:45,623 ERROR org.apache.hadoop.hbase.master.HMaster: Region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044 reported a fatal error:",
                "ABORTING region server workernode4.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845421044: IOE in log roller",
                "Cause:",
                "org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2367)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1960)",
                "at org.apache.hadoop.hbase.util.FSUtils.renameAndSetModifyTime(FSUtils.java:1719)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.archiveLogFile(FSHLog.java:798)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.cleanOldLogs(FSHLog.java:656)",
                "at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:593)",
                "at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:97)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)",
                "at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)",
                "at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)",
                "at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)",
                "at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)",
                "... 8 more",
                "2015-03-01 18:43:29,072 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN",
                "java.io.IOException: failed log splitting for workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1424845307901, will retry",
                "at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:71)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2446)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem$FolderRenamePending.execute(NativeAzureFileSystem.java:393)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.rename(NativeAzureFileSystem.java:1973)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.getLogDirs(MasterFileSystem.java:319)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:406)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:302)",
                "at org.apache.hadoop.hbase.master.MasterFileSystem.splitMetaLog(MasterFileSystem.java:293)",
                "at org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.process(MetaServerShutdownHandler.java:64)",
                "... 4 more",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: The server is busy.",
                "at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)",
                "at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)",
                "at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)",
                "at com.microsoft.windowsazure.storage.blob.CloudBlob.startCopyFromBlob(CloudBlob.java:762)",
                "at org.apache.hadoop.fs.azurenative.StorageInterfaceImpl$CloudBlobWrapperImpl.startCopyFromBlob(StorageInterfaceImpl.java:350)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.rename(AzureNativeFileSystemStore.java:2439)",
                "... 11 more",
                "Sun Mar 01 18:59:51 GMT 2015, org.apache.hadoop.hbase.client.RpcRetryingCaller@aa93ac7, org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on workernode13.hbaseproddb4001.f5.internal.cloudapp.net,60020,1425235081338",
                "at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2676)",
                "at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:4095)",
                "at org.apache.hadoop.hbase.regionserver.HRegionServer.scan(HRegionServer.java:3076)",
                "at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28861)",
                "at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2008)",
                "at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:92)",
                "at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)",
                "at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)",
                "at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up an HBase cluster with Azure storage as the backend.",
                "2. Archive old WALs while the Azure storage is under heavy load.",
                "3. Monitor the HBase region server logs for errors related to throttling."
            ],
            "ExpectedBehavior": "HBase should successfully archive old WALs without aborting the region server, even under load from Azure storage.",
            "ObservedBehavior": "HBase region server aborts and fails to archive WALs due to Azure storage throttling, leading to the hbase:meta table going offline.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-12441.json",
        "creation_time": "2015-09-25T23:26:43.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12441",
            "Title": "Fix kill command behavior under some Linux distributions",
            "Description": "After HADOOP-12317, the kill command's execution fails under Ubuntu 12. After the NodeManager (NM) restarts, it cannot determine if a process is alive via the PID of containers, leading to incorrect process termination when the ResourceManager (RM) or ApplicationMaster (AM) instructs NM to kill a container.",
            "StackTrace": [
                "2015-09-25 21:58:59,348 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:containerIsAlive(431)) -  ================== check alive cmd:[[Ljava.lang.String;@496e442d]",
                "2015-09-25 21:58:59,349 INFO  nodemanager.NMAuditLogger (NMAuditLogger.java:logSuccess(89)) - USER=hrt_qa       IP=10.0.1.14    OPERATION=Stop Container Request        TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1443218269460_0001    CONTAINERID=container_1443218269460_0001_01_000001",
                "2015-09-25 21:58:59,363 INFO  nodemanager.DefaultContainerExecutor (DefaultContainerExecutor.java:containerIsAlive(438)) -  ===========================",
                "ExitCodeException exitCode=1: ERROR: garbage process ID '--'.",
                "Usage:",
                "  kill pid ...              Send SIGTERM to every process listed.",
                "  kill signal pid ...       Send a signal to every process listed.",
                "  kill -s signal pid ...    Send a signal to every process listed.",
                "  kill -l                   List all signal names.",
                "  kill -L                   List all signal names in a nice table.",
                "  kill -l signal            Convert between signal numbers and names.",
                "        at org.apache.hadoop.util.Shell.runCommand(Shell.java:550)",
                "        at org.apache.hadoop.util.Shell.run(Shell.java:461)",
                "        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:727)",
                "        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.containerIsAlive(DefaultContainerExecutor.java:432)",
                "        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.signalContainer(DefaultContainerExecutor.java:401)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:419)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Deploy the application on Ubuntu 12.",
                "2. Execute the kill command on a running container.",
                "3. Restart the NodeManager.",
                "4. Observe the behavior of the kill command."
            ],
            "ExpectedBehavior": "The kill command should successfully terminate the specified container process.",
            "ObservedBehavior": "The kill command fails with an error indicating a garbage process ID, and the container process is not terminated.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11685.json",
        "creation_time": "2015-03-06T22:00:14.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11685",
            "Title": "StorageException: No Lease ID during HBase Log Splitting",
            "Description": "During HBase distributed log splitting, multiple threads access the same folder called 'recovered.edits'. Many parts of the WASB code do not acquire a lease and pass null to Azure storage, leading to a StorageException.",
            "StackTrace": [
                "2015-02-26 03:21:28,871 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: log splitting of WALs/workernode4.xxx.g6.internal.cloudapp.net,60020,1422071058425-splitting/workernode4.xxx.g6.internal.cloudapp.net%2C60020%2C1422071058425.1424914216773 failed, returning error",
                "java.io.IOException: org.apache.hadoop.fs.azure.AzureException: java.io.IOException",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.checkForErrors(HLogSplitter.java:633)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.access$000(HLogSplitter.java:121)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$OutputSink.finishWriting(HLogSplitter.java:964)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.finishWritingAndClose(HLogSplitter.java:1019)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:359)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:223)",
                "at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:142)",
                "at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:79)",
                "at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.fs.azure.AzureException: java.io.IOException",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1477)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1862)",
                "at org.apache.hadoop.fs.azurenative.NativeAzureFileSystem.mkdirs(NativeAzureFileSystem.java:1812)",
                "at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1815)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getRegionSplitEditsPath(HLogSplitter.java:502)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.createWAP(HLogSplitter.java:1211)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.getWriterAndPath(HLogSplitter.java:1200)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$LogRecoveredEditsOutputSink.append(HLogSplitter.java:1243)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.writeBuffer(HLogSplitter.java:851)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.doRun(HLogSplitter.java:843)",
                "at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter$WriterThread.run(HLogSplitter.java:813)",
                "Caused by: java.io.IOException",
                "at com.microsoft.windowsazure.storage.core.Utility.initIOException(Utility.java:493)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:282)",
                "at org.apache.hadoop.fs.azurenative.AzureNativeFileSystemStore.storeEmptyFolder(AzureNativeFileSystemStore.java:1472)",
                "... 10 more",
                "Caused by: com.microsoft.windowsazure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.windowsazure.storage.StorageException.translateException(StorageException.java:163)",
                "at com.microsoft.windowsazure.storage.core.StorageRequest.materializeException(StorageRequest.java:306)",
                "at com.microsoft.windowsazure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:229)",
                "at com.microsoft.windowsazure.storage.blob.CloudBlockBlob.commitBlockList(CloudBlockBlob.java:248)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.commit(BlobOutputStream.java:319)",
                "at com.microsoft.windowsazure.storage.blob.BlobOutputStream.close(BlobOutputStream.java:279)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "1. Set up HBase with Azure storage.",
                "2. Initiate log splitting on HBase.",
                "3. Monitor the logs for any StorageException related to lease IDs."
            ],
            "ExpectedBehavior": "The log splitting process should complete without any exceptions related to lease IDs.",
            "ObservedBehavior": "The log splitting process fails with a StorageException indicating that no lease ID was specified.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-8589.json",
        "creation_time": "2012-07-11T23:27:00.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8589",
            "Title": "ViewFs Tests Fail When Home Directory is Nested Deeper Than Two Levels",
            "Description": "The `TestFSMainOperationsLocalFileSystem` fails when the test root directory is located under the user's home directory, specifically when the user's home directory is nested deeper than two levels from the root directory. This issue occurs with the default one-node installation of Jenkins. The failure is due to an attempt to mount links for both '/var' and '/var/lib', resulting in a conflict as '/var' is already mounted.",
            "StackTrace": [
                "org.apache.hadoop.fs.FileAlreadyExistsException: Path /var already exists as dir; cannot create link here",
                "at org.apache.hadoop.fs.viewfs.InodeTree.createLink(InodeTree.java:244)",
                "at org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:334)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:167)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2094)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:79)",
                "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2128)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2110)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:290)",
                "at org.apache.hadoop.fs.viewfs.ViewFileSystemTestSetup.setupForViewFileSystem(ViewFileSystemTestSetup.java:76)",
                "at org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem.setUp(TestFSMainOperationsLocalFileSystem.java:40)"
            ],
            "StepsToReproduce": [
                "1. Set up a default one-node installation of Jenkins.",
                "2. Create a user's home directory nested deeper than two levels from the root directory.",
                "3. Run the `TestFSMainOperationsLocalFileSystem` test."
            ],
            "ExpectedBehavior": "The test should pass without any exceptions, and the links should be created successfully.",
            "ObservedBehavior": "The test fails with a `FileAlreadyExistsException` indicating that the path '/var' already exists as a directory.",
            "Resolution": "The issue was addressed in HADOOP-8036, but the fix was later reverted in HADOOP-8129."
        }
    },
    {
        "filename": "HADOOP-11754.json",
        "creation_time": "2015-03-26T05:22:54.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11754",
            "Title": "ResourceManager Fails to Start in Non-Secure Mode Due to Authentication Filter Error",
            "Description": "The ResourceManager fails to start in non-secure mode, resulting in a ServletException caused by an inability to read the signature secret file. This issue appears to be a regression introduced by HADOOP-10670.",
            "StackTrace": [
                "2015-03-25 22:02:42,526 WARN org.mortbay.log: failed RMAuthenticationFilter: javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "2015-03-25 22:02:42,526 WARN org.mortbay.log: Failed startup of context org.mortbay.jetty.webapp.WebAppContext@6de50b08{/,jar:file:/Users/sjlee/hadoop-3.0.0-SNAPSHOT/share/hadoop/yarn/hadoop-yarn-common-3.0.0-SNAPSHOT.jar!/webapps/cluster}",
                "javax.servlet.ServletException: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:266)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:225)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.init(DelegationTokenAuthenticationFilter.java:161)",
                "at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.init(RMAuthenticationFilter.java:53)",
                "at org.mortbay.jetty.servlet.FilterHolder.doStart(FilterHolder.java:97)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.servlet.ServletHandler.initialize(ServletHandler.java:713)",
                "at org.mortbay.jetty.servlet.Context.startContext(Context.java:140)",
                "at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)",
                "at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)",
                "at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.handler.HandlerCollection.doStart(HandlerCollection.java:152)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:156)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)",
                "at org.mortbay.jetty.Server.doStart(Server.java:224)",
                "at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)",
                "at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:773)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)",
                "Caused by: java.lang.RuntimeException: Could not read signature secret file: /Users/sjlee/hadoop-http-auth-signature-secret",
                "at org.apache.hadoop.security.authentication.util.FileSignerSecretProvider.init(FileSignerSecretProvider.java:59)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeSecretProvider(AuthenticationFilter.java:264)",
                "... 23 more",
                "2015-03-25 22:02:42,538 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error starting ResourceManager",
                "org.apache.hadoop.yarn.webapp.WebAppException: Error starting http server",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:279)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:974)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1074)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)",
                "Caused by: java.io.IOException: Problem in starting http server. Server handlers failed",
                "at org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:785)",
                "at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:274)",
                "... 4 more"
            ],
            "StepsToReproduce": [
                "Attempt to start the ResourceManager in non-secure mode.",
                "Ensure that the signature secret file is not accessible or does not exist."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully in non-secure mode without any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing a ServletException due to an inability to read the signature secret file.",
            "Resolution": "Fixed in version 2.7.0"
        }
    },
    {
        "filename": "HADOOP-8225.json",
        "creation_time": "2012-03-13T20:33:53.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8225",
            "Title": "DistCp Fails When Invoked by Oozie Due to Missing Delegation Token",
            "Description": "When DistCp is invoked through a proxy-user (e.g., through Oozie), the delegation-token-store isn't picked up by DistCp correctly, leading to failures. The error message indicates a security exception related to System.exit being intercepted.",
            "StackTrace": [
                "ERROR [main] org.apache.hadoop.tools.DistCp: Couldn't complete DistCp operation:",
                "java.lang.SecurityException: Intercepted System.exit(-999)",
                "    at org.apache.oozie.action.hadoop.LauncherSecurityManager.checkExit(LauncherMapper.java:651)",
                "    at java.lang.Runtime.exit(Runtime.java:88)",
                "    at java.lang.System.exit(System.java:904)",
                "    at org.apache.hadoop.tools.DistCp.main(DistCp.java:357)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "    at java.lang.reflect.Method.invoke(Method.java:597)",
                "    at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:394)",
                "    at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:396)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with Oozie configured.",
                "2. Create a DistCp job that uses a proxy-user.",
                "3. Execute the DistCp job through Oozie."
            ],
            "ExpectedBehavior": "The DistCp job should complete successfully without any security exceptions.",
            "ObservedBehavior": "The DistCp job fails with a SecurityException indicating that System.exit was intercepted.",
            "Resolution": "A patch is being developed to ensure that HADOOP_TOKEN_FILE_LOCATION is copied to mapreduce.job.credentials.binary in the job configuration."
        }
    },
    {
        "filename": "HADOOP-10866.json",
        "creation_time": "2014-07-18T22:00:15.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10866",
            "Title": "Intermittent Symlink Test Failures in TestSymlinkLocalFSFileContext",
            "Description": "The symlink tests in the Hadoop project occasionally fail due to issues with symbolic links not being created correctly. This has been observed in multiple builds, leading to inconsistent test results.",
            "StackTrace": [
                "java.io.IOException: Path file:/home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/trunk/hadoop-common-project/hadoop-common/target/test/data/RtGBheUh4y/test1/linkToFile is not a symbolic link",
                "\tat org.apache.hadoop.fs.FileStatus.getSymlink(FileStatus.java:266)",
                "\tat org.apache.hadoop.fs.TestSymlinkLocalFS.testDanglingLink(TestSymlinkLocalFS.java:163)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "\tat java.lang.reflect.Method.invoke(Method.java:597)",
                "\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)",
                "\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)",
                "\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)",
                "\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)",
                "\tat org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)"
            ],
            "StepsToReproduce": [
                "Run the test suite for TestSymlinkLocalFSFileContext.",
                "Observe the test results for failures related to symbolic links."
            ],
            "ExpectedBehavior": "The symlink tests should pass without any errors, confirming that symbolic links are handled correctly.",
            "ObservedBehavior": "The tests intermittently fail with an IOException indicating that the specified path is not a symbolic link.",
            "Resolution": "The issue has been resolved in the latest commit, and the tests are now passing consistently."
        }
    },
    {
        "filename": "HADOOP-12089.json",
        "creation_time": "2015-06-15T17:34:46.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12089",
            "Title": "StorageException: No Lease ID Specified When Updating Folder Last Modified Time in WASB",
            "Description": "This issue occurs when HBase is attempting to delete old Write Ahead Logs (WALs) and update the /hbase/oldWALs folder. The error is similar to HADOOP-11523, which happens during distributed log splitting. The underlying cause is a lease on the blob without a specified lease ID in the request.",
            "StackTrace": [
                "2015-06-10 08:11:40,636 WARN org.apache.hadoop.hbase.master.cleaner.CleanerChore: Error while deleting: wasb://basecus1-1@basestoragecus1.blob.core.windows.net/hbase/oldWALs/workernode10.dthbasecus1.g1.internal.cloudapp.net%2C60020%2C1433908062461.1433921692855",
                "org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2602)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2613)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1505)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.delete(NativeAzureFileSystem.java:1437)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteFiles(CleanerChore.java:256)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.checkAndDeleteEntries(CleanerChore.java:157)",
                "at org.apache.hadoop.hbase.master.cleaner.CleanerChore.chore(CleanerChore.java:124)",
                "at org.apache.hadoop.hbase.Chore.run(Chore.java:80)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: com.microsoft.azure.storage.StorageException: There is currently a lease on the blob and no lease ID was specified in the request.",
                "at com.microsoft.azure.storage.StorageException.translateException(StorageException.java:162)",
                "at com.microsoft.azure.storage.core.StorageRequest.materializeException(StorageRequest.java:307)",
                "at com.microsoft.azure.storage.core.ExecutionEngine.executeWithRetry(ExecutionEngine.java:177)",
                "at com.microsoft.azure.storage.blob.CloudBlob.uploadProperties(CloudBlob.java:2991)",
                "at org.apache.hadoop.fs.azure.StorageInterfaceImpl$CloudBlobWrapperImpl.uploadProperties(StorageInterfaceImpl.java:372)",
                "at org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.updateFolderLastModifiedTime(AzureNativeFileSystemStore.java:2597)",
                "... 8 more"
            ],
            "StepsToReproduce": [
                "1. Set up HBase with a configured WASB storage.",
                "2. Trigger the deletion of old WALs in HBase.",
                "3. Monitor the logs for any warnings or errors related to folder updates."
            ],
            "ExpectedBehavior": "HBase should successfully delete old WALs and update the /hbase/oldWALs folder without any errors.",
            "ObservedBehavior": "An error occurs indicating that there is a lease on the blob and no lease ID was specified, preventing the deletion of old WALs.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-11934.json",
        "creation_time": "2015-05-07T00:38:53.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11934",
            "Title": "Infinite Loop in LdapGroupsMapping with JavaKeyStoreProvider",
            "Description": "When using the LdapGroupsMapping code alongside the JavaKeyStoreProvider, an infinite loop occurs, leading to a stack overflow. This issue arises due to recursive calls between the two components.",
            "StackTrace": [
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:88)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:65)",
                "at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:291)",
                "at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:58)",
                "at org.apache.hadoop.conf.Configuration.getPasswordFromCredentialProviders(Configuration.java:1863)",
                "at org.apache.hadoop.conf.Configuration.getPassword(Configuration.java:1843)",
                "at org.apache.hadoop.security.LdapGroupsMapping.getPassword(LdapGroupsMapping.java:386)",
                "at org.apache.hadoop.security.LdapGroupsMapping.setConf(LdapGroupsMapping.java:349)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:70)",
                "at org.apache.hadoop.security.Groups.<init>(Groups.java:66)",
                "at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)",
                "at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)",
                "at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:804)",
                "at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)",
                "at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2753)",
                "at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2745)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2611)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)"
            ],
            "StepsToReproduce": [
                "1. Configure LdapGroupsMapping with JavaKeyStoreProvider.",
                "2. Attempt to retrieve user group information.",
                "3. Observe the application behavior."
            ],
            "ExpectedBehavior": "The application should retrieve user group information without entering an infinite loop.",
            "ObservedBehavior": "The application enters an infinite loop, resulting in a stack overflow and termination.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11722.json",
        "creation_time": "2015-03-16T21:39:07.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11722",
            "Title": "Service Instances Crash When Deleting Expired Tokens in ZKDelegationTokenSecretManager",
            "Description": "When multiple instances of a service using ZKDelegationTokenSecretManager attempt to delete the same expired token simultaneously, only one instance succeeds while the others throw an exception, causing those instances to crash.",
            "StackTrace": [
                "2015-03-15 10:24:54,000 ERROR org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: ExpiredTokenRemover thread received unexpected exception",
                "java.lang.RuntimeException: Could not remove Stored Token ZKDTSMDelegationToken_28",
                "at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:770)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.removeExpiredToken(AbstractDelegationTokenSecretManager.java:605)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.access$400(AbstractDelegationTokenSecretManager.java:54)",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:656)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /zkdtsm/ZKDTSMRoot/ZKDTSMTokensRoot/DT_28",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:238)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:233)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:214)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:41)",
                "at org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.removeStoredToken(ZKDelegationTokenSecretManager.java:764)",
                "... 4 more"
            ],
            "StepsToReproduce": [
                "1. Start multiple instances of a service that uses ZKDelegationTokenSecretManager.",
                "2. Ensure that there are expired tokens present in the ZKDelegationTokenSecretManager.",
                "3. Trigger the deletion of the expired tokens simultaneously from all instances."
            ],
            "ExpectedBehavior": "Only one instance should successfully delete the expired token, while the others should handle the situation gracefully without crashing.",
            "ObservedBehavior": "All instances enter the deletion loop, leading to multiple exceptions being thrown, causing the non-successful instances to crash.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-15331.json",
        "creation_time": "2018-03-21T01:15:56.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15331",
            "Title": "Race Condition in Configuration Class Causes Stream Closed Exception",
            "Description": "There is a race condition in the way Hadoop handles the Configuration class. The scenario is as follows: two threads share the same Configuration class. One thread adds resources to the configuration while the other clones it. Resources are loaded lazily in a deferred call to `loadResources()`. If cloning occurs after adding resources but before parsing them, temporary resources like input stream pointers are cloned. Eventually, both copies will load the input stream resources pointing to the same input streams. When one thread parses the input stream XML and closes it, the other thread, which still has a pointer to the same input stream, will crash with a stream closed exception.",
            "StackTrace": [
                "2018-02-28 08:23:19,589 ERROR org.apache.hadoop.conf.Configuration: error parsing conf java.io.BufferedInputStream@7741d346",
                "com.ctc.wstx.exc.WstxIOException: Stream closed",
                "\tat com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:578)",
                "\tat com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)",
                "\tat org.apache.hadoop.conf.Configuration.parse(Configuration.java:2803)",
                "\tat org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2853)",
                "\tat org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2817)",
                "\tat org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2689)",
                "\tat org.apache.hadoop.conf.Configuration.get(Configuration.java:1420)",
                "\tat org.apache.hadoop.security.authorize.ServiceAuthorizationManager.refreshWithLoadedConfiguration(ServiceAuthorizationManager.java:161)",
                "\tat org.apache.hadoop.ipc.Server.refreshServiceAclWithLoadedConfiguration(Server.java:607)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:586)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer(AdminService.java:188)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceStart(AdminService.java:165)",
                "\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "\tat org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1231)",
                "\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1421)"
            ],
            "StepsToReproduce": [
                "Create a Configuration object.",
                "In one thread, add a resource to the Configuration object.",
                "In another thread, clone the Configuration object.",
                "In the second thread, attempt to access a property from the cloned Configuration object.",
                "In the first thread, access a property from the original Configuration object."
            ],
            "ExpectedBehavior": "The Configuration object should handle concurrent access without throwing exceptions.",
            "ObservedBehavior": "A Stream closed exception is thrown when accessing the input stream after it has been closed by another thread.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-14062.json",
        "creation_time": "2016-12-20T00:35:48.000+0000",
        "bug_report": {
            "BugID": "HADOOP-14062",
            "Title": "EOFException in ApplicationMasterProtocolPBClientImpl.allocate with RPC Privacy Enabled",
            "Description": "When privacy is enabled for RPC (hadoop.rpc.protection = privacy), the method ApplicationMasterProtocolPBClientImpl.allocate sometimes fails with an EOFException. This issue has been reproduced with Spark 2.0.2 built against the latest branch-2.8 and during a simple distcp job on the same branch. This bug is critical as it blocks YARN users from encrypting RPC in their Hadoop clusters.",
            "StackTrace": [
                "java.io.EOFException: End of File Exception between local host is: \"<application_master_host>/<ip_addr>\"; destination host is: \"<rm_host>\":8030; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException",
                "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
                "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)",
                "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)",
                "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)",
                "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1486)",
                "\tat org.apache.hadoop.ipc.Client.call(Client.java:1428)",
                "\tat org.apache.hadoop.ipc.Client.call(Client.java:1338)",
                "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)",
                "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "\tat com.sun.proxy.$Proxy80.allocate(Unknown Source)",
                "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "\tat java.lang.reflect.Method.invoke(Method.java:497)",
                "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)",
                "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)",
                "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)",
                "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "\tat com.sun.proxy.$Proxy81.allocate(Unknown Source)",
                "\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.makeRemoteRequest(RMContainerRequestor.java:204)",
                "\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:735)",
                "\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:269)",
                "\tat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$AllocatorRunnable.run(RMCommunicator.java:281)",
                "\tat java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.EOFException",
                "\tat java.io.DataInputStream.readInt(DataInputStream.java:392)",
                "\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1785)",
                "\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1156)",
                "\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1053)"
            ],
            "StepsToReproduce": [
                "Set hadoop.rpc.protection equal to privacy.",
                "Write data to HDFS using Spark as follows: sc.parallelize(1 to (5*1024*1024)).map(k => Seq(k, org.apache.commons.lang.RandomStringUtils.random(1024, \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWxyZ0123456789\")).mkString(\"|\")).toDF().repartition(100).write.parquet(\"hdfs:///tmp/testData\").",
                "Attempt to distcp that data to another location in HDFS using the command: hadoop distcp -Dmapreduce.framework.name=yarn hdfs:///tmp/testData hdfs:///tmp/testDataCopy."
            ],
            "ExpectedBehavior": "The distcp command should successfully copy the data from the source to the destination without any exceptions.",
            "ObservedBehavior": "The distcp command fails with an EOFException, preventing the data from being copied.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11149.json",
        "creation_time": "2014-09-27T16:37:06.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11149",
            "Title": "TestZKFailoverController Timeout Error",
            "Description": "The test 'testGracefulFailover' in the 'TestZKFailoverController' class fails due to a timeout after 25000 milliseconds. This issue occurs when running the tests in a Jenkins environment on CentOS 6.5.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 25000 milliseconds",
                "\tat java.lang.Object.wait(Native Method)",
                "\tat org.apache.hadoop.ha.ZKFailoverController.waitForActiveAttempt(ZKFailoverController.java:467)",
                "\tat org.apache.hadoop.ha.ZKFailoverController.doGracefulFailover(ZKFailoverController.java:657)",
                "\tat org.apache.hadoop.ha.ZKFailoverController.access$400(ZKFailoverController.java:61)",
                "\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:602)",
                "\tat org.apache.hadoop.ha.ZKFailoverController$3.run(ZKFailoverController.java:599)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:415)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1621)",
                "\tat org.apache.hadoop.ha.ZKFailoverController.gracefulFailoverToYou(ZKFailoverController.java:599)",
                "\tat org.apache.hadoop.ha.ZKFCRpcServer.gracefulFailover(ZKFCRpcServer.java:94)",
                "\tat org.apache.hadoop.ha.TestZKFailoverController.testGracefulFailover(TestZKFailoverController.java:448)"
            ],
            "StepsToReproduce": [
                "Run the test suite for org.apache.hadoop.ha.TestZKFailoverController.",
                "Observe the test 'testGracefulFailover'."
            ],
            "ExpectedBehavior": "The test 'testGracefulFailover' should complete successfully without timing out.",
            "ObservedBehavior": "The test 'testGracefulFailover' fails with a timeout error after 25000 milliseconds.",
            "Resolution": "Increase the timeout duration for the test to prevent it from failing."
        }
    },
    {
        "filename": "HADOOP-15059.json",
        "creation_time": "2017-11-21T20:54:52.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15059",
            "Title": "3.0 Deployment Fails with Old Version MR Tarball Due to Token Incompatibility",
            "Description": "When attempting to deploy a Hadoop 3.0 cluster using a 2.9 MR tarball, the MR job fails with a runtime exception related to user token incompatibility. This issue arises from changes in the delegation token format between versions 2.9 and 3.0, which impacts the rolling upgrade capability.",
            "StackTrace": [
                "2017-11-21 12:42:50,911 INFO [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Created MRAppMaster for application appattempt_1511295641738_0003_000001",
                "2017-11-21 12:42:51,070 WARN [main] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable",
                "2017-11-21 12:42:51,118 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster",
                "java.lang.RuntimeException: Unable to determine current user",
                "\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:254)",
                "\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:220)",
                "\tat org.apache.hadoop.conf.Configuration$Resource.<init>(Configuration.java:212)",
                "\tat org.apache.hadoop.conf.Configuration.addResource(Configuration.java:888)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1638)",
                "Caused by: java.io.IOException: Exception reading /tmp/nm-local-dir/usercache/jdu/appcache/application_1511295641738_0003/container_e03_1511295641738_0003_01_000001/container_tokens",
                "\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:208)",
                "\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:907)",
                "\tat org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:820)",
                "\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:689)",
                "\tat org.apache.hadoop.conf.Configuration$Resource.getRestrictParserDefault(Configuration.java:252)",
                "\t... 4 more",
                "Caused by: java.io.IOException: Unknown version 1 in token storage.",
                "\tat org.apache.hadoop.security.Credentials.readTokenStorageStream(Credentials.java:226)",
                "\tat org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:205)",
                "\t... 8 more",
                "2017-11-21 12:42:51,122 INFO [main] org.apache.hadoop.util.ExitUtil: Exiting with status 1: java.lang.RuntimeException: Unable to determine current user"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop 3.0 cluster.",
                "2. Use a 2.9 MR tarball for the deployment.",
                "3. Submit an MR job."
            ],
            "ExpectedBehavior": "The MR job should run successfully without any errors related to user tokens.",
            "ObservedBehavior": "The MR job fails with a runtime exception indicating an inability to determine the current user due to token incompatibility.",
            "Resolution": "The issue has been resolved in a subsequent release by ensuring compatibility of the delegation token format during rolling upgrades."
        }
    },
    {
        "filename": "HADOOP-15307.json",
        "creation_time": "2018-03-12T14:56:59.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15307",
            "Title": "NFS Gateway Fails to Start Due to Unsupported AUTH_SYS Verifier",
            "Description": "When the NFS gateway starts, if the portmapper request is denied by rpcbind (e.g., due to /etc/hosts.allow not including localhost), the NFS gateway fails with an obscure exception related to unsupported verifier flavor AUTH_SYS.",
            "StackTrace": [
                "2018-03-05 12:49:31,976 INFO org.apache.hadoop.oncrpc.SimpleUdpServer: Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1",
                "2018-03-05 12:49:31,988 INFO org.apache.hadoop.oncrpc.SimpleTcpServer: Started listening to TCP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1",
                "2018-03-05 12:49:31,993 TRACE org.apache.hadoop.oncrpc.RpcCall: Xid:692394656, messageType:RPC_CALL, rpcVersion:2, program:100000, version:2, procedure:1, credential:(AuthFlavor:AUTH_NONE), verifier:(AuthFlavor:AUTH_NONE)",
                "2018-03-05 12:49:31,998 FATAL org.apache.hadoop.mount.MountdBase: Failed to start the server. Cause:",
                "java.lang.UnsupportedOperationException: Unsupported verifier flavor AUTH_SYS",
                "at org.apache.hadoop.oncrpc.security.Verifier.readFlavorAndVerifier(Verifier.java:45)",
                "at org.apache.hadoop.oncrpc.RpcDeniedReply.read(RpcDeniedReply.java:50)",
                "at org.apache.hadoop.oncrpc.RpcReply.read(RpcReply.java:67)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:71)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:130)",
                "at org.apache.hadoop.oncrpc.RpcProgram.register(RpcProgram.java:101)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:83)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:60)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "2018-03-05 12:49:32,007 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1"
            ],
            "StepsToReproduce": [
                "1. Ensure that the NFS gateway is configured to start.",
                "2. Modify /etc/hosts.allow to deny access to localhost.",
                "3. Start the NFS gateway service."
            ],
            "ExpectedBehavior": "The NFS gateway should start successfully without any exceptions.",
            "ObservedBehavior": "The NFS gateway fails to start with an UnsupportedOperationException related to the AUTH_SYS verifier.",
            "Resolution": "The verifier should be updated to handle AUTH_SYS as a valid authentication flavor."
        }
    },
    {
        "filename": "HADOOP-11446.json",
        "creation_time": "2014-12-23T22:15:23.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11446",
            "Title": "S3AOutputStream Should Use Shared Thread Pool to Avoid OutOfMemoryError",
            "Description": "When working with Terry Padgett who used s3a for HBase snapshot, an OutOfMemoryError (OOME) was encountered during the export of HBase snapshots to S3A. The issue arises because each TransferManager creates its own thread pool, leading to excessive thread creation and ultimately resulting in OOME. Increasing the nofile ulimit to 102400 did not resolve the issue.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:713)",
                "at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1360)",
                "at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)",
                "at com.amazonaws.services.s3.transfer.internal.UploadMonitor.<init>(UploadMonitor.java:129)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:449)",
                "at com.amazonaws.services.s3.transfer.TransferManager.upload(TransferManager.java:382)",
                "at org.apache.hadoop.fs.s3a.S3AOutputStream.close(S3AOutputStream.java:127)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)",
                "at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:54)",
                "at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:112)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:356)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.run(ExportSnapshot.java:791)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.innerMain(ExportSnapshot.java:882)",
                "at org.apache.hadoop.hbase.snapshot.ExportSnapshot.main(ExportSnapshot.java:886)"
            ],
            "StepsToReproduce": [
                "1. Increase the nofile ulimit to 102400.",
                "2. Use s3a to export an HBase snapshot.",
                "3. Observe the logs for any OutOfMemoryError."
            ],
            "ExpectedBehavior": "The HBase snapshot should be exported to S3A without encountering an OutOfMemoryError.",
            "ObservedBehavior": "An OutOfMemoryError occurs during the export process due to excessive thread creation by multiple TransferManager instances.",
            "Resolution": "One solution is to pass a shared thread pool to the TransferManager to prevent the creation of multiple thread pools."
        }
    },
    {
        "filename": "HADOOP-12689.json",
        "creation_time": "2016-01-04T23:30:49.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12689",
            "Title": "S3 Filesystem Operations Fail Due to IOException Instead of Null Return",
            "Description": "The issue arises from the resolution of HADOOP-10542, where returning null was replaced with throwing an IOException. This change has caused several S3 filesystem operations to fail, as they now throw IOException instead of returning expected values. The affected methods include:\n- S3FileSystem.getFileStatus() (no longer raises FileNotFoundException)\n- FileSystem.exists() (no longer returns false)\n- S3FileSystem.create() (no longer succeeds)\n\nTo reproduce the issue, run the following command:\n\n`hadoop distcp hdfs://localhost:9000/test s3://xxx:yyy@com.bar.foo/`\n\nChanging the code to return null instead of raising IOException resolves the issue and allows distcp to succeed.",
            "StackTrace": [
                "2015-12-11 10:04:34,030 FATAL [IPC Server handler 6 on 44861] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1449826461866_0005_m_000006_0 - exited : java.io.IOException: /test doesn't exist",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:170)",
                "at org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:221)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy17.retrieveINode(Unknown Source)",
                "at org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:340)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:230)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "StepsToReproduce": [
                "1. Ensure Hadoop is set up and running.",
                "2. Execute the command: `hadoop distcp hdfs://localhost:9000/test s3://xxx:yyy@com.bar.foo/`.",
                "3. Observe the resulting IOException in the logs."
            ],
            "ExpectedBehavior": "The command should successfully copy files from HDFS to S3 without throwing an IOException.",
            "ObservedBehavior": "The command fails with an IOException indicating that the specified path doesn't exist.",
            "Resolution": "Changing the code to return null instead of raising IOException resolves the issue."
        }
    },
    {
        "filename": "HADOOP-13132.json",
        "creation_time": "2016-05-11T11:42:30.000+0000",
        "bug_report": {
            "BugID": "HADOOP-13132",
            "Title": "ClassCastException in LoadBalancingKMSClientProvider during Oozie job execution",
            "Description": "An Oozie job with a single shell action fails due to a ClassCastException. The error originates from the NodeManager, which throws an uncaught exception that prevents the job from completing successfully. This results in the YARN logs not being reported or saved, and the exact cause of the failure is obscured.",
            "StackTrace": [
                "2016-05-10 11:10:14,290 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[LogAggregationService #652,5,main] threw an Exception.",
                "java.lang.ClassCastException: org.apache.hadoop.security.authentication.client.AuthenticationException cannot be cast to java.security.GeneralSecurityException",
                "at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.decryptEncryptedKey(LoadBalancingKMSClientProvider.java:189)",
                "at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.decryptEncryptedKey(KeyProviderCryptoExtension.java:388)",
                "at org.apache.hadoop.hdfs.DFSClient.decryptEncryptedDataEncryptionKey(DFSClient.java:1419)",
                "at org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1521)",
                "at org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:108)",
                "at org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:59)",
                "at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:577)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:683)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:679)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.create(FileContext.java:679)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:382)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter$1.run(AggregatedLogFormat.java:377)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogFormat$LogWriter.<init>(AggregatedLogFormat.java:376)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:246)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:456)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:421)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$2.run(LogAggregationService.java:384)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Submit an Oozie job with a single shell action.",
                "Monitor the job execution through YARN logs."
            ],
            "ExpectedBehavior": "The Oozie job should complete successfully without any exceptions, and YARN logs should be reported and saved correctly.",
            "ObservedBehavior": "The Oozie job fails with a ClassCastException, preventing the job from completing and obscuring the actual cause of the failure.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-15121.json",
        "creation_time": "2017-12-15T07:41:39.000+0000",
        "bug_report": {
            "BugID": "HADOOP-15121",
            "Title": "NullPointerException in DecayRpcScheduler Metrics Initialization",
            "Description": "When setting `ipc.8020.scheduler.impl` to `org.apache.hadoop.ipc.DecayRpcScheduler`, a NullPointerException occurs in the NameNode. The issue seems to stem from the `metricsProxy` in `DecayRpcScheduler` not initializing its `delegate` field properly during its initialization method.",
            "StackTrace": [
                "2017-12-15 15:26:34,662 ERROR impl.MetricsSourceAdapter (MetricsSourceAdapter.java:getMetrics(202)) - Error getting metrics from source DecayRpcSchedulerMetrics2.ipc.8020",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getMetrics(DecayRpcScheduler.java:781)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMetrics(MetricsSourceAdapter.java:199)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.updateJmxCache(MetricsSourceAdapter.java:182)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.getMBeanInfo(MetricsSourceAdapter.java:155)",
                "    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getNewMBeanClassName(DefaultMBeanServerInterceptor.java:333)",
                "    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:319)",
                "    at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:66)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:222)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:100)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:268)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:233)",
                "    at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.registerMetrics2Source(DecayRpcScheduler.java:709)",
                "    at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.<init>(DecayRpcScheduler.java:685)",
                "    at org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy.getInstance(DecayRpcScheduler.java:693)",
                "    at org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:236)",
                "    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)",
                "    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)",
                "    at org.apache.hadoop.ipc.CallQueueManager.createScheduler(CallQueueManager.java:102)",
                "    at org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:76)",
                "    at org.apache.hadoop.ipc.Server.<init>(Server.java:2612)",
                "    at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)",
                "    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:374)",
                "    at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:349)",
                "    at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:415)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:755)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:697)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:905)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:884)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1610)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1678)"
            ],
            "StepsToReproduce": [
                "Set the configuration property `ipc.8020.scheduler.impl` to `org.apache.hadoop.ipc.DecayRpcScheduler`.",
                "Start the NameNode."
            ],
            "ExpectedBehavior": "The NameNode should start without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the initialization of the NameNode.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-8110.json",
        "creation_time": "2012-02-24T18:49:27.000+0000",
        "bug_report": {
            "BugID": "HADOOP-8110",
            "Title": "Intermittent Failure in TestViewFsTrash",
            "Description": "The TestViewFsTrash test occasionally fails due to an assertion error indicating that the expected value does not match the actual value. This issue has been observed in multiple builds, suggesting a potential instability in the test or the underlying code.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: -expunge failed expected:<0> but was:<1>",
                "at junit.framework.Assert.fail(Assert.java:47)",
                "at junit.framework.Assert.failNotEquals(Assert.java:283)",
                "at junit.framework.Assert.assertEquals(Assert.java:64)",
                "at junit.framework.Assert.assertEquals(Assert.java:195)",
                "at org.apache.hadoop.fs.TestTrash.trashShell(TestTrash.java:322)",
                "at org.apache.hadoop.fs.viewfs.TestViewFsTrash.testTrash(TestViewFsTrash.java:73)"
            ],
            "StepsToReproduce": [
                "Run the TestViewFsTrash test suite.",
                "Observe the test results for intermittent failures."
            ],
            "ExpectedBehavior": "The TestViewFsTrash test should pass without any assertion errors.",
            "ObservedBehavior": "The TestViewFsTrash test fails intermittently with an assertion error indicating a mismatch between expected and actual values.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-11400.json",
        "creation_time": "2014-12-12T10:05:52.000+0000",
        "bug_report": {
            "BugID": "HADOOP-11400",
            "Title": "GraphiteSink Fails to Reconnect After 'Broken Pipe' Error",
            "Description": "After a network error, the GraphiteSink does not reconnect to the Graphite server, resulting in metrics not being sent. This issue is observed in the Hadoop 2.6.0 version, specifically in the GraphiteSinkFixed class, which lacks reconnection logic.",
            "StackTrace": [
                "2014-12-11 16:39:21,655 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception, retry in 4806ms",
                "org.apache.hadoop.metrics2.MetricsException: Error flushing metrics",
                "    at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)",
                "    at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)",
                "Caused by: java.net.SocketException: Broken pipe",
                "    at java.net.SocketOutputStream.socketWrite0(Native Method)",
                "    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)",
                "    at java.net.SocketOutputStream.write(SocketOutputStream.java:159)",
                "    at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)",
                "    at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)",
                "    at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)",
                "    at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)",
                "    at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)",
                "    at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)",
                "    ... 5 more",
                "2014-12-11 16:39:26,463 ERROR org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Got sink exception and over retry limit, suppressing further error messages",
                "org.apache.hadoop.metrics2.MetricsException: Error flushing metrics",
                "    at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:120)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:184)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.consume(MetricsSinkAdapter.java:43)",
                "    at org.apache.hadoop.metrics2.impl.SinkQueue.consumeAll(SinkQueue.java:87)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter.publishMetricsFromQueue(MetricsSinkAdapter.java:129)",
                "    at org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$1.run(MetricsSinkAdapter.java:88)",
                "Caused by: java.net.SocketException: Broken pipe",
                "    at java.net.SocketOutputStream.socketWrite0(Native Method)",
                "    at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:113)",
                "    at java.net.SocketOutputStream.write(SocketOutputStream.java:159)",
                "    at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)",
                "    at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)",
                "    at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:295)",
                "    at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)",
                "    at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)",
                "    at org.apache.hadoop.metrics2.sink.GraphiteSinkFixed.flush(GraphiteSinkFixed.java:118)",
                "    ... 5 more"
            ],
            "StepsToReproduce": [
                "1. Start the application using GraphiteSink.",
                "2. Simulate a network error that causes a 'broken pipe'.",
                "3. Observe the behavior of the GraphiteSink after the error."
            ],
            "ExpectedBehavior": "The GraphiteSink should automatically attempt to reconnect to the Graphite server after a network error.",
            "ObservedBehavior": "The GraphiteSink fails to reconnect, and metrics are not sent after a network error.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-9865.json",
        "creation_time": "2013-08-12T23:22:58.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9865",
            "Title": "Regression in FileContext.globStatus() with Relative Paths on Windows",
            "Description": "A regression was discovered in the FileContext.globStatus() method when running unit tests on Windows. The issue arises when a job fails to launch due to a relative path being passed to FileContext.globStatus(), which is not supported. This results in the unit test failing, while it passes on Linux. The problem is linked to the changes made in HADOOP-9817.",
            "StackTrace": [
                "2013-08-12 16:12:05,937 WARN  [ContainersLauncher #0] launcher.ContainerLaunch (ContainerLaunch.java:call(270)) - Failed to launch container.",
                "org.apache.hadoop.HadoopIllegalArgumentException: Path is relative",
                "\tat org.apache.hadoop.fs.Path.checkNotRelative(Path.java:74)",
                "\tat org.apache.hadoop.fs.FileContext.getFSofPath(FileContext.java:304)",
                "\tat org.apache.hadoop.fs.Globber.schemeFromPath(Globber.java:107)",
                "\tat org.apache.hadoop.fs.Globber.glob(Globber.java:128)",
                "\tat org.apache.hadoop.fs.FileContext$Util.globStatus(FileContext.java:1908)",
                "\tat org.apache.hadoop.fs.FileUtil.createJarWithClassPath(FileUtil.java:1247)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.sanitizeEnv(ContainerLaunch.java:679)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:232)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:1)",
                "\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the unit test TestMRJobClient on a Windows environment.",
                "Observe the job launch process and the subsequent status listing.",
                "Check the logs for any warnings or errors related to path handling."
            ],
            "ExpectedBehavior": "The job should launch successfully and the status should be retrievable without errors.",
            "ObservedBehavior": "The job fails to launch due to a relative path error, causing the unit test to fail.",
            "Resolution": "A patch has been created to address this issue, and additional unit tests will be added to verify the behavior."
        }
    },
    {
        "filename": "HADOOP-9977.json",
        "creation_time": "2013-09-17T22:11:54.000+0000",
        "bug_report": {
            "BugID": "HADOOP-9977",
            "Title": "Hadoop Services Fail to Start with Mismatched Keypass and Keystore Password",
            "Description": "When enabling SSL in the Hadoop configuration and using different passwords for the keystore and key, the Hadoop services (NameNode, ResourceManager, DataNode, NodeManager, SecondaryNameNode) fail to start. The error indicates an issue with recovering the key due to the mismatched passwords.",
            "StackTrace": [
                "2013-09-17 21:39:00,794 FATAL namenode.NameNode (NameNode.java:main(1325)) - Exception in namenode join",
                "java.io.IOException: java.security.UnrecoverableKeyException: Cannot recover key",
                "    at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:222)",
                "    at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:174)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer$1.<init>(NameNodeHttpServer.java:76)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:74)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:626)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:488)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:684)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:669)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1254)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1320)",
                "Caused by: java.security.UnrecoverableKeyException: Cannot recover key",
                "    at sun.security.provider.KeyProtector.recover(KeyProtector.java:328)",
                "    at sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:138)",
                "    at sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:55)",
                "    at java.security.KeyStore.getKey(KeyStore.java:792)",
                "    at sun.security.ssl.SunX509KeyManagerImpl.<init>(SunX509KeyManagerImpl.java:131)",
                "    at sun.security.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:68)",
                "    at javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:259)",
                "    at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:170)",
                "    at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:121)",
                "    at org.apache.hadoop.http.HttpServer.<init>(HttpServer.java:220)",
                "    ... 9 more"
            ],
            "StepsToReproduce": [
                "Enable SSL in the Hadoop configuration.",
                "Create a keystore with different keypass and storepass using the following command:",
                "keytool -genkey -alias host1 -keyalg RSA -keysize 1024 -dname 'CN=host1,OU=cm,O=cm,L=san jose,ST=ca,C=us' -keypass hadoop -keystore keystore.jks -storepass hadoopKey",
                "Set the following properties in ssl-server.xml:",
                "<property><name>ssl.server.keystore.keypassword</name><value>hadoop</value></property>",
                "<property><name>ssl.server.keystore.password</name><value>hadoopKey</value></property>",
                "Attempt to start the Hadoop services (NameNode, ResourceManager, DataNode, NodeManager, SecondaryNameNode)."
            ],
            "ExpectedBehavior": "Hadoop services should start successfully with the provided keystore and key passwords.",
            "ObservedBehavior": "Hadoop services fail to start with an UnrecoverableKeyException indicating that the key cannot be recovered.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-12611.json",
        "creation_time": "2015-12-01T17:14:38.000+0000",
        "bug_report": {
            "BugID": "HADOOP-12611",
            "Title": "Intermittent Test Failure in TestZKSignerSecretProvider#testMultipleInit",
            "Description": "The test TestZKSignerSecretProvider#testMultipleInit occasionally fails with an assertion error indicating that a null value was expected but a non-null value was returned. This issue may have been introduced after the changes made in HADOOP-12181.",
            "StackTrace": [
                "java.lang.AssertionError: expected null, but was:<[B@142bad79>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotNull(Assert.java:664)",
                "at org.junit.Assert.assertNull(Assert.java:646)",
                "at org.junit.Assert.assertNull(Assert.java:656)",
                "at org.apache.hadoop.security.authentication.util.TestZKSignerSecretProvider.testMultipleInit(TestZKSignerSecretProvider.java:149)",
                "2015-11-29 00:24:33,325 ERROR ZKSignerSecretProvider - An unexpected exception occurred while pulling data from ZooKeeper",
                "java.lang.IllegalStateException: instance must be started before calling this method",
                "at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "at org.apache.curator.framework.imps.CuratorFrameworkImpl.getData(CuratorFrameworkImpl.java:363)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.pullFromZK(ZKSignerSecretProvider.java:341)",
                "at org.apache.hadoop.security.authentication.util.ZKSignerSecretProvider.rollSecret(ZKSignerSecretProvider.java:264)",
                "at org.apache.hadoop.security.authentication.util.RolloverSignerSecretProvider$1.run(RolloverSignerSecretProvider.java:97)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Run the test suite for TestZKSignerSecretProvider.",
                "Observe the test results for TestZKSignerSecretProvider#testMultipleInit."
            ],
            "ExpectedBehavior": "The test TestZKSignerSecretProvider#testMultipleInit should pass without any assertion errors.",
            "ObservedBehavior": "The test occasionally fails with an AssertionError indicating that a null value was expected but a non-null value was returned.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HADOOP-10142.json",
        "creation_time": "2013-12-03T06:33:32.000+0000",
        "bug_report": {
            "BugID": "HADOOP-10142",
            "Title": "Reduce log verbosity for unprivileged user group lookups",
            "Description": "The current implementation of ShellBasedUnixGroupsMapping generates excessive logs when attempting to retrieve groups for unprivileged users, such as 'dr.who'. This results in a large number of warning messages being logged for each request, which can clutter log files and make it difficult to identify other important log entries.",
            "StackTrace": [
                "2013-12-03 11:34:56,589 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user dr.who",
                "org.apache.hadoop.util.Shell$ExitCodeException: id: dr.who: No such user",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:504)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:417)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:636)",
                "at org.apache.hadoop.util.Shell.execCommand(Shell.java:725)",
                "at org.apache.hadoop.util.Shell.execCommand(Shell.java:708)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)",
                "at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)",
                "at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)",
                "at org.apache.hadoop.security.Groups.getGroups(Groups.java:95)",
                "at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1376)",
                "at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.<init>(FSPermissionChecker.java:63)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getPermissionChecker(FSNamesystem.java:3228)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListingInt(FSNamesystem.java:4063)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getListing(FSNamesystem.java:4052)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing(NameNodeRpcServer.java:748)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getDirectoryListing(NamenodeWebHdfsMethods.java:715)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getListingStream(NamenodeWebHdfsMethods.java:727)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:675)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.access$400(NamenodeWebHdfsMethods.java:114)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:623)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods$3.run(NamenodeWebHdfsMethods.java:618)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1515)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.get(NamenodeWebHdfsMethods.java:618)",
                "at org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.getRoot(NamenodeWebHdfsMethods.java:586)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:384)",
                "at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:85)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1310)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "Use WebHDFS from a Windows environment.",
                "Attempt to access a resource as an unprivileged user (e.g., 'dr.who').",
                "Observe the logs generated during the request."
            ],
            "ExpectedBehavior": "The system should not generate excessive warning logs for unprivileged users when attempting to retrieve group information.",
            "ObservedBehavior": "The system generates multiple warning logs indicating that the user 'dr.who' does not exist, cluttering the log files.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HADOOP-14727.json",
        "creation_time": "2017-08-02T21:56:38.000+0000",
        "bug_report": {
            "BugID": "HADOOP-14727",
            "Title": "Socket Not Closed Properly When Reading Configurations with BlockReaderRemote",
            "Description": "This issue was identified during Cloudera's internal testing of the alpha4 release. Reports indicated that some hosts ran out of file descriptors (FDs). Investigation revealed that both the Oozie server and Yarn JobHistoryServer had numerous sockets in the CLOSE_WAIT state. The issue was consistently reproducible by visiting the JobHistoryServer web UI and navigating through a job and its logs. Debugging indicated that the CLOSE_WAIT sockets were created from the BlockReaderRemote implementation, but no leaks were found in the code. Further analysis showed that reverting recent commits to the Configuration class eliminated the CLOSE_WAIT sockets, indicating a potential issue with socket management in the configuration parsing process.",
            "StackTrace": [
                "java.lang.Exception: test",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:745)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)",
                "at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:636)",
                "at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:566)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:749)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:807)",
                "at java.io.DataInputStream.read(DataInputStream.java:149)",
                "at com.ctc.wstx.io.StreamBootstrapper.ensureLoaded(StreamBootstrapper.java:482)",
                "at com.ctc.wstx.io.StreamBootstrapper.resolveStreamEncoding(StreamBootstrapper.java:306)",
                "at com.ctc.wstx.io.StreamBootstrapper.bootstrapInput(StreamBootstrapper.java:167)",
                "at com.ctc.wstx.stax.WstxInputFactory.doCreateSR(WstxInputFactory.java:573)",
                "at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:633)",
                "at com.ctc.wstx.stax.WstxInputFactory.createSR(WstxInputFactory.java:647)",
                "at com.ctc.wstx.stax.WstxInputFactory.createXMLStreamReader(WstxInputFactory.java:366)",
                "at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2649)",
                "at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2697)",
                "at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2662)",
                "at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2545)",
                "at org.apache.hadoop.conf.Configuration.get(Configuration.java:1076)",
                "at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1126)",
                "at org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1344)",
                "at org.apache.hadoop.mapreduce.counters.Limits.init(Limits.java:45)",
                "at org.apache.hadoop.mapreduce.counters.Limits.reset(Limits.java:130)",
                "at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.loadFullHistoryData(CompletedJob.java:363)",
                "at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.<init>(CompletedJob.java:105)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager$HistoryFileInfo.loadJob(HistoryFileManager.java:473)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.loadJob(CachedHistoryStorage.java:180)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.access$000(CachedHistoryStorage.java:52)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage$1.load(CachedHistoryStorage.java:103)",
                "at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3568)",
                "at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2350)",
                "at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2313)",
                "at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2228)",
                "at com.google.common.cache.LocalCache.get(LocalCache.java:3965)",
                "at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3969)",
                "at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4829)",
                "at com.google.common.cache.LocalCache$LocalManualCache.getUnchecked(LocalCache.java:4834)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getFullJob(CachedHistoryStorage.java:193)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getJob(JobHistory.java:220)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:277)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsController.attempts(HsController.java:152)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:162)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.doServiceImpl.java:287)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.dispatch(119)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter$1.call(130)",
                "at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$Context.call(203)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.doFilter(130)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler$CachedChain.doFilter(1759)",
                "at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.doFilter(57)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler$CachedChain.doFilter(1759)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.doFilter(109)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler$CachedChain.doFilter(1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2$QuotingInputFilter.doFilter(1552)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler$CachedChain.doFilter(1759)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.doFilter(45)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler$CachedChain.doFilter(1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.doHandle(582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.handle(143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.handle(548)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.doHandle(226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.doHandle(1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.doScope(512)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.doScope(185)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.doScope(1112)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.handle(141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.handle(119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.handle(134)",
                "at org.eclipse.jetty.server.Server.handle(Server.handle(534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.handle(320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.onFillable(251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection$ReadCallback.succeeded(283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.fillable(108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint$2.run(93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.executeProduceConsume(303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.produceConsume(148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.run(136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.runJob(671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool$2.run(589)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the Oozie server and Yarn JobHistoryServer.",
                "2. Access the JobHistoryServer web UI.",
                "3. Click through a job and its logs."
            ],
            "ExpectedBehavior": "Sockets should be closed properly after reading configurations, preventing excessive CLOSE_WAIT states.",
            "ObservedBehavior": "Numerous sockets remain in CLOSE_WAIT state, leading to exhaustion of file descriptors.",
            "Resolution": "Fixed"
        }
    }
]