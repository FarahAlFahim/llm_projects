[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6633",
            "Title": "AM Should Retry Map Attempts on Compression Errors in Reduce Tasks",
            "Description": "When a reduce task encounters compression-related errors, the Application Master (AM) does not retry the corresponding map task. This can lead to job failures, especially if the node running the map task has issues, such as a bad drive. The following stack trace was generated during one such failure.",
            "StackTrace": [
                "2016-01-27 13:44:28,915 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)",
                "at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)",
                "at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)",
                "at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)",
                "at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job that involves compression.",
                "2. Ensure that one of the map tasks runs on a node with a bad drive.",
                "3. Monitor the reduce task for errors related to compression."
            ],
            "ExpectedBehavior": "The Application Master should retry the failed map task on a different node if a compression-related error occurs during the reduce task.",
            "ObservedBehavior": "The Application Master does not retry the map task, leading to job failure.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6577",
            "Title": "MR AM Fails to Load Native Library Without MR_AM_ADMIN_USER_ENV Set",
            "Description": "If `yarn.app.mapreduce.am.admin.user.env` (or `yarn.app.mapreduce.am.env`) is not configured to set `LD_LIBRARY_PATH`, the MR AM will fail to load the native library. This results in failures for any code that requires the Hadoop native library in the MR AM, such as an uber-AM with lz4 compression for the mapper task.",
            "StackTrace": [
                "2015-12-15 21:30:17,575 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: native lz4 library not available",
                "at org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Ensure that `yarn.app.mapreduce.am.admin.user.env` or `yarn.app.mapreduce.am.env` is not configured to set `LD_LIBRARY_PATH`.",
                "Run a MapReduce job that requires the Hadoop native library, such as one using lz4 compression."
            ],
            "ExpectedBehavior": "The MR AM should successfully load the native library and execute the MapReduce job without errors.",
            "ObservedBehavior": "The MR AM fails to load the native library, resulting in a RuntimeException indicating that the native lz4 library is not available.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5137",
            "Title": "500 Error When Accessing Map Task in AM Web UI",
            "Description": "When accessing the Map Task page for a running MapReduce job in the Application Master web UI, a 500 error is encountered. This issue does not occur in version 0.23.6.",
            "StackTrace": [
                "2013-04-09 13:53:01,587 DEBUG [1088374@qtp-13877033-2 - /mapreduce/task/task_1365457322543_0004_m_000000] org.apache.hadoop.yarn.webapp.GenericExceptionHandler: GOT EXCEPTION",
                "com.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:123)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1069)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job and ensure it is running.",
                "2. Navigate to the Application Master web UI.",
                "3. Click on the job to view its details.",
                "4. Click on the 'MAP' task type to view the list of map tasks.",
                "5. Attempt to click on a specific map task."
            ],
            "ExpectedBehavior": "The map task details should be displayed without any errors.",
            "ObservedBehavior": "A 500 error is displayed when attempting to access the map task details.",
            "Resolution": "Fixed in version 0.23.7"
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4008",
            "Title": "ResourceManager MetricsException on Startup: QueueMetrics MBean Already Exists",
            "Description": "The ResourceManager fails to start due to a MetricsException indicating that the QueueMetrics MBean already exists. This issue prevents the ResourceManager from initializing properly.",
            "StackTrace": [
                "2012-03-14 15:22:23,089 WARN org.apache.hadoop.metrics2.util.MBeans: Error creating MBean object name: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default",
                "org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)",
                "at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)",
                "at $Proxy6.postStart(Unknown Source)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)",
                "Caused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)",
                "... 19 more",
                "2012-03-14 15:22:23,090 WARN org.apache.hadoop.metrics2.util.MBeans: Failed to register MBean \"null\"",
                "javax.management.RuntimeOperationsException: Exception occurred trying to register the MBean",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:969)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)",
                "at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:57)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.start(MetricsSourceAdapter.java:93)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)",
                "at $Proxy6.postStart(Unknown Source)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)",
                "Caused by: java.lang.IllegalArgumentException: No object name specified",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:967)",
                "... 20 more"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager service.",
                "Monitor the logs for any warnings or errors."
            ],
            "ExpectedBehavior": "The ResourceManager should start without any errors or warnings related to MBeans.",
            "ObservedBehavior": "The ResourceManager fails to start and logs a MetricsException indicating that the QueueMetrics MBean already exists.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6259",
            "Title": "IllegalArgumentException due to missing job submit time in JobHistory",
            "Description": "An IllegalArgumentException occurs when parsing the job history file name due to a missing job submit time, which defaults to -1. This issue leads to incorrect job state handling and can cause jobs to be marked as failed without proper submission time recorded.",
            "StackTrace": [
                "2015-02-10 04:54:01,863 WARN org.apache.hadoop.mapreduce.v2.hs.PartialJob: Exception while parsing job state. Defaulting to KILLED",
                "java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0",
                "at java.lang.Enum.valueOf(Enum.java:236)",
                "at org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)",
                "at sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1223)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "Create a job in the Hadoop MapReduce framework.",
                "Submit the job and ensure that the job history file name is formatted incorrectly, leading to a missing job submit time.",
                "Check the job state after submission."
            ],
            "ExpectedBehavior": "The job should have a valid submit time recorded and should not throw an IllegalArgumentException when parsing the job history.",
            "ObservedBehavior": "The job submit time is recorded as -1, leading to an IllegalArgumentException and incorrect job state handling.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3333",
            "Title": "OutOfMemoryError during sort job on 350 node cluster",
            "Description": "The sort job on a 350 node cluster hangs due to an OutOfMemoryError, leading to a failure after an hour instead of the usual 20 minutes. This issue affects the performance and reliability of the MapReduce application.",
            "StackTrace": [
                "2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002_01_001434 : java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)",
                "at $Proxy20.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)",
                "... 4 more",
                "Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1089)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)",
                "... 6 more",
                "Caused by: java.io.IOException: Couldn't set up IO streams",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1065)",
                "... 7 more",
                "Caused by: java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:597)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)",
                "... 10 more"
            ],
            "StepsToReproduce": [
                "1. Set up a 350 node cluster.",
                "2. Submit a sort job to the cluster.",
                "3. Monitor the job execution."
            ],
            "ExpectedBehavior": "The sort job should complete successfully within the usual time frame of approximately 20 minutes.",
            "ObservedBehavior": "The sort job hangs and eventually fails after about an hour due to an OutOfMemoryError.",
            "Resolution": "Fixed in version 0.23.1."
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7059",
            "Title": "MR Job Fails Due to Unknown setErasureCodingPolicy Method in HDFS 2.x",
            "Description": "Running teragen fails in Hadoop version 3.1 when connected to an HDFS server version 2.8. The failure occurs because HDFS 2.8 does not support the setErasureCodingPolicy method, which is called by the MR job. A potential solution involves parsing RemoteException in JobResourceUploader to handle this case gracefully.",
            "StackTrace": [
                "2018-02-26 11:22:53,178 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1518615699369_0006",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol protocol.",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1437)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)",
                "at com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)",
                "at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)",
                "at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)",
                "at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)",
                "at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:304)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:218)"
            ],
            "StepsToReproduce": [
                "Run the following command: bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0-SNAPSHOT.jar teragen 1000000 /teragen",
                "Ensure the HDFS server is version 2.8.",
                "Observe the failure in the job submission."
            ],
            "ExpectedBehavior": "The teragen job should complete successfully without errors.",
            "ObservedBehavior": "The teragen job fails with a RemoteException indicating that the setErasureCodingPolicy method is unknown.",
            "Resolution": "A fix has been implemented to handle the RemoteException gracefully."
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4843",
            "Title": "JobLocalizer Not Thread Safe in DefaultTaskController",
            "Description": "In our cluster, sometimes jobs fail due to a DiskErrorException when initializing tasks. The root cause is that JobLocalizer is not thread safe, leading to potential conflicts when multiple TaskLauncher threads attempt to initialize jobs simultaneously.",
            "StackTrace": [
                "2012-12-03 23:11:54,811 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing attempt_201212031626_1115_r_000023_0:",
                "org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)",
                "at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)",
                "at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)",
                "at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop cluster with multiple TaskLauncher threads.",
                "2. Submit multiple jobs that require localization at the same time.",
                "3. Monitor the TaskTracker logs for DiskErrorException warnings."
            ],
            "ExpectedBehavior": "Jobs should initialize successfully without any DiskErrorException, and job.xml should be correctly localized in the appropriate user's directory.",
            "ObservedBehavior": "Jobs occasionally fail with a DiskErrorException due to conflicts in JobLocalizer when multiple threads attempt to initialize jobs simultaneously.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5028",
            "Title": "Maps Fail with IOException When io.sort.mb is Set to High Value",
            "Description": "The issue occurs when running a MapReduce job with a high value for io.sort.mb. The maps fail during execution, resulting in an IOException. This has been verified on branch-1 with specific configurations.",
            "StackTrace": [
                "java.io.IOException: Spill failed",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)",
                "at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "at org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)",
                "at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)"
            ],
            "StepsToReproduce": [
                "Set up a Hadoop cluster with branch-1.",
                "Configure the following settings: Pseudo-dist mode: 2 maps/1 reduce, mapred.child.java.opts=-Xmx2048m, io.sort.mb=1280, dfs.block.size=2147483648.",
                "Run the teragen command to generate 4 GB of data.",
                "Execute the wordcount job on the generated data."
            ],
            "ExpectedBehavior": "The wordcount job should complete successfully without any IOException.",
            "ObservedBehavior": "The wordcount job fails with an IOException indicating 'Spill failed'.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4300",
            "Title": "OutOfMemoryError in Application Master Causes Zombie State",
            "Description": "The Application Master (AM) encounters an OutOfMemoryError (OOM) which leads to multiple threads dying, resulting in the AM entering a zombie state. This issue affects the stability of the Hadoop MapReduce application.",
            "StackTrace": [
                "Exception in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space",
                "\tat com.google.protobuf.CodedInputStream.<init>(CodedInputStream.java:538)",
                "\tat com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)",
                "\tat com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)",
                "\tat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)",
                "\tat org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)",
                "\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)",
                "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)",
                "Exception in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space",
                "\tat java.util.HashMap.resize(HashMap.java:462)",
                "\tat java.util.HashMap.addEntry(HashMap.java:755)",
                "\tat java.util.HashMap.put(HashMap.java:385)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)",
                "\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)",
                "\tat java.lang.Thread.run(Thread.java:619)",
                "Exception in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space",
                "Exception in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space"
            ],
            "StepsToReproduce": [
                "Start the Application Master for a Hadoop MapReduce job.",
                "Submit a job that requires significant memory resources.",
                "Monitor the logs for any OutOfMemoryError messages."
            ],
            "ExpectedBehavior": "The Application Master should handle memory allocation properly without entering a zombie state.",
            "ObservedBehavior": "The Application Master encounters OutOfMemoryError, leading to multiple threads dying and the AM becoming unresponsive.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3241",
            "Title": "IllegalArgumentException in TraceBuilder when processing job history",
            "Description": "When running the TraceBuilder, an IllegalArgumentException is thrown, and the output does not contain the map and reduce task information. This issue occurs while processing a specific job history file.",
            "StackTrace": [
                "2011-10-21 22:07:17,268 WARN  rumen.TraceBuilder (TraceBuilder.java:run(272)) - TraceBuilder got an error while processing the [possibly virtual] file job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist within Path hdfs://10.18.52.57:9000/user/root/null/history/done_intermediate/root/job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist",
                "java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type",
                "at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:189)"
            ],
            "StepsToReproduce": [
                "Run the TraceBuilder with the job history file: job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist",
                "Observe the output for exceptions."
            ],
            "ExpectedBehavior": "The TraceBuilder should process the job history file without throwing exceptions and should include map and reduce task information in the output.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, and the output does not contain the map and reduce task information.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6002",
            "Title": "MR Task Reports Error to AM During Shutdown Instead of Preemption",
            "Description": "With MAPREDUCE-5900, preempted MR tasks should not be treated as failed. However, it is still possible for an MR task to fail and report to the Application Master (AM) when preemption takes effect and the AM hasn't received the completed container from the Resource Manager (RM) yet. This can lead to the task attempt being marked as failed instead of preempted.\n\nFor example, if the FileSystem has a shutdown hook that closes all FileSystem instances while one is in use (e.g., reading split details from HDFS), the MR task may fail and report a fatal error to the MR AM. An exception will be raised as shown in the stack trace below. We should prevent this behavior, as other exceptions may occur during shutdown, and we shouldn't report any such exceptions to the AM.",
            "StackTrace": [
                "2014-07-22 01:46:19,613 FATAL [IPC Server handler 10 on 56903] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1405985051088_0018_m_000025_0 - exited : java.io.IOException: Filesystem closed",
                "at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)",
                "at java.io.DataInputStream.readByte(DataInputStream.java:265)",
                "at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)",
                "at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)",
                "at org.apache.hadoop.io.Text.readString(Text.java:464)",
                "at org.apache.hadoop.io.Text.readString(Text.java:457)",
                "at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job that involves reading from HDFS.",
                "2. Trigger a preemption of the MR task while it is in progress.",
                "3. Ensure that the FileSystem is in use during the shutdown process.",
                "4. Observe the behavior of the task and the error reported to the AM."
            ],
            "ExpectedBehavior": "The MR task should be marked as preempted instead of failed when preemption occurs.",
            "ObservedBehavior": "The MR task is marked as failed and reports a fatal error to the AM during the shutdown process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6452",
            "Title": "NullPointerException when enabling intermediate encryption in LocalRunner",
            "Description": "When running a MapReduce job with intermediate encryption enabled, a NullPointerException occurs, causing the job to fail. This issue arises specifically when the following properties are set:\n\n- mapreduce.framework.name=local\n- mapreduce.job.encrypted-intermediate-data=true",
            "StackTrace": [
                "2015-08-14 16:27:25,248 WARN  [Thread-21] mapred.LocalJobRunner (LocalJobRunner.java:run(561)) - job_local473843898_0001",
                "java.lang.Exception: java.lang.NullPointerException",
                "    at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)",
                "    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)",
                "Caused by: java.lang.NullPointerException",
                "    at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)",
                "    at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)",
                "    at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)",
                "    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)",
                "    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)",
                "    at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)",
                "    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)",
                "    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "    at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)",
                "    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Set the following properties in your MapReduce job configuration:",
                "1. mapreduce.framework.name=local",
                "2. mapreduce.job.encrypted-intermediate-data=true",
                "3. Run the MapReduce job."
            ],
            "ExpectedBehavior": "The MapReduce job should run successfully without any exceptions.",
            "ObservedBehavior": "The job fails with a NullPointerException, preventing successful execution.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6649",
            "Title": "getFailureInfo Command Fails to Return Failure Details",
            "Description": "The command executed does not provide any failure information regarding why the job failed. This lack of detail makes it difficult to diagnose issues with the job execution.",
            "StackTrace": [
                "ExitCodeException exitCode=1:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Execute the following command: $HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.jobtracker.split.metainfo.maxsize=10 -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1",
                "Observe the output for failure information."
            ],
            "ExpectedBehavior": "The command should return detailed failure information when the job fails.",
            "ObservedBehavior": "The command does not return any failure information, making it difficult to understand why the job failed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4048",
            "Title": "NullPointerException when accessing Application Master UI",
            "Description": "A NullPointerException occurs when trying to access the Application Master UI in Hadoop MapReduce, leading to an error in handling the URI.",
            "StackTrace": [
                "2012-03-21 10:21:31,838 ERROR [2145015588@qtp-957250718-801] org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /mapreduce/attempts/job_1332261815858_2_8/m/KILLED",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "....",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at com.google.common.base.Joiner.toString(Joiner.java:317)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:97)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:127)",
                "at com.google.common.base.Joiner.join(Joiner.java:158)",
                "at com.google.common.base.Joiner.join(Joiner.java:166)",
                "at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)",
                "... 36 more"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job in Hadoop.",
                "2. Attempt to access the Application Master UI for the job.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The Application Master UI should display the job details without any errors.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing access to the Application Master UI.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5198",
            "Title": "Race Condition in Cleanup During Task Tracker Reinitialization with LinuxTaskController",
            "Description": "This issue occurs when the job tracker is restarted while jobs are running, causing the task tracker to reinitialize. The task tracker fails with a ClosedChannelException, leading to a fatal exception during reinitialization.",
            "StackTrace": [
                "2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075 caught: java.nio.channels.ClosedChannelException",
                "\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)",
                "\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)",
                "\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)",
                "\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:98)",
                "\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)",
                "\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)",
                "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)",
                "2013-04-27 20:19:10,414 ERROR org.apache.hadoop.mapred.TaskTracker: Got fatal exception while reinitializing TaskTracker: org.apache.hadoop.util.Shell$ExitCodeException",
                "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:255)",
                "\tat org.apache.hadoop.util.Shell.run(Shell.java:182)",
                "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)",
                "\tat org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)",
                "\tat org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)",
                "\tat org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)",
                "\tat org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)",
                "\tat org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)"
            ],
            "StepsToReproduce": [
                "1. Start a job on the Hadoop Map/Reduce framework.",
                "2. While the job is running, restart the job tracker.",
                "3. Observe the behavior of the task tracker during reinitialization."
            ],
            "ExpectedBehavior": "The task tracker should reinitialize without errors and continue processing jobs.",
            "ObservedBehavior": "The task tracker fails with a ClosedChannelException and a fatal exception during reinitialization.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "bug_report": {
            "BugID": "13126315",
            "Title": "Concurrent Task Progress Updates Cause NullPointerException in Application Master",
            "Description": "Concurrent task progress updates can cause a NullPointerException in the Application Master. This issue occurs during large word count runs and can be reproduced by artificially increasing the frequency of task updates.",
            "StackTrace": [
                "2017-12-20 06:49:42,369 INFO [IPC Server handler 9 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,369 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,383 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "    at java.lang.Thread.run(Thread.java:748)",
                "2017-12-20 06:49:42,385 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,386 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "Run a large word count job in the Hadoop MapReduce framework.",
                "Increase the frequency of task progress updates artificially.",
                "Observe the logs for NullPointerException in the Application Master."
            ],
            "ExpectedBehavior": "The Application Master should handle concurrent task progress updates without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the Application Master when concurrent task progress updates occur.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3790",
            "Title": "Streaming Job Success Despite Truncated Output Due to Broken Pipe",
            "Description": "When a streaming job does not consume all of its input, it can be incorrectly marked as successful, leading to truncated output. This issue arises from the handling of IOExceptions in the PipeMapRed class, which allows the job to finish without waiting for output threads to complete, potentially resulting in lost job output.",
            "StackTrace": [
                "2012-02-02 11:27:25,054 WARN [main] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Broken pipe",
                "2012-02-02 11:27:25,054 INFO [main] org.apache.hadoop.streaming.PipeMapRed: mapRedFinished",
                "2012-02-02 11:27:25,056 WARN [Thread-12] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Bad file descriptor",
                "2012-02-02 11:27:25,124 INFO [main] org.apache.hadoop.mapred.Task: Task:attempt_1328203555769_0001_m_000000_0 is done. And is in the process of committing",
                "2012-02-02 11:27:25,127 WARN [Thread-11] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: DFSOutputStream is closed",
                "2012-02-02 11:27:25,199 INFO [main] org.apache.hadoop.mapred.Task: Task attempt_1328203555769_0001_m_000000_0 is allowed to commit now",
                "2012-02-02 11:27:25,225 INFO [main] org.apache.hadoop.mapred.FileOutputCommitter: Saved output of task 'attempt_1328203555769_0001_m_000000_0' to hdfs://localhost:9000/user/somebody/out/_temporary/1",
                "2012-02-02 11:27:27,834 INFO [main] org.apache.hadoop.mapred.Task: Task 'attempt_1328203555769_0001_m_000000_0' done."
            ],
            "StepsToReproduce": [
                "Create an input file with content: 'foo'.",
                "Run the following command: $ hdfs dfs -cat in",
                "Execute the streaming job with the command: $ yarn jar ./share/hadoop/tools/lib/hadoop-streaming-0.24.0-SNAPSHOT.jar -Dmapred.map.tasks=1 -Dmapred.reduce.tasks=1 -mapper /bin/env -reducer NONE -input in -output out.",
                "Check the output and logs for warnings and errors."
            ],
            "ExpectedBehavior": "The streaming job should consume all input and produce complete output without being marked successful if output is truncated.",
            "ObservedBehavior": "The streaming job is marked successful even when it does not consume all input, leading to truncated output.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6357",
            "Title": "Documentation Update Needed for MultipleOutputs.write() Regarding Absolute Paths",
            "Description": "After debugging a user job where reduce tasks were failing on retry, it was found that the MultipleOutputs.write() documentation does not clarify that using absolute paths may lead to improper execution of tasks on retry or when MapReduce speculative execution is enabled. This issue was highlighted in the stack trace below, indicating a java.io.IOException due to a file already existing.",
            "StackTrace": [
                "2015-04-28 23:13:10,452 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File already exists: wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "StepsToReproduce": [
                "1. Set up a MapReduce job using MultipleOutputs.write() with an absolute output path.",
                "2. Run the job and observe the behavior during task retries."
            ],
            "ExpectedBehavior": "The job should execute without errors, and output committing should be utilized correctly.",
            "ObservedBehavior": "The job fails with a java.io.IOException indicating that the file already exists due to improper handling of absolute paths.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6091",
            "Title": "YARNRunner.getJobStatus() fails with ApplicationNotFoundException for jobs rolled off RM view",
            "Description": "When querying the job status of a job that has rolled off the Resource Manager (RM) view, the system throws an ApplicationNotFoundException. This issue was introduced in YARN-873, which changed the behavior of ClientRMService to throw an exception for unknown application IDs instead of returning null. This affects the ability to retrieve job status from the job history server.",
            "StackTrace": [
                "2014-09-15 07:09:51,084 ERROR org.apache.pig.tools.grunt.Grunt: ERROR 6017: JobID: job_1410289045532_90542 Reason: java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)",
                "at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)",
                "at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)",
                "at java.lang.Thread.run(Thread.java:662)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)"
            ],
            "StepsToReproduce": [
                "Submit a job to YARN.",
                "Wait for the job to complete.",
                "Query the job status using YARNRunner.getJobStatus() after the job has rolled off the RM view."
            ],
            "ExpectedBehavior": "The job status should be retrievable from the job history server even after the job has rolled off the RM view.",
            "ObservedBehavior": "An ApplicationNotFoundException is thrown, indicating that the application ID does not exist in the Resource Manager.",
            "Resolution": "This issue has been fixed in version 2.6.0."
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4825",
            "Title": "JobImpl.finished does not handle ERROR as a final job state",
            "Description": "The test case TestMRApp.testJobError is causing the AsyncDispatcher to exit unexpectedly due to an exception being thrown. The console output indicates that an invalid event is being processed, leading to a fatal error in the dispatcher thread.",
            "StackTrace": [
                "2012-11-27 18:46:15,240 ERROR [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:internalError(665)) - Invalid event T_SCHEDULE on Task task_0_0000_m_000000",
                "2012-11-27 18:46:15,242 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(132)) - Error in dispatcher thread",
                "java.lang.IllegalArgumentException: Illegal job state: ERROR",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "\tat java.lang.Thread.run(Thread.java:662)",
                "2012-11-27 18:46:15,242 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(135)) - Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "Run the test case TestMRApp.testJobError.",
                "Observe the console output for errors related to AsyncDispatcher."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should handle job states correctly without exiting unexpectedly.",
            "ObservedBehavior": "The AsyncDispatcher exits with a fatal error when encountering an ERROR job state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3319",
            "Title": "ClassCastException in multifilewc example of Hadoop 0.20.205.0",
            "Description": "When running the multifilewc example from Hadoop's examples, a ClassCastException occurs, preventing the job from completing successfully.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable",
                "at org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)",
                "at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "1. Execute the command: /usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.205.0.22.jar multifilewc examples/text examples-output/multifilewc",
                "2. Monitor the job execution in the Hadoop console."
            ],
            "ExpectedBehavior": "The multifilewc job should complete successfully and produce the expected output files in the specified output directory.",
            "ObservedBehavior": "The job fails with a ClassCastException, indicating a type mismatch in the reducer.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6361",
            "Title": "NullPointerException in Shuffle due to Concurrent Execution",
            "Description": "A NullPointerException occurs in the shuffle process when there is a concurrent issue between the copySucceeded() and copyFailed() methods running on the same host. This leads to a failure in the task execution.",
            "StackTrace": [
                "2015-05-08 21:00:00,513 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job that involves shuffling data.",
                "2. Ensure that multiple threads are executing copySucceeded() and copyFailed() concurrently on the same host.",
                "3. Monitor the logs for any warnings or errors related to shuffle."
            ],
            "ExpectedBehavior": "The shuffle process should complete without any exceptions, and the MapReduce job should succeed.",
            "ObservedBehavior": "A NullPointerException is thrown during the shuffle process, causing the job to fail.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4848",
            "Title": "ClassCastException during Application Master Recovery",
            "Description": "An Application Master (AM) failed and subsequent recovery attempts resulted in a ClassCastException. The error occurs when trying to recover a task, leading to a failure in the recovery process.",
            "StackTrace": [
                "2012-12-05 02:33:36,752 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext",
                "at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)",
                "2012-12-05 02:33:36,752 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start an Application Master (AM) for a job.",
                "2. Allow the AM to fail.",
                "3. Attempt to recover the AM.",
                "4. Observe the logs for any ClassCastException during the recovery process."
            ],
            "ExpectedBehavior": "The Application Master should recover successfully without throwing a ClassCastException.",
            "ObservedBehavior": "The recovery process fails with a ClassCastException, preventing the job from progressing.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3226",
            "Title": "Hanging Reducer Tasks in Gridmix Run",
            "Description": "During a Gridmix run with approximately 1000 jobs, several reducer tasks are hanging after downloading all map outputs. This issue is causing one job to get stuck due to 2-3 reducers not completing. The following thread dump indicates the state of the hanging threads.",
            "StackTrace": [
                "\"EventFetcher for fetching Map Completion Events\" daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]",
                "   java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "        at java.lang.Thread.sleep(Native Method)",
                "        at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)",
                "",
                "\"main\" prio=10 tid=0x080ed400 nid=0x1c71 in Object.wait() [0xf73a2000]",
                "   java.lang.Thread.State: WAITING (on object monitor)",
                "        at java.lang.Object.wait(Native Method)",
                "        - waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)",
                "        at java.lang.Thread.join(Thread.java:1143)",
                "        - locked <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)",
                "        at java.lang.Thread.join(Thread.java:1196)",
                "        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)",
                "        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)",
                "        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "        at java.security.AccessController.doPrivileged(Native Method)",
                "        at javax.security.auth.Subject.doAs(Subject.java:396)",
                "        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "StepsToReproduce": [
                "1. Initiate a Gridmix run with approximately 1000 jobs.",
                "2. Monitor the reducer tasks during execution.",
                "3. Observe the state of the reducer tasks after all map outputs have been downloaded."
            ],
            "ExpectedBehavior": "All reducer tasks should complete successfully without hanging.",
            "ObservedBehavior": "2-3 reducer tasks are hanging, causing one job to get stuck.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3070",
            "Title": "NodeManager Fails to Register with ResourceManager After Restart",
            "Description": "After stopping the NodeManager (NM) gracefully and then starting it again, the NM registration fails with the error: 'Duplicate registration from the node!'. This issue prevents the NodeManager from successfully starting and registering with the ResourceManager (RM).",
            "StackTrace": [
                "2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)",
                "at $Proxy13.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more"
            ],
            "StepsToReproduce": [
                "1. Stop the NodeManager gracefully.",
                "2. Start the NodeManager.",
                "3. Observe the logs for registration errors."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager without any errors.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager, resulting in a 'Duplicate registration from the node!' error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "bug_report": {
            "BugID": "12544334",
            "Title": "AM Crash Due to MR Task Preemption When Headroom is Zero",
            "Description": "[~karams] reported this offline. One reduce task gets preempted because of zero headRoom and crashes the Application Master (AM).",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit a MapReduce job with multiple reduce tasks.",
                "2. Monitor the resource allocation and ensure that the available headroom reaches zero.",
                "3. Observe the behavior of the Application Master and the reduce tasks."
            ],
            "ExpectedBehavior": "The Application Master should handle the preemption of tasks without crashing.",
            "ObservedBehavior": "The Application Master crashes when a reduce task is preempted due to zero headroom.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4062",
            "Title": "AM Launcher Thread Hangs Indefinitely Due to Resource Manager Issue",
            "Description": "An instance was observed where the Resource Manager (RM) stopped launching Application Masters (AMs) because the launcher thread was hung. This occurred due to an issue with the Node Manager (NM) node. Currently, there is only one launcher thread, which can lead to the RM being stuck if multiple nodes fail. This issue persisted for approximately 9 hours.",
            "StackTrace": [
                "\"pool-1-thread-1\" prio=10 tid=0x000000004343e800 nid=0x3a4c in Object.wait()",
                "[0x000000004fad2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Object.wait(Object.java:485)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1076)",
                "- locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)",
                "at $Proxy76.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start the Resource Manager with a single Application Master launcher thread.",
                "2. Simulate a failure in one or more Node Managers.",
                "3. Observe the behavior of the Resource Manager and the Application Master launcher thread."
            ],
            "ExpectedBehavior": "The Resource Manager should be able to launch Application Masters even if some Node Managers fail.",
            "ObservedBehavior": "The Application Master launcher thread hangs indefinitely, preventing the Resource Manager from launching new Application Masters.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3066",
            "Title": "YARN NodeManager Fails to Start Due to Invalid Configuration",
            "Description": "The YARN NodeManager fails to start after performing an SVN update. The error indicates that the configuration for the resource tracker address is invalid.",
            "StackTrace": [
                "2011-09-21 15:36:33,534 INFO  service.AbstractService (AbstractService.java:stop(71)) - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.",
                "2011-09-21 15:36:33,534 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "\t... 2 more",
                "Caused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "\t... 3 more",
                "2011-09-21 15:36:33,535 INFO  service.CompositeService (CompositeService.java:stop(97)) - Error stopping org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl",
                "java.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED",
                "\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)",
                "\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:95)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:85)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)",
                "\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)",
                "2011-09-21 15:36:33,535 INFO  nodemanager.NodeManager (StringUtils.java:run(605)) - SHUTDOWN_MSG: ",
                "/************************************************************",
                "SHUTDOWN_MSG: Shutting down NodeManager at criccomi-ld/127.0.0.1",
                "************************************************************/",
                "2011-09-21 15:36:33,536 INFO  ipc.Server (Server.java:stop(1708)) - Stopping server on 45454",
                "2011-09-21 15:36:33,536 INFO  logaggregation.LogAggregationService (LogAggregationService.java:stop(116)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit",
                "2011-09-21 15:36:33,536 INFO  ipc.Server (Server.java:stop(1708)) - Stopping server on 4344",
                "2011-09-21 15:36:33,536 INFO  service.CompositeService (CompositeService.java:run(120)) - Error stopping org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "java.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED",
                "\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)",
                "\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:87)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)",
                "\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)"
            ],
            "StepsToReproduce": [
                "Perform an SVN update on the Hadoop project.",
                "Attempt to start the YARN NodeManager."
            ],
            "ExpectedBehavior": "The YARN NodeManager should start successfully without errors.",
            "ObservedBehavior": "The YARN NodeManager fails to start with a fatal error indicating an invalid host:port configuration for the resource tracker address.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3123.json",
        "creation_time": "2011-09-29T19:03:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3123",
            "Title": "Container Task Failure Due to Special Characters in Symbolic Links",
            "Description": "The job fails with an exception when special characters are included in the symbolic link path. This issue occurs during the execution of a Hadoop streaming job.",
            "StackTrace": [
                "2011-09-27 20:58:48,903 INFO org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: launchContainer:",
                "[container-executor, hadoopuser, 1, application_1317077272567_0239,",
                "container_1317077272567 0239_01_000001,",
                "tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001,",
                "tmp/mapred-local/nmPrivate/application_1317077272567_0239/container_1317077272567_0239_01 000001/task.sh,",
                "tmp/mapred-local/nmPrivate/container_1317077272567_0239_01_000001/container_1317077272567_0239_01_000001.tokens]1109221111-tests.jar:hadoop-mapreduce-p2011-09-27",
                "20:58:48,944 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exit code from container is : 2",
                "2011-09-27 20:58:48,946 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exception from container-launch :",
                "org.apache.hadoop.util.Shell$ExitCodeException:",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:",
                "line 26: syntax error near unexpected token `-_+='",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:",
                "line 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir test",
                "ink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:188)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:174)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:62)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)",
                "2011-09-27 20:58:48,951 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:",
                "2011-09-27 20:58:48,951 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Processing",
                "container_1317077272567_0239_01_000001 of type UPDATE_DIAGNOSTICS_MSG",
                "2011-09-27 20:58:48,951 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:",
                "Container exited with a non-zero exit code 2"
            ],
            "StepsToReproduce": [
                "Run the following Hadoop streaming command:",
                "hadoop jar hadoop-streaming.jar -Dmapreduce.job.acl-view-job=* -Dmapreduce.job.queuename=queue1 -files file:///homes/user/hadoop/Streaming/data/streaming-980//InputDir#testlink!@$&*()-_+= -input Streaming/streaming-980/input.txt -mapper 'xargs cat' -reducer cat -output Streaming/streaming-980/Output -jobconf mapred.job.name=streamingTest-980 -jobconf mapreduce.job.acl-view-job=*"
            ],
            "ExpectedBehavior": "The job should execute successfully without throwing any exceptions.",
            "ObservedBehavior": "The job fails with a syntax error in the task.sh script due to special characters in the symbolic link.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "bug_report": {
            "BugID": "12850372",
            "Title": "YarnRuntimeException Causes Application Master to Fail Instead of Retry",
            "Description": "The Application Master (AM) encounters a YarnRuntimeException during communication with the Resource Manager (RM), leading it to incorrectly assume that it has exhausted its retry attempts. This results in the AM failing instead of retrying the operation.",
            "StackTrace": [
                "2015-07-25 20:07:27,346 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Error communicating with RM: java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)",
                "... 11 more",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job that requires communication with the Resource Manager.",
                "2. Simulate a shutdown of the Resource Manager during the allocate call.",
                "3. Observe the behavior of the Application Master."
            ],
            "ExpectedBehavior": "The Application Master should retry the allocate call instead of failing.",
            "ObservedBehavior": "The Application Master fails and does not retry after encountering a YarnRuntimeException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4490",
            "Title": "JVM Reuse Causes Task Failures with LinuxTaskController",
            "Description": "When using LinuxTaskController with JVM reuse (mapred.job.reuse.jvm.num.tasks > 1), the second map task in each JVM fails immediately due to the absence of the userlog directory for the task attempt. This results in an ENOENT error when attempting to write the log.index file, causing the JVM to exit. This issue does not occur with DefaultTaskController, which creates userlog directories for each task.",
            "StackTrace": [
                "2012-07-24 14:29:11,914 INFO org.apache.hadoop.mapred.TaskLog: Starting logging for a new task attempt_201207241401_0013_m_000027_0 in the same JVM as that of the first task /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_000006_0",
                "2012-07-24 14:29:11,915 WARN org.apache.hadoop.mapred.Child: Error running child",
                "ENOENT: No such file or directory",
                "    at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)",
                "    at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)",
                "    at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)",
                "    at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)",
                "    at org.apache.hadoop.mapred.Child.main(Child.java:229)"
            ],
            "StepsToReproduce": [
                "Set mapred.job.reuse.jvm.num.tasks to a value greater than 1.",
                "Submit a job with more map tasks than available map slots in the cluster.",
                "Monitor the task attempts in the logs."
            ],
            "ExpectedBehavior": "The job should complete successfully without task failures.",
            "ObservedBehavior": "The second task in each JVM fails with an ENOENT error due to the missing userlog directory.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3744",
            "Title": "Error retrieving application logs via 'yarn logs' and 'mapred job -logs'",
            "Description": "When attempting to retrieve application logs using the 'yarn logs' command, a FileNotFoundException occurs. Additionally, using the 'mapred job -logs' command results in a warning about the Job History Server not being configured, despite the process running.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.",
                "\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)",
                "\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)",
                "\tat org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)",
                "\tat org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)",
                "\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)",
                "\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)",
                "\tat org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)",
                "\tat org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)",
                "\tat org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)",
                "\tat org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)",
                "\tat org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)",
                "2012-01-27 14:05:52,040 INFO  mapred.ClientServiceDelegate (ClientServiceDelegate:getProxy(246)) - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server",
                "2012-01-27 14:05:52,041 WARN  mapred.ClientServiceDelegate (ClientServiceDelegate:checkAndGetHSProxy(257)) - Job History Server is not configured.",
                "Unable to get log information for job: job_1327694122989_0001"
            ],
            "StepsToReproduce": [
                "Run the command 'yarn logs -applicationId application_1327694122989_0001'.",
                "Observe the error message regarding the missing log file.",
                "Run the command 'mapred job -logs job_1327694122989_0001'.",
                "Observe the warning about the Job History Server."
            ],
            "ExpectedBehavior": "The application logs should be retrieved successfully without any errors.",
            "ObservedBehavior": "An error occurs indicating that the log file does not exist, and a warning is shown about the Job History Server not being configured.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6815.json",
        "creation_time": "2016-12-02T00:30:37.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6815",
            "Title": "Flaky TestKill.testKillTask() Fails with Job State Assertion Error",
            "Description": "The test case TestKill.testKillTask() intermittently fails due to an assertion error related to the job state. The expected state is 'SUCCEEDED', but the actual state is 'ERROR'. This issue needs to be addressed to ensure the reliability of the test.",
            "StackTrace": [
                "java.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)"
            ],
            "StepsToReproduce": [
                "Run the TestKill.testKillTask() test case multiple times.",
                "Observe the job state after execution."
            ],
            "ExpectedBehavior": "The job should complete successfully with the state 'SUCCEEDED'.",
            "ObservedBehavior": "The job fails with the state 'ERROR' intermittently.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3053",
            "Title": "YARN ApplicationMaster Registration Fails with NullPointerException",
            "Description": "When attempting to register the ApplicationMaster with YARN's ResourceManager, the registration fails due to a NullPointerException. This issue occurs during the execution of the ApplicationMaster's main method, specifically when calling the registerApplicationMaster method.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)",
                "at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)",
                "at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)",
                "at kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)",
                "Caused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1084)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager.",
                "2. Execute the ApplicationMaster with the following parameters: [Provide application ID, attempt ID, timestamp, streamer class, number of tasks].",
                "3. Observe the logs for the ApplicationMaster."
            ],
            "ExpectedBehavior": "The ApplicationMaster should successfully register with the ResourceManager without any exceptions.",
            "ObservedBehavior": "The ApplicationMaster fails to register, throwing a NullPointerException in the logs.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5884",
            "Title": "AccessControlException when Cancelling Delegation Token due to User Name Mismatch",
            "Description": "When the owner of a token tries to explicitly cancel the token, an AccessControlException is thrown due to a mismatch between the full principal name of the owner and the short name of the canceller.",
            "StackTrace": [
                "2014-04-14 20:07:35,744 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:<someuser>/<machine_name>.linkedin.com@<realm>.LINKEDIN.COM (auth:KERBEROS) cause:org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "2014-04-14 20:07:35,744 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 10020, call org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB.cancelDelegationToken from 172.20.158.61:49042 Call#4 Retry#0: error: org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)",
                "at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)"
            ],
            "StepsToReproduce": [
                "1. Obtain a delegation token as a user.",
                "2. Attempt to cancel the token using a different user name (short name).",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The token should be successfully cancelled if the user is authorized.",
            "ObservedBehavior": "An AccessControlException is thrown indicating that the user is not authorized to cancel the token.",
            "Resolution": "Consider implementing one of the following options: \nOption 1: Modify the cancelToken() method to compare both short name and full principal name. \nOption 2: Ensure that all callers send a consistent value for 'canceller' (either short name or full principal name)."
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3706",
            "Title": "HTTP Circular Redirect Error on Job Attempts Page",
            "Description": "When attempting to access the job attempts page for a submitted job, a circular redirect error occurs, preventing access to the page.",
            "StackTrace": [
                "org.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'",
                "at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
            ],
            "StepsToReproduce": [
                "Submit a job in the Hadoop MapReduce application.",
                "Navigate to the job attempts page using the following URL: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW."
            ],
            "ExpectedBehavior": "The job attempts page should load successfully, displaying the relevant job attempt information.",
            "ObservedBehavior": "An HTTP ERROR 500 occurs with a message indicating a circular redirect.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "bug_report": {
            "BugID": "12650061",
            "Title": "MapReduce Fails on Windows Due to Incorrect LD_LIBRARY_PATH Handling",
            "Description": "In order to set the path for loading native libraries, MapReduce relies on the default value of the mapreduce.admin.user.env configuration setting the LD_LIBRARY_PATH environment entry. This approach has two issues on Windows:\n1. LD_LIBRARY_PATH is not applicable in Windows.\n2. It incorrectly uses $HADOOP_COMMON_HOME instead of %HADOOP_COMMON_HOME%.\n\nAs a result, unless this configuration is overridden, MapReduce jobs fail with an UnsatisfiedLinkError.",
            "StackTrace": [
                "2013-05-29 13:51:41,049 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z",
                "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)",
                "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)",
                "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)",
                "\tat org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)",
                "\tat org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)",
                "\tat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)",
                "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)",
                "\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)",
                "\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)",
                "\tat org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)",
                "\tat org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)",
                "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)"
            ],
            "StepsToReproduce": [
                "Set up a MapReduce job on a Windows environment.",
                "Ensure that the mapreduce.admin.user.env configuration is set without overriding the default LD_LIBRARY_PATH.",
                "Run the MapReduce job."
            ],
            "ExpectedBehavior": "The MapReduce job should run successfully without errors related to native library loading.",
            "ObservedBehavior": "The MapReduce job fails with an UnsatisfiedLinkError due to incorrect handling of the LD_LIBRARY_PATH.",
            "Resolution": "Fixed in version 2.3.0"
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "bug_report": {
            "BugID": "12505901",
            "Title": "Job History Files Fail to Move to DONE Folder When Configured to HDFS Location",
            "Description": "When the configuration for 'mapreduce.jobtracker.jobhistory.location' is set to an HDFS path, the job history files do not move to the DONE folder during Job Tracker initialization or after job completion. This results in an error indicating a wrong filesystem type.",
            "StackTrace": [
                "2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.",
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)",
                "at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)",
                "at org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "Set the configuration 'mapreduce.jobtracker.jobhistory.location' to an HDFS path.",
                "Start the Job Tracker.",
                "Submit a job that generates history files.",
                "Check the DONE folder for the expected history files."
            ],
            "ExpectedBehavior": "The job history files should move to the DONE folder without any errors.",
            "ObservedBehavior": "The job history files do not move to the DONE folder, and an error is logged indicating a wrong filesystem type.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3531",
            "Title": "IllegalArgumentException: Invalid key to HMAC computation during NODE_UPDATE causing ResourceManager to stop scheduling",
            "Description": "This bug report details an issue where an IllegalArgumentException is thrown during the NODE_UPDATE event, which prevents the ResourceManager from allocating resources to jobs. This issue was observed after submitting a large sleep job on a 350 cluster setup.",
            "StackTrace": [
                "2011-12-01 11:56:25,200 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <NMHost>:48490 clusterResources: memory: 3225600",
                "2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Invalid key to HMAC computation",
                "    at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)",
                "    at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)",
                "    at java.lang.Thread.run(Thread.java:619)",
                "Caused by: java.security.InvalidKeyException: Secret key expected",
                "    at com.sun.crypto.provider.HmacCore.a(DashoA13*..)",
                "    at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)",
                "    at javax.crypto.Mac.init(DashoA13*..)",
                "    at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)",
                "    ... 14 more"
            ],
            "StepsToReproduce": [
                "1. Start a 350 cluster.",
                "2. Submit a large sleep job.",
                "3. Monitor the ResourceManager logs for NODE_UPDATE events."
            ],
            "ExpectedBehavior": "The ResourceManager should allocate resources to the submitted job without errors.",
            "ObservedBehavior": "The ResourceManager fails to allocate resources and throws an IllegalArgumentException during NODE_UPDATE events.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3931",
            "Title": "MR Tasks Fail Due to Resource Timestamp Changes",
            "Description": "Tasks are randomly failing during gridmix runs due to changing timestamps on resources that are being downloaded. This issue was reported by [~karams] offline.",
            "StackTrace": [
                "2012-02-24 21:03:34,912 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1330116323296_0140_m_003868_0: RemoteTrace:",
                "java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)",
                "at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)",
                "at LocalTrace:",
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)",
                "at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)",
                "at org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)",
                "at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)"
            ],
            "StepsToReproduce": [
                "Run gridmix jobs on the Hadoop cluster.",
                "Monitor the task attempts for failures.",
                "Check the logs for any IOException related to resource timestamp changes."
            ],
            "ExpectedBehavior": "Tasks should complete successfully without failing due to resource timestamp issues.",
            "ObservedBehavior": "Tasks are failing intermittently with IOException indicating that the resource has changed on the source filesystem.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4467",
            "Title": "IndexCache Synchronization Error Causes Test Failures",
            "Description": "The TestMRJobs.testSleepJob fails randomly due to a synchronization error in IndexCache. The issue arises from the removal of the 'synchronized' keyword in a related change (MAPREDUCE-4384), which leads to a failure in the 'info.wait()' call. This needs to be wrapped into a 'synchronized' block to prevent the IllegalMonitorStateException.",
            "StackTrace": [
                "2012-07-20 19:32:34,627 ERROR [New I/O server worker #2-1] mapred.ShuffleHandler (ShuffleHandler.java:exceptionCaught(528)) - Shuffle error:",
                "java.lang.IllegalMonitorStateException",
                "\tat java.lang.Object.wait(Native Method)",
                "\tat org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)",
                "\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)",
                "\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)",
                "\tat org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)",
                "\tat org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)",
                "\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)",
                "\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)",
                "\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)",
                "\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)",
                "\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)",
                "\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)",
                "\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)",
                "\tat org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)",
                "\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the TestMRJobs.testSleepJob.",
                "Observe the random failures due to synchronization issues."
            ],
            "ExpectedBehavior": "The test should pass consistently without any synchronization errors.",
            "ObservedBehavior": "The test fails randomly with an IllegalMonitorStateException due to missing synchronization in IndexCache.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3463",
            "Title": "Second AM Fails to Recover Properly When First AM is Killed",
            "Description": "When the first Application Master (AM) is killed using 'kill -9', the second AM fails to recover properly, resulting in a lost job due to a java.lang.IllegalArgumentException. This issue occurs in a YARN cluster setup with multiple nodes.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port",
                "at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:985)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:128)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:845)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml.",
                "Start a YARN cluster with 4 nodes.",
                "Run a job using Randowriter/Sort/Sort-validate successfully.",
                "While the job is running (approximately 50% complete), kill the Application Master (AM) using 'kill -9'.",
                "Observe the client side logs for errors."
            ],
            "ExpectedBehavior": "The second Application Master should recover and continue the job without errors.",
            "ObservedBehavior": "The second Application Master fails to recover, resulting in a lost job and an IllegalArgumentException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4164",
            "Title": "IOException during task completion leads to reexecution in Hadoop 0.22",
            "Description": "An IOException occurs during the task completion phase, causing the task to be reexecuted. This issue was observed in Hadoop version 0.22. The stack trace indicates a communication exception due to a closed channel.",
            "StackTrace": [
                "2012-02-28 19:17:08,504 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 1969310 bytes",
                "2012-02-28 19:17:08,694 INFO org.apache.hadoop.mapred.Task: Task:attempt_201202272306_0794_m_000094_0 is done. And is in the process of committing",
                "2012-02-28 19:18:08,774 INFO org.apache.hadoop.mapred.Task: Communication exception: java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1062)",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)",
                "at $Proxy0.statusUpdate(Unknown Source)",
                "at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)",
                "at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)",
                "at java.io.DataOutputStream.flush(DataOutputStream.java:106)",
                "at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1040)",
                "... 4 more"
            ],
            "StepsToReproduce": [
                "Run a Hadoop job using version 0.22.",
                "Monitor the task completion process.",
                "Observe the logs for any IOException during the task completion."
            ],
            "ExpectedBehavior": "The task should complete successfully without any exceptions, and no reexecution should occur.",
            "ObservedBehavior": "An IOException is thrown during task completion, leading to the task being reexecuted.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "bug_report": {
            "BugID": "12648389",
            "Title": "Job Failure Due to JvmManager Inconsistent State",
            "Description": "In our cluster, jobs failed due to random task initialization failures caused by JvmManager running into an inconsistent state. The TaskTracker failed to exit, leading to a NullPointerException.",
            "StackTrace": [
                "java.lang.Throwable: Child Error",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)",
                "Caused by: java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)",
                "\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)",
                "\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)",
                "",
                "java.lang.Throwable: Child Error",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)",
                "Caused by: java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)",
                "\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)",
                "\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
            ],
            "StepsToReproduce": [
                "1. Deploy the Hadoop cluster with the current configuration.",
                "2. Submit a job that requires task initialization.",
                "3. Monitor the TaskTracker logs for errors."
            ],
            "ExpectedBehavior": "The job should initialize tasks successfully without any errors.",
            "ObservedBehavior": "The job fails with a NullPointerException due to JvmManager being in an inconsistent state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4774",
            "Title": "JobImpl Fails to Handle Asynchronous Task Events in FAILED State",
            "Description": "The test `org.apache.hadoop.mapred.TestClusterMRNotification.testMR` frequently fails in the mapred build due to incorrect job state transitions. The test checks job status notifications received through an HTTP Servlet and runs three jobs: successful, killed, and failed. It expects the servlet to receive notifications in a specific order. However, the actual notifications differ from the expected ones, leading to test failures. The root cause is an invalid state transition when a job task fails, resulting in an `InvalidStateTransitonException`.",
            "StackTrace": [
                "2012-11-06 12:22:02,335 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the test `org.apache.hadoop.mapred.TestClusterMRNotification.testMR` in the mapred build.",
                "Observe the notifications received by the HTTP Servlet during the execution of the three jobs."
            ],
            "ExpectedBehavior": "The servlet should receive job status notifications in the expected order for successful, killed, and failed jobs.",
            "ObservedBehavior": "The servlet receives notifications that differ from the expected order, leading to test failures.",
            "Resolution": "The issue has been fixed and is now closed."
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3005",
            "Title": "ResourceManager NPE Causes Application Hang",
            "Description": "The application hangs intermittently due to a NullPointerException (NPE) in the ResourceManager. This issue was observed during sort runs on a large cluster, occurring two out of five times.",
            "StackTrace": [
                "2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)",
                "    at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "Run a sort job on a large cluster using the MR application.",
                "Monitor the ResourceManager logs for errors.",
                "Observe if the application hangs and check for NPE in the logs."
            ],
            "ExpectedBehavior": "The application should run without hanging and the ResourceManager should handle node updates without throwing exceptions.",
            "ObservedBehavior": "The application hangs intermittently, and the ResourceManager logs show a NullPointerException during node updates.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6895",
            "Title": "YarnRuntimeException Prevents Job End Notification from Being Sent",
            "Description": "The method `MRAppMaster.this.stop()` throws a `YarnRuntimeException`, which results in the job end notification not being sent. This issue was logged with the following warning and stack trace.",
            "StackTrace": [
                "2017-05-24 12:14:02,165 WARN [Thread-693] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Graceful stop failed",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException",
                "    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)",
                "    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)",
                "    at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "    at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "    at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "    at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "    at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)",
                "    at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$1.run(MRAppMaster.java:605)",
                "Caused by: java.nio.channels.ClosedChannelException",
                "    at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)",
                "    at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)",
                "    at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)",
                "    at java.io.DataOutputStream.write(DataOutputStream.java:107)",
                "    at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)",
                "    at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)",
                "    at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)",
                "    at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)",
                "    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)",
                "    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)",
                "    ... 11 more",
                "2017-05-24 12:14:02,165 INFO [Thread-693] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Exiting MR AppMaster..GoodBye!"
            ],
            "StepsToReproduce": [
                "1. Start a job using the MRAppMaster.",
                "2. Trigger the stop method on the MRAppMaster.",
                "3. Observe the logs for any warnings or exceptions."
            ],
            "ExpectedBehavior": "The job end notification should be sent successfully when the MRAppMaster stops gracefully.",
            "ObservedBehavior": "The job end notification is not sent due to a YarnRuntimeException being thrown.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4451",
            "Title": "FairScheduler Job Initialization Fails with Kerberos Authentication",
            "Description": "When using FairScheduler in Hadoop 1.0.3 with Kerberos authentication configured, job initialization fails due to the JobInitializer threads running as the RPC user instead of the JobTracker. This results in a failure to obtain valid Kerberos credentials.",
            "StackTrace": [
                "2012-07-17 15:15:09,220 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:",
                "java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1097)",
                "at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)",
                "at $Proxy7.getProtocolVersion(Unknown Source)",
                "at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)",
                "at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)",
                "at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)",
                "at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)",
                "at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)",
                "at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)",
                "at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:543)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:488)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:590)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:187)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1228)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1072)",
                "... 20 more",
                "Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:385)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1200(Client.java:187)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:583)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:580)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:579)",
                "... 23 more",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)",
                "... 32 more"
            ],
            "StepsToReproduce": [
                "1. Configure Kerberos authentication in Hadoop 1.0.3.",
                "2. Submit a job using FairScheduler.",
                "3. Observe the job initialization process."
            ],
            "ExpectedBehavior": "The job should initialize successfully with valid Kerberos credentials.",
            "ObservedBehavior": "Job initialization fails with an IOException indicating that no valid credentials were provided.",
            "Resolution": "The issue was fixed by ensuring that the JobInitializer threads run as the JobTracker user, which has the necessary Kerberos credentials."
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4144",
            "Title": "ResourceManager NullPointerException on NODE_UPDATE Event",
            "Description": "The ResourceManager on one of our clusters has exited twice in the past few days due to a NullPointerException (NPE) while trying to handle a NODE_UPDATE event. This issue is similar to the failure reported in MAPREDUCE-3005.",
            "StackTrace": [
                "2012-04-12 02:09:01,672 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "[ResourceManager Event Processor]java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager on a Hadoop cluster.",
                "2. Trigger a NODE_UPDATE event.",
                "3. Monitor the ResourceManager logs for any exceptions."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the NODE_UPDATE event without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException and exits unexpectedly.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5924",
            "Title": "Sort Job Fails with InvalidStateTransitonException on Large Data Set",
            "Description": "The Sort job over 1GB of data failed with the following error:\n\n```\n2014-06-09 09:15:38,746 INFO [Socket Reader #1 for port 63415] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1402304714683_0002 (auth:SIMPLE)\n2014-06-09 09:15:38,750 INFO [IPC Server handler 13 on 63415] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1402304714683_0002_r_000015_1000\n2014-06-09 09:15:38,751 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1402304714683_0002_r_000015_1000\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:722)\n2014-06-09 09:15:38,753 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1402304714683_0002 Job Transitioned from RUNNING to ERROR\n```\n\nThe JobHistory URL indicates the job state as ERROR.",
            "StackTrace": [
                "2014-06-09 09:15:38,746 INFO [Socket Reader #1 for port 63415] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1402304714683_0002 (auth:SIMPLE)",
                "2014-06-09 09:15:38,750 INFO [IPC Server handler 13 on 63415] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1402304714683_0002_r_000015_1000",
                "2014-06-09 09:15:38,751 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1402304714683_0002_r_000015_1000",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:722)",
                "2014-06-09 09:15:38,753 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1402304714683_0002 Job Transitioned from RUNNING to ERROR"
            ],
            "StepsToReproduce": [
                "Submit a Sort job with a data size of over 1GB.",
                "Monitor the job execution through the JobHistory URL.",
                "Observe the logs for any errors related to state transitions."
            ],
            "ExpectedBehavior": "The Sort job should complete successfully without any state transition errors.",
            "ObservedBehavior": "The Sort job fails with an InvalidStateTransitonException, transitioning to ERROR state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3649",
            "Title": "Job End Notification Fails with UnknownServiceException",
            "Description": "When calling the job end notification for Oozie, the Application Master (AM) fails with an UnknownServiceException. This issue occurs during the callback to the specified URL.",
            "StackTrace": [
                "2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&status=SUCCEEDED& failed",
                "java.net.UnknownServiceException: no content-type",
                "\tat java.net.URLConnection.getContentHandler(URLConnection.java:1192)",
                "\tat java.net.URLConnection.getContent(URLConnection.java:689)",
                "\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)",
                "\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)"
            ],
            "StepsToReproduce": [
                "1. Submit a job to Oozie.",
                "2. Wait for the job to complete.",
                "3. Observe the job end notification callback to the specified URL."
            ],
            "ExpectedBehavior": "The job end notification should successfully call back to the specified URL without errors.",
            "ObservedBehavior": "The job end notification fails with an UnknownServiceException indicating 'no content-type'.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-7077",
            "Title": "Pipe MapReduce Job Fails with Permission Denied for jobTokenPassword",
            "Description": "When launching the wordcount example with pipes, the application fails due to a permission issue when trying to access the jobTokenPassword file.",
            "StackTrace": [
                "2018-02-02 02:40:51,071 ERROR [IPC Server handler 16 on 43391] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1517534613368_0041_r_000000_2 - exited : java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1026)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:703)",
                "at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)",
                "at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)"
            ],
            "StepsToReproduce": [
                "Launch the wordcount example with pipes using the following command:",
                "/usr/hdp/current/hadoop-client/bin/hadoop pipes \"-Dhadoop.pipes.java.recordreader=true\" \"-Dhadoop.pipes.java.recordwriter=true\" -input pipeInput -output pipeOutput -program bin/wordcount"
            ],
            "ExpectedBehavior": "The wordcount application should execute successfully without any permission errors.",
            "ObservedBehavior": "The application fails with a FileNotFoundException due to permission denied for the jobTokenPassword file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "bug_report": {
            "BugID": "12549389",
            "Title": "Job Counters Not Available in Job History Web UI for Killed Jobs",
            "Description": "When a job is killed before completion, the Job History Web UI displays a '500 error' when attempting to view the job counters. This issue occurs despite the availability of task counters for successful tasks.",
            "StackTrace": [
                "Caused by: com.google.inject.ProvisionException: Guice provision errors:",
                "2012-04-03 19:42:53,148 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /jobhistory/jobcounters/job_1333482028750_0001",
                "java.lang.reflect.InvocationTargetException",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "    at java.lang.reflect.Method.invoke(Method.java:597)",
                "...",
                "1) Error injecting constructor, java.lang.NullPointerException",
                "  at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)",
                "  while locating org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock",
                "...",
                "Caused by: java.lang.NullPointerException",
                "    at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)",
                "    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)",
                "    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)",
                "    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)"
            ],
            "StepsToReproduce": [
                "Run a simple wordcount or sleep job.",
                "Kill the job before it finishes.",
                "Go to the Job History Web UI.",
                "Click the 'Counters' link for the killed job."
            ],
            "ExpectedBehavior": "The Job History Web UI should display the job counters without errors, even for killed jobs.",
            "ObservedBehavior": "The Job History Web UI displays a '500 error' when attempting to view the job counters for killed jobs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6353",
            "Title": "ArithmeticException: Divide by Zero in ResourceCalculatorUtils",
            "Description": "When running a sleep job with zero CPU vcores, an ArithmeticException occurs due to a division by zero error in the ResourceCalculatorUtils class.",
            "StackTrace": [
                "2015-04-30 06:41:06,954 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Configure a sleep job with zero CPU vcores.",
                "2. Execute the job.",
                "3. Observe the logs for any exceptions."
            ],
            "ExpectedBehavior": "The job should run without errors, and the ResourceCalculatorUtils should handle zero vcores gracefully.",
            "ObservedBehavior": "An ArithmeticException is thrown, indicating a division by zero error.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4913",
            "Title": "TestMRAppMaster#testMRAppMasterMissingStaging Causes JVM Exit",
            "Description": "The test case `testMRAppMasterMissingStaging` occasionally causes the JVM to exit due to an error from `AsyncDispatcher`. This can lead to a build failure since the test process exits without unregistering from Surefire, which treats it as a build error rather than a test failure.",
            "StackTrace": [
                "2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "Run the test suite that includes `TestMRAppMaster`.",
                "Ensure that the test case `testMRAppMasterMissingStaging` is executed.",
                "Observe the behavior of the JVM during the test execution."
            ],
            "ExpectedBehavior": "The test case should complete without causing the JVM to exit unexpectedly.",
            "ObservedBehavior": "The JVM exits with a fatal error from `AsyncDispatcher`, leading to a build failure.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6492",
            "Title": "NullPointerException in AsyncDispatcher due to unassigned TaskAttempt",
            "Description": "The AsyncDispatcher encounters a NullPointerException when processing an event for a TaskAttempt that is in the UNASSIGNED state. This occurs in the method `sendJHStartEventForAssignedFailTask` of the `TaskAttemptImpl` class, leading to a failure in log aggregation for the mapreduce application.",
            "StackTrace": [
                "2015-09-28 18:01:48,656 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "\tat java.lang.Thread.run(Thread.java:745)",
                "2015-09-28 18:01:48,660 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..",
                "2015-09-28 18:01:48,660 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_e04_1443430524957_0006_01_000059 taskAttempt attempt_1443430524957_0006_m_000000_9"
            ],
            "StepsToReproduce": [
                "1. Submit a mapreduce job that results in a TaskAttempt being in the UNASSIGNED state.",
                "2. Monitor the logs for the AsyncDispatcher.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should handle all TaskAttempt states without throwing exceptions.",
            "ObservedBehavior": "The AsyncDispatcher throws a NullPointerException when processing an event for a TaskAttempt in the UNASSIGNED state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5744",
            "Title": "Job Hangs Due to Comparator Contract Violation in RMContainerAllocator",
            "Description": "We encountered a situation where tasks are not getting assigned because the method RMContainerAllocator$AssignedRequests.preemptReduce() fails repeatedly with an IllegalArgumentException. This is caused by a comparator that does not adhere to its general contract, specifically when the comparison result is zero.",
            "StackTrace": [
                "2014-02-06 16:43:45,183 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeLo(TimSort.java:747)",
                "at java.util.TimSort.mergeAt(TimSort.java:483)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:408)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start a Hadoop MapReduce job that requires task assignment.",
                "2. Monitor the RMContainerAllocator for errors.",
                "3. Observe the IllegalArgumentException being thrown repeatedly."
            ],
            "ExpectedBehavior": "Tasks should be assigned without errors, and the job should complete successfully.",
            "ObservedBehavior": "Tasks are not getting assigned, and the job hangs due to repeated IllegalArgumentException.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "bug_report": {
            "BugID": "12535850",
            "Title": "NumberFormatException in ProcfsBasedProcessTree#constructProcessInfo() due to large ppid",
            "Description": "HBase PreCommit builds frequently encounter a NumberFormatException when processing large parent process IDs (ppid). This issue arises because the ppid exceeds the maximum value that a Java long can handle, leading to a failure in parsing the integer value.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"18446743988060683582\"",
                "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)",
                "\tat java.lang.Long.parseLong(Long.java:422)",
                "\tat java.lang.Long.parseLong(Long.java:468)",
                "\tat org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)",
                "\tat org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)",
                "\tat org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)",
                "\tat org.apache.hadoop.mapred.Task.initialize(Task.java:536)",
                "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)",
                "\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:396)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "\tat org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "StepsToReproduce": [
                "Run HBase PreCommit builds on a system with a process that has a ppid exceeding 63 bits.",
                "Observe the logs for NumberFormatException related to ProcfsBasedProcessTree."
            ],
            "ExpectedBehavior": "The system should handle large ppid values without throwing a NumberFormatException.",
            "ObservedBehavior": "A NumberFormatException is thrown when attempting to parse a ppid that exceeds the maximum value for a Java long.",
            "Resolution": "Proposed to change allProcessInfo to Map<String, ProcessInfo> to avoid parsing large integers."
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-2238",
            "Title": "Undeletable Build Directories Causing Job Failures",
            "Description": "The MR Hudson job is failing due to a test that is changing permissions on a build directory, preventing the checkout process from cleaning the build directory. This issue leads to job failures in the Hadoop MapReduce project.",
            "StackTrace": [
                "hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7",
                "\tat hudson.FilePath.act(FilePath.java:749)",
                "\tat hudson.FilePath.act(FilePath.java:735)",
                "\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)",
                "\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)",
                "\tat hudson.model.AbstractProject.checkout(AbstractProject.java:1116)",
                "\tat hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild.java:479)",
                "\tat hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild.java:411)",
                "\tat hudson.model.Run.run(Run.java:1324)",
                "\tat hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)",
                "\tat hudson.model.ResourceController.execute(ResourceController.java:88)",
                "\tat hudson.model.Executor.run(Executor.java:139)",
                "Caused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
            ],
            "StepsToReproduce": [
                "1. Trigger a build job in Hudson for the Hadoop MapReduce project.",
                "2. Ensure that the test modifies the permissions of the build directory.",
                "3. Observe the job failure in the console output."
            ],
            "ExpectedBehavior": "The build job should complete successfully without any permission-related errors.",
            "ObservedBehavior": "The build job fails with an IOException indicating that it cannot delete the specified build directory.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6410",
            "Title": "GSSException Thrown During Log Deletion After Refreshing Log Retention Settings in Secure Cluster",
            "Description": "A GSSException is thrown every time log aggregation deletion is attempted after executing the command `bin/mapred hsadmin -refreshLogRetentionSettings` in a secure cluster. This issue occurs specifically when the history server is started in secure mode and log deletion is enabled.",
            "StackTrace": [
                "2015-06-04 14:14:40,070 | ERROR | Timer-3 | Error reading root log dir this deletion attempt is being aborted | AggregatedLogDeletionService.java:127",
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"vm-31/9.91.12.31\"; destination host is: \"vm-33\":25000;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy9.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy10.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)",
                "at java.util.TimerThread.mainLoop(Timer.java:555)",
                "at java.util.TimerThread.run(Timer.java:505)",
                "Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)",
                "at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1381)",
                "... 21 more",
                "Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:411)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:550)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:367)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:716)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:712)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)",
                "... 24 more",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)",
                "... 33 more"
            ],
            "StepsToReproduce": [
                "Start the history server in a secure cluster.",
                "Perform log deletion as per expectation.",
                "Execute the command `mapred hsadmin -refreshLogRetentionSettings` to refresh the configuration value.",
                "Attempt log deletion again and observe the failure with GSSException."
            ],
            "ExpectedBehavior": "Log deletion should succeed after refreshing log retention settings.",
            "ObservedBehavior": "Log deletion fails with a GSSException indicating no valid credentials provided.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6693",
            "Title": "ArrayIndexOutOfBoundsException when job name exceeds character limit",
            "Description": "An ArrayIndexOutOfBoundsException occurs when the length of the job name is equal to the limit defined by {{mapreduce.jobhistory.jobname.limit}}. This results in a failure to create the job history entry.",
            "StackTrace": [
                "java.lang.ArrayIndexOutOfBoundsException: 50",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Set the job name to a string that is equal to the value of mapreduce.jobhistory.jobname.limit.",
                "Submit the job to the Hadoop MapReduce framework.",
                "Check the job history for the created entry."
            ],
            "ExpectedBehavior": "The job history entry should be created successfully without any exceptions.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown, preventing the job history entry from being created.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5912",
            "Title": "Task.calculateOutputSize fails to handle Windows file paths after MAPREDUCE-5196",
            "Description": "The method Task.calculateOutputSize does not correctly handle Windows file paths, leading to exceptions when attempting to retrieve file status. This issue arises after the changes made in MAPREDUCE-5196.",
            "StackTrace": [
                "2014-06-02 00:14:53,891 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)",
                "at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)",
                "at org.apache.hadoop.mapred.Task.done(Task.java:1048)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop job on a Windows environment.",
                "2. Submit a job that generates output files.",
                "3. Monitor the job execution and observe the logs for any warnings or errors."
            ],
            "ExpectedBehavior": "The Task.calculateOutputSize method should correctly identify and handle Windows file paths, returning the appropriate file status without errors.",
            "ObservedBehavior": "The method throws an IllegalArgumentException indicating that the provided pathname is not a valid DFS filename.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "bug_report": {
            "BugID": "12724589",
            "Title": "LocalContainerLauncher#renameMapOutputForReduce Incorrectly Assumes Single Directory for Map Output Index",
            "Description": "The javadoc comment for `renameMapOutputForReduce` incorrectly refers to a single map output directory, whereas this depends on `LOCAL_DIRS`. The method `mapOutIndex` should be set to `subMapOutputFile.getOutputIndexFile()`.",
            "StackTrace": [
                "2014-06-30 14:48:35,574 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "Set up a Hadoop MapReduce job with multiple reducers.",
                "Run the job and monitor the output directories.",
                "Check the logs for any warnings or errors related to map output indexing."
            ],
            "ExpectedBehavior": "The job should correctly handle multiple map output directories without throwing a FileNotFoundException.",
            "ObservedBehavior": "The job fails with a FileNotFoundException indicating that the expected output index file does not exist.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3306",
            "Title": "NoSuchElementException in NodeManager when running applications",
            "Description": "The NodeManager logs show a NoSuchElementException when attempting to run jobs, which prevents applications from executing properly.",
            "StackTrace": [
                "java.util.NoSuchElementException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:822)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager.",
                "Submit a job to the NodeManager.",
                "Check the NodeManager logs for errors."
            ],
            "ExpectedBehavior": "The job should run successfully without any exceptions in the logs.",
            "ObservedBehavior": "The job fails to run, and the logs show a NoSuchElementException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6554",
            "Title": "NullPointerException during MRAppMaster Service Start",
            "Description": "The MRAppMaster fails to start due to a NullPointerException when attempting to recover the previous job history file. This issue occurs in the method `MRAppMaster#parsePreviousJobHistory`.",
            "StackTrace": [
                "2015-11-21 13:52:27,722 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STARTED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "    at java.io.StringReader.<init>(StringReader.java:50)",
                "    at org.apache.avro.Schema$Parser.parse(Schema.java:917)",
                "    at org.apache.avro.Schema.parse(Schema.java:966)",
                "    at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)",
                "    at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)",
                "    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)",
                "2015-11-21 13:52:27,725 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0"
            ],
            "StepsToReproduce": [
                "Create a scenario where the MR app master gets preempted.",
                "On the next MRAppMaster launch, attempt to recover the previous job history file."
            ],
            "ExpectedBehavior": "The MRAppMaster should start successfully and recover the previous job history without errors.",
            "ObservedBehavior": "The MRAppMaster fails to start and throws a NullPointerException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4457",
            "Title": "Invalid State Transition in TaskAttemptImpl Leading to Error State",
            "Description": "A job transitioned into the ERROR state due to an invalid state transition caused by receiving multiple TA_TOO_MANY_FETCH_FAILURE events. The first event caused the task to fail, and the second event failed because there was no valid transition available.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_TOO_MANY_FETCH_FAILURE at FAILED",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "    at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "    at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "    at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Submit a job that is expected to run successfully.",
                "2. Monitor the job's state transitions.",
                "3. Trigger conditions that would lead to multiple TA_TOO_MANY_FETCH_FAILURE events."
            ],
            "ExpectedBehavior": "The job should handle state transitions correctly without entering an ERROR state due to invalid transitions.",
            "ObservedBehavior": "The job enters an ERROR state due to an invalid state transition after receiving multiple TA_TOO_MANY_FETCH_FAILURE events.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-2716",
            "Title": "MR279: MRReliabilityTest Fails Due to Missing jobFile",
            "Description": "The ApplicationReport should include the jobFile (e.g. hdfs://localhost:9000/tmp/hadoop-<USER>/mapred/staging/<USER>/.staging/job_201107121640_0001/job.xml). Without it, jobs such as MRReliabilityTest fail with the following error: java.lang.IllegalArgumentException: Can not create a Path from an empty string. This issue is caused by the jobFile being hardcoded to an empty string in TypeConverter.java.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Can not create a Path from an empty string",
                "at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:96)",
                "at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)",
                "at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)",
                "at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)",
                "at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)",
                "at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)",
                "at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)",
                "at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)",
                "at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)",
                "at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:192)"
            ],
            "StepsToReproduce": [
                "Run the MRReliabilityTest job.",
                "Observe the error message related to the jobFile."
            ],
            "ExpectedBehavior": "The MRReliabilityTest should complete successfully without errors related to the jobFile.",
            "ObservedBehavior": "The MRReliabilityTest fails with an IllegalArgumentException due to the jobFile being an empty string.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-6702.json",
        "creation_time": "2016-05-17T22:11:38.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-6702",
            "Title": "TestMiniMRChildTask Tests Fail Due to Environment Variable Issues",
            "Description": "The tests `testTaskEnv` and `testTaskOldEnv` in `org.apache.hadoop.mapred.TestMiniMRChildTask` are failing due to the environment checker job not passing. This is likely related to the fact that YARN containers do not inherit NodeManager's environment variables in Hadoop 3.",
            "StackTrace": [
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)",
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop 3 environment with YARN.",
                "2. Run the tests in `org.apache.hadoop.mapred.TestMiniMRChildTask`.",
                "3. Observe the test results."
            ],
            "ExpectedBehavior": "The tests should pass without any assertion errors related to environment variables.",
            "ObservedBehavior": "The tests fail with an assertion error indicating that the environment checker job failed.",
            "Resolution": "The issue is related to the environment variable inheritance in YARN containers. Ensure that the necessary parameters (e.g., `mapreduce.admin.user.env` and `yarn.app.mapreduce.am.env`) are set correctly."
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-4748",
            "Title": "Invalid State Transition Error in MapReduce Task Execution",
            "Description": "An error occurs during the execution of a large Pig script, specifically when a task attempts to transition to a succeeded state but encounters an invalid state transition exception.",
            "StackTrace": [
                "2012-10-23 22:45:24,986 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Can't handle this event at current state for task_1350837501057_21978_m_040453",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Enable speculative execution in the MapReduce configuration.",
                "2. Run a large Pig script that triggers multiple task attempts.",
                "3. Monitor the logs for any state transition errors."
            ],
            "ExpectedBehavior": "The task should successfully transition to the SUCCEEDED state without any errors.",
            "ObservedBehavior": "An InvalidStateTransitonException is thrown, indicating that the task cannot handle the event at its current state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3062",
            "Title": "ResourceManager Fails to Start Due to Invalid Configuration",
            "Description": "The ResourceManager fails to start with a fatal error indicating that the configuration for 'yarn.resourcemanager.admin.address' is not a valid host:port pair. This issue appears to be related to a copy-paste error in the configuration file.",
            "StackTrace": [
                "2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager",
                "java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)"
            ],
            "StepsToReproduce": [
                "1. Open the configuration file for YARN.",
                "2. Check the value of 'yarn.resourcemanager.admin.address'.",
                "3. Attempt to start the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any fatal errors.",
            "ObservedBehavior": "The ResourceManager fails to start with a fatal error indicating an invalid configuration for 'yarn.resourcemanager.admin.address'.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5724",
            "Title": "JobHistoryServer Fails to Start When HDFS is Not Running",
            "Description": "Starting the Job History Server (JHS) without HDFS running results in a failure with a connection error. The server attempts to create a done directory in HDFS but fails due to the absence of an active HDFS instance.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)",
                "Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused"
            ],
            "StepsToReproduce": [
                "Ensure that HDFS is not running.",
                "Start the Job History Server (JHS).",
                "Observe the logs for error messages."
            ],
            "ExpectedBehavior": "The Job History Server should start successfully, or provide a clear error message indicating that HDFS is required.",
            "ObservedBehavior": "The Job History Server fails to start and logs an error indicating that it cannot create the done directory due to a connection refusal.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5358",
            "Title": "Invalid State Transition Errors in MRAppMaster for JobImpl",
            "Description": "The MRAppMaster is throwing invalid state transition errors when handling job events, specifically for JOB_TASK_ATTEMPT_COMPLETED and JOB_MAP_TASK_RESCHEDULED events. This issue occurs when the job is in the SUCCEEDED state, leading to exceptions that prevent proper job handling.",
            "StackTrace": [
                "2013-06-26 11:39:50,128 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-06-26 11:39:50,129 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Submit a job to the MRAppMaster.",
                "Ensure the job reaches the SUCCEEDED state.",
                "Trigger events JOB_TASK_ATTEMPT_COMPLETED and JOB_MAP_TASK_RESCHEDULED."
            ],
            "ExpectedBehavior": "The MRAppMaster should handle job events appropriately without throwing invalid state transition errors.",
            "ObservedBehavior": "The MRAppMaster throws InvalidStateTransitonException errors when handling specific job events while in the SUCCEEDED state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5837",
            "Title": "MRAppMaster NoClassDefError when checking for Uber Mode",
            "Description": "The MRAppMaster fails to determine if a job should run in uber mode due to a NoClassDefError when the required class is not found. This occurs when the additional dependent jar is unavailable to the MRAppMaster, leading to job failures.",
            "StackTrace": [
                "2014-04-15 11:52:55,877 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster",
                "java.lang.NoClassDefFoundError: scala/Function1",
                "    at java.lang.Class.forName0(Native Method)",
                "    at java.lang.Class.forName(Class.java:190)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)",
                "    at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)",
                "    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:415)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)",
                "    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)",
                "Caused by: java.lang.ClassNotFoundException: scala.Function1",
                "    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)",
                "    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)",
                "    ... 22 more"
            ],
            "StepsToReproduce": [
                "1. Configure a MapReduce job that requires the Scala library.",
                "2. Ensure that the Scala library jar is not included in the classpath.",
                "3. Submit the job to the MRAppMaster."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully determine if the job can run in uber mode without throwing an error.",
            "ObservedBehavior": "The MRAppMaster throws a NoClassDefError, preventing the job from starting.",
            "Resolution": "Catch NoClassDefError in the relevant places to handle missing dependencies gracefully."
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-3058",
            "Title": "Reducer Task Stuck for Extended Duration Despite Shutdown Logs",
            "Description": "While running GridMixV3, one of the jobs got stuck for over 15 hours. The job page indicated that one of its reducers was stuck. Upon checking the syslog of the stuck reducer, it was found that the task was supposed to have stopped within 20 seconds, but the process remained alive for an extended period. The logs indicate an IOException related to data node connectivity issues.",
            "StackTrace": [
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)",
                "2011-09-19 18:06:49,837 WARN org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)"
            ],
            "StepsToReproduce": [
                "Run GridMixV3 with a job that includes a reducer.",
                "Monitor the job page for the reducer's status.",
                "Check the syslog for the reducer after it is expected to have stopped."
            ],
            "ExpectedBehavior": "The reducer task should stop within 20 seconds after completion.",
            "ObservedBehavior": "The reducer task remained alive for over 15 hours despite logs indicating it should have stopped.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "bug_report": {
            "BugID": "MAPREDUCE-5088",
            "Title": "UninitializedMessageException during Oozie job submission",
            "Description": "After the fix for HADOOP-9299, an UninitializedMessageException occurs in Oozie when attempting to submit a job. This issue appears to be related to Kerberos authentication.",
            "StackTrace": [
                "org.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer",
                "at org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)",
                "at org.apache.oozie.command.XCommand.call(XCommand.java:277)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)",
                "at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer",
                "at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)",
                "at org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)",
                "at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)",
                "... 10 more"
            ],
            "StepsToReproduce": [
                "1. Ensure the environment is set up with Kerberos authentication.",
                "2. Attempt to submit a job using Oozie.",
                "3. Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The job should be submitted successfully without any exceptions.",
            "ObservedBehavior": "An UninitializedMessageException is thrown indicating that the message is missing required fields, specifically 'renewer'.",
            "Resolution": "Fixed"
        }
    }
]