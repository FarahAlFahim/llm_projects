[
    {
        "filename": "HDFS-4558.json",
        "creation_time": "2013-03-06T11:10:08.000+0000",
        "bug_report": {
            "BugID": "HDFS-4558",
            "Title": "NullPointerException when starting balancer",
            "Description": "The balancer fails to start due to a NullPointerException. This issue needs to be tracked for Quality Engineering (QE) and development review.",
            "StackTrace": [
                "2013-03-06 00:19:55,174 ERROR org.apache.hadoop.hdfs.server.balancer.Balancer: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:165)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility(Balancer.java:799)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.<init>(Balancer.java:808)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:831)"
            ],
            "StepsToReproduce": [
                "1. Attempt to start the balancer.",
                "2. Monitor the logs for any errors."
            ],
            "ExpectedBehavior": "The balancer should start without any errors.",
            "ObservedBehavior": "The balancer fails to start and logs a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-13039.json",
        "creation_time": "2018-01-19T18:52:12.000+0000",
        "bug_report": {
            "BugID": "HDFS-13039",
            "Title": "DataNode Exhausts File Descriptors Due to Excessive CLOSE_WAIT Connections",
            "Description": "When running Erasure Coding (EC) on a cluster, the DataNode accumulates millions of CLOSE_WAIT connections, leading to an inability to open new files or sockets. This results in a critical failure as indicated by the logs.",
            "StackTrace": [
                "2018-01-19 06:47:09,424 WARN io.netty.channel.DefaultChannelPipeline: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.",
                "java.io.IOException: Too many open files",
                "at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:422)",
                "at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:250)",
                "at io.netty.channel.socket.nio.NioServerSocketChannel.doReadMessages(NioServerSocketChannel.java:135)",
                "at io.netty.channel.nio.AbstractNioMessageChannel$NioMessageUnsafe.read(AbstractNioMessageChannel.java:75)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:563)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:504)",
                "at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:418)",
                "at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:390)",
                "at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:742)",
                "at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Start the DataNode on a cluster with Erasure Coding enabled.",
                "2. Monitor the number of connections using the command: `grep CLOSE_WAIT lsof.out | wc -l`.",
                "3. Observe the accumulation of CLOSE_WAIT connections over time."
            ],
            "ExpectedBehavior": "The DataNode should manage connections efficiently without reaching the limit of open files.",
            "ObservedBehavior": "The DataNode reaches millions of CLOSE_WAIT connections, resulting in an IOException indicating 'Too many open files'.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-13023.json",
        "creation_time": "2018-01-16T20:27:39.000+0000",
        "bug_report": {
            "BugID": "HDFS-13023",
            "Title": "Journal Sync Fails on Secure Cluster Due to Authorization Exception",
            "Description": "The Journal Node synchronization fails with an authorization exception when attempting to sync with the journal on a secure cluster. The error indicates that the user is not authorized for the protocol interface.",
            "StackTrace": [
                "2018-01-10 01:15:40,517 INFO server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(235)) - Syncing Journal /0.0.0.0:8485 with xxx, journal id: mycluster",
                "2018-01-10 01:15:40,583 ERROR server.JournalNodeSyncer (JournalNodeSyncer.java:syncWithJournalAtIndex(259)) - Could not sync with Journal at xxx/xxx:8485",
                "com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx@EXAMPLE.COM",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy16.getEditLogManifest(Unknown Source)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex(JournalNodeSyncer.java:254)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncJournals(JournalNodeSyncer.java:230)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.lambda$startSyncJournalsDaemon$0(JournalNodeSyncer.java:190)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User nn/xxx (auth:PROXY) via jn/xxx (auth:KERBEROS) is not authorized for protocol interface org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol: this service is only accessible by nn/xxx",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1437)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)",
                "... 6 more"
            ],
            "StepsToReproduce": [
                "1. Set up a secure Hadoop cluster with Kerberos authentication.",
                "2. Attempt to sync the Journal Node with the journal using the command: [Provide command details].",
                "3. Observe the logs for any errors during the sync process."
            ],
            "ExpectedBehavior": "The Journal Node should successfully sync with the journal without any authorization errors.",
            "ObservedBehavior": "The sync fails with an authorization exception indicating that the user is not authorized for the protocol interface.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-3157.json",
        "creation_time": "2012-03-28T14:35:53.000+0000",
        "bug_report": {
            "BugID": "HDFS-3157",
            "Title": "IOException when deleting block from DataNode after replication",
            "Description": "An IOException occurs on the DataNode when attempting to delete a block that has already been replicated. This issue arises after a file is written and the blocks are deleted from one of the DataNodes, leading to inconsistencies in the block management.",
            "StackTrace": [
                "java.io.IOException: Error in deleting blocks.",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:2061)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActive(BPOfferService.java:581)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.processCommandFromActor(BPOfferService.java:545)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processCommand(BPServiceActor.java:690)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:522)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:662)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Set up a cluster with 1 NameNode and 3 DataNodes (DN1, DN2, DN3) with a replication factor of 2.",
                "2. Write a file named 'a.txt' and do not close it (keep it open with sync).",
                "3. Delete the blocks from one of the DataNodes (e.g., DN1) that were replicated.",
                "4. Close the file."
            ],
            "ExpectedBehavior": "The block should be deleted without any IOException, and the DataNode should correctly manage the block replication.",
            "ObservedBehavior": "An IOException is thrown indicating that the block cannot be deleted because it is not found in the volume map.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-4850.json",
        "creation_time": "2013-05-25T00:18:15.000+0000",
        "bug_report": {
            "BugID": "HDFS-4850",
            "Title": "NegativeArraySizeException in OfflineImageViewer when processing fsimages with empty files",
            "Description": "When using the OfflineImageViewer on fsimages that contain empty files, a NegativeArraySizeException is thrown, preventing the successful loading of the image. This issue occurs after creating an empty file in HDFS and forcing a checkpoint.",
            "StackTrace": [
                "2013-05-24 17:01:13,622 ERROR [main] offlineImageViewer.OfflineImageViewer (OfflineImageViewer.java:go(140)) - image loading failed at offset 402",
                "Exception in thread \"main\" java.lang.NegativeArraySizeException",
                "at org.apache.hadoop.io.Text.readString(Text.java:458)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processPermission(ImageLoaderCurrent.java:370)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode(ImageLoaderCurrent.java:671)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processChildren(ImageLoaderCurrent.java:557)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:464)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryWithSnapshot(ImageLoaderCurrent.java:470)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processLocalNameINodesWithSnapshot(ImageLoaderCurrent.java:444)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINodes(ImageLoaderCurrent.java:398)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage(ImageLoaderCurrent.java:199)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.go(OfflineImageViewer.java:136)",
                "at org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer.main(OfflineImageViewer.java:260)"
            ],
            "StepsToReproduce": [
                "Deploy hadoop-trunk HDFS.",
                "Create the directory /user/schu.",
                "Force a checkpoint and fetch the fsimage.",
                "Run the OfflineImageViewer on the fsimage and verify it runs successfully.",
                "Create an empty file /user/schu/testFile1.",
                "Force another checkpoint and fetch the fsimage again.",
                "Run the OfflineImageViewer on the new fsimage."
            ],
            "ExpectedBehavior": "The OfflineImageViewer should successfully load the fsimage and include the empty file in the output.",
            "ObservedBehavior": "The OfflineImageViewer throws a NegativeArraySizeException and fails to load the fsimage containing the empty file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-3415.json",
        "creation_time": "2012-05-13T13:44:43.000+0000",
        "bug_report": {
            "BugID": "HDFS-3415",
            "Title": "NullPointerException during NameNode startup with inconsistent storage layout versions",
            "Description": "When starting the NameNode after modifying the layout version of one of the storage directories, a NullPointerException occurs. This issue arises when the layout versions of the storage directories are different, leading to the NameNode picking the wrong storage directory inspector.",
            "StackTrace": [
                "2012-05-13 19:01:41,483 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.namenode.NNStorage.getStorageFile(NNStorage.java:686)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditsInStorageDir(FSImagePreTransactionalStorageInspector.java:243)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getLatestEditsFiles(FSImagePreTransactionalStorageInspector.java:261)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImagePreTransactionalStorageInspector.getEditLogStreams(FSImagePreTransactionalStorageInspector.java:276)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:596)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:368)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:402)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:564)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:545)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1093)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1151)",
                "2012-05-13 19:01:41,485 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:"
            ],
            "StepsToReproduce": [
                "Start the NameNode and DataNode by configuring three storage directories for the NameNode.",
                "Write 10 files to the NameNode.",
                "Edit the version file of one of the storage directories and set the layout version to 123, which is different from the default (-40).",
                "Stop the NameNode.",
                "Start the NameNode again."
            ],
            "ExpectedBehavior": "The NameNode should start without any exceptions, correctly identifying the storage directory inspector based on the layout versions.",
            "ObservedBehavior": "A NullPointerException is thrown during the startup of the NameNode, preventing it from initializing properly.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-2245.json",
        "creation_time": "2011-08-10T22:55:33.000+0000",
        "bug_report": {
            "BugID": "HDFS-2245",
            "Title": "NullPointerException in BlockManager.chooseTarget() during addBlock",
            "Description": "A NullPointerException is thrown in the BlockManager.chooseTarget() method when attempting to add a block to the HDFS. This issue occurs under certain conditions when the system is unable to place replicas.",
            "StackTrace": [
                "java.io.IOException: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)"
            ],
            "StepsToReproduce": [
                "1. Start the HDFS Namenode.",
                "2. Attempt to add a block to the HDFS with insufficient available nodes for replication.",
                "3. Monitor the logs for exceptions."
            ],
            "ExpectedBehavior": "The system should log a warning indicating that it was unable to place replicas without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the operation to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-10320.json",
        "creation_time": "2016-04-20T23:02:09.000+0000",
        "bug_report": {
            "BugID": "HDFS-10320",
            "Title": "Network NameNode Termination Due to Rack Failures",
            "Description": "When there are rack failures that leave only one rack available, the BlockPlacementPolicyDefault#chooseRandom method may encounter an InvalidTopologyException. This exception propagates up to the BlockManager's ReplicationMonitor thread, leading to the termination of the NameNode.",
            "StackTrace": [
                "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to find datanode (scope=\"\" excludedScope=\"/rack_a5\").",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:729)",
                "at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:694)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:635)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:580)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:348)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:214)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:111)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3746)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3711)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1400)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1306)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3682)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3634)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Simulate rack failures in the Hadoop cluster.",
                "2. Ensure that only one rack remains available.",
                "3. Monitor the NameNode's behavior during this condition."
            ],
            "ExpectedBehavior": "The NameNode should continue to operate without termination, even when rack failures occur.",
            "ObservedBehavior": "The NameNode terminates due to an InvalidTopologyException thrown by the BlockPlacementPolicyDefault#chooseRandom method.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-4201.json",
        "creation_time": "2012-11-16T21:28:57.000+0000",
        "bug_report": {
            "BugID": "HDFS-4201",
            "Title": "NullPointerException in BPServiceActor#sendHeartBeat",
            "Description": "A NullPointerException (NPE) was observed in the logs during the operation of the DataNode. This issue is likely caused by either the 'dn' or 'dn.getFSDataset()' being null, possibly due to a configuration or local directory failure.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:434)",
                "    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:520)",
                "    at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)",
                "    at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Start the DataNode service.",
                "2. Monitor the logs for any exceptions.",
                "3. Observe the occurrence of the NullPointerException."
            ],
            "ExpectedBehavior": "The DataNode should operate without throwing a NullPointerException during the heartbeat process.",
            "ObservedBehavior": "A NullPointerException is thrown in the BPServiceActor#sendHeartBeat method, causing potential disruptions in service.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-6904.json",
        "creation_time": "2014-08-21T10:17:21.000+0000",
        "bug_report": {
            "BugID": "HDFS-6904",
            "Title": "YARN Fails to Renew WebHDFS Delegation Token Due to Incorrect Service Port",
            "Description": "YARN is unable to renew delegation tokens obtained via the WebHDFS REST API. The issue arises when a user creates a delegation token using the WebHDFS REST API and then submits an application to YARN with this token. When YARN attempts to renew the token, it fails because the token service is pointing to the RPC port instead of the WebHDFS port.",
            "StackTrace": [
                "2014-08-19 03:12:54,733 WARN  security.DelegationTokenRenewer (DelegationTokenRenewer.java:handleDTRenewerAppSubmitEvent(661)) - Unable to add the application to the delegation token renewer.",
                "java.io.IOException: Failed to renew token: Kind: WEBHDFS delegation, Service: NameNodeIP:8020, Ident: (WEBHDFS delegation token 2222 for hrt_qa)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:394)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$5(DelegationTokenRenewer.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:657)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:638)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.io.IOException: Unexpected HTTP response: code=-1 != 200, op=RENEWDELEGATIONTOKEN, message=null",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:331)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$200(WebHdfsFileSystem.java:90)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:598)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:448)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:477)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:473)",
                "at org.apache.hadoop.hdfs.web.TokenAspect$TokenManager.renew(TokenAspect.java:73)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:477)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:1)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:473)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:392)",
                "... 6 more",
                "Caused by: java.io.IOException: The error stream is null.",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.jsonParse(WebHdfsFileSystem.java:304)",
                "at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:329)",
                "... 24 more",
                "2014-08-19 03:12:54,735 DEBUG event.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent.EventType: APP_REJECTED"
            ],
            "StepsToReproduce": [
                "1. Create a delegation token using the WebHDFS REST API.",
                "2. Pass this token to YARN as part of application submission via the YARN REST API.",
                "3. Observe the failure when YARN tries to renew the delegation token."
            ],
            "ExpectedBehavior": "YARN should successfully renew the WebHDFS delegation token without errors.",
            "ObservedBehavior": "YARN fails to renew the token due to an incorrect service port being used.",
            "Resolution": "The issue has been fixed in version 2.6.0."
        }
    },
    {
        "filename": "HDFS-13721.json",
        "creation_time": "2018-07-05T20:11:57.000+0000",
        "bug_report": {
            "BugID": "HDFS-13721",
            "Title": "NullPointerException in DataNode#getDiskBalancerStatus() during startup",
            "Description": "A NullPointerException occurs when attempting to retrieve the DiskBalancerStatus of the DataNode during startup. This issue has been observed consistently and needs to be addressed to improve the stability of the DataNode service.",
            "StackTrace": [
                "2018-06-28 05:11:47,650 ERROR org.apache.hadoop.jmx.JMXJsonServlet: getting attribute DiskBalancerStatus of Hadoop:service=DataNode,name=DataNodeInfo threw an exception",
                "javax.management.RuntimeMBeanException: java.lang.NullPointerException",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.writeAttribute(JMXJsonServlet.java:338)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.listBeans(JMXJsonServlet.java:316)",
                "at org.apache.hadoop.jmx.JMXJsonServlet.doGet(JMXJsonServlet.java:210)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:110)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1537)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus(DataNode.java:3146)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)",
                "at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)",
                "at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)",
                "at com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)",
                "at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)",
                "at com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)",
                "at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)",
                "at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)",
                "at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)",
                "at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)"
            ],
            "StepsToReproduce": [
                "Start the DataNode service.",
                "Monitor the logs for any exceptions related to DiskBalancerStatus."
            ],
            "ExpectedBehavior": "The DataNode should start without throwing any exceptions, and the DiskBalancerStatus should be retrievable without errors.",
            "ObservedBehavior": "A NullPointerException is thrown during the startup of the DataNode when attempting to access the DiskBalancerStatus.",
            "Resolution": "The issue has been resolved by initializing the DiskBalancer properly to prevent the NullPointerException."
        }
    },
    {
        "filename": "HDFS-7180.json",
        "creation_time": "2014-10-02T03:07:04.000+0000",
        "bug_report": {
            "BugID": "HDFS-7180",
            "Title": "NFSv3 Gateway Becomes Unresponsive After Extended Use",
            "Description": "The NFSv3 daemon frequently becomes unresponsive after approximately one day of operation while handling large data uploads via rsync. The HDFS remains functional, but the NFS mount point cannot be accessed, leading to stuck commands such as 'ls' and 'df -hT'.",
            "StackTrace": [
                "java.io.IOException: Bad response ERROR for block BP-1960069741-10.0.3.170-1410430543652:blk_1074363564_623643 from datanode 10.0.3.176:50010",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:828)",
                "WARN org.apache.hadoop.hdfs.DFSClient: Slow ReadProcessor read fields took 60062ms (threshold=30000ms); ack: seqno: -2 status: SUCCESS status: ERROR downstreamAckTimeNanos: 0, targets: [10.0.3.172:50010, 10.0.3.176:50010]"
            ],
            "StepsToReproduce": [
                "1. Start the NFSv3 gateway on a node in the Hadoop cluster.",
                "2. Mount the NFS on the same node.",
                "3. Use rsync to upload several hundreds of GBs of data to the NFS mount point.",
                "4. Wait for approximately 24 hours.",
                "5. Attempt to access the mounted directory using 'ls' or 'df -hT'."
            ],
            "ExpectedBehavior": "The NFSv3 gateway should remain responsive and allow for normal operations such as listing files and checking disk usage.",
            "ObservedBehavior": "The NFSv3 daemon becomes unresponsive, resulting in commands like 'ls' and 'df -hT' getting stuck. The logs indicate that the NFS server is not responding.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-6102.json",
        "creation_time": "2014-03-13T18:27:36.000+0000",
        "bug_report": {
            "BugID": "HDFS-6102",
            "Title": "Protocol Buffer Size Limit Exceeded When Loading Large FSImage",
            "Description": "During testing, a bug was discovered when creating a large number of directories, which caused the fsimage size to exceed the protocol buffer size limit. This results in an error when attempting to load the fsimage.",
            "StackTrace": [
                "2014-03-13 13:57:03,901 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 24523605 INodes.",
                "2014-03-13 13:57:59,038 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Failed to load image from FSImageFile(file=/dfs/nn/current/fsimage_0000000000024532742, cpktTxId=0000000000024532742)",
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message was too large.  May be malicious.  Use CodedInputStream.setSizeLimit() to increase the size limit.",
                "at com.google.protobuf.InvalidProtocolBufferException.sizeLimitExceeded(InvalidProtocolBufferException.java:110)",
                "at com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:755)",
                "at com.google.protobuf.CodedInputStream.readRawByte(CodedInputStream.java:769)",
                "at com.google.protobuf.CodedInputStream.readRawVarint64(CodedInputStream.java:462)",
                "at com.google.protobuf.CodedInputStream.readUInt64(CodedInputStream.java:188)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9839)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry.<init>(FsImageProto.java:9770)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9901)",
                "at org.apache.hadoop.hdfs.server.namenode.FsImageProto$INodeDirectorySection$DirEntry$1.parsePartialFrom(FsImageProto.java:9896)"
            ],
            "StepsToReproduce": [
                "1. Create a large number of directories within a single directory.",
                "2. Attempt to load the fsimage after the directories have been created."
            ],
            "ExpectedBehavior": "The fsimage should load successfully without exceeding protocol buffer size limits.",
            "ObservedBehavior": "The fsimage fails to load with an error indicating that the protocol message was too large.",
            "Resolution": "A fix has been implemented to lower the default maximum items per directory to prevent exceeding the protocol buffer size limit."
        }
    },
    {
        "filename": "HDFS-6250.json",
        "creation_time": "2014-04-16T16:14:32.000+0000",
        "bug_report": {
            "BugID": "HDFS-6250",
            "Title": "AssertionError in TestBalancerWithNodeGroup.testBalancerWithRackLocality",
            "Description": "An AssertionError occurs during the execution of the test case TestBalancerWithNodeGroup.testBalancerWithRackLocality. The expected value does not match the actual value, leading to test failure.",
            "StackTrace": [
                "java.lang.AssertionError: expected:<1800> but was:<1810>",
                "\tat org.junit.Assert.fail(Assert.java:93)",
                "\tat org.junit.Assert.failNotEquals(Assert.java:647)",
                "\tat org.junit.Assert.assertEquals(Assert.java:128)",
                "\tat org.junit.Assert.assertEquals(Assert.java:147)",
                "\tat org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup.testBalancerWithRackLocality(TestBalancerWithNodeGroup.java:253)"
            ],
            "StepsToReproduce": [
                "1. Navigate to the test suite for Hadoop HDFS.",
                "2. Execute the test case TestBalancerWithNodeGroup.testBalancerWithRackLocality.",
                "3. Observe the output for any AssertionError."
            ],
            "ExpectedBehavior": "The test should pass without any assertion errors, confirming that the expected value matches the actual value.",
            "ObservedBehavior": "The test fails with an AssertionError indicating that the expected value was 1800 but the actual value was 1810.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-11377.json",
        "creation_time": "2017-01-26T23:40:53.000+0000",
        "bug_report": {
            "BugID": "HDFS-11377",
            "Title": "Balancer Hangs Due to No Available Mover Threads",
            "Description": "When running the balancer on a large cluster with more than 3000 Datanodes, the process may hang due to the message 'No mover threads available'. The stack trace indicates that the balancer is waiting indefinitely, which leads to operational issues.",
            "StackTrace": [
                "\"main\" #1 prio=5 os_prio=0 tid=0x00007ff6cc014800 nid=0x6b2c waiting on condition [0x00007ff6d1bad000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion(Dispatcher.java:1043)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchBlockMoves(Dispatcher.java:1017)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatchAndCheckContinue(Dispatcher.java:981)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.runOneIteration(Balancer.java:611)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:663)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer$Cli.run(Balancer.java:776)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.hdfs.server.balancer.Balancer.main(Balancer.java:905)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with more than 3000 Datanodes.",
                "2. Initiate the balancer process.",
                "3. Monitor the logs for warnings related to mover threads."
            ],
            "ExpectedBehavior": "The balancer should complete its operations without hanging, successfully moving blocks between Datanodes.",
            "ObservedBehavior": "The balancer hangs indefinitely with the message 'No mover threads available', leading to operational delays.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-6753.json",
        "creation_time": "2014-07-25T11:36:43.000+0000",
        "bug_report": {
            "BugID": "HDFS-6753",
            "Title": "DataNode Not Shutting Down on Full Disk with Permission Denied",
            "Description": "The DataNode does not shut down when all configured volumes fail due to full disk and permission issues. This leads to potential data loss and system instability.",
            "StackTrace": [
                "2014-07-21 14:10:52,814 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: XX1.XX1.XX1.XX1:50010:DataXceiver error processing WRITE_BLOCK operation src: /XX2.XX2.XX2.XX2:10106 dst: /XX1.XX1.XX1.XX1:50010",
                "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException: Out of space: The volume with the most available space (=4096 B) is less than the block size (=134217728 B).",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.chooseVolume(RoundRobinVolumeChoosingPolicy.java:60)",
                "2014-07-21 14:13:00,180 WARN org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Exception occurred while compiling report:",
                "java.io.IOException: Invalid directory or I/O error occurred for dir: /mnt/tmp_Datanode/current/BP-1384489961-XX2.XX2.XX2.XX2-845784615183/current/finalized",
                "at org.apache.hadoop.fs.FileUtil.listFiles(FileUtil.java:1164)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner$ReportCompiler.compileReport(DirectoryScanner.java:596)"
            ],
            "StepsToReproduce": [
                "Step 1: Change the permissions of /mnt/tmp_Datanode to root.",
                "Step 2: Perform write operations (DataNode detects that all configured volumes have failed and shuts down).",
                "Step 3: Make /mnt/tmp_Datanode disk full and change the permissions to root.",
                "Step 4: Perform client write operations (disk full exception is thrown, but DataNode does not shut down)."
            ],
            "ExpectedBehavior": "The DataNode should shut down when all configured volumes fail due to full disk and permission issues.",
            "ObservedBehavior": "The DataNode does not shut down even when all configured volumes have failed due to full disk and permission denied errors.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-3443.json",
        "creation_time": "2012-05-18T11:30:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-3443",
            "Title": "NullPointerException during NameNode transition to active state",
            "Description": "A NullPointerException (NPE) occurs when the NameNode transitions to an active state during startup, specifically in the method `editLogTailer.catchupDuringFailover()`. This issue arises if the edit log tailer is initialized before the active services are started.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:602)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1287)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:63)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1219)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:978)",
                "at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:3633)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:427)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:916)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1688)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1686)"
            ],
            "StepsToReproduce": [
                "Start the NameNode (NN).",
                "Start the NameNode standby services.",
                "Before initializing the editLogTailer, start the ZKFC (ZooKeeper Failover Controller).",
                "Allow the active services to start."
            ],
            "ExpectedBehavior": "The NameNode should transition to active state without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the transition to active state, causing the active services to fail to start.",
            "Resolution": "A fix has been implemented by adding a checkNNStartup() method in NameNodeRpcServer to prevent the NPE."
        }
    },
    {
        "filename": "HDFS-11479.json",
        "creation_time": "2017-03-01T11:08:07.000+0000",
        "bug_report": {
            "BugID": "HDFS-11479",
            "Title": "UDP Server Fails to Start Due to Bind Exception in SimpleUdpServer",
            "Description": "The NFS gateway restart can fail because of a bind error in SimpleUdpServer. The re-use address option should be used in SimpleUdpServer to allow socket binding when it is in TIME_WAIT state.",
            "StackTrace": [
                "2017-02-28 04:19:53,495 FATAL mount.MountdBase (MountdBase.java:startUDPServer(66)) - Failed to start the UDP server.",
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:204)",
                "at org.apache.hadoop.oncrpc.SimpleUdpServer.run(SimpleUdpServer.java:68)",
                "at org.apache.hadoop.mount.MountdBase.startUDPServer(MountdBase.java:64)",
                "at org.apache.hadoop.mount.MountdBase.start(MountdBase.java:97)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "at sun.nio.ch.Net.bind0(Native Method)",
                "at sun.nio.ch.Net.bind(Net.java:433)",
                "at sun.nio.ch.DatagramChannelImpl.bind(DatagramChannelImpl.java:691)",
                "at sun.nio.ch.DatagramSocketAdaptor.bind(DatagramSocketAdaptor.java:91)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.bind(NioDatagramPipelineSink.java:129)",
                "at org.jboss.netty.channel.socket.nio.NioDatagramPipelineSink.eventSunk(NioDatagramPipelineSink.java:77)",
                "at org.jboss.netty.channel.Channels.bind(Channels.java:561)",
                "at org.jboss.netty.channel.AbstractChannel.bind(AbstractChannel.java:189)",
                "at org.jboss.netty.bootstrap.ConnectionlessBootstrap.bind(ConnectionlessBootstrap.java:198)",
                "... 11 more"
            ],
            "StepsToReproduce": [
                "Attempt to restart the NFS gateway.",
                "Observe the logs for any binding errors related to the UDP server."
            ],
            "ExpectedBehavior": "The UDP server should start successfully without any binding errors.",
            "ObservedBehavior": "The UDP server fails to start due to a bind exception indicating that the address is already in use.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-8055.json",
        "creation_time": "2015-04-02T23:21:55.000+0000",
        "bug_report": {
            "BugID": "HDFS-8055",
            "Title": "NullPointerException in NameNode when topology script is missing",
            "Description": "Reports indicate that the NameNode can throw a NullPointerException (NPE) when the topology script is missing. This issue aims to investigate the validation logic and improve the error messaging to be more informative.",
            "StackTrace": [
                "2015-02-06 23:02:12,250 ERROR [pool-4-thread-1] util.HFileV1Detector: Got exception while reading trailer for file:hdfs://hqhd02nm01.pclc0.merkle.local:8020/hbase/.META./1028785192/info/1490a396aea448b693da563f76a28486",
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:359)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:542)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:362)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1468)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1399)",
                "at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1220)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1210)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1200)",
                "at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:271)",
                "at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:238)",
                "at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:231)",
                "at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1498)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:302)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:298)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:298)",
                "at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)",
                "at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:320)",
                "at org.apache.hadoop.hbase.util.HFileV1Detector$1.call(HFileV1Detector.java:300)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Ensure that the topology script is missing from the configuration.",
                "Attempt to access the NameNode.",
                "Observe the logs for any NullPointerException errors."
            ],
            "ExpectedBehavior": "The system should validate the presence of the topology script and provide a clear error message indicating the issue.",
            "ObservedBehavior": "A NullPointerException is thrown, leading to confusion and lack of clarity on the actual problem.",
            "Resolution": "The issue has been fixed and the validation logic has been improved to provide more informative error messages."
        }
    },
    {
        "filename": "HDFS-6533.json",
        "creation_time": "2014-06-14T16:15:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-6533",
            "Title": "Intermittent Failure in TestBPOfferService#testBasicFunctionality",
            "Description": "The test 'TestBPOfferService#testBasicFunctionality' fails intermittently on the CI server but passes when run locally. This inconsistency needs to be investigated to ensure reliable test outcomes.",
            "StackTrace": [
                "org.mockito.exceptions.verification.WantedButNotInvoked: ",
                "Wanted but not invoked:",
                "datanodeProtocolClientSideTranslatorPB.registerDatanode(<any>);",
                "-> at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)",
                "Actually, there were zero interactions with this mock.",
                "at org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBasicFunctionality(TestBPOfferService.java:175)"
            ],
            "StepsToReproduce": [
                "Run the test suite on the CI server.",
                "Observe the failure of 'TestBPOfferService#testBasicFunctionality'.",
                "Rerun the same test locally to confirm it passes."
            ],
            "ExpectedBehavior": "The test 'TestBPOfferService#testBasicFunctionality' should pass consistently without failures.",
            "ObservedBehavior": "The test fails intermittently on the CI server but passes when run locally.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-10609.json",
        "creation_time": "2016-07-11T17:14:39.000+0000",
        "bug_report": {
            "BugID": "HDFS-10609",
            "Title": "Uncaught InvalidEncryptionKeyException during pipeline recovery aborts downstream applications",
            "Description": "In normal operations, if SASL negotiation fails due to InvalidEncryptionKeyException, it is typically a benign exception that is caught and retried. However, if the exception is thrown during pipeline recovery, the corresponding code does not handle it properly, leading to the exception being spilled out to downstream applications, such as SOLR, which aborts its operation. This issue needs to be addressed to ensure that the exception is contained within HDFS and retried appropriately.",
            "StackTrace": [
                "2016-07-06 12:12:51,992 ERROR org.apache.solr.update.HdfsTransactionLog: Exception closing tlog.",
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=557709482) doesn't exist. Current key: 1350592619",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:1308)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:1272)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1433)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1147)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:632)"
            ],
            "StepsToReproduce": [
                "1. Start a long-running application that relies on HDFS.",
                "2. Allow the application to run until the encryption key expires.",
                "3. Trigger a pipeline recovery in HDFS.",
                "4. Observe the behavior of the downstream applications (e.g., SOLR)."
            ],
            "ExpectedBehavior": "The InvalidEncryptionKeyException should be caught and retried within HDFS, allowing the downstream applications to continue functioning without interruption.",
            "ObservedBehavior": "The InvalidEncryptionKeyException is not handled properly during pipeline recovery, causing downstream applications to abort their operations.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-2310.json",
        "creation_time": "2011-09-05T15:31:00.000+0000",
        "bug_report": {
            "BugID": "HDFS-2310",
            "Title": "JournalProtocol Not Registered Causes IOException in IPC Server",
            "Description": "The logs indicate an error related to the JournalProtocol not being registered with the server, leading to an IOException when attempting to start a log segment.",
            "StackTrace": [
                "2011-09-05 10:44:36,811 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 60758, call: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3), rpc version=2, client version=1, methodsFingerPrint=-852377201 from 127.0.0.1:60760, error:",
                "java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol",
                "\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)",
                "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:396)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "2011-09-05 10:44:36,813 ERROR namenode.FSEditLog (FSEditLog.java:mapJournalsAndReportErrors(1048)) - Error starting log segment 3 (journal JournalAndStream(mgr=org.apache.hadoop.hdfs.server.namenode.BackupJournalManager@2876b359, stream=null))",
                "java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol",
                "\tat org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)",
                "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:396)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)"
            ],
            "StepsToReproduce": [
                "1. Start the IPC server.",
                "2. Attempt to start a log segment using the command: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3).",
                "3. Observe the logs for any errors."
            ],
            "ExpectedBehavior": "The IPC server should successfully start the log segment without any errors related to the JournalProtocol.",
            "ObservedBehavior": "An IOException is thrown indicating that the JournalProtocol is unknown, preventing the log segment from starting.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-3385.json",
        "creation_time": "2012-05-08T12:45:31.000+0000",
        "bug_report": {
            "BugID": "HDFS-3385",
            "Title": "ClassCastException when appending a file in HDFS",
            "Description": "When attempting to append a file in HDFS, a ClassCastException occurs, indicating a type mismatch in the block management system.",
            "StackTrace": [
                "2012-05-08 18:13:40,506 WARN  util.KerberosName (KerberosName.java:<clinit>(87)) - Kerberos krb5 configuration not found, setting default realm to empty",
                "Exception in thread \"main\" java.lang.ClassCastException: org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo cannot be cast to org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1787)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1584)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:1824)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:425)",
                "...",
                "at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1150)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1189)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1177)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:221)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:981)",
                "at org.apache.hadoop.hdfs.server.datanode.DeleteMe.main(DeleteMe.java:26)"
            ],
            "StepsToReproduce": [
                "1. Start the HDFS service.",
                "2. Attempt to append a file using the HDFS API.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The file should be appended successfully without any exceptions.",
            "ObservedBehavior": "A ClassCastException is thrown, preventing the file from being appended.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-4006.json",
        "creation_time": "2012-10-04T22:05:14.000+0000",
        "bug_report": {
            "BugID": "HDFS-4006",
            "Title": "NPE in TestCheckpoint#testSecondaryHasVeryOutOfDateImage Causes Unexpected Exit",
            "Description": "The test TestCheckpoint#testSecondaryHasVeryOutOfDateImage occasionally fails due to an unexpected exit caused by a NullPointerException (NPE) during the checkpointing process. This issue arises when the background checkpoint conflicts with explicit checkpoints performed by the tests.",
            "StackTrace": [
                "2012-09-16 01:55:05,901 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit",
                "org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:480)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:331)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$2.run(SecondaryNameNode.java:298)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:452)",
                "at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:294)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Run the test suite that includes TestCheckpoint#testSecondaryHasVeryOutOfDateImage.",
                "Observe the behavior of the checkpointing process during the test execution."
            ],
            "ExpectedBehavior": "The test should complete successfully without any unexpected exits or exceptions.",
            "ObservedBehavior": "The test occasionally fails with an unexpected exit due to a NullPointerException during the checkpointing process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-6715.json",
        "creation_time": "2014-07-21T21:26:25.000+0000",
        "bug_report": {
            "BugID": "HDFS-6715",
            "Title": "WebHDFS Fails to Handle Namenode Startup Mode Exception",
            "Description": "During high availability testing, running a MapReduce job with the WebHDFS file system occasionally results in a failure due to the Namenode being in startup mode. This leads to a failure in committing the job.",
            "StackTrace": [
                "2014-04-17 05:08:06,346 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1397710493213_0001_r_000008_0: Container killed by the ApplicationMaster.",
                "Container killed on request. Exit code is 143",
                "Container exited with a non-zero exit code 143",
                "2014-04-17 05:08:10,205 ERROR [CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job",
                "java.io.IOException: Namenode is in startup mode",
                "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with high availability enabled.",
                "2. Start the Namenode in startup mode.",
                "3. Submit a MapReduce job that uses the WebHDFS file system.",
                "4. Monitor the job execution and check for errors."
            ],
            "ExpectedBehavior": "The MapReduce job should complete successfully without errors, even if the Namenode is in startup mode.",
            "ObservedBehavior": "The job fails with an IOException indicating that the Namenode is in startup mode, preventing the job from committing.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-2392.json",
        "creation_time": "2011-09-30T20:29:09.000+0000",
        "bug_report": {
            "BugID": "HDFS-2392",
            "Title": "DistCp Fails with IOException When Using HFTP",
            "Description": "The DistCp command fails when attempting to copy files from an HFTP source to a destination, resulting in multiple IOException errors.",
            "StackTrace": [
                "java.io.IOException: Copied: 0 Skipped: 0 Failed: 1",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Copy failed: java.io.IOException: Job failed!",
                "at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1257)",
                "at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)",
                "at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)",
                "at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)"
            ],
            "StepsToReproduce": [
                "Run the following command: hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3",
                "Monitor the job status in the Hadoop job tracker."
            ],
            "ExpectedBehavior": "The files should be copied successfully from the HFTP source to the specified HDFS destination.",
            "ObservedBehavior": "The job fails with multiple IOException errors indicating that no files were copied.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-11472.json",
        "creation_time": "2017-02-28T18:39:13.000+0000",
        "bug_report": {
            "BugID": "HDFS-11472",
            "Title": "Inconsistent Replica Size After Data Pipeline Failure",
            "Description": "A case was observed where a replica's on-disk length is less than the acknowledged length, breaking the assumption in the recovery code. This issue can lead to data integrity problems during recovery processes.",
            "StackTrace": [
                "2017-01-08 01:41:03,532 WARN org.apache.hadoop.hdfs.server.protocol.InterDatanodeProtocol: Failed to obtain replica info for block (=BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394519586) from datanode (=DatanodeInfoWithStorage[10.204.138.17:1004,null,null])",
                "java.io.IOException: THIS IS NOT SUPPOSED TO HAPPEN: getBytesOnDisk() < getVisibleLength(), rip=ReplicaBeingWritten, blk_2526438952_1101394519586, RBW",
                "  getNumBytes()     = 27530",
                "  getBytesOnDisk()  = 27006",
                "  getVisibleLength()= 27268",
                "  getVolume()       = /data/6/hdfs/datanode/current",
                "  getBlockFile()    = /data/6/hdfs/datanode/current/BP-947993742-10.204.0.136-1362248978912/current/rbw/blk_2526438952",
                "  bytesAcked=27268",
                "  bytesOnDisk=27006",
                "        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2284)",
                "        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery(FsDatasetImpl.java:2260)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.initReplicaRecovery(DataNode.java:2566)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.callInitReplicaRecovery(DataNode.java:2577)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2645)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$400(DataNode.java:245)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataNode$5.run(DataNode.java:2551)",
                "        at java.lang.Thread.run(Thread.java:745)",
                "2017-01-08 01:40:59,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-947993742-10.204.0.136-1362248978912:blk_2526438952_1101394499067",
                "java.nio.channels.ClosedByInterruptException",
                "        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)",
                "        at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:269)",
                "        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.adjustCrcChannelPosition(FsDatasetImpl.java:1484)",
                "        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.adjustCrcFilePosition(BlockReceiver.java:994)",
                "        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:670)",
                "        at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:857)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:797)",
                "        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:169)",
                "        at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:106)",
                "        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:244)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Trigger a data pipeline failure in the Hadoop HDFS environment.",
                "Attempt to recover the affected block.",
                "Observe the on-disk length of the replica compared to the acknowledged length."
            ],
            "ExpectedBehavior": "The on-disk length of the replica should be equal to or greater than the acknowledged length during recovery.",
            "ObservedBehavior": "The on-disk length of the replica is less than the acknowledged length, leading to potential data integrity issues.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-10760.json",
        "creation_time": "2016-08-12T08:29:24.000+0000",
        "bug_report": {
            "BugID": "HDFS-10760",
            "Title": "DataXceiver#run() Should Not Log InvalidToken Exception as Error",
            "Description": "The DataXceiver#run() method currently logs the InvalidToken exception as an error when a client has an expired token and refetches a new token. This behavior is misleading as it is not a server error. The InvalidToken exception should be caught and only the warning logged by DataXceiver#checkAccess() should be retained in the DataNode log.",
            "StackTrace": [
                "org.apache.hadoop.security.token.SecretManager$InvalidToken: Block token with block_token_identifier (expiryDate=1470850746803, keyId=-2093956963, userId=hbase, blockPoolId=BP-641703426-10.17.1.2-1468517918886, blockId=1077120201, access modes=[READ]) is expired.",
                "at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:280)",
                "at org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.checkAccess(BlockTokenSecretManager.java:301)",
                "at org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.checkAccess(BlockPoolTokenSecretManager.java:97)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1236)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:481)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:242)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a DataNode with an expired token.",
                "2. Attempt to read a block using the DataXceiver.",
                "3. Observe the logs for the InvalidToken exception."
            ],
            "ExpectedBehavior": "The InvalidToken exception should be logged as a warning, not as an error.",
            "ObservedBehavior": "The InvalidToken exception is logged as an error, which is misleading.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-13635.json",
        "creation_time": "2018-05-29T19:54:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-13635",
            "Title": "Incorrect Error Message When Block is Not Found",
            "Description": "When a client opens a file, it requests the DataNode to check the blocks' visible length. If the block is not present on the DataNode, it incorrectly throws a 'Cannot append to a non-existent replica' message. The expected behavior is to simply state 'block is not found'. This issue has been observed in CDH5.13 and appears to exist in the Apache Hadoop trunk as well.",
            "StackTrace": [
                "2018-05-29 09:23:41,966 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 50020, call org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol.getReplicaVisibleLength from 10.0.0.14:53217 Call#38334117 Retry#0",
                "org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-725378529-10.236.236.8-1410027444173:13276792346",
                " at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:792)",
                " at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaVisibleLength(FsDatasetImpl.java:2588)",
                " at org.apache.hadoop.hdfs.server.datanode.DataNode.getReplicaVisibleLength(DataNode.java:2756)",
                " at org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolServerSideTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolServerSideTranslatorPB.java:107)",
                " at org.apache.hadoop.hdfs.protocol.proto.ClientDatanodeProtocolProtos$ClientDatanodeProtocolService$2.callBlockingMethod(ClientDatanodeProtocolProtos.java:17873)",
                " at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                " at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                " at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)",
                " at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)",
                " at java.security.AccessController.doPrivileged(Native Method)",
                " at javax.security.auth.Subject.doAs(Subject.java:422)",
                " at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                " at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2211)"
            ],
            "StepsToReproduce": [
                "Open a file using the client.",
                "Observe the request sent to the DataNode to check the blocks' visible length.",
                "Ensure that the block is not present on the DataNode."
            ],
            "ExpectedBehavior": "The system should return a message stating 'block is not found'.",
            "ObservedBehavior": "The system throws an error message: 'Cannot append to a non-existent replica'.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-11608.json",
        "creation_time": "2017-03-31T17:37:20.000+0000",
        "bug_report": {
            "BugID": "HDFS-11608",
            "Title": "HDFS Write Crashes with Block Size Greater than 2 GB",
            "Description": "When attempting to write a file larger than 2 GB using a block size greater than 2 GB (e.g., 3 GB), the HDFS client throws an OutOfMemoryException, and the DataNode returns an IOException. This issue leads to a failure in the DFSOutputStream ResponseProcessor, resulting in a broken pipe and pipeline recovery errors.",
            "StackTrace": [
                "2017-03-30 16:34:33,828 ERROR datanode.DataNode (DataXceiver.java:run(278)) - c6401.ambari.apache.org:50010:DataXceiver error processing WRITE_BLOCK operation src: /192.168.64.101:47167 dst: /192.168.64.101:50010",
                "java.io.IOException: Incorrect value for packet payload size: 2147483128",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:159)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:502)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:898)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:806)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:251)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up an HDFS environment.",
                "2. Attempt to write a file larger than 2 GB with a block size greater than 2 GB (e.g., 3 GB).",
                "3. Observe the error messages in the logs."
            ],
            "ExpectedBehavior": "The HDFS client should successfully write the file without throwing an OutOfMemoryException or IOException.",
            "ObservedBehavior": "The HDFS client throws an OutOfMemoryException, and the DataNode returns an IOException, leading to a broken pipe and pipeline recovery errors.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-12638.json",
        "creation_time": "2017-10-11T11:29:04.000+0000",
        "bug_report": {
            "BugID": "HDFS-12638",
            "Title": "NullPointerException in ReplicationMonitor due to null BlockCollection",
            "Description": "The Active NameNode exits unexpectedly due to a NullPointerException (NPE) when the ReplicationMonitor thread attempts to process replication work. The BlockCollection passed in when creating ReplicationWork is null, leading to this error. The issue may be related to changes made in HDFS-9754, which removed checks for null BlockCollection.",
            "StackTrace": [
                "2017-10-11 16:29:06,161 ERROR [ReplicationMonitor] org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.ReplicationWork.chooseTargets(ReplicationWork.java:55)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1532)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1491)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3792)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3744)",
                "    at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS cluster with an active NameNode.",
                "2. Trigger a replication event that involves a BlockCollection.",
                "3. Monitor the logs for any NullPointerException related to ReplicationMonitor."
            ],
            "ExpectedBehavior": "The ReplicationMonitor should handle replication work without encountering a NullPointerException, ensuring the NameNode remains active.",
            "ObservedBehavior": "The NameNode exits unexpectedly due to a NullPointerException when the ReplicationMonitor attempts to process replication work with a null BlockCollection.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-12383.json",
        "creation_time": "2017-08-31T20:43:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-12383",
            "Title": "Re-encryption Updater Exits Due to Cancellation Exception",
            "Description": "An instance was observed where the re-encryption updater exited unexpectedly due to a CancellationException, leading to subsequent tasks not executing. The logs indicate that the updater should be improved to handle canceled tasks more effectively.",
            "StackTrace": [
                "java.util.concurrent.CancellationException",
                "        at java.util.concurrent.FutureTask.report(FutureTask.java:121)",
                "        at java.util.concurrent.FutureTask.get(FutureTask.java:192)",
                "        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks(ReencryptionUpdater.java:404)",
                "        at org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run(ReencryptionUpdater.java:250)",
                "        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Submit a re-encryption task for a zone.",
                "2. Cancel the re-encryption task before it completes.",
                "3. Observe the behavior of the re-encryption updater."
            ],
            "ExpectedBehavior": "The re-encryption updater should handle canceled tasks gracefully without exiting unexpectedly.",
            "ObservedBehavior": "The re-encryption updater exits with a CancellationException, causing subsequent tasks to not execute.",
            "Resolution": "The issue has been fixed in the codebase to improve handling of canceled tasks."
        }
    },
    {
        "filename": "HDFS-5322.json",
        "creation_time": "2013-10-07T22:52:54.000+0000",
        "bug_report": {
            "BugID": "HDFS-5322",
            "Title": "HDFS Delegation Token Not Found in Cache Errors on Secure HA Clusters",
            "Description": "While running High Availability (HA) tests, we encountered errors indicating that the HDFS delegation token could not be found in the cache, leading to job failures.",
            "StackTrace": [
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)",
                "2013-10-06 20:14:51,193 INFO  [main] mapreduce.Job: Task Id : attempt_1381090351344_0001_m_000007_0, Status : FAILED",
                "Error: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 11 for hrt_qa) can't be found in cache",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1300)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)"
            ],
            "StepsToReproduce": [
                "1. Set up a secure HA cluster.",
                "2. Run HA tests that require HDFS delegation tokens.",
                "3. Observe the logs for errors related to delegation tokens."
            ],
            "ExpectedBehavior": "The HDFS delegation tokens should be found in the cache, allowing jobs to run successfully without errors.",
            "ObservedBehavior": "Jobs fail with errors indicating that the HDFS delegation token cannot be found in the cache.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-11741.json",
        "creation_time": "2017-05-02T17:57:25.000+0000",
        "bug_report": {
            "BugID": "HDFS-11741",
            "Title": "Long Running Balancer Fails Due to Expired DataEncryptionKey",
            "Description": "A long-running balancer may fail despite using a keytab because the KeyManager returns an expired DataEncryptionKey, leading to an InvalidEncryptionKeyException. This issue is similar to HDFS-10609, where the balancer's KeyManager does not update the DataEncryptionKey in sync with the NameNode.",
            "StackTrace": [
                "2017-04-30 05:03:58,661 WARN  [pool-1464-thread-10] balancer.Dispatcher (Dispatcher.java:dispatch(325)) - Failed to move blk_1067352712_3913241 with size=546650 from 10.0.0.134:50010:DISK to 10.0.0.98:50010:DISK through 10.0.0.134:50010",
                "org.apache.hadoop.hdfs.protocol.datatransfer.InvalidEncryptionKeyException: Can't re-compute encryption key for nonce, since the required block key (keyID=1005215027) doesn't exist. Current key: 1005215030",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil.readSaslMessageAndNegotiatedCipherOption(DataTransferSaslUtil.java:417)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.doSaslHandshake(SaslDataTransferClient.java:474)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.getEncryptedStreams(SaslDataTransferClient.java:299)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.send(SaslDataTransferClient.java:242)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:211)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.socketSend(SaslDataTransferClient.java:183)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:311)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$2300(Dispatcher.java:182)",
                "at org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:899)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Set up a cluster with Kerberos authentication and Data transfer encryption enabled.",
                "2. Start a long-running balancer using a keytab.",
                "3. Wait for 20-30 hours to observe the failure."
            ],
            "ExpectedBehavior": "The balancer should continue to operate without failure, successfully moving blocks as needed.",
            "ObservedBehavior": "The balancer fails with an InvalidEncryptionKeyException after 20-30 hours of operation due to an expired DataEncryptionKey.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-3936.json",
        "creation_time": "2012-09-14T05:35:12.000+0000",
        "bug_report": {
            "BugID": "HDFS-3936",
            "Title": "MiniDFSCluster Shutdown Races with BlocksMap Usage",
            "Description": "The issue arises when the MiniDFSCluster shuts down, causing a race condition with the BlocksMap usage. This results in a NullPointerException during the shutdown process, leading to unexpected exits in tests.",
            "StackTrace": [
                "2012-09-13 18:54:12,526 FATAL hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1355)) - Test resulted in an unexpected exit",
                "org.apache.hadoop.util.ExitUtil$ExitException: Fatal exception with message null",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.getBlockCollection(BlocksMap.java:101)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1132)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1107)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3061)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3023)",
                "\tat java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start the MiniDFSCluster.",
                "2. Trigger a shutdown of the MiniDFSCluster while replication tasks are ongoing.",
                "3. Observe the logs for any fatal exceptions or unexpected exits."
            ],
            "ExpectedBehavior": "The MiniDFSCluster should shut down gracefully without causing any exceptions or unexpected exits.",
            "ObservedBehavior": "The MiniDFSCluster experiences a NullPointerException during shutdown, leading to an unexpected exit.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-6348.json",
        "creation_time": "2014-05-07T12:34:54.000+0000",
        "bug_report": {
            "BugID": "HDFS-6348",
            "Title": "SecondaryNameNode Fails to Terminate on RuntimeException During Startup",
            "Description": "The SecondaryNameNode process remains alive even after a RuntimeException occurs during startup due to incorrect configuration. This is caused by a non-daemon RMI thread that prevents the JVM from exiting. A thread dump is attached for further analysis.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1900)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.getInstance(BlockPlacementPolicy.java:199)",
                "\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:256)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:635)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:260)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.<init>(SecondaryNameNode.java:205)",
                "\tat org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main(SecondaryNameNode.java:695)",
                "Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1868)",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1892)",
                "\t... 6 more",
                "Caused by: java.lang.ClassNotFoundException: Class com.huawei.hadoop.hdfs.server.blockmanagement.MyBlockPlacementPolicy not found",
                "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1774)",
                "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1866)",
                "\t... 7 more",
                "2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state",
                "2014-05-07 14:27:04,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state",
                "2014-05-07 14:31:04,926 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG:"
            ],
            "StepsToReproduce": [
                "Configure the SecondaryNameNode with an incorrect configuration that leads to a validation failure.",
                "Start the SecondaryNameNode process.",
                "Observe the process state after the RuntimeException is thrown."
            ],
            "ExpectedBehavior": "The SecondaryNameNode should terminate upon encountering a RuntimeException during startup.",
            "ObservedBehavior": "The SecondaryNameNode process remains alive, and the JVM does not exit due to a non-daemon RMI thread.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-7884.json",
        "creation_time": "2015-03-04T15:47:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-7884",
            "Title": "NullPointerException in BlockSender during Data Transfer",
            "Description": "A NullPointerException occurs in the BlockSender class when attempting to read a block from the datanode. This issue may impact data transfer operations, leading to potential data loss or service interruptions.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.<init>(BlockSender.java:264)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:506)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opReadBlock(Receiver.java:116)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:71)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:249)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop HDFS cluster.",
                "2. Attempt to read a block from the datanode using a client.",
                "3. Monitor the logs for any exceptions."
            ],
            "ExpectedBehavior": "The block should be read successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating a failure in the BlockSender initialization.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-7996.json",
        "creation_time": "2015-03-26T21:19:55.000+0000",
        "bug_report": {
            "BugID": "HDFS-7996",
            "Title": "BlockReceiver Throws ReplicaNotFoundException After Volume Removal",
            "Description": "When removing a disk from an actively writing DataNode, the BlockReceiver working on the disk throws a ReplicaNotFoundException because the replicas are removed from memory. This occurs due to the timing of volume removal and block finalization processes.",
            "StackTrace": [
                "org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException: Cannot append to a non-existent replica BP-51301509-10.20.202.114-1427296597742:blk_1073742979_2160",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.getReplicaInfo(FsDatasetImpl.java:615)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:1362)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1281)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1241)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a DataNode with an active disk.",
                "2. Begin writing data to the DataNode.",
                "3. Remove the disk from the DataNode while it is actively writing.",
                "4. Observe the logs for any exceptions thrown by BlockReceiver."
            ],
            "ExpectedBehavior": "The BlockReceiver should handle the removal of the disk gracefully without throwing a ReplicaNotFoundException.",
            "ObservedBehavior": "The BlockReceiver throws a ReplicaNotFoundException when attempting to finalize a block after the disk has been removed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-4302.json",
        "creation_time": "2012-12-11T19:44:59.000+0000",
        "bug_report": {
            "BugID": "HDFS-4302",
            "Title": "Fatal Exception During NameNode Startup Due to Early Precondition Check in EditLogFileInputStream",
            "Description": "When bringing up a namenode in standby mode with DEBUG enabled, the namenode encounters a fatal exception due to a precondition check in the EditLogFileInputStream's length() method being evaluated too early. This occurs before the advertisedSize is initialized, leading to a shutdown of the namenode.",
            "StackTrace": [
                "2012-12-11 10:45:33,464 FATAL namenode.NameNode (NameNode.java:main(1224)) - Exception in namenode join",
                "java.lang.IllegalStateException: must get input stream before length is available",
                "    at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "    at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream$URLLog.length(EditLogFileInputStream.java:405)",
                "    at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.length(EditLogFileInputStream.java:258)",
                "    at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "    at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.length(RedundantEditLogInputStream.java:256)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:125)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:88)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:697)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:604)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:447)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:409)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:400)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:434)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:606)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:591)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1153)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1219)",
                "2012-12-11 10:45:33,470 INFO  util.ExitUtil (ExitUtil.java:terminate(84)) - Exiting with status 1",
                "2012-12-11 10:45:33,471 INFO  namenode.NameNode (StringUtils.java:run(620)) - SHUTDOWN_MSG:",
                "/************************************************************",
                "SHUTDOWN_MSG: Shutting down NameNode at Eugenes-MacBook-Pro.local/172.16.175.1",
                "************************************************************/"
            ],
            "StepsToReproduce": [
                "1. Start the namenode in standby mode.",
                "2. Ensure DEBUG logging is enabled for the namenode.",
                "3. Monitor the logs for any exceptions during startup."
            ],
            "ExpectedBehavior": "The namenode should start successfully without any fatal exceptions.",
            "ObservedBehavior": "The namenode fails to start and logs a fatal exception due to an early precondition check in the EditLogFileInputStream's length() method.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-11849.json",
        "creation_time": "2017-05-18T11:31:48.000+0000",
        "bug_report": {
            "BugID": "HDFS-11849",
            "Title": "JournalNode Startup Failure Not Logged in Log File",
            "Description": "The JournalNode fails to start due to a Kerberos login issue, but the exception is not recorded in the log file. This leads to difficulties in diagnosing the problem.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.IOException: Login failure for xxx/yyyy@ZZZZ.COM from keytab dummy.keytab: javax.security.auth.login.LoginException: host1",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:994)",
                "at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:281)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:153)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.run(JournalNode.java:132)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)",
                "at org.apache.hadoop.hdfs.qjournal.server.JournalNode.main(JournalNode.java:318)"
            ],
            "StepsToReproduce": [
                "1. Attempt to start the JournalNode service.",
                "2. Ensure that the Kerberos configuration is set up with a valid keytab.",
                "3. Observe the startup process and check the log files."
            ],
            "ExpectedBehavior": "The JournalNode should start successfully and log any exceptions encountered during the startup process.",
            "ObservedBehavior": "The JournalNode fails to start due to a Kerberos login issue, and the exception is not logged in the log file.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-4841.json",
        "creation_time": "2013-05-22T17:01:36.000+0000",
        "bug_report": {
            "BugID": "HDFS-4841",
            "Title": "Warning on ShutdownHook 'ClientFinalizer' when using FsShell with secure WebHDFS",
            "Description": "When executing FsShell commands using the webhdfs:// URI with security enabled, a warning is generated indicating that the ShutdownHook 'ClientFinalizer' has failed. This issue does not occur when security is disabled.",
            "StackTrace": [
                "java.lang.IllegalStateException: Shutdown in progress, cannot add a shutdownHook",
                "\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:152)",
                "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2400)",
                "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2372)",
                "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:352)",
                "\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.getWebHdfs(WebHdfsFileSystem.java:1001)",
                "\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem$DtRenewer.cancel(WebHdfsFileSystem.java:1013)",
                "\tat org.apache.hadoop.security.token.Token.cancel(Token.java:382)",
                "\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.cancel(DelegationTokenRenewer.java:152)",
                "\tat org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction.access$200(DelegationTokenRenewer.java:58)",
                "\tat org.apache.hadoop.fs.DelegationTokenRenewer.removeRenewAction(DelegationTokenRenewer.java:241)",
                "\tat org.apache.hadoop.hdfs.web.WebHdfsFileSystem.close(WebHdfsFileSystem.java:822)",
                "\tat org.apache.hadoop.fs.FileSystem$Cache.closeAll(FileSystem.java:2446)",
                "\tat org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer.run(FileSystem.java:2463)",
                "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "StepsToReproduce": [
                "Enable security in the Hadoop configuration.",
                "Run the command: `hadoop fs -ls webhdfs://hdfs-upgrade-pseudo.ent.cloudera.com:50070/`",
                "Observe the warning message regarding the ShutdownHook 'ClientFinalizer'."
            ],
            "ExpectedBehavior": "The command should execute without any warnings related to ShutdownHook failures.",
            "ObservedBehavior": "The command executes successfully but generates a warning: 'ShutdownHook 'ClientFinalizer' failed'.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-3384.json",
        "creation_time": "2012-05-08T09:01:10.000+0000",
        "bug_report": {
            "BugID": "HDFS-3384",
            "Title": "DataStreamer Thread Fails to Close on Pipeline Setup Failure",
            "Description": "When attempting to append to a file after manually corrupting a block, the DataStreamer thread does not close immediately upon failure to set up a pipeline for append or recovery. This leads to multiple exceptions being thrown, including EOFException and NullPointerException.",
            "StackTrace": [
                "2012-04-19 09:33:10,776 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1059)) - Exception in createBlockOutputStream",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "    at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1039)",
                "    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:939)",
                "    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:510)",
                "2012-04-19 09:33:10,807 WARN  hdfs.DFSClient (DFSOutputStream.java:hflush(1511)) - Error while syncing",
                "java.io.IOException: All datanodes 10.18.40.20:50010 are bad. Aborting...",
                "    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:908)",
                "    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)"
            ],
            "StepsToReproduce": [
                "1. Write a file to HDFS.",
                "2. Manually corrupt a block of the file.",
                "3. Attempt to append to the corrupted file."
            ],
            "ExpectedBehavior": "The DataStreamer thread should close immediately upon failure to set up a pipeline for append or recovery, preventing further exceptions.",
            "ObservedBehavior": "The DataStreamer thread continues to run and throws multiple exceptions, including EOFException and NullPointerException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-5657.json",
        "creation_time": "2013-12-11T21:59:46.000+0000",
        "bug_report": {
            "BugID": "HDFS-5657",
            "Title": "Race Condition Causes Writeback State Error in NFS Gateway",
            "Description": "A race condition between the NFS gateway writeback executor thread and the new write handler thread can lead to a writeback state check failure. This issue manifests as an IllegalStateException indicating that the openFileCtx has a false async status.",
            "StackTrace": [
                "2013-11-26 10:34:07,871 ERROR nfs3.AsyncDataService (AsyncDataService.java:run(136)) - Asyn data service got error:java.lang.IllegalStateException: The openFileCtx has false async status",
                "        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)",
                "        at org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack(OpenFileCtx.java:890)",
                "        at org.apache.hadoop.hdfs.nfs.nfs3.AsyncDataService$WriteBackTask.run(AsyncDataService.java:134)",
                "        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "        at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Set up an NFS gateway with multiple concurrent write operations.",
                "2. Trigger the writeback executor thread while simultaneously invoking the new write handler thread.",
                "3. Monitor the logs for any IllegalStateException related to the openFileCtx."
            ],
            "ExpectedBehavior": "The NFS gateway should handle concurrent write operations without throwing an IllegalStateException.",
            "ObservedBehavior": "An IllegalStateException is thrown indicating that the openFileCtx has a false async status, leading to a writeback state check failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-11827.json",
        "creation_time": "2017-05-16T06:28:49.000+0000",
        "bug_report": {
            "BugID": "HDFS-11827",
            "Title": "NullPointerException in BlockPlacementPolicyDefault when changing log level",
            "Description": "A NullPointerException (NPE) is thrown when changing the log level of BlockPlacementPolicy using the 'hadoop daemonlog' command. This issue was identified by a colleague during testing. The root cause appears to be a missing null check in the chooseRandom() method of the BlockPlacementPolicyDefault class.",
            "StackTrace": [
                "2017-05-16 13:36:51,053 ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: ReplicationMonitor thread received Runtime exception.",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:666)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRemoteRack(BlockPlacementPolicyDefault.java:607)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:375)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:446)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:240)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:119)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTargetInternal(BlockPlacementPolicyWithMultiDC.java:263)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyWithMultiDC.chooseTarget(BlockPlacementPolicyWithMultiDC.java:214)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.chooseTargets(BlockManager.java:3836)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationWork.access$200(BlockManager.java:3801)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1394)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1300)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3764)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3711)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Execute the command 'hadoop daemonlog -setlevel <daemon> DEBUG' to change the log level of BlockPlacementPolicy.",
                "2. Monitor the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The log level should change without any exceptions being thrown.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the ReplicationMonitor thread to fail.",
            "Resolution": "A fix has been implemented to add null checks in the chooseRandom() method of BlockPlacementPolicyDefault."
        }
    },
    {
        "filename": "HDFS-6804.json",
        "creation_time": "2014-08-01T05:37:23.000+0000",
        "bug_report": {
            "BugID": "HDFS-6804",
            "Title": "Unexpected Checksum Mismatch Exception During Block Transfer",
            "Description": "A race condition between transferring a block and appending a block leads to an 'Unexpected checksum mismatch exception'. The source datanode logs indicate that the block is transmitted successfully, but the destination datanode reports a checksum mismatch, causing the NameNode to incorrectly mark the replica on the source datanode as corrupt.",
            "StackTrace": [
                "java.io.IOException: Terminating due to a checksum error.",
                "java.io.IOException: Unexpected checksum mismatch while writing BP-2072804351-192.168.2.104-1406008383435:blk_1073741997_9248 from /192.168.2.101:39495",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:536)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:703)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:575)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:115)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:68)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start a block transfer from the source datanode.",
                "2. Simultaneously initiate an append operation on the same block.",
                "3. Monitor the logs of both the source and destination datanodes."
            ],
            "ExpectedBehavior": "The block should be transferred without any checksum errors, and the replica on the source datanode should remain valid.",
            "ObservedBehavior": "The destination datanode reports a checksum mismatch, leading to the source datanode's replica being marked as corrupt by the NameNode.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-5843.json",
        "creation_time": "2014-01-28T06:05:16.000+0000",
        "bug_report": {
            "BugID": "HDFS-5843",
            "Title": "IOException Thrown by DFSClient.getFileChecksum() When Checksum is Disabled",
            "Description": "When a file is created with checksum disabled (using ChecksumOpt.disabled()), calling FileSystem.getFileChecksum() results in an IOException. Additionally, the datanode logs indicate an ArithmeticException due to division by zero during the BLOCK_CHECKSUM operation.",
            "StackTrace": [
                "java.io.IOException: Fail to get block MD5 for BP-341493254-192.168.1.10-1390888724459:blk_1073741825_1001",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1965)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileChecksum(DFSClient.java:1771)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1186)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:1)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1194)",
                "2014-01-27 21:58:46,329 ERROR datanode.DataNode (DataXceiver.java:run(225)) - 127.0.0.1:52398:DataXceiver error processing BLOCK_CHECKSUM operation src: /127.0.0.1:52407 dest: /127.0.0.1:52398",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum(DataXceiver.java:658)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opBlockChecksum(Receiver.java:169)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:77)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:221)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "Create a file with checksum disabled using ChecksumOpt.disabled()",
                "Call FileSystem.getFileChecksum() on the created file"
            ],
            "ExpectedBehavior": "The system should return the file checksum without throwing an IOException.",
            "ObservedBehavior": "An IOException is thrown indicating failure to get block MD5, and an ArithmeticException is logged in the datanode.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-8070.json",
        "creation_time": "2015-04-06T19:57:58.000+0000",
        "bug_report": {
            "BugID": "HDFS-8070",
            "Title": "ShortCircuitShm Locking Issue in HDFS During Multi-threaded Split Generation",
            "Description": "The HDFS ShortCircuitShm layer locks the task during multi-threaded split-generation, potentially due to compatibility issues between different versions of DataNode and Client. This issue was encountered immediately after upgrading the data, raising concerns about the ShortCircuitShim wire protocol when a 2.8.0 DataNode communicates with a 2.7.0 Client.",
            "StackTrace": [
                "2015-04-06 00:04:30,781 ERROR [ShortCircuitCache_SlotReleaser] shortcircuit.ShortCircuitCache: ShortCircuitCache(0x29e82045): failed to release short-circuit shared memory slot Slot(slotIdx=2, shm=DfsClientShm(a86ee34576d93c4964005d90b0d97c38)) by sending ReleaseShortCircuitAccessRequestProto to /grid/0/cluster/hdfs/dn_socket.  Closing shared memory segment.",
                "java.io.IOException: ERROR_INVALID: there is no shared memory segment registered with shmId a86ee34576d93c4964005d90b0d97c38",
                "at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache$SlotReleaser.run(ShortCircuitCache.java:208)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Upgrade the HDFS DataNode to version 2.8.0.",
                "Run a multi-threaded split-generation task using a 2.7.0 Client.",
                "Monitor the logs for any errors related to ShortCircuitShm."
            ],
            "ExpectedBehavior": "The DataNode should successfully release short-circuit shared memory slots without errors during multi-threaded operations.",
            "ObservedBehavior": "The DataNode fails to release short-circuit shared memory slots, resulting in errors and potential task locking.",
            "Resolution": "Investigating further, as the exact exception from the DataNode call is not logged. [Provide additional details on resolution status]"
        }
    },
    {
        "filename": "HDFS-1085.json",
        "creation_time": "2010-04-06T22:49:40.000+0000",
        "bug_report": {
            "BugID": "HDFS-1085",
            "Title": "HFTP Read Fails Silently with File Size Mismatch",
            "Description": "When performing a massive distcp through hftp, many tasks fail due to a file size mismatch. The read operation does not fail, but the resulting file is smaller than expected.",
            "StackTrace": [
                "2010-04-06 17:56:43,005 INFO org.apache.hadoop.tools.DistCp: FAIL 2010/0/part-00032 : java.io.IOException: File size not matched: copied 193855488 bytes (184.9m) to tmpfile (=hdfs://omehost.com:8020/somepath/part-00032)",
                "but expected 1710327403 bytes (1.6g) from hftp://someotherhost/somepath/part-00032",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.copy(DistCp.java:435)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:543)",
                "at org.apache.hadoop.tools.DistCp$CopyFilesMapper.map(DistCp.java:310)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)",
                "at org.apache.hadoop.mapred.Child.main(Child.main:159)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with HFTP enabled.",
                "2. Execute a distcp command to copy a large file using HFTP.",
                "3. Monitor the logs for any file size mismatch errors."
            ],
            "ExpectedBehavior": "The file should be copied successfully with the correct size matching the source file.",
            "ObservedBehavior": "The file is copied but the size does not match the expected size, leading to a silent failure.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-12339.json",
        "creation_time": "2017-08-23T03:16:49.000+0000",
        "bug_report": {
            "BugID": "HDFS-12339",
            "Title": "NFS Gateway Unregistration Failure on Shutdown",
            "Description": "When stopping the NFS Gateway, an error is thrown in the NFS gateway role logs indicating an unregistration failure with the rpcbind portmapper. This issue prevents proper unregistration of the NFS service.",
            "StackTrace": [
                "2017-08-17 18:09:16,529 ERROR org.apache.hadoop.oncrpc.RpcProgram: Unregistration failure with localhost:2049, portmap entry: (PortmapMapping-100003:3:6:2049)",
                "2017-08-17 18:09:16,531 WARN org.apache.hadoop.util.ShutdownHookManager: ShutdownHook 'NfsShutdownHook' failed, java.lang.RuntimeException: Unregistration failure",
                "java.lang.RuntimeException: Unregistration failure",
                "Caused by: java.net.SocketException: Socket is closed",
                "at java.net.DatagramSocket.send(DatagramSocket.java:641)",
                "at org.apache.hadoop.oncrpc.SimpleUdpClient.run(SimpleUdpClient.java:62)"
            ],
            "StepsToReproduce": [
                "1. Start the NFS Gateway service.",
                "2. Attempt to stop the NFS Gateway service.",
                "3. Check the NFS gateway role logs for errors."
            ],
            "ExpectedBehavior": "The NFS Gateway should unregister successfully with the rpcbind portmapper without throwing any errors.",
            "ObservedBehavior": "An unregistration failure error is logged, indicating that the NFS Gateway did not unregister properly.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-6520.json",
        "creation_time": "2014-06-12T02:32:29.000+0000",
        "bug_report": {
            "BugID": "HDFS-6520",
            "Title": "Error during 'fsck -move' due to invalid length value in BlockReader",
            "Description": "When running the 'fsck -move' command on a corrupted file in HDFS, an IOException is thrown indicating an expected empty end-of-read packet. This occurs after the filesystem is identified as corrupt.",
            "StackTrace": [
                "java.io.IOException: Expected empty end-of-read packet! Header: PacketHeader with packetLen=66048 header data: offsetInBlock: 65536",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readTrailingEmptyPacket(RemoteBlockReader2.java:259)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.readNextPacket(RemoteBlockReader2.java:220)",
                "at org.apache.hadoop.hdfs.RemoteBlockReader2.read(RemoteBlockReader2.java:138)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock(NamenodeFsck.java:649)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlocksToLostFound(NamenodeFsck.java:543)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:460)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.check(NamenodeFsck.java:324)",
                "at org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.fsck(NamenodeFsck.java:233)",
                "at org.apache.hadoop.hdfs.server.namenode.FsckServlet$1.run(FsckServlet.java:67)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.hdfs.server.namenode.FsckServlet.doGet(FsckServlet.java:58)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1192)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(HttpServer2.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "Set up a pseudo cluster.",
                "Copy a file to HDFS.",
                "Corrupt a block of the file.",
                "Run 'fsck' to check the file system.",
                "Run 'fsck -move' to move the corrupted file to /lost+found."
            ],
            "ExpectedBehavior": "The corrupted file should be moved to /lost+found without errors.",
            "ObservedBehavior": "An IOException is thrown indicating an expected empty end-of-read packet, preventing the move operation.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-10715.json",
        "creation_time": "2016-08-02T08:10:23.000+0000",
        "bug_report": {
            "BugID": "HDFS-10715",
            "Title": "NullPointerException in AvailableSpaceBlockPlacementPolicy",
            "Description": "The introduction of the AvailableSpaceBlockPlacementPolicy in HDFS-8131 has led to a NullPointerException (NPE) in certain scenarios. The issue arises when the method `chooseDataNode` attempts to compare a potentially null value, which results in an NPE. This bug needs to be addressed to prevent the application from crashing.",
            "StackTrace": [
                "2016-08-02 13:05:03,271 WARN org.apache.hadoop.ipc.Server: IPC Server handler 13 on 8020, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 10.132.89.79:14001 Call#56 Retry#0",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.compareDataNode(AvailableSpaceBlockPlacementPolicy.java:95)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode(AvailableSpaceBlockPlacementPolicy.java:80)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:691)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom(BlockPlacementPolicyDefault.java:665)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseLocalRack(BlockPlacementPolicyDefault.java:572)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTargetInOrder(BlockPlacementPolicyDefault.java:457)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:367)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:242)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:114)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseTarget(BlockPlacementPolicyDefault.java:130)",
                "    at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1606)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3315)",
                "    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:679)",
                "    at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)",
                "    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:489)",
                "    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)",
                "    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)"
            ],
            "StepsToReproduce": [
                "1. Deploy the Hadoop HDFS version 2.7.4 or later.",
                "2. Configure the AvailableSpaceBlockPlacementPolicy.",
                "3. Attempt to add a block to the HDFS while the cluster is in a state that causes the `chooseDataNode` method to return null."
            ],
            "ExpectedBehavior": "The block should be added successfully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the `chooseDataNode` method attempts to compare a null value.",
            "Resolution": "The issue has been fixed in the subsequent release. Ensure to update to the latest version."
        }
    },
    {
        "filename": "HDFS-3332.json",
        "creation_time": "2012-04-27T04:42:38.000+0000",
        "bug_report": {
            "BugID": "HDFS-3332",
            "Title": "NullPointerException in DataNode when DirectoryScanner reports bad blocks",
            "Description": "A NullPointerException occurs in the DataNode when the DirectoryScanner attempts to report bad blocks. This issue arises when there is a corrupted block in the system. The logs indicate that the DirectoryScanner is unable to handle the corrupted block properly, leading to the exception.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.protocol.DatanodeID.<init>(DatanodeID.java:66)",
                "at org.apache.hadoop.hdfs.protocol.DatanodeInfo.<init>(DatanodeInfo.java:87)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.reportBadBlocks(BPServiceActor.java:238)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.reportBadBlocks(BPOfferService.java:187)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:559)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate(FsDatasetImpl.java:1377)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.reconcile(DirectoryScanner.java:318)",
                "at org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.run(DirectoryScanner.java:284)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)",
                "at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Start the NameNode (NN) and DataNode (DN) with HA configuration.",
                "2. Corrupt one block in the HDFS.",
                "3. Trigger the DirectoryScanner to report bad blocks."
            ],
            "ExpectedBehavior": "The DirectoryScanner should handle the corrupted block gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the DirectoryScanner when it attempts to report bad blocks.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-6130.json",
        "creation_time": "2014-03-20T07:15:47.000+0000",
        "bug_report": {
            "BugID": "HDFS-6130",
            "Title": "NullPointerException when initializing shared edits during namenode upgrade with HA enabled",
            "Description": "When upgrading an old cluster (0.20.2-cdh3u1) to a trunk instance with High Availability (HA) enabled, a NullPointerException (NPE) occurs when executing the command 'hdfs namenode -initializeSharedEdits'. The upgrade is successful without HA configuration.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.isReservedName(FSDirectory.java:2984)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.addToParent(FSImageFormatPBINode.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode$Loader.loadINodeDirectorySection(FSImageFormatPBINode.java:162)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.loadInternal(FSImageFormatProtobuf.java:243)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf$Loader.load(FSImageFormatProtobuf.java:168)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$LoaderDelegator.load(FSImageFormat$LoaderDelegator.java:120)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:895)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:881)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:704)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:642)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:271)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:894)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:653)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initializeSharedEdits(NameNode.java:912)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1276)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1360)"
            ],
            "StepsToReproduce": [
                "1. Upgrade an old Hadoop cluster (version 0.20.2-cdh3u1) to the trunk instance.",
                "2. Configure High Availability (HA) settings.",
                "3. Run the command 'hdfs namenode -initializeSharedEdits'."
            ],
            "ExpectedBehavior": "The namenode should initialize shared edits without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the initialization to fail.",
            "Resolution": "Fixed in version 2.4.0"
        }
    },
    {
        "filename": "HDFS-2827.json",
        "creation_time": "2012-01-23T09:34:51.000+0000",
        "bug_report": {
            "BugID": "HDFS-2827",
            "Title": "Checkpointing Fails After Renaming Directory with Open Lease",
            "Description": "When executing a series of file system operations, checkpointing fails with an IOException indicating that a path was found but no matching entry exists in the namespace. This occurs after renaming a directory that contains a file with an open lease.",
            "StackTrace": [
                "2012-01-23 15:03:14,204 ERROR namenode.FSImage (FSImage.java:run(795)) - Unable to save image for E:\\HDFS-1623\\hadoop-hdfs-project\\hadoop-hdfs\\build\\test\\data\\dfs\\name3",
                "java.io.IOException: saveLeases found path /test1/est/abc.txt but no matching entry in namespace.[/test1/est/abc.txt]",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveFilesUnderConstruction(FSNamesystem.java:4336)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Saver.save(FSImageFormat.java:588)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImage(FSImage.java:761)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage$FSImageSaver.run(FSImage$FSImageSaver.java:789)",
                "at java.lang.Thread.run(Unknown Source)"
            ],
            "StepsToReproduce": [
                "1. Execute the following operations in the Hadoop file system:",
                "   - fs.mkdirs(new Path('/test1'));",
                "   - FSDataOutputStream create = fs.create(new Path('/test/abc.txt')); // do not close the stream",
                "   - fs.rename(new Path('/test/'), new Path('/test1/'));",
                "2. Wait for the checkpoint to complete."
            ],
            "ExpectedBehavior": "The checkpoint should complete successfully without any exceptions.",
            "ObservedBehavior": "Checkpointing fails with an IOException indicating a missing entry in the namespace for the renamed path.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-11056.json",
        "creation_time": "2016-10-25T23:00:33.000+0000",
        "bug_report": {
            "BugID": "HDFS-11056",
            "Title": "Checksum Error During Concurrent Append and Read Operations",
            "Description": "When two clients are simultaneously performing operations on the same file\u2014one continuously appending and the other continuously reading\u2014the reader eventually encounters a checksum error. This issue has been observed with httpfs clients, and it is likely to affect other append clients as well. A unit test demonstrating the checksum error will be attached later.",
            "StackTrace": [
                "Exception in thread \"Thread-144\" java.lang.RuntimeException: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C",
                "at org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:309)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.fs.ChecksumException: Checksum CRC32C not matched for file /tmp/bar.txt at position 0: expected=C893FEDE but computed=69322F90, algorithm=PureJavaCrc32C",
                "at org.apache.hadoop.util.DataChecksum.throwChecksumException(DataChecksum.java:407)",
                "at org.apache.hadoop.util.DataChecksum.verifyChunked(DataChecksum.java:351)",
                "at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:311)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:216)",
                "at org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.read(BlockReaderRemote.java:144)",
                "at org.apache.hadoop.hdfs.ByteArrayStrategy.readFromBlock(ReaderStrategy.java:119)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:704)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:765)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:814)",
                "at java.io.DataInputStream.read(DataInputStream.java:149)",
                "at org.apache.hadoop.fs.http.client.BaseTestHttpFSWith$1ReaderRunnable.run(BaseTestHttpFSWith.java:302)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start two clients.",
                "2. On the first client, continuously open, append, and close a file (e.g., /tmp/bar.txt).",
                "3. On the second client, continuously open, read, and close the same file.",
                "4. Monitor the output for checksum errors."
            ],
            "ExpectedBehavior": "The reader should be able to read the file without encountering any checksum errors, regardless of concurrent append operations.",
            "ObservedBehavior": "The reader encounters a checksum error after a few minutes of operation, indicating a mismatch between the expected and computed checksum values.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-6825.json",
        "creation_time": "2014-08-06T01:31:34.000+0000",
        "bug_report": {
            "BugID": "HDFS-6825",
            "Title": "FileNotFoundException during block synchronization due to delayed block removal",
            "Description": "A FileNotFoundException is thrown when attempting to synchronize a block after a file has been deleted, leading to potential edit log corruption.",
            "StackTrace": [
                "2014-08-04 23:49:44,133 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: commitBlockSynchronization(lastblock=BP-.., newgenerationstamp=..., newlength=..., newtargets=..., closeFile=true, deleteBlock=false)",
                "2014-08-04 23:49:44,133 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Unexpected exception while updating disk space.",
                "java.io.FileNotFoundException: Path not found: /solr/hierarchy/core_node1/data/tlog/tlog.xyz",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateSpaceConsumed(FSDirectory.java:1807)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitOrCompleteLastBlock(FSNamesystem.java:3975)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.closeFileCommitBlocks(FSNamesystem.java:4178)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization(FSNamesystem.java:4146)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization(NameNodeRpcServer.java:662)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.commitBlockSynchronization(DatanodeProtocolServerSideTranslatorPB.java:270)",
                "at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28073)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1986)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1982)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1554)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1980)"
            ],
            "StepsToReproduce": [
                "1. Create a file at /solr/hierarchy/core_node1/data/tlog/tlog.xyz.",
                "2. Attempt to append to the file after the lease has expired.",
                "3. Delete the file while there are still pending blocks.",
                "4. Trigger the commitBlockSynchronization method."
            ],
            "ExpectedBehavior": "The system should handle the deletion of the file gracefully without throwing a FileNotFoundException during block synchronization.",
            "ObservedBehavior": "A FileNotFoundException is thrown, indicating that the path was not found, leading to potential corruption in the edit log.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-5710.json",
        "creation_time": "2014-01-01T04:06:03.000+0000",
        "bug_report": {
            "BugID": "HDFS-5710",
            "Title": "NullPointerException in FSDirectory#getFullPathName due to missing null check",
            "Description": "A NullPointerException occurs in the FSDirectory#getFullPathName method when the getRelativePathINodes() method returns null. The method does not check if the inodes are null before attempting to access them, leading to a runtime exception.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathName(FSDirectory.java:1871)",
                "at org.apache.hadoop.hdfs.server.namenode.INode.getFullPathName(INode.java:482)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.getName(INodeFile.java:316)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy.chooseTarget(BlockPlacementPolicy.java:118)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWorkForBlocks(BlockManager.java:1259)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeReplicationWork(BlockManager.java:1167)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.computeDatanodeWork(BlockManager.java:3158)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$ReplicationMonitor.run(BlockManager.java:3112)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "Run the test suite for the Hadoop HDFS project.",
                "Observe the logs for any NullPointerException related to FSDirectory#getFullPathName."
            ],
            "ExpectedBehavior": "The FSDirectory#getFullPathName method should handle null inodes gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when getRelativePathINodes() returns null, causing the replication monitor thread to fail.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-3555.json",
        "creation_time": "2012-06-20T17:36:48.000+0000",
        "bug_report": {
            "BugID": "HDFS-3555",
            "Title": "SocketTimeoutException logged at ERROR level instead of INFO",
            "Description": "The Datanode service is logging a java.net.SocketTimeoutException at ERROR level, indicating that the datanode is unable to send data to the client because the client has stopped reading. This message should be logged at INFO level instead of ERROR.",
            "StackTrace": [
                "2012-06-18 17:47:13 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode DatanodeRegistration(x.x.x.x:50010, storageID=DS-196671195-10.10.120.67-50010-1334328338972, infoPort=50075, ipcPort=50020):DataXceiver",
                "java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/10.10.120.67:50010 remote=/10.10.120.67:59282]",
                "at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:246)",
                "at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)",
                "at org.apache.hadoop.net.SocketOutputStream.transferToFully(SocketOutputStream.java:198)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendChunks(BlockSender.java:397)",
                "at org.apache.hadoop.hdfs.server.datanode.BlockSender.sendBlock(BlockSender.java:493)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:267)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:163)"
            ],
            "StepsToReproduce": [
                "Start the Datanode service.",
                "Ensure a client is connected and then stop the client from reading data.",
                "Monitor the Datanode logs for any ERROR level messages."
            ],
            "ExpectedBehavior": "The Datanode should log the SocketTimeoutException at INFO level instead of ERROR level.",
            "ObservedBehavior": "The Datanode logs the SocketTimeoutException at ERROR level, which is misleading and may cause unnecessary concern.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-10962.json",
        "creation_time": "2016-10-05T04:31:45.000+0000",
        "bug_report": {
            "BugID": "HDFS-10962",
            "Title": "Flaky Test in TestRequestHedgingProxyProvider",
            "Description": "The test 'testHedgingWhenOneFails' in the 'TestRequestHedgingProxyProvider' class fails intermittently with a verification error indicating that a method was expected to be invoked but was not.",
            "StackTrace": [
                "org.mockito.exceptions.verification.WantedButNotInvoked: Wanted but not invoked: namenodeProtocols.getStats();",
                "-> at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)",
                "Actually, there were zero interactions with this mock.",
                "at org.apache.hadoop.hdfs.server.namenode.ha.TestRequestHedgingProxyProvider.testHedgingWhenOneFails(TestRequestHedgingProxyProvider.java:78)"
            ],
            "StepsToReproduce": [
                "Run the test suite for the Hadoop HDFS project.",
                "Observe the test 'testHedgingWhenOneFails' in the 'TestRequestHedgingProxyProvider' class."
            ],
            "ExpectedBehavior": "The test should pass without any verification errors, indicating that the method 'namenodeProtocols.getStats()' was invoked as expected.",
            "ObservedBehavior": "The test fails with a verification error stating that 'namenodeProtocols.getStats()' was not invoked, despite the expectation.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-12363.json",
        "creation_time": "2017-08-28T05:34:21.000+0000",
        "bug_report": {
            "BugID": "HDFS-12363",
            "Title": "NullPointerException in BlockManager$StorageInfoDefragmenter during Storage Scanning",
            "Description": "The NameNode encountered a NullPointerException (NPE) while executing the scanAndCompactStorages method in BlockManager$StorageInfoDefragmenter. This issue leads to the NameNode going down unexpectedly.",
            "StackTrace": [
                "ERROR org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Thread received Runtime exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages(BlockManager.java:3897)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.run(BlockManager.java:3852)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-08-21 22:14:05,303 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1",
                "2017-08-21 22:14:05,313 INFO org.apache.hadoop.hdfs.server.namenode.NameNode:"
            ],
            "StepsToReproduce": [
                "1. Start the NameNode service.",
                "2. Trigger a storage scanning operation.",
                "3. Monitor the logs for any exceptions."
            ],
            "ExpectedBehavior": "The NameNode should complete the storage scanning operation without encountering any exceptions.",
            "ObservedBehavior": "The NameNode crashes with a NullPointerException, leading to service downtime.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-7916.json",
        "creation_time": "2015-03-11T10:40:00.000+0000",
        "bug_report": {
            "BugID": "HDFS-7916",
            "Title": "Infinite Loop in Reporting Bad Blocks to Standby Node",
            "Description": "When a bad block is detected, the BPSA for the Standby Node enters an infinite loop while attempting to report it. This issue can lead to performance degradation and potential system instability.",
            "StackTrace": [
                "2015-03-11 19:43:41,528 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode: stobdtserver3/10.224.54.70:18010",
                "org.apache.hadoop.hdfs.server.datanode.BPServiceActorActionException: Failed to report bad block BP-1384821822-10.224.54.68-1422634566395:blk_1079544278_5812006 to namenode:",
                "at org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo(ReportBadBlockAction.java:63)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.processQueueMessages(BPServiceActor.java:1020)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:762)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:856)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Deploy a Hadoop HDFS cluster with a Standby Node.",
                "2. Introduce a bad block in the data node.",
                "3. Monitor the reporting process from the data node to the Standby Node."
            ],
            "ExpectedBehavior": "The data node should successfully report the bad block to the Standby Node without entering an infinite loop.",
            "ObservedBehavior": "The data node enters an infinite loop while trying to report the bad block, leading to repeated warnings in the logs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-9549.json",
        "creation_time": "2015-12-11T16:25:08.000+0000",
        "bug_report": {
            "BugID": "HDFS-9549",
            "Title": "Flaky Test: TestCacheDirectives#testExceedsCapacity Fails in Jenkins",
            "Description": "The test 'TestCacheDirectives.testExceedsCapacity' fails intermittently in Jenkins environments (trunk, trunk-Java8). This issue is characterized by an assertion error indicating that the pending cached list is not empty when it should be.",
            "StackTrace": [
                "java.lang.AssertionError: Pending cached list of 127.0.0.1:54134 is not empty, [{blockId=1073741841, replication=1, mark=true}]",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.junit.Assert.assertTrue(Assert.java:41)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.checkPendingCachedEmpty(TestCacheDirectives.java:1479)",
                "\tat org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testExceedsCapacity(TestCacheDirectives.java:1502)"
            ],
            "StepsToReproduce": [
                "1. Set up a Jenkins environment with the trunk or trunk-Java8 branch.",
                "2. Run the test suite that includes 'TestCacheDirectives.testExceedsCapacity'.",
                "3. Observe the test results for intermittent failures."
            ],
            "ExpectedBehavior": "The test 'TestCacheDirectives.testExceedsCapacity' should pass without any assertion errors, indicating that the pending cached list is empty.",
            "ObservedBehavior": "The test fails with an assertion error stating that the pending cached list is not empty.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-11164.json",
        "creation_time": "2016-11-22T05:58:03.000+0000",
        "bug_report": {
            "BugID": "HDFS-11164",
            "Title": "Mover Should Avoid Unnecessary Retries for Pinned Blocks",
            "Description": "When the mover attempts to move a pinned block to another datanode, it encounters an IOException, marking the block movement as failure. The mover continues to retry moving the block based on the configured maximum attempts, which is unnecessary if the failure is due to block pinning. This issue aims to prevent retry attempts for pinned blocks, as they cannot be moved to a different node.",
            "StackTrace": [
                "2016-11-22 10:56:10,537 WARN org.apache.hadoop.hdfs.server.balancer.Dispatcher: Failed to move blk_1073741825_1001 with size=52 from 127.0.0.1:19501:DISK to 127.0.0.1:19758:ARCHIVE through 127.0.0.1:19501",
                "java.io.IOException: Got error, status=ERROR, status message opReplaceBlock BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 received exception java.io.IOException: Got error, status=ERROR, status message Not able to copy block 1073741825 to /127.0.0.1:19826 because it's pinned , copy block BP-1772076264-10.252.146.200-1479792322960:blk_1073741825_1001 from /127.0.0.1:19501, reportedBlock move is failed",
                "\tat org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:118)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.receiveResponse(Dispatcher.java:417)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.dispatch(Dispatcher.java:358)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$PendingMove.access$5(Dispatcher.java:322)",
                "\tat org.apache.hadoop.hdfs.server.balancer.Dispatcher$1.run(Dispatcher.java:1075)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Pin a block on a datanode.",
                "2. Attempt to move the pinned block to another datanode using the mover.",
                "3. Observe the logs for IOException related to the pinned block."
            ],
            "ExpectedBehavior": "The mover should not attempt to retry moving a pinned block and should log a message indicating that the block cannot be moved due to pinning.",
            "ObservedBehavior": "The mover continues to retry moving the pinned block, resulting in repeated IOException logs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-5291.json",
        "creation_time": "2013-10-02T23:52:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-5291",
            "Title": "Active NN Enters SafeMode, Causing Client Timeouts",
            "Description": "In our test, we observed that the NameNode (NN) immediately entered SafeMode after transitioning to the active state. This behavior can lead to HBase region servers timing out and subsequently killing themselves. We should implement a mechanism that allows clients to retry when High Availability (HA) is enabled and the Active NameNode is in SafeMode.",
            "StackTrace": [
                "java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:334)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:279)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:296)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:356)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1463)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:454)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:296)"
            ],
            "StepsToReproduce": [
                "1. Start the NameNode in standby mode.",
                "2. Transition the NameNode to active state.",
                "3. Observe the logs for the transition and subsequent behavior."
            ],
            "ExpectedBehavior": "Clients should be able to retry operations when the Active NameNode is in SafeMode.",
            "ObservedBehavior": "The Active NameNode enters SafeMode immediately after transitioning from standby, causing client operations to fail and leading to timeouts.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-12836.json",
        "creation_time": "2017-11-17T20:26:32.000+0000",
        "bug_report": {
            "BugID": "HDFS-12836",
            "Title": "startTxId can exceed endTxId during in-progress edit log tailing",
            "Description": "When the configuration {{dfs.ha.tail-edits.in-progress}} is set to true, the edit log tailer attempts to tail in-progress edit log segments. However, there is a potential issue in the code where {{remoteLog.getStartTxId()}} can be greater than {{endTxId}}, leading to errors during log replay.",
            "StackTrace": [
                "2017-11-17 19:55:41,165 ERROR org.apache.hadoop.hdfs.server.namenode.FSImage: Error replaying edit log at offset 1048576.  Expected transaction ID was 87",
                "Recent opcode offsets: 1048576",
                "org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:158)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:882)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:863)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:293)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:427)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$400(EditLogTailer.java:380)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:397)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:481)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:393)",
                "Caused by: org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream$PrematureEOFException: got premature end-of-file at txid 86; expected file to go up to 85",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:197)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:189)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:205)",
                "... 9 more"
            ],
            "StepsToReproduce": [
                "Set the configuration property dfs.ha.tail-edits.in-progress to true.",
                "Trigger the edit log tailing process.",
                "Ensure that the start transaction ID is greater than the end transaction ID."
            ],
            "ExpectedBehavior": "The edit log tailer should successfully tail the in-progress edit log segments without errors.",
            "ObservedBehavior": "An error occurs indicating that the expected transaction ID does not match the actual transaction ID, leading to a premature end-of-file exception.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-8113.json",
        "creation_time": "2015-04-09T11:29:31.000+0000",
        "bug_report": {
            "BugID": "HDFS-8113",
            "Title": "NullPointerException in BlockInfoContiguous Constructor",
            "Description": "The copy constructor for BlockInfoContiguous can throw a NullPointerException if the 'bc' field is null. This issue has been observed to cause DataNodes to fail during block reports with the NameNode.",
            "StackTrace": [
                "2015-03-08 19:28:13,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: RemoteException in offerService",
                "org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo.(BlockInfo.java:80)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockToMarkCorrupt.(BlockManager.java:1696)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt(BlockManager.java:2185)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReportedBlock(BlockManager.java:2047)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.reportDiff(BlockManager.java:1950)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1823)",
                "at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.processReport(BlockManager.java:1750)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport(NameNodeRpcServer.java:1069)",
                "at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.blockReport(DatanodeProtocolServerSideTranslatorPB.java:152)",
                "at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:26382)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1623)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the 'bc' field in BlockInfoContiguous is null.",
                "2. Attempt to create an instance of BlockInfoContiguous using the copy constructor.",
                "3. Observe the resulting NullPointerException."
            ],
            "ExpectedBehavior": "The BlockInfoContiguous constructor should handle null 'bc' gracefully without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown when the 'bc' field is null, causing DataNodes to fail during block reports.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-10512.json",
        "creation_time": "2016-06-09T14:34:04.000+0000",
        "bug_report": {
            "BugID": "HDFS-10512",
            "Title": "VolumeScanner Termination Due to NullPointerException in DataNode.reportBadBlocks",
            "Description": "The VolumeScanner may terminate unexpectedly due to a NullPointerException thrown in the DataNode.reportBadBlocks() method. This issue has been observed in a production CDH 5.5.1 cluster and persists in the upstream trunk.",
            "StackTrace": [
                "2016-04-07 20:30:53,830 WARN org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Reporting bad BP-1800173197-10.204.68.5-1444425156296:blk_1170134484_96468685 on /dfs/dn",
                "2016-04-07 20:30:53,831 ERROR org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting because of exception",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.hdfs.server.datanode.DataNode.reportBadBlocks(DataNode.java:1018)",
                "    at org.apache.hadoop.hdfs.server.datanode.VolumeScanner$ScanResultHandler.handle(VolumeScanner.java:287)",
                "    at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.scanBlock(VolumeScanner.java:443)",
                "    at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.runLoop(VolumeScanner.java:547)",
                "    at org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:621)",
                "2016-04-07 20:30:53,832 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/dfs/dn, DS-89b72832-2a8c-48f3-8235-48e6c5eb5ab3) exiting."
            ],
            "StepsToReproduce": [
                "1. Start the VolumeScanner on a DataNode.",
                "2. Ensure that there are bad blocks to report.",
                "3. Monitor the logs for any NullPointerException related to DataNode.reportBadBlocks() method."
            ],
            "ExpectedBehavior": "The VolumeScanner should report bad blocks without terminating unexpectedly.",
            "ObservedBehavior": "The VolumeScanner terminates with a NullPointerException when attempting to report bad blocks.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-10729.json",
        "creation_time": "2016-08-08T02:36:05.000+0000",
        "bug_report": {
            "BugID": "12995526",
            "Title": "Standby NameNode Crashes Due to Null Pointer Exception When Loading Edits",
            "Description": "We encountered a bug where the Standby NameNode crashes due to a Null Pointer Exception (NPE) when loading edits. This issue arises when the maximum number of items per directory is exceeded, leading to an NPE during the edit log loading process.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFileEncryptionInfo(FSDirectory.java:2914)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.createFileStatus(FSDirectory.java:2469)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:375)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:230)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:139)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:829)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:810)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.doTailEdits(EditLogTailer.java:232)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:331)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:360)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1651)",
                "at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:410)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)"
            ],
            "StepsToReproduce": [
                "1. Start the Standby NameNode.",
                "2. Load a directory with more than 1,048,576 items.",
                "3. Observe the logs for any exceptions."
            ],
            "ExpectedBehavior": "The Standby NameNode should load edits without crashing.",
            "ObservedBehavior": "The Standby NameNode crashes with a Null Pointer Exception when attempting to load edits due to exceeding the maximum directory item limit.",
            "Resolution": "A possible workaround is to increase the value of dfs.namenode.fs-limits.max-directory-items to 6,400,000. However, further testing is required to ensure there are no side effects."
        }
    },
    {
        "filename": "HDFS-13040.json",
        "creation_time": "2018-01-19T19:47:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-13040",
            "Title": "Kerberized iNotify Client Fails Despite Valid Kerberos Credentials",
            "Description": "The iNotify client fails to operate correctly even when valid Kerberos credentials are present. This issue arises after the NameNodes have been running longer than the Kerberos ticket lifetime, leading to a failure in reading edit logs. The problem is exacerbated by the fact that the NameNode cannot re-login on behalf of the client due to principal mismatches.",
            "StackTrace": [
                "18/01/19 11:23:02 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs@GCE.CLOUDERA.COM (auth:KERBEROS) cause:org.apache.hadoop.ipc.RemoteException(java.io.IOException): We encountered an error reading https://nn2.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3, https://nn1.example.com:8481/getJournal?jid=ns1&segmentTxId=8662&storageInfo=-60%3A353531113%3A0%3Acluster3. During automatic edit log failover, we noticed that all of the remaining edit log streams are shorter than the current one! The best remaining edit log ends at transaction 8683, but we thought we could read up to transaction 8684. If you continue, metadata will be lost forever!",
                "at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:213)",
                "at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp(NameNodeRpcServer.java:1701)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid(NameNodeRpcServer.java:1763)",
                "at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getEditsFromTxid(AuthorizationProviderProxyClientProtocol.java:1011)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getEditsFromTxid(ClientNamenodeProtocolServerSideTranslatorPB.java:1490)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)"
            ],
            "StepsToReproduce": [
                "1. Set up a Kerberized HA cluster with two NameNodes.",
                "2. Ensure the iNotify client is configured to use valid Kerberos credentials.",
                "3. Start the NameNodes and let them run for a duration longer than the Kerberos ticket lifetime.",
                "4. Attempt to perform an operation using the iNotify client."
            ],
            "ExpectedBehavior": "The iNotify client should successfully read edit logs and perform operations without errors.",
            "ObservedBehavior": "The iNotify client fails with a RemoteException indicating issues with reading edit logs due to mismatched principals and expired Kerberos tickets.",
            "Resolution": "A patch has been created to address this issue by implementing proxy user functionality, allowing the NameNode to retrieve edits on behalf of the client."
        }
    },
    {
        "filename": "HDFS-3374.json",
        "creation_time": "2012-05-04T22:08:19.000+0000",
        "bug_report": {
            "BugID": "HDFS-3374",
            "Title": "Intermittent Failure in TestDelegationToken Due to Race Condition",
            "Description": "The testcase is failing because the MiniDFSCluster is shutdown before the secret manager can change the key, which calls system.exit with no edit streams available.",
            "StackTrace": [
                "2012-05-04 15:03:51,521 WARN  common.Storage (FSImage.java:updateRemovedDirs(224)) - Removing storage dir /home/horton/src/hadoop/build/test/data/dfs/name1",
                "2012-05-04 15:03:51,522 FATAL namenode.FSNamesystem (FSEditLog.java:fatalExit(388)) - No edit streams are accessible",
                "java.lang.Exception: No edit streams are accessible",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.fatalExit(FSEditLog.java:388)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.exitIfNoStreams(FSEditLog.java:407)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsAndStorageDir(FSEditLog.java:432)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.removeEditsStreamsAndStorageDirs(FSEditLog.java:468)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:1028)",
                "    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logUpdateMasterKey(FSNamesystem.java:5641)",
                "    at org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.logUpdateMasterKey(DelegationTokenSecretManager.java:286)",
                "    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:150)",
                "    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.rollMasterKey(AbstractDelegationTokenSecretManager.java:174)",
                "    at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover.run(AbstractDelegationTokenSecretManager.java:385)",
                "    at java.lang.Thread.run(Thread.java:662)",
                "Running org.apache.hadoop.hdfs.security.TestDelegationToken",
                "Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0 sec",
                "Test org.apache.hadoop.hdfs.security.TestDelegationToken FAILED (crashed)"
            ],
            "StepsToReproduce": [
                "Run the TestDelegationToken test case in the Hadoop HDFS module.",
                "Ensure that the MiniDFSCluster is initialized and running.",
                "Observe the behavior when the secret manager attempts to change the key."
            ],
            "ExpectedBehavior": "The test should complete successfully without crashing, and the secret manager should be able to change the key without issues.",
            "ObservedBehavior": "The test fails intermittently with a crash due to the MiniDFSCluster shutting down before the key change is completed, resulting in a fatal exit with no edit streams available.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-2359.json",
        "creation_time": "2011-09-23T00:20:53.000+0000",
        "bug_report": {
            "BugID": "HDFS-2359",
            "Title": "NullPointerException in Datanode Log During Disk Failure in HDFS Operation",
            "Description": "During the execution of a distcp operation in an HDFS cluster, a NullPointerException is logged in the Datanode when three disks are intentionally failed by changing their permissions to 000. This occurs despite the distcp job completing successfully.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)",
                "at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "StepsToReproduce": [
                "1. Set up a cluster of 4 Datanodes, each with 12 disks.",
                "2. Configure hdfs-site.xml with 'dfs.datanode.failed.volumes.tolerated=3'.",
                "3. In one terminal, execute the command: $hadoop distcp /user/$HADOOPQA_USER/data1 /user/$HADOOPQA_USER/data3.",
                "4. In another terminal, change the permissions of three disks on one Datanode to 000 using: $ chmod 000 /xyz/{0,1,2}/hadoop/var/hdfs/data.",
                "5. Monitor the Datanode logs for any exceptions."
            ],
            "ExpectedBehavior": "The distcp job should complete successfully without any exceptions in the Datanode logs.",
            "ObservedBehavior": "The distcp job completes successfully, but a NullPointerException is logged in the Datanode logs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-10986.json",
        "creation_time": "2016-10-08T00:50:36.000+0000",
        "bug_report": {
            "BugID": "HDFS-10986",
            "Title": "DFSAdmin Should Log Detailed Error Messages for IOException",
            "Description": "Certain subcommands in DFSAdmin swallow IOException and provide limited error messages to stderr, making it difficult for users to diagnose issues. This bug report highlights the need for improved error logging in these scenarios.",
            "StackTrace": [
                "java.net.ConnectException: Connection refused",
                "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "\tat org.apache.hadoop.hdfs.tools.DFSAdmin.run(DFSAdmin.java:2073)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "\tat org.apache.hadoop.hdfs.tools.DFSAdmin.main(DFSAdmin.java:2225)"
            ],
            "StepsToReproduce": [
                "Run the command: hdfs dfsadmin -getBalancerBandwidth 127.0.0.1:9866",
                "Observe the output: 'Datanode unreachable.'",
                "Run the command: hdfs dfsadmin -getDatanodeInfo localhost:9866",
                "Observe the output: 'Datanode unreachable.'",
                "Run the command: hdfs dfsadmin -evictWriters 127.0.0.1:9866",
                "Check the exit code using: echo $?"
            ],
            "ExpectedBehavior": "The command should provide a detailed error message indicating the reason for the failure, including the exception stack trace.",
            "ObservedBehavior": "The command fails with a generic message 'Datanode unreachable.' without providing any stack trace or detailed error information.",
            "Resolution": "The issue has been fixed by ensuring that detailed error messages are logged when exceptions occur."
        }
    },
    {
        "filename": "HDFS-6455.json",
        "creation_time": "2014-05-28T18:01:15.000+0000",
        "bug_report": {
            "BugID": "HDFS-6455",
            "Title": "NFS: Log Error for Invalid Separator in dfs.nfs.exports.allowed.hosts",
            "Description": "The error for an invalid separator in the dfs.nfs.exports.allowed.hosts property should be logged in the NFS log file instead of the nfs.out file. Currently, the NFS server fails to start without providing a clear error message in the appropriate log file.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.IllegalArgumentException: Incorrectly formatted line 'host1 ro:host2 rw'",
                "at org.apache.hadoop.nfs.NfsExports.getMatch(NfsExports.java:356)",
                "at org.apache.hadoop.nfs.NfsExports.<init>(NfsExports.java:151)",
                "at org.apache.hadoop.nfs.NfsExports.getInstance(NfsExports.java:54)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.<init>(RpcProgramNfs3.java:176)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.<init>(Nfs3.java:43)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.main(Nfs3.java:59)"
            ],
            "StepsToReproduce": [
                "1. Pass an invalid separator in dfs.nfs.exports.allowed.hosts.",
                "   Example: <property><name>dfs.nfs.exports.allowed.hosts</name><value>host1 ro:host2 rw</value></property>",
                "2. Restart the NFS server.",
                "3. Observe the console output and log files."
            ],
            "ExpectedBehavior": "The NFS server should log an error message in the NFS log file indicating the invalid separator in the dfs.nfs.exports.allowed.hosts property.",
            "ObservedBehavior": "The NFS server fails to start and prints an exception in the console, but does not log any error message in the NFS log file. The error is only found in the nfs.out file.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-2882.json",
        "creation_time": "2012-02-02T18:11:20.000+0000",
        "bug_report": {
            "BugID": "HDFS-2882",
            "Title": "DataNode Initialization Fails When Disk is Full",
            "Description": "When starting a DataNode (DN) on a machine that is completely out of space on one of its drives, the initialization fails for the block pool, leading to subsequent NullPointerExceptions (NPEs) during block reports. This issue was observed on the HDFS-1623 branch and may also affect the trunk.",
            "StackTrace": [
                "2012-02-02 09:56:50,499 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool BP-448349972-172.29.5.192-1323816762969 (storage id DS-507718931-172.29.5.194-11072-1297842002148) service to styx01.sf.cloudera.com/172.29.5.192:8021",
                "java.io.IOException: Mkdirs failed to create /data/1/scratch/todd/styx-datadir/current/BP-448349972-172.29.5.192-1323816762969/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.FSDataset$BlockPoolSlice.<init>(FSDataset.java:335)"
            ],
            "StepsToReproduce": [
                "1. Start a DataNode on a machine with no available disk space.",
                "2. Monitor the logs for initialization messages."
            ],
            "ExpectedBehavior": "The DataNode should not start if there is insufficient disk space for initialization.",
            "ObservedBehavior": "The DataNode starts but fails to initialize the block pool, resulting in fatal errors and NPEs during operation.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-5185.json",
        "creation_time": "2013-09-11T14:38:23.000+0000",
        "bug_report": {
            "BugID": "HDFS-5185",
            "Title": "DataNode Fails to Start Up When One Data Directory is Full",
            "Description": "The DataNode fails to start up if one of the configured data directories is out of space, resulting in a fatal error. The expected behavior is for the DataNode to continue starting up using the available data directories.",
            "StackTrace": [
                "2013-09-11 17:48:43,680 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for block pool Block pool <registering> (storage id DS-308316523-xx.xx.xx.xx-64015-1378896293604) service to /nn1:65110",
                "java.io.IOException: Mkdirs failed to create /opt/nish/data/current/BP-123456-1234567/tmp",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:105)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:216)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.addBlockPool(FsVolumeList.java:155)",
                "at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.addBlockPool(FsDatasetImpl.java:1593)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:834)",
                "at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:311)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:217)",
                "at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:660)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Configure a DataNode with multiple data directories.",
                "2. Fill one of the data directories to its capacity.",
                "3. Attempt to start the DataNode."
            ],
            "ExpectedBehavior": "The DataNode should continue to start up using the available data directories.",
            "ObservedBehavior": "The DataNode fails to start up and logs a fatal error indicating that it could not create a necessary directory due to insufficient space.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-13164.json",
        "creation_time": "2018-02-17T00:40:21.000+0000",
        "bug_report": {
            "BugID": "HDFS-13164",
            "Title": "File Not Closed When Streamer Fails with DSQuotaExceededException",
            "Description": "This issue occurs during YARN log aggregation but could theoretically happen to any client. When the directory's space quota is exceeded, the following sequence occurs when a file is created:\n\n1. The client sends a `startFile` RPC to the NameNode (NN) and receives a `DFSOutputStream`.\n2. Writing to the stream triggers the streamer to call `getAdditionalBlock` RPC to the NN, which results in a `DSQuotaExceededException`.\n3. The client then attempts to close the stream.\n\nThe problem arises because the stream may be left in an open-for-write status, potentially leaking the leaseRenewer. This happens due to the implementation of the close method, which checks if the stream is already closed and may skip necessary operations if the quota exception has occurred during the streaming process.",
            "StackTrace": [
                "2018-02-16 15:59:32,916 DEBUG org.apache.hadoop.hdfs.DFSClient: DataStreamer Quota Exception",
                "org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /DIR is exceeded: quota = 2000000 B = 1.91 MB but diskspace consumed = 404139552 B = 385.42 MB",
                "at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)",
                "at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2124)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1991)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1966)",
                "at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:463)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3896)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3484)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:686)",
                "at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:217)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:506)",
                "at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2226)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2222)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2220)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:423)",
                "at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)",
                "at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1833)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1626)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:788)"
            ],
            "StepsToReproduce": [
                "Set a directory quota in HDFS to a specific limit.",
                "Attempt to create a file that exceeds the quota limit.",
                "Observe the behavior when the file creation fails due to DSQuotaExceededException."
            ],
            "ExpectedBehavior": "The file should be closed properly, and no open-for-write status should remain in HDFS.",
            "ObservedBehavior": "The file remains in an open-for-write status, potentially leaking leaseRenewer.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-11508.json",
        "creation_time": "2017-03-07T04:55:50.000+0000",
        "bug_report": {
            "BugID": "HDFS-11508",
            "Title": "Bind Failure in SimpleTCPServer & Portmap Due to TIME_WAIT State",
            "Description": "The bind operation in SimpleTCPServer and Portmap fails because the socket is in the TIME_WAIT state. This issue can be resolved by setting the socket option to use setReuseAddress.",
            "StackTrace": [
                "2017-03-05 08:37:13,283 INFO  oncrpc.SimpleUdpServer (SimpleUdpServer.java:run(73)) - Started listening to UDP requests at port 4242 for Rpc program: mountd at localhost:4242 with workerCount 1",
                "2017-03-05 08:37:13,298 FATAL mount.MountdBase (MountdBase.java:startTCPServer(85)) - Failed to start the TCP server.",
                "org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:4242",
                "\tat org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272)",
                "\tat org.apache.hadoop.oncrpc.SimpleTcpServer.run(SimpleTcpServer.java:86)",
                "\tat org.apache.hadoop.mount.MountdBase.startTCPServer(MountdBase.java:83)",
                "\tat org.apache.hadoop.mount.MountdBase.start(MountdBase.java:98)",
                "\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal(Nfs3.java:56)",
                "\tat org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService(Nfs3.java:69)",
                "\tat org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.start(PrivilegedNfsGatewayStarter.java:71)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "\tat java.lang.reflect.Method.invoke(Method.java:498)",
                "\tat org.apache.commons.daemon.support.DaemonLoader.start(DaemonLoader.java:243)",
                "Caused by: java.net.BindException: Address already in use",
                "\tat sun.nio.ch.Net.bind0(Native Method)",
                "\tat sun.nio.ch.Net.bind(Net.java:433)",
                "\tat sun.nio.ch.Net.bind(Net.java:425)",
                "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)",
                "\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)",
                "\tat org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)",
                "\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)",
                "\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)",
                "\tat org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)",
                "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "\tat java.lang.Thread.run(Thread.java:745)",
                "2017-03-05 08:37:13,304 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1",
                "2017-03-05 08:37:13,308 INFO  nfs3.Nfs3Base (LogAdapter.java:info(45)) - SHUTDOWN_MSG: \n/************************************************************"
            ],
            "StepsToReproduce": [
                "Start the SimpleTCPServer on port 4242.",
                "Attempt to bind the server to the port while another instance is already using it."
            ],
            "ExpectedBehavior": "The server should bind successfully to the specified port without any errors.",
            "ObservedBehavior": "The server fails to bind to the port with a 'Address already in use' error.",
            "Resolution": "The issue has been fixed by implementing the setReuseAddress option for the socket."
        }
    },
    {
        "filename": "HDFS-2991.json",
        "creation_time": "2012-02-23T02:06:51.000+0000",
        "bug_report": {
            "BugID": "HDFS-2991",
            "Title": "ClassCastException during Edit Log Replay in HDFS",
            "Description": "During scale testing of the HDFS trunk at revision r1291606, an IOException occurred while replaying the edit log, leading to a ClassCastException. This issue prevents the system from loading edits correctly.",
            "StackTrace": [
                "java.io.IOException: Error replaying edit log at offset 1354251",
                "Recent opcode offsets: 1350014 1350176 1350312 1354251",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:418)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:79)",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hdfs.server.namenode.INodeFile cannot be cast to org.apache.hadoop.hdfs.server.namenode.INodeFileUnderConstruction",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:213)",
                "... 13 more"
            ],
            "StepsToReproduce": [
                "1. Set up a scale testing environment for HDFS.",
                "2. Run the trunk version at revision r1291606.",
                "3. Trigger the edit log replay process."
            ],
            "ExpectedBehavior": "The edit log should replay successfully without any exceptions.",
            "ObservedBehavior": "An IOException occurs, followed by a ClassCastException, preventing the edit log from being loaded.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-4404.json",
        "creation_time": "2013-01-15T01:43:17.000+0000",
        "bug_report": {
            "BugID": "HDFS-4404",
            "Title": "File Creation Failure When Primary NameNode is Down",
            "Description": "In a test environment with multiple NameNodes and DataNodes, a file creation attempt fails when the primary NameNode is down. The system throws a SocketTimeoutException indicating a timeout while trying to connect to the NameNode.",
            "StackTrace": [
                "java.net.SocketTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=/160.161.0.155:8020]",
                " at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)",
                " at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:524)",
                " at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:489)",
                " at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:474)",
                " at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:568)",
                " at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:217)",
                " at org.apache.hadoop.ipc.Client.getConnection(Client.java:1286)",
                " at org.apache.hadoop.ipc.Client.call(Client.java:1156)",
                " at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:184)",
                " at $Proxy9.create(Unknown Source)",
                " at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:187)",
                " at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                " at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                " at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                " at java.lang.reflect.Method.invoke(Method.java:597)",
                " at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:165)",
                " at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:84)",
                " at $Proxy10.create(Unknown Source)",
                " at org.apache.hadoop.hdfs.DFSOutputStream.<init>(DFSOutputStream.java:1261)",
                " at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1280)",
                " at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1128)",
                " at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1086)",
                " at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:232)",
                " at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:75)",
                " at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:806)",
                " at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)",
                " at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:715)",
                " at test.TestLease.main(TestLease.java:45)"
            ],
            "StepsToReproduce": [
                "Set up a test environment with two NameNodes (NN1, NN2) and three DataNodes (DN1, DN2, DN3).",
                "Shut down the first NameNode (NN1).",
                "Attempt to create a file in HDFS using the Hadoop client."
            ],
            "ExpectedBehavior": "The file should be created successfully using the secondary NameNode (NN2) when the primary NameNode (NN1) is down.",
            "ObservedBehavior": "The file creation fails with a SocketTimeoutException indicating a timeout while trying to connect to the downed NameNode (NN1).",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "HDFS-8276.json",
        "creation_time": "2015-04-28T10:11:35.000+0000",
        "bug_report": {
            "BugID": "HDFS-8276",
            "Title": "Namenode Startup Fails When Scrub Interval is Configured to Zero",
            "Description": "The namenode fails to start when the configuration parameter *dfs.namenode.lazypersist.file.scrub.interval.sec* is set to zero. This is contrary to the expected behavior where a zero value should disable the scrubber functionality.",
            "StackTrace": [
                "2015-04-27 23:47:31,744 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.",
                "java.lang.IllegalArgumentException: dfs.namenode.lazypersist.file.scrub.interval.sec must be non-zero.",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:828)"
            ],
            "StepsToReproduce": [
                "Set the configuration parameter dfs.namenode.lazypersist.file.scrub.interval.sec to 0.",
                "Start the namenode.",
                "Observe the startup process."
            ],
            "ExpectedBehavior": "The namenode should start successfully and the scrubber should be disabled.",
            "ObservedBehavior": "The namenode fails to start with an IllegalArgumentException indicating that the scrub interval must be non-zero.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-12369.json",
        "creation_time": "2017-08-28T22:56:02.000+0000",
        "bug_report": {
            "BugID": "HDFS-12369",
            "Title": "NameNode Fails to Start Due to FileNotFoundException on Unclosed File",
            "Description": "The NameNode fails to start with a FileNotFoundException due to an unclosed file that has snapshots. This issue arises from the hard lease recovery process when the lease reaches its limit, leading to potential edit log corruption.",
            "StackTrace": [
                "2017-08-17 14:32:18,418 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.",
                "java.io.FileNotFoundException: File does not exist: /home/Events/CancellationSurvey_MySQL/2015/12/31/.part-00000.9nlJ3M",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.applyEditLogOp(FSEditLogLoader.java:429)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:232)",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:141)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:897)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:750)",
                "at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:318)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1125)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:789)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:614)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:676)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:844)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:823)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1547)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1615)"
            ],
            "StepsToReproduce": [
                "1. Start the NameNode service.",
                "2. Ensure there is an unclosed file with snapshots in the specified directory.",
                "3. Observe the logs for any FileNotFoundException."
            ],
            "ExpectedBehavior": "The NameNode should start successfully without any exceptions.",
            "ObservedBehavior": "The NameNode fails to start and logs a FileNotFoundException indicating that a specific file does not exist.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-6462.json",
        "creation_time": "2014-05-29T00:19:03.000+0000",
        "bug_report": {
            "BugID": "HDFS-6462",
            "Title": "NFS fsstat Request Fails in Secure HDFS Environment",
            "Description": "The fsstat command fails in a secure HDFS environment with an input/output error due to Kerberos authentication issues.",
            "StackTrace": [
                "2014-05-29 00:09:13,698 DEBUG nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1654)) - NFS FSSTAT fileId: 16385",
                "2014-05-29 00:09:13,706 WARN  ipc.Client (Client.java:run(672)) - Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "2014-05-29 00:09:13,710 WARN  nfs3.RpcProgramNfs3 (RpcProgramNfs3.java:fsstat(1681)) - Exception",
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: \"host1/0.0.0.0\"; destination host is: \"host1\":8020;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)",
                "at com.sun.proxy.$Proxy14.getFsStats(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getStats(ClientNamenodeProtocolTranslatorPB.java:554)",
                "at org.apache.hadoop.hdfs.DFSClient.getDiskStatus(DFSClient.java:2165)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat(RpcProgramNfs3.java:1659)",
                "at org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.handleInternal(RpcProgramNfs3.java:1961)",
                "at org.apache.hadoop.oncrpc.RpcProgram.messageReceived(RpcProgram.java:162)",
                "at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)",
                "at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)",
                "at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:281)",
                "at org.apache.hadoop.oncrpc.RpcUtil$RpcMessageParserStage.messageReceived(RpcUtil.java:132)",
                "at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)",
                "at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)",
                "at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)",
                "at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)",
                "at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)",
                "at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)",
                "at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)",
                "at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)",
                "at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)",
                "at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)",
                "at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)",
                "at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1381)",
                "... 42 more"
            ],
            "StepsToReproduce": [
                "Create user named UserB and UserA.",
                "Create group named GroupB.",
                "Add root and UserB users to GroupB, ensuring UserA is not in GroupB.",
                "Set the following properties in hdfs-site.xml:",
                "  <property>",
                "    <name>dfs.nfs.keytab.file</name>",
                "    <value>/tmp/keytab/UserA.keytab</value>",
                "  </property>",
                "  <property>",
                "    <name>dfs.nfs.kerberos.principal</name>",
                "    <value>UserA@EXAMPLE.COM</value>",
                "  </property>",
                "Set the following properties in core-site.xml:",
                "  <property>",
                "    <name>hadoop.proxyuser.UserA.groups</name>",
                "    <value>GroupB</value>",
                "  </property>",
                "  <property>",
                "    <name>hadoop.proxyuser.UserA.hosts</name>",
                "    <value>*</value>",
                "  </property>",
                "Start the NFS server as UserA.",
                "Mount NFS as the root user.",
                "Run the command: df /tmp/tmp_mnt/"
            ],
            "ExpectedBehavior": "The df command should return the disk usage statistics without any errors.",
            "ObservedBehavior": "The df command returns an input/output error and no file systems are processed.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-5425.json",
        "creation_time": "2013-10-25T06:53:28.000+0000",
        "bug_report": {
            "BugID": "HDFS-5425",
            "Title": "NameNode Failure on Restart After Renaming Under-Construction Files with Snapshots",
            "Description": "When performing snapshot operations such as createSnapshot and renameSnapshot, the NameNode fails to restart after being shut down. This issue occurs specifically when there are under-construction files involved in the snapshot operations.",
            "StackTrace": [
                "2013-10-24 21:07:03,040 FATAL org.apache.hadoop.hdfs.server.namenode.NameNode: Exception in namenode join",
                "java.lang.IllegalStateException",
                "\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.replace(INodeDirectoryWithSnapshot.java:82)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$ChildrenDiff.access$700(INodeDirectoryWithSnapshot.java:62)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.replaceChild(INodeDirectoryWithSnapshot.java:397)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot$DirectoryDiffList.access$900(INodeDirectoryWithSnapshot.java:376)",
                "\tat org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.replaceChild(INodeDirectoryWithSnapshot.java:598)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.unprotectedReplaceINodeFile(FSDirectory.java:1548)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.replaceINodeFile(FSDirectory.java:1537)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.loadFilesUnderConstruction(FSImageFormat.java:855)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImageFormat$Loader.load(FSImageFormat.java:350)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:910)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:899)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImageFile(FSImage.java:751)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:720)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:266)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:784)",
                "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:563)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:422)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:472)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:670)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:655)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1245)",
                "\tat org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1311)",
                "2013-10-24 21:07:03,050 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1",
                "2013-10-24 21:07:03,052 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:"
            ],
            "StepsToReproduce": [
                "1. Perform a snapshot operation (createSnapshot) on a directory with under-construction files.",
                "2. Rename the snapshot created in step 1.",
                "3. Restart the NameNode."
            ],
            "ExpectedBehavior": "The NameNode should restart successfully without any exceptions.",
            "ObservedBehavior": "The NameNode fails to restart and throws an IllegalStateException related to snapshot operations.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-13145.json",
        "creation_time": "2018-02-13T19:22:56.000+0000",
        "bug_report": {
            "BugID": "HDFS-13145",
            "Title": "SBN Crash During Transition to ANN with In-Progress Edit Tailing Enabled",
            "Description": "When edit log tailing is enabled, the Standby NameNode (SBN) crashes during the transition to Active NameNode (ANN) if the ANN crashes in between sending batches. This occurs due to a failure in the check for the committed transaction ID, leading to an IllegalStateException.",
            "StackTrace": [
                "java.lang.IllegalStateException: Cannot start writing at txid 24312595802 when there is a stream available for read: ......",
                "at org.apache.hadoop.hdfs.server.namenode.FSEditLog.openForWrite(FSEditLog.java:329)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices(FSNamesystem.java:1196)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.startActiveServices(NameNode.java:1839)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.enterState(ActiveState.java:61)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.HAState.setStateInternal(HAState.java:64)",
                "at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.setState(StandbyState.java:49)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNode.transitionToActive(NameNode.java:1707)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.transitionToActive(NameNodeRpcServer.java:1622)",
                "at org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB.transitionToActive(HAServiceProtocolServerSideTranslatorPB.java:107)",
                "at org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceProtocolService$2.callBlockingMethod(HAServiceProtocolProtos.java:4460)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:851)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:794)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:249)"
            ],
            "StepsToReproduce": [
                "Enable in-progress edit log tailing.",
                "Trigger a crash in the Active NameNode (ANN).",
                "Observe the Standby NameNode (SBN) during its transition to Active NameNode (ANN)."
            ],
            "ExpectedBehavior": "The Standby NameNode (SBN) should transition to Active NameNode (ANN) without crashing.",
            "ObservedBehavior": "The Standby NameNode (SBN) crashes with an IllegalStateException during the transition to Active NameNode (ANN).",
            "Resolution": "Fixed in version 3.1.0."
        }
    },
    {
        "filename": "HDFS-8807.json",
        "creation_time": "2015-07-22T20:45:55.000+0000",
        "bug_report": {
            "BugID": "HDFS-8807",
            "Title": "Datanode Fails to Start Due to Space in dfs.datanode.data.dir Configuration",
            "Description": "When a space is added between the storage type and the file URI in the dfs.datanode.data.dir configuration, the datanode fails to start. This issue leads to a parsing error, as shown in the stack trace from the datanode logs.",
            "StackTrace": [
                "2015-07-22 13:34:36,812 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain",
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:204)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:170)",
                "at org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse(StorageLocation.java:97)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.getStorageLocations(DataNode.java:2314)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2298)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2349)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2529)",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2553)",
                "Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 0:  file://tmp/hadoop-aengineer/disk1/dfs/data",
                "at java.net.URI$Parser.fail(URI.java:2829)",
                "at java.net.URI$Parser.checkChars(URI.java:300)",
                "at java.net.URI$Parser.checkChar(URI.java:3012)",
                "at java.net.URI$Parser.parse(URI.java:3028)",
                "at java.net.URI.<init>(URI.java:753)",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:201)",
                "... 7 more"
            ],
            "StepsToReproduce": [
                "Open the configuration file for the datanode.",
                "Add a space between the storage type and the file URI in the dfs.datanode.data.dir property.",
                "Attempt to start the datanode."
            ],
            "ExpectedBehavior": "The datanode should start successfully without any errors.",
            "ObservedBehavior": "The datanode fails to start and logs a parsing error due to an illegal character in the URI.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "HDFS-3436.json",
        "creation_time": "2012-05-17T10:36:31.000+0000",
        "bug_report": {
            "BugID": "HDFS-3436",
            "Title": "Failure to Append to File After Stopping DataNode in Existing Pipeline",
            "Description": "When attempting to append to a file in a Hadoop cluster with an existing pipeline, the operation fails if one of the DataNodes in the pipeline is stopped. This issue occurs due to a failure in adding a new DataNode to the existing pipeline.",
            "StackTrace": [
                "2012-04-24 22:06:09,947 INFO  hdfs.DFSClient (DFSOutputStream.java:createBlockOutputStream(1063)) - Exception in createBlockOutputStream",
                "java.io.IOException: Bad connect ack with firstBadLink as *******:50010",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1053)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:943)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "2012-04-24 22:06:09,947 WARN  hdfs.DFSClient (DFSOutputStream.java:setupPipelineForAppendOrRecovery(916)) - Error Recovery for block BP-1023239-10.18.40.233-1335275282109:blk_296651611851855249_1253 in pipeline *****:50010, ******:50010, *****:50010: bad datanode ******:50010",
                "2012-04-24 22:06:10,072 WARN  hdfs.DFSClient (DFSOutputStream.java:run(549)) - DataStreamer Exception",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:162)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:866)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:843)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:934)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:461)",
                "2012-05-17 15:39:12,261 ERROR datanode.DataNode (DataXceiver.java:run(193)) - host0.foo.com:49744:DataXceiver error processing TRANSFER_BLOCK operation  src: /127.0.0.1:49811 dest: /127.0.0.1:49744",
                "java.io.IOException: BP-2001850558-xx.xx.xx.xx-1337249347060:blk_-8165642083860293107_1002 is neither a RBW nor a Finalized, r=ReplicaBeingWritten, blk_-8165642083860293107_1003, RBW",
                "getNumBytes()     = 1024",
                "getBytesOnDisk()  = 1024",
                "getVisibleLength()= 1024",
                "getVolume()       = E:\\MyWorkSpace\\branch-2\\Test\\build\\test\\data\\dfs\\data\\data1\\current",
                "getBlockFile()    = E:\\MyWorkSpace\\branch-2\\Test\\build\\test\\data\\dfs\\data\\data1\\current\\BP-2001850558-xx.xx.xx.xx-1337249347060\\current\\rbw\\blk_-8165642083860293107",
                "bytesAcked=1024",
                "bytesOnDisk=102",
                "at org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery(DataNode.java:2038)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.transferBlock(DataXceiver.java:525)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opTransferBlock(Receiver.java:114)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:78)",
                "at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:189)",
                "at java.lang.Thread.run(Unknown Source)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with 4 DataNodes.",
                "2. Write a file to 3 DataNodes in the following order: DN1 -> DN2 -> DN3.",
                "3. Stop DN3.",
                "4. Attempt to append to the file."
            ],
            "ExpectedBehavior": "The file should be successfully appended to, even if one DataNode in the pipeline is stopped.",
            "ObservedBehavior": "The append operation fails with an IOException indicating issues with adding a DataNode to the existing pipeline.",
            "Resolution": "Fixed"
        }
    }
]