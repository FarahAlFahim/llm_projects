[
    {
        "filename": "YARN-5918.json",
        "creation_time": "2016-11-20T14:19:00.000+0000",
        "bug_report": {
            "BugID": "YARN-5918",
            "Title": "NullPointerException during Opportunistic Container Allocation when NodeManager is Lost",
            "Description": "This bug occurs when there is a failure in allocating requests during the Opportunistic container allocation process, specifically when the NodeManager is lost. The system logs indicate a NullPointerException that arises during the conversion of remote nodes.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1857)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2539)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN ResourceManager.",
                "2. Submit an application that requires opportunistic container allocation.",
                "3. Simulate a loss of the NodeManager while the application is running.",
                "4. Monitor the logs for any allocation failures."
            ],
            "ExpectedBehavior": "The system should handle the loss of the NodeManager gracefully and continue to allocate containers without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown in the logs, indicating a failure in the opportunistic container allocation process.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-8629.json",
        "creation_time": "2018-08-07T00:14:14.000+0000",
        "bug_report": {
            "BugID": "YARN-8629",
            "Title": "Container Cleanup Fails Due to Missing Cgroup Tasks File",
            "Description": "When an application fails to launch a container successfully, the cleanup of the container also fails with a warning message indicating that the cgroup tasks file could not be found.",
            "StackTrace": [
                "2018-08-06 03:28:20,351 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file.",
                "java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:93)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)",
                "2018-08-06 03:28:20,372 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file."
            ],
            "StepsToReproduce": [
                "Attempt to launch a container that is expected to fail.",
                "Observe the logs for cleanup operations related to the failed container."
            ],
            "ExpectedBehavior": "The cleanup process should complete successfully without warnings or errors.",
            "ObservedBehavior": "The cleanup process fails with a warning about a missing cgroup tasks file.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4431.json",
        "creation_time": "2015-12-07T18:31:36.000+0000",
        "bug_report": {
            "BugID": "YARN-4431",
            "Title": "Unnecessary unregistration of NodeManager on connection failure to ResourceManager",
            "Description": "When the NodeManager (NM) fails to connect to the ResourceManager (RM), it retries the connection according to a defined policy. However, after exhausting the retry attempts, the NM attempts to unregister itself from the RM, which is unnecessary and leads to additional retries and delays. This behavior should be modified to skip the unregistration process when the NM is shutting down due to connection issues.",
            "StackTrace": [
                "2015-12-07 12:16:57,873 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)",
                "2015-12-07 12:16:58,874 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)",
                "2015-12-07 12:16:58,876 WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Unregistration of the Node 10.200.10.53:25454 failed.",
                "java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:408)",
                "at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1452)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1385)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)",
                "at com.sun.proxy.$Proxy74.unRegisterNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)",
                "at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:255)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)",
                "at com.sun.proxy.$Proxy75.unRegisterNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop(NodeStatusUpdaterImpl.java:245)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:377)"
            ],
            "StepsToReproduce": [
                "Start the NodeManager and ensure it cannot connect to the ResourceManager.",
                "Observe the logs for connection retry attempts.",
                "Wait for the NodeManager to exhaust its retry attempts."
            ],
            "ExpectedBehavior": "The NodeManager should stop without attempting to unregister itself from the ResourceManager when it fails to connect.",
            "ObservedBehavior": "The NodeManager attempts to unregister itself from the ResourceManager after failing to connect, leading to unnecessary retries and delays.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-2273.json",
        "creation_time": "2014-07-10T18:38:53.000+0000",
        "bug_report": {
            "BugID": "YARN-2273",
            "Title": "NullPointerException in ContinuousScheduling when Node is Lost",
            "Description": "A DataNode (DN) experienced memory errors, leading to a cycle of rebooting and rejoining the cluster. After the second disconnection, the ResourceManager (RM) produced a series of logs indicating a NullPointerException in the FairScheduler's continuousScheduling thread. This issue resulted in YARN becoming unresponsive, where jobs could be submitted but no containers were assigned, halting progress until the RM was restarted.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)",
                "at java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)",
                "at java.util.TimSort.sort(TimSort.java:203)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start a YARN cluster with multiple DataNodes.",
                "2. Simulate memory errors on one of the DataNodes.",
                "3. Allow the DataNode to reboot and rejoin the cluster.",
                "4. Observe the ResourceManager logs for errors."
            ],
            "ExpectedBehavior": "The ResourceManager should handle node failures gracefully without throwing exceptions, and jobs should continue to be scheduled and executed.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException in the continuousScheduling thread, leading to unresponsive behavior where jobs can be submitted but no containers are assigned.",
            "Resolution": "Fixed in version 2.6.0"
        }
    },
    {
        "filename": "YARN-2834.json",
        "creation_time": "2014-11-09T06:07:01.000+0000",
        "bug_report": {
            "BugID": "YARN-2834",
            "Title": "Resource Manager Crashes with Null Pointer Exception on Restart",
            "Description": "The Resource Manager fails to start properly after a restart, resulting in a Null Pointer Exception. This issue impacts the stability of the Hadoop YARN system.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1005)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:843)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1091)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1226)"
            ],
            "StepsToReproduce": [
                "Restart the Resource Manager service.",
                "Monitor the logs for any errors during startup."
            ],
            "ExpectedBehavior": "The Resource Manager should start without errors and be able to handle application attempts.",
            "ObservedBehavior": "The Resource Manager fails to start and logs a Null Pointer Exception.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-370.json",
        "creation_time": "2013-02-01T04:02:58.000+0000",
        "bug_report": {
            "BugID": "YARN-370",
            "Title": "CapacityScheduler App Submission Fails Due to Resource Allocation Mismatch",
            "Description": "When running version 2.0.3-SNAPSHOT of Hadoop YARN with the capacity scheduler configured for a minimum allocation size of 1G, the application master (AM) fails to launch due to a resource allocation mismatch. The AM size is set to 1.5G, but the expected resource allocation does not match the available resources, leading to an unauthorized request error.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unauthorized request to start container. Expected resource <memory:2048, vCores:1> but found <memory:1536, vCores:1>",
                "at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:383)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:400)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:68)",
                "at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1735)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1731)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1729)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)",
                "at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)",
                "at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:123)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:109)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:255)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Configure the capacity scheduler with a minimum allocation size of 1G.",
                "2. Set the application master size to 1.5G.",
                "3. Launch an application using the configured settings.",
                "4. Observe the failure in launching the application master."
            ],
            "ExpectedBehavior": "The application master should launch successfully with the specified resource allocations.",
            "ObservedBehavior": "The application master fails to launch due to a resource allocation mismatch, resulting in an unauthorized request error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3675.json",
        "creation_time": "2015-05-18T22:38:39.000+0000",
        "bug_report": {
            "BugID": "YARN-3675",
            "Title": "FairScheduler: ResourceManager crashes when node removal races with continuous scheduling",
            "Description": "When continuous scheduling is enabled, the ResourceManager can crash if a scheduling event occurs on a node that has just been removed. This leads to a NullPointerException in the FairScheduler.",
            "StackTrace": [
                "12:28:53.782 AM FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                "Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserve(FSAppAttempt.java:469)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:815)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:763)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1217)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "\tat java.lang.Thread.run(Thread.java:745)",
                "12:28:53.783 AM\t INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Enable continuous scheduling in the FairScheduler.",
                "2. Remove a node from the cluster.",
                "3. Trigger a scheduling event on the removed node."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the node removal gracefully without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when handling the APP_ATTEMPT_REMOVED event.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4763.json",
        "creation_time": "2016-03-04T10:03:56.000+0000",
        "bug_report": {
            "BugID": "YARN-4763",
            "Title": "NullPointerException on RMApps Page",
            "Description": "The RMApps page crashes with a NullPointerException when attempting to render application data. This issue occurs when the application state is NEW and the application attempts are empty.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)",
                "at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet._(Hamlet.java:30354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics.render(AppsBlockWithMetrics.java:30)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:848)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:156)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)"
            ],
            "StepsToReproduce": [
                "Navigate to the RMApps page in the Hadoop YARN web interface.",
                "Ensure that there are applications in the NEW state with no attempts.",
                "Observe the behavior when the page attempts to render application data."
            ],
            "ExpectedBehavior": "The RMApps page should display a list of applications without crashing.",
            "ObservedBehavior": "The RMApps page crashes with a NullPointerException, preventing any application data from being displayed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8202.json",
        "creation_time": "2018-04-24T15:52:00.000+0000",
        "bug_report": {
            "BugID": "YARN-8202",
            "Title": "Invalid Resource Request Exception in YARN due to Custom Resource Type Validation",
            "Description": "When executing a YARN job with specific resource arguments, an InvalidResourceRequestException is thrown, causing the job to hang. This issue arises because the resource validation does not account for the units of the requested custom resource types.",
            "StackTrace": [
                "2018-04-24 08:42:03,694 INFO org.apache.hadoop.ipc.Server: IPC Server handler 20 on 8030, call Call#386 Retry#0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate from 172.31.119.172:58138",
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation. Requested resource=<memory:200, vCores:1, resource1: 500M>, maximum allowed allocation=<memory:6144, vCores:8, resource1: 5G>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:8192, resource1: 9223372036854775807G>",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)",
                "at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:433)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)"
            ],
            "StepsToReproduce": [
                "Execute a YARN job with the following arguments: -Dmapreduce.map.resource.memory-mb=200 -Dmapreduce.map.resource.resource1=500M 1 1000",
                "Ensure that there is one node with 5GB of resource1 available.",
                "Observe the job execution and note the exception thrown."
            ],
            "ExpectedBehavior": "The job should execute successfully without throwing an InvalidResourceRequestException.",
            "ObservedBehavior": "The job hangs and throws an InvalidResourceRequestException due to incorrect validation of the custom resource type.",
            "Resolution": "The issue has been fixed in the latest version by ensuring that resource units are properly accounted for during validation."
        }
    },
    {
        "filename": "YARN-7118.json",
        "creation_time": "2017-08-29T12:04:01.000+0000",
        "bug_report": {
            "BugID": "YARN-7118",
            "Title": "NullPointerException in AHS REST API",
            "Description": "The ApplicationHistoryService REST API returns a NullPointerException when attempting to retrieve application history. This issue occurs when the API is called with a specific queue parameter.",
            "StackTrace": [
                "2017-08-17 17:54:54,128 WARN  webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)",
                "    at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:497)",
                "    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)"
            ],
            "StepsToReproduce": [
                "1. Open a terminal or command prompt.",
                "2. Execute the following curl command: curl --negotiate -u: 'http://<ATS IP>:8188/ws/v1/applicationhistory/apps?queue=test'",
                "3. Observe the response from the API."
            ],
            "ExpectedBehavior": "The API should return a valid JSON response containing application history data.",
            "ObservedBehavior": "The API returns a NullPointerException, indicating an internal server error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4743.json",
        "creation_time": "2016-02-27T09:12:28.000+0000",
        "bug_report": {
            "BugID": "YARN-4743",
            "Title": "FairSharePolicy Breaks TimSort Assumption Leading to IllegalArgumentException",
            "Description": "The FairSharePolicy implementation is causing a failure in the TimSort algorithm due to a violation of its general contract. This results in an IllegalArgumentException when both memorySize and weight are zero.",
            "StackTrace": [
                "2016-02-26 14:08:50,821 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:868)",
                "at java.util.TimSort.mergeAt(TimSort.java:485)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:410)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-02-26 14:08:50,822 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Set memorySize to 0 and weight to 0 in the FairSharePolicy.",
                "2. Trigger a NODE_UPDATE event in the ResourceManager.",
                "3. Observe the logs for the IllegalArgumentException."
            ],
            "ExpectedBehavior": "The FairSharePolicy should handle cases where memorySize and weight are both zero without throwing an exception.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, indicating that the comparison method violates its general contract.",
            "Resolution": "Fixed in version 2.9.0 and later."
        }
    },
    {
        "filename": "YARN-2414.json",
        "creation_time": "2014-08-12T23:48:48.000+0000",
        "bug_report": {
            "BugID": "YARN-2414",
            "Title": "NPE in RM Web UI when app fails before attempts are created",
            "Description": "The ResourceManager web UI crashes when an application fails before any attempts have been created, resulting in a NullPointerException.",
            "StackTrace": [
                "2014-08-12 16:45:13,573 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/app/application_1407887030038_0001",
                "java.lang.reflect.InvocationTargetException",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "\tat java.lang.reflect.Method.invoke(Method.java:597)",
                "\tat org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)",
                "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:84)",
                "\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:460)",
                "\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1191)",
                "\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "\tat org.mortbay.jetty.Server.handle(Server.java:326)",
                "\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)",
                "\tat org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)",
                "\tat org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)",
                "\tat org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "\tat org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "\tat org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "\tat org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)",
                "\tat org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)",
                "\tat org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "\tat org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:55)",
                "\t... 44 more"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the ResourceManager.",
                "2. Ensure the application fails before any attempts are created.",
                "3. Access the application page in the ResourceManager web UI."
            ],
            "ExpectedBehavior": "The application page should display the application's status without crashing.",
            "ObservedBehavior": "The web UI crashes with a NullPointerException when trying to render the application page.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3878.json",
        "creation_time": "2015-07-02T00:20:59.000+0000",
        "bug_report": {
            "BugID": "YARN-3878",
            "Title": "AsyncDispatcher Hangs During Shutdown When Draining Events",
            "Description": "The AsyncDispatcher can hang indefinitely during the shutdown process if it is configured to drain events. This occurs when the ResourceManager (RM) is stopped while attempting to post an event to the RMStateStore's AsyncDispatcher, leading to an InterruptedException. The dispatcher waits for the event queue to drain, but this condition never becomes true, causing the dispatcher to wait until the JVM exits.",
            "StackTrace": [
                "2015-06-27 20:08:35,922 DEBUG [main] service.AbstractService (AbstractService.java:enterState(452)) - Service: Dispatcher entered state STOPPED",
                "2015-06-27 20:08:35,923 WARN  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:handle(247)) - AsyncDispatcher thread interrupted",
                "java.lang.InterruptedException",
                "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.access$3300(RMAppAttemptImpl.java:109)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1619)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:786)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:108)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:838)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager (RM).",
                "2. While the RM is running, initiate a stop command for the RM.",
                "3. Observe the logs for any InterruptedException and the state of the AsyncDispatcher."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should stop gracefully without hanging, allowing all events to be processed before shutdown.",
            "ObservedBehavior": "The AsyncDispatcher hangs indefinitely, waiting for the event queue to drain, which never occurs due to the InterruptedException.",
            "Resolution": "A fix for this issue has been implemented and tested."
        }
    },
    {
        "filename": "YARN-6683.json",
        "creation_time": "2017-06-02T00:29:13.000+0000",
        "bug_report": {
            "BugID": "YARN-6683",
            "Title": "Invalid State Transition on COLLECTOR_UPDATE Event in KILLED State",
            "Description": "An error occurs when attempting to handle a COLLECTOR_UPDATE event while the application is in the KILLED state. This results in an InvalidStateTransitionException, indicating that the event cannot be processed in the current state.",
            "StackTrace": [
                "2017-06-01 20:01:22,686 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(905)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)"
            ],
            "StepsToReproduce": [
                "1. Start an application in YARN.",
                "2. Transition the application to the KILLED state.",
                "3. Attempt to send a COLLECTOR_UPDATE event to the application."
            ],
            "ExpectedBehavior": "The application should handle the COLLECTOR_UPDATE event without throwing an exception, regardless of its state.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the COLLECTOR_UPDATE event cannot be processed in the KILLED state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-2910.json",
        "creation_time": "2014-11-27T06:19:00.000+0000",
        "bug_report": {
            "BugID": "YARN-2910",
            "Title": "ConcurrentModificationException in FSLeafQueue due to unsynchronized access",
            "Description": "The list that maintains the runnable and non-runnable apps is a standard ArrayList, but there is no guarantee that it will only be manipulated by one thread in the system. This can lead to a ConcurrentModificationException when multiple threads access the list simultaneously. The following exception was observed:\n\n```\n2014-11-12 02:29:01,169 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.\njava.util.ConcurrentModificationException: java.util.ConcurrentModificationException\nat java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)\nat java.util.ArrayList$Itr.next(ArrayList.java:831)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)\nat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)\n```\n\nTo prevent this issue, we should use a thread-safe version such as `java.util.concurrent.CopyOnWriteArrayList`.",
            "StackTrace": [
                "2014-11-12 02:29:01,169 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.util.ConcurrentModificationException: java.util.ConcurrentModificationException",
                "at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)",
                "at java.util.ArrayList$Itr.next(ArrayList.java:831)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN application with multiple threads accessing the FSLeafQueue.",
                "2. Monitor the application for any exceptions thrown.",
                "3. Observe the logs for ConcurrentModificationException."
            ],
            "ExpectedBehavior": "The FSLeafQueue should handle concurrent access without throwing exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown when multiple threads access the FSLeafQueue simultaneously.",
            "Resolution": "Consider using a thread-safe collection such as java.util.concurrent.CopyOnWriteArrayList to manage the list of runnable and non-runnable apps."
        }
    },
    {
        "filename": "YARN-192.json",
        "creation_time": "2012-11-01T05:00:41.000+0000",
        "bug_report": {
            "BugID": "YARN-192",
            "Title": "NullPointerException in Fair Scheduler on Node Update",
            "Description": "The exception occurs when `unreserve` is called on an `FSSchedulerApp` with a `NodeId` that it does not recognize. The ResourceManager appears to have a different understanding of which applications are reserved for which node compared to the scheduler.",
            "StackTrace": [
                "2012-10-29 22:30:52,901 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.unreserve(FSSchedulerApp.java:356)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.unreserve(AppSchedulable.java:214)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:266)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:330)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueSchedulable.assignContainer(FSQueueSchedulable.java:161)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:759)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:836)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:329)",
                "    at java.lang.Thread.run(Thread.java:662)",
                "2012-10-29 22:30:52,903 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager and Fair Scheduler.",
                "2. Reserve a NodeId with an FSSchedulerApp.",
                "3. Call the `unreserve` method on the FSSchedulerApp with a NodeId that is not recognized by the scheduler."
            ],
            "ExpectedBehavior": "The unreserve operation should complete without throwing an exception, even if the NodeId is not recognized.",
            "ObservedBehavior": "A NullPointerException is thrown when unreserving an FSSchedulerApp with an unknown NodeId.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4581.json",
        "creation_time": "2016-01-12T03:37:40.000+0000",
        "bug_report": {
            "BugID": "YARN-4581",
            "Title": "AHS Writer Thread Leak Causes ResourceManager Crash During Recovery",
            "Description": "Enabling ApplicationHistoryWriter results in thousands of errors, leading to ResourceManager (RM) crashes during recovery. This issue is characterized by an OutOfMemoryError due to thread leaks in the RM after several failovers.",
            "StackTrace": [
                "2016-01-08 03:13:03,441 ERROR org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore: Error when opening history file of application application_1451878591907_0197",
                "java.io.IOException: Output file not at zero offset.",
                "        at org.apache.hadoop.io.file.tfile.BCFile$Writer.<init>(BCFile.java:288)",
                "        at org.apache.hadoop.io.file.tfile.TFile$Writer.<init>(TFile.java:288)",
                "        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:728)",
                "        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "        at java.lang.Thread.run(Thread.java:745)",
                "2016-01-08 03:13:08,335 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.OutOfMemoryError: unable to create new native thread",
                "        at java.lang.Thread.start0(Native Method)",
                "        at java.lang.Thread.start(Thread.java:714)",
                "        at org.apache.hadoop.hdfs.DFSOutputStream.start(DFSOutputStream.java:2033)",
                "        at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForAppend(DFSOutputStream.java:1652)",
                "        at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1573)",
                "        at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1603)",
                "        at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1591)",
                "        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:328)",
                "        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:324)",
                "        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "        at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:324)",
                "        at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)",
                "        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:723)",
                "        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "        at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Enable ApplicationHistoryWriter in the configuration.",
                "Submit an application to the ResourceManager.",
                "Monitor the logs for errors related to application history."
            ],
            "ExpectedBehavior": "The ResourceManager should handle application history without errors and should not crash.",
            "ObservedBehavior": "The ResourceManager crashes with an OutOfMemoryError due to thread leaks after enabling ApplicationHistoryWriter.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7786.json",
        "creation_time": "2018-01-22T14:29:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7786",
            "Title": "NullPointerException Occurs When Launching ApplicationMaster After Kill Command",
            "Description": "A NullPointerException is thrown when attempting to launch the ApplicationMaster after sending a kill command to the job. This issue needs to be addressed to prevent application failures.",
            "StackTrace": [
                "2017-11-25 21:27:25,333 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1511616410268_0001_000001. Got exception: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the application that utilizes the ApplicationMaster.",
                "2. Send a kill command to the job before the ApplicationMaster is launched.",
                "3. Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The ApplicationMaster should launch successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the ApplicationMaster from launching.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8035.json",
        "creation_time": "2018-03-16T12:02:04.000+0000",
        "bug_report": {
            "BugID": "YARN-8035",
            "Title": "MetricsException due to ContainerPid tag conflict during container relaunch",
            "Description": "In the case of a container relaunch event, the container ID is reused but a new process is spawned. For resource monitoring, ContainersMonitorImpl will obtain the new PID post relaunch and initialize the process tree monitoring. As part of this initialization, a tag called ContainerPid, whose value is the PID for the container, is populated for the metrics associated with the container. If the prior container failed after its process started, the original PID will already be populated for the container, resulting in the MetricsException below.\n\nThe MetricsRegistry provides a tag method that allows for updating the value of an existing tag. Updating the value ensures that the PID associated with the container is the currently running process, which appears to be an appropriate fix. However, it's unclear how this tag might be being used by other systems. I'm not finding any usage in Hadoop itself.",
            "StackTrace": [
                "2018-03-16 11:59:02,563 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Uncaught exception in ContainersMonitorImpl while monitoring resource of container_1521201379995_0001_01_000002",
                "org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448)"
            ],
            "StepsToReproduce": [
                "1. Start a container in the Hadoop YARN environment.",
                "2. Allow the container to run until it fails.",
                "3. Relaunch the container.",
                "4. Monitor the logs for any warnings or exceptions related to the ContainersMonitorImpl."
            ],
            "ExpectedBehavior": "The ContainersMonitorImpl should successfully monitor the new container without throwing a MetricsException.",
            "ObservedBehavior": "A MetricsException is thrown indicating that the tag ContainerPid already exists.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4152.json",
        "creation_time": "2015-09-12T15:02:22.000+0000",
        "bug_report": {
            "BugID": "YARN-4152",
            "Title": "NullPointerException in LogAggregationService when stopping absent container",
            "Description": "The NodeManager crashes with a NullPointerException when the LogAggregationService's stopContainer method is called for a container that does not exist. This issue occurs during log aggregation when an application is killed unexpectedly.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "    at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "Run a Pi job with 500 containers.",
                "Kill the application while it is running.",
                "Observe the NodeManager logs for errors related to log aggregation."
            ],
            "ExpectedBehavior": "The NodeManager should handle the absence of a container gracefully without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when attempting to stop a non-existent container.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3697.json",
        "creation_time": "2015-05-21T18:05:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3697",
            "Title": "FairScheduler: ContinuousSchedulingThread Fails to Shutdown",
            "Description": "The ContinuousSchedulingThread in FairScheduler sometimes fails to shut down due to an InterruptedException being blocked in the continuousSchedulingAttempt method. This issue can lead to resource management problems in the YARN scheduler.",
            "StackTrace": [
                "2015-05-17 23:30:43,065 WARN  [FairSchedulerContinuousScheduling] event.AsyncDispatcher (AsyncDispatcher.java:handle(247)) - AsyncDispatcher thread interrupted",
                "java.lang.InterruptedException",
                "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)",
                "2015-05-17 23:30:43,066 ERROR [FairSchedulerContinuousScheduling] fair.FairScheduler (FairScheduler.java:continuousSchedulingAttempt(1017)) - Error while attempting scheduling for node host: 127.0.0.2:2 #containers=1 available=<memory:7168, vCores:7> used=<memory:1024, vCores:1>: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:249)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)",
                "Caused by: java.lang.InterruptedException",
                "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "\tat ... 17 more"
            ],
            "StepsToReproduce": [
                "Start the FairScheduler with a configured ContinuousSchedulingThread.",
                "Trigger a stop command for the scheduler.",
                "Observe the logs for any InterruptedException warnings."
            ],
            "ExpectedBehavior": "The ContinuousSchedulingThread should shut down gracefully without any InterruptedException warnings.",
            "ObservedBehavior": "The ContinuousSchedulingThread fails to shut down and logs an InterruptedException warning.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2340.json",
        "creation_time": "2014-07-23T15:18:38.000+0000",
        "bug_report": {
            "BugID": "YARN-2340",
            "Title": "NullPointerException on ResourceManager Restart After Stopping Queue",
            "Description": "When the ResourceManager (RM) is restarted after the queue state is set to STOPPED, it fails to come up as active, resulting in a NullPointerException (NPE). This issue occurs specifically when the CapacityScheduler is in use with queues configured as 'a' and 'b'.",
            "StackTrace": [
                "2014-07-23 18:43:24,432 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1406116264351_0014_000002 State change from NEW to SUBMITTED",
                "2014-07-23 18:43:24,433 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                " at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:568)",
                " at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:916)",
                " at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:101)",
                " at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:602)",
                " at java.lang.Thread.run(Thread.java:662)",
                "2014-07-23 18:43:24,434 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with the CapacityScheduler configured.",
                "2. Submit a job to the queue.",
                "3. While the job is in progress, change the queue state to STOPPED.",
                "4. Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully and recover the application's state.",
            "ObservedBehavior": "The ResourceManager fails to come up as active and throws a NullPointerException.",
            "Resolution": "Fixed in version 2.7.0"
        }
    },
    {
        "filename": "YARN-8022.json",
        "creation_time": "2018-03-10T19:29:27.000+0000",
        "bug_report": {
            "BugID": "YARN-8022",
            "Title": "ResourceManager UI fails to render application attempts",
            "Description": "The ResourceManager UI fails to display the application attempts, resulting in a NullPointerException. The error message displayed is: \"Failed to read the attempts of the application\".",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.webapp.AppBlock: Failed to read the attempts of the application application_1520597233415_0002.",
                "java.lang.NullPointerException",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:283)",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:280)",
                " at java.security.AccessController.doPrivileged(Native Method)",
                " at javax.security.auth.Subject.doAs(Subject.java:422)",
                " at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock.render(AppBlock.java:279)",
                " at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock.render(RMAppBlock.java:71)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                " at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                " at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)",
                " at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)",
                " at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                " at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                " at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:54)"
            ],
            "StepsToReproduce": [
                "Navigate to the ResourceManager UI.",
                "Access the cluster/app/<app-id> page.",
                "Observe the error message displayed."
            ],
            "ExpectedBehavior": "The ResourceManager UI should display the application attempts without errors.",
            "ObservedBehavior": "The UI displays a message indicating failure to read the application attempts, along with a NullPointerException in the logs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3793.json",
        "creation_time": "2015-06-10T20:52:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3793",
            "Title": "Null Pointer Exceptions during NM Recovery when Work-Preserving Restart is Enabled",
            "Description": "When NM work-preserving restart is enabled, several Null Pointer Exceptions (NPEs) occur during recovery. These exceptions seem to correspond to sub-directories that need to be deleted, indicating potential incorrect tracking of resources and a possible resource leak. This report aims to investigate and resolve the underlying issues.",
            "StackTrace": [
                "2015-05-18 07:06:10,225 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : null",
                "2015-05-18 07:06:10,224 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "    at org.apache.hadoop.fs.FileContext.delete(FileContext.java:755)",
                "    at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)",
                "    at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)"
            ],
            "StepsToReproduce": [
                "Enable NM work-preserving restart.",
                "Trigger a recovery process on the NodeManager.",
                "Monitor the logs for any Null Pointer Exceptions."
            ],
            "ExpectedBehavior": "The NodeManager should successfully delete sub-directories without throwing any exceptions.",
            "ObservedBehavior": "Several Null Pointer Exceptions are logged during the deletion process, indicating a failure to delete certain sub-directories.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-6102.json",
        "creation_time": "2017-01-17T09:36:29.000+0000",
        "bug_report": {
            "BugID": "YARN-6102",
            "Title": "Error in AsyncDispatcher during RM failover leading to abnormal exits",
            "Description": "An error occurs in the AsyncDispatcher when handling node heartbeats during ResourceManager failover, resulting in abnormal exits of the TestResourceTrackerOnHA. The dispatcher is reset during failover, causing events to be sent to an unregistered dispatcher.",
            "StackTrace": [
                "2017-01-17 16:42:17,911 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(200)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-17 16:42:17,914 INFO  [AsyncDispatcher ShutDown handler] event.AsyncDispatcher (AsyncDispatcher.java:run(303)) - Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Send a node heartbeat to ResourceTrackerService.",
                "3. Trigger a ResourceManager failover before the heartbeat is processed.",
                "4. Observe the logs for the AsyncDispatcher error."
            ],
            "ExpectedBehavior": "The ResourceManager should handle node heartbeats without errors during failover.",
            "ObservedBehavior": "The AsyncDispatcher throws an error indicating no handler is registered for RMNodeEventType, leading to abnormal exits.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8409.json",
        "creation_time": "2018-06-08T20:36:32.000+0000",
        "bug_report": {
            "BugID": "YARN-8409",
            "Title": "NullPointerException in ActiveStandbyElectorBasedElectorService during RM failover",
            "Description": "In a ResourceManager High Availability (RM-HA) environment, killing the ZooKeeper leader and then performing a ResourceManager failover sometimes results in a NullPointerException (NPE) that prevents the active ResourceManager from starting successfully.",
            "StackTrace": [
                "2018-06-08 10:31:03,007 INFO client.ZooKeeperSaslClient (ZooKeeperSaslClient.java:run(289)) - Client will use GSSAPI as SASL mechanism.",
                "2018-06-08 10:31:03,008 INFO zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server xxx/xxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'",
                "2018-06-08 10:31:03,009 WARN zookeeper.ClientCnxn (ClientCnxn.java:run(1146)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect",
                "java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)",
                "at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)",
                "2018-06-08 10:31:03,344 INFO service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService failed in state INITED",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)",
                "2018-06-08 10:31:03,345 INFO ha.ActiveStandbyElector (ActiveStandbyElector.java:quitElection(409)) - Yielding from election"
            ],
            "StepsToReproduce": [
                "1. Set up a ResourceManager High Availability (RM-HA) environment.",
                "2. Kill the ZooKeeper leader.",
                "3. Perform a ResourceManager failover."
            ],
            "ExpectedBehavior": "The active ResourceManager should come up successfully after the failover.",
            "ObservedBehavior": "The active ResourceManager fails to start and throws a NullPointerException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-8223.json",
        "creation_time": "2018-04-27T11:49:02.000+0000",
        "bug_report": {
            "BugID": "YARN-8223",
            "Title": "ClassNotFoundException when loading auxiliary service from HDFS",
            "Description": "When attempting to load an auxiliary jar from HDFS, a ClassNotFoundException occurs, indicating that the class cannot be found due to an empty classpath. This issue does not occur when loading the jar from a local location.",
            "StackTrace": [
                "java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal",
                "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)",
                "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)",
                "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)",
                "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)",
                "\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)",
                "\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)",
                "\tat java.lang.Class.forName0(Native Method)",
                "\tat java.lang.Class.forName(Class.java:348)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)"
            ],
            "StepsToReproduce": [
                "1. Configure the auxiliary service to load a jar from HDFS.",
                "2. Start the NodeManager.",
                "3. Observe the logs for any ClassNotFoundException."
            ],
            "ExpectedBehavior": "The auxiliary service should load the jar from HDFS without any ClassNotFoundException.",
            "ObservedBehavior": "A ClassNotFoundException is thrown, indicating that the class cannot be found due to an empty classpath.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8331.json",
        "creation_time": "2018-05-21T05:19:35.000+0000",
        "bug_report": {
            "BugID": "YARN-8331",
            "Title": "Race Condition in NodeManager Container Launch After Done State",
            "Description": "When a container is launching, it transitions through various states. If a kill event is sent while the container is in the SCHEDULED state, it transitions to KILLING and then to DONE. However, after this transition, the ContainerLaunch sends a CONTAINER_LAUNCHED event, which leads to an InvalidStateTransitionException because the container is already in the DONE state. This results in the absence of cleanup for the container processes that should have been terminated.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Launch a container in the NodeManager.",
                "2. While the container is in the SCHEDULED state, send a kill event to the container.",
                "3. Observe the state transitions of the container.",
                "4. Check for the CONTAINER_LAUNCHED event after the container has transitioned to DONE."
            ],
            "ExpectedBehavior": "The container should properly handle the CONTAINER_LAUNCHED event and clean up any associated processes after transitioning to DONE.",
            "ObservedBehavior": "The container throws an InvalidStateTransitionException when receiving a CONTAINER_LAUNCHED event after it has transitioned to DONE, leading to uncleaned processes.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2931.json",
        "creation_time": "2014-12-08T21:09:13.000+0000",
        "bug_report": {
            "BugID": "YARN-2931",
            "Title": "PublicLocalizer Fails Due to Missing Local Directories After Cleanup",
            "Description": "When the data directory is cleaned up and the NodeManager (NM) is started with an existing recovery state, it fails to recreate the local directories due to YARN-90. This results in a PublicLocalizer failure until the getInitializedLocalDirs method is called by a LocalizeRunner for private localization.",
            "StackTrace": [
                "2014-12-02 22:57:32,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Failed to download rsrc { { hdfs:/<blah machine>:8020/tmp/hive-hive/hive_2014-12-02_22-56-58_741_2045919883676051996-3/-mr-10004/8060c9dd-54b6-42fc-9d77-34b655fa5e82/reduce.xml, 1417589819618, FILE, null },pending,[(container_1417589109512_0001_02_000003)],119413444132127,DOWNLOADING}",
                "java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:720)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)",
                "at org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2014-12-02 22:57:32,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1417589109512_0001_02_000003 transitioned from LOCALIZING to LOCALIZATION_FAILED"
            ],
            "StepsToReproduce": [
                "1. Clean up the data directory used by the NodeManager.",
                "2. Start the NodeManager with an existing recovery state.",
                "3. Attempt to localize resources that require the local directories."
            ],
            "ExpectedBehavior": "The NodeManager should recreate the necessary local directories and successfully localize resources.",
            "ObservedBehavior": "The NodeManager fails to recreate the local directories, resulting in a PublicLocalizer failure and a FileNotFoundException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6837.json",
        "creation_time": "2017-07-18T11:17:55.000+0000",
        "bug_report": {
            "BugID": "YARN-6837",
            "Title": "Null LocalResource Visibility Causes NodeManager Crash",
            "Description": "When creating a LocalResource in a Yarn application, failing to set the visibility can lead to a NullPointerException in the NodeManager, causing it to shut down unexpectedly. This issue was observed when the visibility was set to null, resulting in a crash during resource management.",
            "StackTrace": [
                "2017-07-18 17:54:09,289 INFO org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=hadoop       IP=10.43.156.177        OPERATION=Start Container Request       TARGET=ContainerManageImpl      RESULT=SUCCESS  APPID=application_1499221670783_0067    CONTAINERID=container_1499221670783_0067_02_000003",
                "2017-07-18 17:54:09,292 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.addResources(ResourceSet.java:84)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:868)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:819)",
                "        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1684)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:96)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1418)",
                "        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1411)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "        at java.lang.Thread.run(Thread.java:745)",
                "2017-07-18 17:54:09,292 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1499221670783_0067_02_000002"
            ],
            "StepsToReproduce": [
                "Create a LocalResource in a Yarn application without setting the visibility.",
                "Submit the Yarn application.",
                "Observe the NodeManager logs for NullPointerException."
            ],
            "ExpectedBehavior": "The NodeManager should handle the LocalResource request without crashing, even if the visibility is not set.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when the visibility of the LocalResource is set to null.",
            "Resolution": "The issue was resolved by ensuring that the visibility of LocalResource is set to a valid value before submission."
        }
    },
    {
        "filename": "YARN-4762.json",
        "creation_time": "2016-03-04T02:24:47.000+0000",
        "bug_report": {
            "BugID": "YARN-4762",
            "Title": "NodeManager Crashes on Initialization with LinuxContainerExecutor Enabled",
            "Description": "The NodeManager fails to initialize when the LinuxContainerExecutor is enabled, resulting in crashes. The logs indicate issues with cgroups setup and initialization of the container executor.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)",
                "Caused by: java.io.IOException: Failed to initialize linux container runtime(s)!",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)",
                "... 3 more"
            ],
            "StepsToReproduce": [
                "1. Enable LinuxContainerExecutor in the NodeManager configuration.",
                "2. Start the NodeManager service.",
                "3. Observe the logs for initialization errors."
            ],
            "ExpectedBehavior": "NodeManager should initialize successfully without crashing.",
            "ObservedBehavior": "NodeManager fails to start and crashes with a YarnRuntimeException indicating failure to initialize the container executor.",
            "Resolution": "Fixed CgroupHandler's creation and usage to avoid NodeManagers crashing when LinuxContainerExecutor is enabled."
        }
    },
    {
        "filename": "YARN-2823.json",
        "creation_time": "2014-11-06T21:38:47.000+0000",
        "bug_report": {
            "BugID": "YARN-2823",
            "Title": "NullPointerException in ResourceManager on Recovery in HA Enabled 3-Node Cluster",
            "Description": "This bug report details a NullPointerException (NPE) encountered in the ResourceManager when attempting to recover application attempts in a high-availability (HA) enabled 3-node cluster setup. The issue arises after the ResourceManagers go down and fail to restart, leading to critical failures in the cluster's operation.",
            "StackTrace": [
                "2014-09-16 01:36:28,037 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(612)) - Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)",
                "    at java.lang.Thread.run(Thread.java:744)",
                "2014-09-16 01:36:28,042 INFO  resourcemanager.ResourceManager (ResourceManager.java:run(616)) - Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Set up a 3-node cluster with ResourceManager high availability (HA) enabled using Ambari.",
                "2. Install HBase using Slider.",
                "3. Allow the ResourceManagers to go down (simulate failure).",
                "4. Attempt to restart the ResourceManagers."
            ],
            "ExpectedBehavior": "The ResourceManagers should recover successfully and handle application attempts without throwing a NullPointerException.",
            "ObservedBehavior": "The ResourceManagers fail to restart and throw a NullPointerException in the logs when handling application attempts.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5098.json",
        "creation_time": "2016-05-17T00:43:08.000+0000",
        "bug_report": {
            "BugID": "YARN-5098",
            "Title": "Yarn Application Log Aggregation Fails Due to Invalid HDFS Delegation Token",
            "Description": "In a High Availability (HA) cluster environment, the Yarn application logs for a long-running application could not be gathered because the NodeManager failed to communicate with HDFS. The error encountered indicates that the HDFS delegation token cannot be found in the cache.",
            "StackTrace": [
                "2016-05-16 18:18:28,533 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(555)) - Application just finished : application_1463170334122_0002",
                "2016-05-16 18:18:28,545 WARN  ipc.Client (Client.java:run(705)) - Exception encountered while connecting to the server :",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:583)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:398)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:752)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:748)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:747)",
                "at org.apache.hadoop.ipc.Client$Connection.access$3100(Client.java:398)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1597)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1439)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1386)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:240)",
                "at com.sun.proxy.$Proxy83.getServerDefaults(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy84.getServerDefaults(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:1018)",
                "at org.apache.hadoop.fs.Hdfs.getServerDefaults(Hdfs.java:156)",
                "at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:550)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:687)"
            ],
            "StepsToReproduce": [
                "Set up a High Availability (HA) cluster environment.",
                "Deploy a long-running Yarn application.",
                "Attempt to gather application logs after the application has finished."
            ],
            "ExpectedBehavior": "The Yarn application logs should be successfully aggregated and accessible after the application completes.",
            "ObservedBehavior": "The application logs could not be gathered due to an error indicating that the HDFS delegation token cannot be found in the cache.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3971.json",
        "creation_time": "2015-07-24T10:17:05.000+0000",
        "bug_report": {
            "BugID": "YARN-3971",
            "Title": "Node Label Recovery Causes ResourceManager to Fail",
            "Description": "When attempting to recover node labels in YARN, the ResourceManager fails to start due to an IOException indicating that a label cannot be removed because it is still in use by a queue. This issue occurs after creating and deleting labels, and then restarting the ResourceManager.",
            "StackTrace": [
                "2015-07-23 14:03:33,627 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager failed in state STARTED; cause: java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label",
                "java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue(RMNodeLabelsManager.java:104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels(RMNodeLabelsManager.java:118)",
                "at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.recover(FileSystemNodeLabelsStore.java:221)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:232)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:245)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:964)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1005)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1001)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1666)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1001)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:832)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:422)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "StepsToReproduce": [
                "Create label x,y",
                "Delete label x,y",
                "Create label x,y and add capacity scheduler XML for labels x and y",
                "Restart ResourceManager"
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without throwing any exceptions related to node labels.",
            "ObservedBehavior": "The ResourceManager fails to start and throws an IOException indicating that the label cannot be removed because it is still in use by a queue.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6948.json",
        "creation_time": "2017-08-04T08:23:46.000+0000",
        "bug_report": {
            "BugID": "YARN-6948",
            "Title": "InvalidStateTransitonException when sending kill command to a running job",
            "Description": "When I send a kill command to a running job, I check the logs and find the following exception:\n\n```\n2017-08-03 01:35:20,485 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING\n    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)\n    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)\n    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)\n    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n    at java.lang.Thread.run(Thread.java:745)\n```\nThis indicates that the system cannot handle the event due to the current state of the application.",
            "StackTrace": [
                "2017-08-03 01:35:20,485 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start a job in the Hadoop YARN environment.",
                "2. While the job is running, send a kill command to the job.",
                "3. Check the logs for any exceptions or errors."
            ],
            "ExpectedBehavior": "The job should be terminated without any exceptions in the logs.",
            "ObservedBehavior": "An InvalidStateTransitonException is logged, indicating that the event cannot be handled in the current state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1409.json",
        "creation_time": "2013-11-13T11:25:56.000+0000",
        "bug_report": {
            "BugID": "YARN-1409",
            "Title": "NonAggregatingLogHandler throws RejectedExecutionException during shutdown",
            "Description": "This issue occurs when handling APPLICATION_FINISHED events after calling `sched.shotdown()` in `NonAggregatingLongHandler#serviceStop()`. The `org.apache.hadoop.mapred.TestJobCleanup` can fail due to a `RejectedExecutionException` thrown by `NonAggregatingLogHandler`.",
            "StackTrace": [
                "2013-11-13 10:53:06,970 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(166)) - Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]",
                "    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)",
                "    at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN application.",
                "2. Trigger an APPLICATION_FINISHED event.",
                "3. Call `sched.shotdown()` in `NonAggregatingLongHandler#serviceStop()`.",
                "4. Observe the logs for any exceptions."
            ],
            "ExpectedBehavior": "The application should handle the APPLICATION_FINISHED event without throwing any exceptions.",
            "ObservedBehavior": "The application throws a `RejectedExecutionException` during the shutdown process.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5545.json",
        "creation_time": "2016-08-21T12:57:35.000+0000",
        "bug_report": {
            "BugID": "YARN-5545",
            "Title": "Application Submission Failure in Capacity Scheduler with Zero Default Capacity",
            "Description": "This bug report addresses issues related to the maximum applications in the Capacity Scheduler. Specifically, it highlights a failure to submit applications when the default partition's capacity is set to zero, despite other partitions having available capacity.",
            "StackTrace": [
                "2016-08-21 18:21:31,375 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1471670113386_0001",
                "java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "\tat org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)",
                "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)",
                "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)",
                "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)",
                "...",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)",
                "\tat org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)",
                "\tat org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:301)",
                "\t... 25 more"
            ],
            "StepsToReproduce": [
                "Configure the capacity scheduler with the following settings:",
                "yarn.scheduler.capacity.root.default.capacity=0",
                "yarn.scheduler.capacity.root.queue1.accessible-node-labels.labelx.capacity=50",
                "yarn.scheduler.capacity.root.default.accessible-node-labels.labelx.capacity=50",
                "Submit an application using the command:",
                "./yarn jar ../share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-alpha2-SNAPSHOT-tests.jar sleep -Dmapreduce.job.node-label-expression=labelx -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 10000000 -rt 1"
            ],
            "ExpectedBehavior": "The application should be submitted successfully to the queue with available capacity.",
            "ObservedBehavior": "The application fails to submit with an AccessControlException indicating that the queue root.default already has 0 applications.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-301.json",
        "creation_time": "2013-01-01T05:40:18.000+0000",
        "bug_report": {
            "BugID": "YARN-301",
            "Title": "ConcurrentModificationException in Fair Scheduler during Node Update",
            "Description": "In my test cluster, the FairScheduler encounters a ConcurrentModificationException, leading to a ResourceManager crash. This issue occurs when handling the NODE_UPDATE event type.",
            "StackTrace": [
                "2012-12-30 17:14:17,171 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.util.ConcurrentModificationException",
                "at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)",
                "at java.util.TreeMap$KeyIterator.nextEntry(TreeMap.java:1154)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:181)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:780)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:842)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:340)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Set up a test cluster with the FairScheduler enabled.",
                "2. Trigger a NODE_UPDATE event in the scheduler.",
                "3. Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The FairScheduler should handle NODE_UPDATE events without throwing exceptions and should not crash the ResourceManager.",
            "ObservedBehavior": "The FairScheduler throws a ConcurrentModificationException, causing the ResourceManager to crash.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7942.json",
        "creation_time": "2018-02-16T19:09:39.000+0000",
        "bug_report": {
            "BugID": "YARN-7942",
            "Title": "ServiceClient Fails to Delete ZNode from Secure ZooKeeper",
            "Description": "The ResourceManager (RM) fails to delete a znode from ZooKeeper even when the appropriate permissions are set. The destroy call succeeds, but the deletion fails with a NoPathPermissionsException.",
            "StackTrace": [
                "2018-02-16 15:49:29,691 WARN  client.ServiceClient (ServiceClient.java:actionDestroy(470)) - Error deleting registry entry /users/hbase/services/yarn-service/hbase-app-test",
                "org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:412)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:390)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:722)",
                "at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.delete(RegistryOperationsService.java:162)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionDestroy(ServiceClient.java:462)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:253)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:243)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.stopService(ApiServer.java:243)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.deleteService(ApiServer.java:223)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:89)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)",
                "at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)",
                "at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$1.call(GuiceFilter.java:203)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.AuthenticationWithProxyUserFilter.doFilter(AuthenticationWithProxyUserFilter.java:101)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1617)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:250)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:244)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:241)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:225)",
                "at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:35)",
                "at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:718)",
                "... 75 more"
            ],
            "StepsToReproduce": [
                "Set the ACLs for the znode to allow access.",
                "Attempt to delete the znode using the ResourceManager.",
                "Observe the logs for any warnings or errors."
            ],
            "ExpectedBehavior": "The znode should be deleted successfully without any permission errors.",
            "ObservedBehavior": "The deletion fails with a NoPathPermissionsException despite correct ACL settings.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7692.json",
        "creation_time": "2017-12-29T06:00:34.000+0000",
        "bug_report": {
            "BugID": "YARN-7692",
            "Title": "Resource Manager Crashes During Application Recovery When ACLs Are Enabled",
            "Description": "When enabling ACLs and submitting a job with a user not included in the priority ACL, the Resource Manager crashes while trying to recover previous applications. This issue occurs even though the job submission is correctly rejected due to lack of permissions.",
            "StackTrace": [
                "2017-12-27 10:52:30,064 INFO  conf.Configuration (Configuration.java:getConfResourceAsInputStream(2659)) - found resource yarn-site.xml at file:/etc/hadoop/3.0.0.0-636/0/yarn-site.xml",
                "2017-12-27 10:52:30,065 INFO  scheduler.AbstractYarnScheduler (AbstractYarnScheduler.java:setClusterMaxPriority(911)) - Updated the cluster max priority to maxClusterLevelAppPriority = 10",
                "2017-12-27 10:52:30,066 INFO  resourcemanager.ResourceManager (ResourceManager.java:transitionToActive(1177)) - Transitioning to active state",
                "2017-12-27 10:52:30,097 INFO  resourcemanager.ResourceManager (ResourceManager.java:serviceStart(765)) - Recovery started",
                "2017-12-27 10:52:30,102 INFO  recovery.RMStateStore (RMStateStore.java:checkVersion(747)) - Loaded RM state version info 1.5",
                "2017-12-27 10:52:30,375 INFO  security.RMDelegationTokenSecretManager (RMDelegationTokenSecretManager.java:recover(196)) - recovering RMDelegationTokenSecretManager.",
                "2017-12-27 10:52:30,380 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(561)) - Recovering 51 applications",
                "2017-12-27 10:52:30,432 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(571)) - Successfully recovered 0 out of 51 applications",
                "2017-12-27 10:52:30,432 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(776)) - Failed to load/recover state",
                "org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0",
                "Caused by: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0",
                "2017-12-27 10:52:30,435 INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping ResourceManager metrics system...",
                "2017-12-27 10:52:30,436 INFO  event.AsyncDispatcher (AsyncDispatcher.java:serviceStop(155)) - AsyncDispatcher is draining to stop, ignoring any new events."
            ],
            "StepsToReproduce": [
                "1. Create a cluster without any ACLs.",
                "2. Submit jobs with an existing user, e.g., 'user_a'.",
                "3. Enable ACLs and create a priority ACL entry via the property 'yarn.scheduler.capacity.priority-acls', excluding 'user_a'.",
                "4. Submit a job with 'user_a'."
            ],
            "ExpectedBehavior": "The job submission should be rejected due to lack of permissions, and the Resource Manager should remain operational.",
            "ObservedBehavior": "The job is rejected as expected, but the Resource Manager crashes while trying to recover previous applications.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3917.json",
        "creation_time": "2015-07-11T00:41:28.000+0000",
        "bug_report": {
            "BugID": "YARN-3917",
            "Title": "Default Resource Calculator Plugin Fails to Determine OS",
            "Description": "The default resource calculator plugin fails to initialize due to an UnsupportedOperationException when the OS cannot be determined. This issue occurs when the user has not configured a specific plugin, leading to a failure in the containers-monitor service.",
            "StackTrace": [
                "2015-07-10 08:16:18,445 INFO org.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.UnsupportedOperationException: Could not determine OS",
                "java.lang.UnsupportedOperationException: Could not determine OS",
                "at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)"
            ],
            "StepsToReproduce": [
                "1. Ensure that no specific resource calculator plugin is configured.",
                "2. Start the Hadoop YARN NodeManager.",
                "3. Observe the logs for any initialization errors related to the containers-monitor service."
            ],
            "ExpectedBehavior": "The default resource calculator plugin should initialize without errors, allowing the containers-monitor service to function correctly.",
            "ObservedBehavior": "The containers-monitor service fails to initialize, resulting in an UnsupportedOperationException due to the inability to determine the OS.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3537.json",
        "creation_time": "2015-04-23T11:34:23.000+0000",
        "bug_report": {
            "BugID": "YARN-3537",
            "Title": "NullPointerException in NodeManager during service initialization",
            "Description": "A NullPointerException occurs in the NodeManager when the serviceInit method fails and the stopRecoveryStore method is invoked. This issue impacts the stability of the NodeManager service.",
            "StackTrace": [
                "2015-04-23 19:30:34,961 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service NodeManager failed in state STOPPED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore(NodeManager.java:181)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:326)",
                "\tat org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:106)"
            ],
            "StepsToReproduce": [
                "1. Start the NodeManager service.",
                "2. Trigger a failure in the service initialization process.",
                "3. Observe the logs for any NullPointerException related to stopRecoveryStore."
            ],
            "ExpectedBehavior": "The NodeManager should handle service initialization failures gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the NodeManager to fail unexpectedly.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7962.json",
        "creation_time": "2018-02-22T22:32:20.000+0000",
        "bug_report": {
            "BugID": "YARN-7962",
            "Title": "Race Condition in DelegationTokenRenewer Causes ResourceManager Crash During Failover",
            "Description": "A race condition occurs in the DelegationTokenRenewer when the service is stopped, leading to a crash of the ResourceManager during failover. The issue arises because the `serviceStop` method does not set the `isServiceStarted` flag to 'false' before shutting down the renewerService thread pool.",
            "StackTrace": [
                "2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]",
                "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "\tat java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Submit a job that utilizes delegation tokens.",
                "3. Trigger a failover scenario while the job is running.",
                "4. Observe the ResourceManager logs for errors."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the failover gracefully without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a RejectedExecutionException due to a race condition in the DelegationTokenRenewer.",
            "Resolution": "The `serviceStop` method should be updated to acquire the `serviceStateLock` and set `isServiceStarted` to false before shutting down the `renewerService` thread pool."
        }
    },
    {
        "filename": "YARN-8357.json",
        "creation_time": "2018-05-24T16:46:57.000+0000",
        "bug_report": {
            "BugID": "YARN-8357",
            "Title": "NullPointerException when starting a Yarn service after saving it",
            "Description": "A NullPointerException (NPE) occurs when attempting to start a Yarn service that has just been saved. The service is returned with a null state, leading to the exception.",
            "StackTrace": [
                "2018-05-24 04:39:22,911 INFO client.ServiceClient (ServiceClient.java:getStatus(1203)) - Service test1 does not have an application ID",
                "2018-05-24 04:39:22,911 ERROR webapp.ApiServer (ApiServer.java:updateService(480)) - Error while performing operation for app: test1",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:644)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:422)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)",
                "    at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:498)",
                "    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)"
            ],
            "StepsToReproduce": [
                "1. Save a Yarn service with a valid configuration.",
                "2. Attempt to start the saved service."
            ],
            "ExpectedBehavior": "The Yarn service should start successfully without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown when trying to start the service.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-6534.json",
        "creation_time": "2017-04-26T21:43:52.000+0000",
        "bug_report": {
            "BugID": "YARN-6534",
            "Title": "ResourceManager Fails to Start Due to SSLFactory Initialization Error",
            "Description": "In a non-secured cluster, the ResourceManager fails consistently because the TimelineServiceV1Publisher attempts to initialize the TimelineClient with SSLFactory without checking if HTTPS is enabled. This results in a FileNotFoundException for the SSL key store.",
            "StackTrace": [
                "2017-04-26 21:09:10,683 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(1457)) - Error starting ResourceManager",
                "org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "    at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "    at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "    at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)",
                "    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "    at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher.serviceInit(AbstractSystemMetricsPublisher.java:59)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.serviceInit(TimelineServiceV1Publisher.java:67)",
                "    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "    at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)",
                "    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1453)",
                "Caused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "    at java.io.FileInputStream.open0(Native Method)",
                "    at java.io.FileInputStream.open(FileInputStream.java:195)",
                "    at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "    at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:168)",
                "    at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:86)",
                "    at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:219)",
                "    at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:179)",
                "    at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.getSSLFactory(TimelineConnector.java:176)",
                "    at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit(TimelineConnector.java:106)",
                "    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "    ... 11 more"
            ],
            "StepsToReproduce": [
                "1. Set up a non-secured Hadoop cluster.",
                "2. Start the ResourceManager.",
                "3. Observe the logs for errors related to SSL initialization."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any SSL-related errors.",
            "ObservedBehavior": "The ResourceManager fails to start due to a FileNotFoundException for the SSL key store.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4227.json",
        "creation_time": "2015-10-06T04:59:10.000+0000",
        "bug_report": {
            "BugID": "YARN-4227",
            "Title": "NullPointerException in FairScheduler when handling expired containers from removed nodes",
            "Description": "Under certain circumstances, the node is removed before an expired container event is processed, causing the ResourceManager (RM) to exit with a NullPointerException. This issue has been observed in versions 2.3.0, 2.5.0, and 2.6.0 by different customers.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:849)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1273)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:122)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:585)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with FairScheduler enabled.",
                "2. Submit a job that allocates containers.",
                "3. Allow a container to expire (wait for the timeout period).",
                "4. Remove the node hosting the expired container before the RM processes the expired event."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the expired container event gracefully without crashing.",
            "ObservedBehavior": "The ResourceManager exits with a NullPointerException when trying to handle the expired container event.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-2649.json",
        "creation_time": "2014-10-06T22:57:46.000+0000",
        "bug_report": {
            "BugID": "YARN-2649",
            "Title": "Flaky Test Failure in TestAMRMRPCNodeUpdates Due to Incorrect AppAttempt State",
            "Description": "The test 'TestAMRMRPCNodeUpdates' intermittently fails with an assertion error indicating that the AppAttempt state is not correct. The expected state is 'ALLOCATED', but the observed state is 'SCHEDULED'. This issue arises when the SchedulerEventType.NODE_UPDATE is processed before RMAppAttemptEvent.ATTEMPT_ADDED, which can occur due to the test's timing with the AsyncDispatcher.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>",
                "at junit.framework.Assert.fail(Assert.java:50)",
                "at junit.framework.Assert.failNotEquals(Assert.java:287)",
                "at junit.framework.Assert.assertEquals(Assert.java:67)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)",
                "at org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)"
            ],
            "StepsToReproduce": [
                "Run the test suite containing 'TestAMRMRPCNodeUpdates'.",
                "Ensure that the test is executed under conditions where the AsyncDispatcher is customized with CountDownLatch.",
                "Observe the test execution to see if the failure occurs."
            ],
            "ExpectedBehavior": "The AppAttempt state should transition to 'ALLOCATED' after being 'SUBMITTED'.",
            "ObservedBehavior": "The AppAttempt state transitions to 'SCHEDULED' instead of 'ALLOCATED', leading to a test failure.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4288.json",
        "creation_time": "2015-10-22T12:30:16.000+0000",
        "bug_report": {
            "BugID": "YARN-4288",
            "Title": "NodeManager Fails to Retry Registration with ResourceManager During Failover",
            "Description": "When the NodeManager (NM) is restarted, it attempts to register with the ResourceManager (RM) via RPC. If the RM is also restarting at the same time, the NM may encounter connection exceptions, leading to a failure in the registration process. This issue can cause the NM restart to fail, impacting the overall cluster stability.",
            "StackTrace": [
                "2015-08-17 14:35:59,434 ERROR nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:rebootNodeStatusUpdaterAndRegisterWithRM(222)) - Unexpected error rebooting NodeStatusUpdater",
                "java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1473)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)",
                "at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)",
                "Caused by: java.io.IOException: Connection reset by peer",
                "at sun.nio.ch.FileDispatcherImpl.read0(Native Method)",
                "at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)",
                "at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)",
                "at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)",
                "at java.io.FilterInputStream.read(FilterInputStream.java:133)",
                "at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)",
                "at java.io.BufferedInputStream.read(BufferedInputStream.java:254)",
                "at java.io.DataInputStream.readInt(DataInputStream.java:387)",
                "at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1072)",
                "at org.apache.hadoop.ipc.Client$Connection.run(Client.java:967)"
            ],
            "StepsToReproduce": [
                "Restart the NodeManager service.",
                "Simultaneously restart the ResourceManager service.",
                "Observe the logs for connection exceptions during the registration process."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager even if the RM is restarting, by retrying the connection.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager and logs connection reset exceptions, leading to a failed restart.",
            "Resolution": "A fix has been implemented to allow the NodeManager to retry registration with the ResourceManager upon encountering connection failures."
        }
    },
    {
        "filename": "YARN-1032.json",
        "creation_time": "2013-08-05T21:10:46.000+0000",
        "bug_report": {
            "BugID": "YARN-1032",
            "Title": "NullPointerException in RackResolver during Host Address Resolution",
            "Description": "A NullPointerException (NPE) occurs in the RackResolver class when attempting to resolve host addresses, which prevents the rack resolve script from functioning correctly. This issue was observed in the RMContainerAllocator component.",
            "StackTrace": [
                "2013-08-01 07:11:37,708 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)",
                "    at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)",
                "    at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)",
                "    at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)",
                "    at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)",
                "    at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$400(RMContainerAllocator.java:681)",
                "    at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)",
                "    at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)",
                "    at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Deploy the Hadoop YARN application on a Linux environment.",
                "2. Attempt to run a job that requires rack awareness.",
                "3. Monitor the logs for any errors related to RMContainerAllocator."
            ],
            "ExpectedBehavior": "The rack resolve script should successfully resolve the host addresses without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the rack resolve script to fail and preventing the job from executing properly.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-5837.json",
        "creation_time": "2016-11-04T16:06:59.000+0000",
        "bug_report": {
            "BugID": "YARN-5837",
            "Title": "NullPointerException when retrieving status of decommissioned node after ResourceManager restart",
            "Description": "When attempting to retrieve the status of a decommissioned node using the `yarn node -status` command after restarting the ResourceManager, a NullPointerException (NPE) occurs. This issue arises specifically for nodes that are marked with a '-1' identifier after decommissioning.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "at org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)"
            ],
            "StepsToReproduce": [
                "Decommission a node using the command: `yarn node -decommission <node-id>`.",
                "Run the command: `yarn node -list -all` to verify the node is decommissioned.",
                "Restart the ResourceManager using the command: `yarn resourcemanager -restart`.",
                "Attempt to retrieve the status of the decommissioned node using: `yarn node -status <node-id>` where <node-id> is the decommissioned node's ID."
            ],
            "ExpectedBehavior": "The command `yarn node -status <node-id>` should return the status of the decommissioned node without throwing an exception.",
            "ObservedBehavior": "The command `yarn node -status <node-id>` throws a NullPointerException instead of returning the node's status.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6827.json",
        "creation_time": "2017-07-15T05:14:25.000+0000",
        "bug_report": {
            "BugID": "YARN-6827",
            "Title": "[ATS1/1.5] NullPointerException during application recovery in ResourceManager",
            "Description": "While recovering applications, a NullPointerException (NPE) is thrown when attempting to publish to ATSv1. This occurs because active services are started before ATSv1 services in non-HA cases, and in HA cases, the transitionToActive event occurs before ATS services are started. This timing issue leads to active services trying to recover applications that attempt to publish to ATSv1 before it is ready, resulting in an NPE.",
            "StackTrace": [
                "017-07-13 14:08:12,476 ERROR org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher: Error when publishing entity [YARN_APPLICATION,application_1499929227397_0001]",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:178)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.putEntity(TimelineServiceV1Publisher.java:368)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager in non-HA mode.",
                "2. Trigger application recovery.",
                "3. Observe the logs for any NPE during the publishing process."
            ],
            "ExpectedBehavior": "The application should recover without throwing a NullPointerException, and the entities should be published successfully to ATSv1.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to publish entities to ATSv1 during application recovery.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3832.json",
        "creation_time": "2015-06-19T13:31:18.000+0000",
        "bug_report": {
            "BugID": "YARN-3832",
            "Title": "Resource Localization Fails Due to Existing Cache Directories",
            "Description": "Resource localization fails on a cluster with the following error. This issue was observed in the Hadoop 2.7.0 release, which was previously fixed in version 2.6.0 (YARN-2624).",
            "StackTrace": [
                "java.io.IOException: Rename cannot overwrite non empty destination directory /opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39",
                "at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:735)",
                "at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:244)",
                "at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:678)",
                "at org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Deploy a cluster with Hadoop 2.7.0.",
                "2. Attempt to run an application that requires resource localization.",
                "3. Observe the error message related to the AM Container exiting."
            ],
            "ExpectedBehavior": "The application should successfully localize resources without errors.",
            "ObservedBehavior": "The application fails with an error indicating that it cannot overwrite a non-empty destination directory.",
            "Resolution": "Fixed in subsequent releases."
        }
    },
    {
        "filename": "YARN-2409.json",
        "creation_time": "2014-08-12T10:53:06.000+0000",
        "bug_report": {
            "BugID": "YARN-2409",
            "Title": "AsyncDispatcher Thread Leak During Active to StandBy Transition",
            "Description": "When transitioning from Active to StandBy state, the rmDispatcher does not stop, leading to a leak of one AsyncDispatcher thread.",
            "StackTrace": [
                "2014-08-12 07:03:00,839 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "    at java.lang.Thread.run(Thread.java:662)",
                "2014-08-12 07:03:00,839 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at LAUNCHED",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "    at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager in Active state.",
                "2. Trigger a transition to StandBy state.",
                "3. Monitor the AsyncDispatcher threads."
            ],
            "ExpectedBehavior": "The rmDispatcher should stop, and no AsyncDispatcher threads should leak during the transition.",
            "ObservedBehavior": "One AsyncDispatcher thread leaks during the transition from Active to StandBy state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-8116.json",
        "creation_time": "2018-04-04T15:30:52.000+0000",
        "bug_report": {
            "BugID": "YARN-8116",
            "Title": "NodeManager Fails to Start with NumberFormatException",
            "Description": "The NodeManager fails to start after updating the debug delay configuration, resulting in a NumberFormatException due to an empty input string.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"\"",
                "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)",
                "\tat java.lang.Long.parseLong(Long.java:601)",
                "\tat java.lang.Long.parseLong(Long.java:631)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)",
                "\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)"
            ],
            "StepsToReproduce": [
                "1) Update NodeManager debug delay config:",
                "   <property>",
                "       <name>yarn.nodemanager.delete.debug-delay-sec</name>",
                "       <value>350</value>",
                "   </property>",
                "2) Launch distributed shell application multiple times using the command:",
                "   /usr/hdp/current/hadoop-yarn-client/bin/yarn jar hadoop-yarn-applications-distributedshell-*.jar -shell_command \"sleep 120\" -num_containers 1 -shell_env YARN_CONTAINER_RUNTIME_TYPE=docker -shell_env YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=centos/httpd-24-centos7:latest -shell_env YARN_CONTAINER_RUNTIME_DOCKER_DELAYED_REMOVAL=true -jar hadoop-yarn-applications-distributedshell-*.jar",
                "3) Restart NodeManager."
            ],
            "ExpectedBehavior": "NodeManager should start successfully without any exceptions.",
            "ObservedBehavior": "NodeManager fails to start with a NumberFormatException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-8403.json",
        "creation_time": "2018-06-06T22:34:42.000+0000",
        "bug_report": {
            "BugID": "YARN-8403",
            "Title": "Nodemanager logs incorrectly log file download failures at INFO level",
            "Description": "Some of the container execution related stack traces are being logged at INFO or WARN level instead of ERROR level. This can lead to difficulties in diagnosing issues as critical errors are not highlighted appropriately in the logs.",
            "StackTrace": [
                "2018-06-06 03:10:40,077 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:writeCredentials(1312)) - Writing credentials to the nmPrivate file /grid/0/hadoop/yarn/local/nmPrivate/container_e02_1528246317583_0048_01_000001.tokens",
                "2018-06-06 03:10:40,087 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(975)) - Failed to download resource { { hdfs://mycluster.example.com:8020/user/hrt_qa/Streaming/InputDir, 1528254452720, FILE, null },pending,[(container_e02_1528246317583_0048_01_000001)],6074418082915225,DOWNLOADING}",
                "org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed",
                "Caused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)",
                "2018-06-06 03:10:41,547 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(182)) - IOException executing command:",
                "java.io.InterruptedIOException: java.lang.InterruptedException",
                "2018-06-06 03:10:41,548 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:startLocalizer(407)) - Exit code from container container_e02_1528246317583_0048_01_000001 startLocalizer is : -1",
                "java.io.IOException: Application application_1528246317583_0048 initialization failed (exitCode=-1) with output: null"
            ],
            "StepsToReproduce": [
                "1. Start a YARN application that requires downloading resources.",
                "2. Monitor the Nodemanager logs during the execution.",
                "3. Observe the log levels of the stack traces related to file download failures."
            ],
            "ExpectedBehavior": "Critical errors related to file download failures should be logged at ERROR level to ensure they are easily identifiable.",
            "ObservedBehavior": "Critical errors are logged at INFO or WARN level, making it difficult to diagnose issues quickly.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1458.json",
        "creation_time": "2013-11-29T03:31:39.000+0000",
        "bug_report": {
            "BugID": "YARN-1458",
            "Title": "FairScheduler: Zero weight can lead to livelock",
            "Description": "The ResourceManager's SchedulerEventDispatcher's EventProcessor gets blocked when clients submit a large number of jobs. This issue is difficult to reproduce consistently; it has been observed to occur after running the test cluster for several days. The output of the jstack command on the ResourceManager process indicates that the EventProcessor thread is blocked while trying to remove an application from the FairScheduler.",
            "StackTrace": [
                "\"ResourceManager Event Processor\" prio=10 tid=0x00002aaab0c5f000 nid=0x5dd3 waiting for monitor entry [0x0000000043aa9000]",
                "java.lang.Thread.State: BLOCKED (on object monitor)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplication(FairScheduler.java:671)",
                "- waiting to lock <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1023)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:440)",
                "at java.lang.Thread.run(Thread.java:744)",
                "\"FairSchedulerUpdateThread\" daemon prio=10 tid=0x00002aaab0a2c800 nid=0x5dc8 runnable [0x00000000433a2000]",
                "java.lang.Thread.State: RUNNABLE",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getAppWeight(FairScheduler.java:545)",
                "- locked <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.getWeights(AppSchedulable.java:129)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShare(ComputeFairShares.java:143)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.resourceUsedWithWeightToResourceRatio(ComputeFairShares.java:131)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShares(ComputeFairShares.java:102)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy.computeShares(FairSharePolicy.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.recomputeShares(FSLeafQueue.java:100)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.recomputeShares(FSParentQueue.java:62)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:282)",
                "- locked <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:255)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN cluster with FairScheduler.",
                "2. Submit a large number of jobs simultaneously from multiple clients.",
                "3. Monitor the ResourceManager for any blocked threads."
            ],
            "ExpectedBehavior": "The ResourceManager should handle job submissions without blocking, allowing for smooth operation and scheduling of jobs.",
            "ObservedBehavior": "The ResourceManager's EventProcessor thread becomes blocked, leading to potential livelock situations where job submissions are not processed in a timely manner.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8209.json",
        "creation_time": "2018-04-26T00:22:23.000+0000",
        "bug_report": {
            "BugID": "YARN-8209",
            "Title": "NullPointerException in DeletionService during Docker container deletion",
            "Description": "A NullPointerException is thrown in the DeletionService when attempting to delete a Docker container. This issue occurs in the Hadoop YARN framework, specifically within the DockerCommandExecutor class.",
            "StackTrace": [
                "2018-04-25 23:38:41,039 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread DeletionService #1:",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile(DockerClient.java:109)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand(DockerCommandExecutor.java:85)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeStatusCommand(DockerCommandExecutor.java:192)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.getContainerStatus(DockerCommandExecutor.java:128)",
                "    at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer(LinuxContainerExecutor.java:935)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask.run(DockerContainerDeletionTask.java:61)",
                "    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "    at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN environment with Docker integration.",
                "2. Attempt to delete a Docker container using the YARN DeletionService.",
                "3. Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The Docker container should be deleted without any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the deletion process to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3804.json",
        "creation_time": "2015-06-15T08:54:42.000+0000",
        "bug_report": {
            "BugID": "YARN-3804",
            "Title": "ResourceManager Stuck in Standby State Due to Insufficient ACL Permissions",
            "Description": "When configuring a YARN cluster in secure mode, both ResourceManagers (RMs) remain in standby state indefinitely if the Kerberos user is not included in the yarn.admin.acl. This leads to an infinite retry loop when attempting to switch to active state.",
            "StackTrace": [
                "2015-06-15 12:20:21,556 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn     OPERATION=refreshAdminAcls      TARGET=AdminService     RESULT=FAILURE  DESCRIPTION=Unauthorized userPERMISSIONS=",
                "2015-06-15 12:20:21,556 WARN org.apache.hadoop.ha.ActiveStandbyElector: Exception handling the winning of election",
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:645)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:518)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Can not execute refreshAdminAcls",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:297)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "    ... 4 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "    at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:230)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAdminAcls(AdminService.java:465)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:295)",
                "    ... 5 more",
                "Caused by: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "    at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:182)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:148)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAccess(AdminService.java:223)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:228)",
                "    ... 7 more"
            ],
            "StepsToReproduce": [
                "1. Configure the YARN cluster in secure mode.",
                "2. Set yarn.admin.acl to 'dsperf' in ResourceManager (RM) configuration.",
                "3. Set yarn.resourcemanager.principal to 'yarn'.",
                "4. Start both ResourceManagers."
            ],
            "ExpectedBehavior": "The ResourceManager should transition to active state after a few retries or at least on the first attempt, shutting down if the user does not have the necessary permissions.",
            "ObservedBehavior": "Both ResourceManagers remain in standby state indefinitely due to insufficient ACL permissions for the user 'yarn'.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1839.json",
        "creation_time": "2014-03-14T23:52:29.000+0000",
        "bug_report": {
            "BugID": "YARN-1839",
            "Title": "InvalidToken Exception when AM attempt 2 is preempted by Capacity Scheduler",
            "Description": "When using a single-node cluster with capacity scheduler preemption enabled, running multiple MapReduce sleep jobs can lead to an InvalidToken exception for the Application Master (AM) when it attempts to launch a task container after being preempted.",
            "StackTrace": [
                "2014-03-13 20:13:50,254 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1394741557066_0001_m_000000_1009: Container launch failed for container_1394741557066_0001_02_000021 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Use a single-node cluster.",
                "2. Enable capacity scheduler preemption.",
                "3. Run a MapReduce sleep job as application 1.",
                "4. Take the entire cluster.",
                "5. Run another MapReduce sleep job as application 2.",
                "6. Preempt application 1 out.",
                "7. Wait until application 2 finishes.",
                "8. Observe the logs for application 1's AM attempt 2."
            ],
            "ExpectedBehavior": "The Application Master should successfully launch task containers after being preempted.",
            "ObservedBehavior": "The Application Master fails to launch task containers with an InvalidToken exception.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6714.json",
        "creation_time": "2017-06-15T09:56:15.000+0000",
        "bug_report": {
            "BugID": "YARN-6714",
            "Title": "IllegalStateException on APP_ATTEMPT_REMOVED Event in Async-Scheduling Mode of CapacityScheduler",
            "Description": "In async-scheduling mode of CapacityScheduler, after Application Master (AM) failover and unreserving all reserved containers, there is a chance to commit an outdated reserve proposal of the failed application attempt. This issue occurs when an application stops, unreserves all reserved containers, and compares the appAttemptId with the current appAttemptId. If they do not match, it throws an IllegalStateException, causing the ResourceManager (RM) to crash.",
            "StackTrace": [
                "2017-06-08 11:02:24,339 FATAL [ResourceManager Event Processor] org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.IllegalStateException: Trying to unreserve for application appattempt_1495188831758_0121_000002 when currently reserved for application application_1495188831758_0121 on node host: node1:45454 #containers=2 available=... used=...",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:152)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "StepsToReproduce": [
                "Enable async-scheduling mode in CapacityScheduler.",
                "Submit an application and allow it to reserve containers.",
                "Force a failover of the Application Master (AM).",
                "Unreserve all reserved containers for the application.",
                "Observe the logs for IllegalStateException."
            ],
            "ExpectedBehavior": "The system should handle the unreservation of containers without throwing an IllegalStateException, allowing the ResourceManager to continue functioning normally.",
            "ObservedBehavior": "The system throws an IllegalStateException, causing the ResourceManager to crash.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3351.json",
        "creation_time": "2015-03-16T14:19:59.000+0000",
        "bug_report": {
            "BugID": "YARN-3351",
            "Title": "AppMaster Tracking URL Broken in High Availability (HA) Mode",
            "Description": "After the implementation of YARN-2713, the AppMaster tracking URL is broken when running in High Availability (HA) mode. This issue occurs when the first ResourceManager (RM) is not active, leading to a failure in binding the socket for the proxy link.",
            "StackTrace": [
                "2015-02-05 20:47:43,478 WARN org.mortbay.log: /proxy/application_1423182188062_0002/: java.net.BindException: Cannot assign requested address",
                "java.net.BindException: Cannot assign requested address",
                "at java.net.PlainSocketImpl.socketBind(Native Method)",
                "at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)",
                "at java.net.Socket.bind(Socket.java:631)",
                "at java.net.Socket.<init>(Socket.java:423)",
                "at java.net.Socket.<init>(Socket.java:280)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)",
                "at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)"
            ],
            "StepsToReproduce": [
                "Set up ResourceManager (RM) High Availability (HA) mode.",
                "Ensure that the first ResourceManager is not active.",
                "Run a long sleep job.",
                "View the tracking URL on the ResourceManager applications page."
            ],
            "ExpectedBehavior": "The AppMaster tracking URL should be accessible and display the job tracking information correctly.",
            "ObservedBehavior": "The AppMaster tracking URL is broken, resulting in a java.net.BindException indicating that the requested address cannot be assigned.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2813.json",
        "creation_time": "2014-11-05T22:29:46.000+0000",
        "bug_report": {
            "BugID": "YARN-2813",
            "Title": "NullPointerException in MemoryTimelineStore.getDomains",
            "Description": "A NullPointerException occurs when accessing the getDomains method in the MemoryTimelineStore class, leading to an INTERNAL_SERVER_ERROR response.",
            "StackTrace": [
                "2014-11-04 20:50:05,146 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "    at java.lang.reflect.Method.invoke(Method.java:606)",
                "    at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "    at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "    at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "    at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "    at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "    at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "    at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "    at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "    at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "    at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "    at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:96)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:572)",
                "    at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:269)",
                "    at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:542)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1204)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "    at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "    at org.mortbay.jetty.Server.handle(Server.java:326)",
                "    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "    at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)",
                "    at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:713)",
                "    at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)",
                "    at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)",
                "    at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:353)",
                "    ... 54 more"
            ],
            "StepsToReproduce": [
                "1. Start the YARN Timeline Server.",
                "2. Attempt to access the timeline domains via the API.",
                "3. Observe the server response."
            ],
            "ExpectedBehavior": "The API should return a list of timeline domains without errors.",
            "ObservedBehavior": "An INTERNAL_SERVER_ERROR is returned due to a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1550.json",
        "creation_time": "2013-12-30T03:58:32.000+0000",
        "bug_report": {
            "BugID": "YARN-1550",
            "Title": "Null Pointer Exception in FairSchedulerAppsBlock#render",
            "Description": "A Null Pointer Exception occurs when rendering the FairSchedulerAppsBlock in the YARN web interface. This issue arises when attempting to submit an application that is already present in the ResourceManager's application context.",
            "StackTrace": [
                "2013-12-30 11:51:43,795 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/scheduler",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock.render(FairSchedulerAppsBlock.java:96)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:66)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:76)"
            ],
            "StepsToReproduce": [
                "1. Debug at RMAppManager#submitApplication after the following code:\n   if (rmContext.getRMApps().putIfAbsent(applicationId, application) != null) {\n       String message = \"Application with id \" + applicationId + \" is already present! Cannot add a duplicate!\";\n       LOG.warn(message);\n       throw RPCUtil.getRemoteException(message);\n   }",
                "2. Submit an application using the command: hadoop jar ~/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.0.0-ydh2.2.0-tests.jar sleep -Dhadoop.job.ugi=test2,#111111 -Dmapreduce.job.queuename=p1 -m 1 -mt 1 -r 1",
                "3. Navigate to the page: http://ip:50030/cluster/scheduler and observe the 500 ERROR."
            ],
            "ExpectedBehavior": "The FairSchedulerAppsBlock should render the application details without throwing an error.",
            "ObservedBehavior": "A 500 ERROR is displayed on the scheduler page due to a Null Pointer Exception.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5006.json",
        "creation_time": "2016-04-28T08:26:38.000+0000",
        "bug_report": {
            "BugID": "YARN-5006",
            "Title": "ResourceManager Crashes Due to ApplicationStateData Exceeding Znode Size Limit",
            "Description": "When a client submits a job that adds a large number of files (10,000) into the DistributedCache, the ResourceManager attempts to store the ApplicationStateData into Zookeeper. If the ApplicationStateData exceeds the znode size limit, the ResourceManager crashes with a ConnectionLossException.",
            "StackTrace": [
                "2016-04-20 11:26:35,732 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore AsyncDispatcher event handler: Maxed out ZK retries. Giving up!",
                "2016-04-20 11:26:35,732 ERROR org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore AsyncDispatcher event handler: Error storing app: application_1461061795989_17671",
                "org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:936)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1075)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1096)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:947)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.createWithRetries(ZKRMStateStore.java:956)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:626)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:138)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:123)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:860)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:855)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "Submit a job that adds 10,000 files to the DistributedCache.",
                "Monitor the ResourceManager logs for errors related to ApplicationStateData storage."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully store the ApplicationStateData without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a ConnectionLossException when the ApplicationStateData exceeds the znode size limit.",
            "Resolution": "The issue has been fixed in version 2.9.0."
        }
    },
    {
        "filename": "YARN-5728.json",
        "creation_time": "2016-10-13T05:16:28.000+0000",
        "bug_report": {
            "BugID": "YARN-5728",
            "Title": "Timeout in TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization",
            "Description": "The test TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization is failing due to a timeout after 60 seconds. This issue occurs during the execution of the test case, leading to a failure in the build process.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 60000 milliseconds",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.processWaitTimeAndRetryInfo(RetryInvocationHandler.java:130)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:107)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy85.nodeHeartbeat(Unknown Source)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:113)"
            ],
            "StepsToReproduce": [
                "Run the test suite for TestMiniYARNClusterNodeUtilization.",
                "Observe the execution of the test case testUpdateNodeUtilization."
            ],
            "ExpectedBehavior": "The test TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization should complete successfully without timing out.",
            "ObservedBehavior": "The test fails with a timeout exception after 60 seconds.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2805.json",
        "creation_time": "2014-11-04T20:37:09.000+0000",
        "bug_report": {
            "BugID": "YARN-2805",
            "Title": "ResourceManager Fails to Start Due to Kerberos Login Issue in HA Setup",
            "Description": "The ResourceManager (RM2) in a High Availability (HA) setup attempts to log in using the Kerberos principal of RM1, leading to a failure during startup. This issue prevents the ResourceManager from initializing properly.",
            "StackTrace": [
                "2014-11-04 08:41:10,636 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service ResourceManager failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)",
                "Caused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)"
            ],
            "StepsToReproduce": [
                "Set up a High Availability (HA) configuration for ResourceManager.",
                "Ensure that RM1 is configured with a Kerberos principal.",
                "Attempt to start RM2."
            ],
            "ExpectedBehavior": "ResourceManager should start successfully without any login errors.",
            "ObservedBehavior": "ResourceManager fails to start with a login failure message indicating it is trying to use RM1's Kerberos principal.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4744.json",
        "creation_time": "2016-02-29T10:08:57.000+0000",
        "bug_report": {
            "BugID": "YARN-4744",
            "Title": "Container Failure Due to Excessive Signals in LCE Mode",
            "Description": "When running a MapReduce application (terasort/teragen) in a High Availability (HA) cluster configured in secure mode with Linux Container Executor (LCE) enabled, the application fails with an excessive signal error. This issue occurs when submitting the application with the user 'yarn' or 'dsperf'.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=9:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer(DefaultLinuxContainerRuntime.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.signalContainer(DelegatingLinuxContainerRuntime.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.signalContainer(LinuxContainerExecutor.java:513)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: ExitCodeException exitCode=9:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:150)",
                "... 9 more"
            ],
            "StepsToReproduce": [
                "1. Install a High Availability (HA) cluster in secure mode.",
                "2. Enable Linux Container Executor (LCE) with cgroups.",
                "3. Start the server with the 'dsperf' user.",
                "4. Submit a MapReduce application (terasort/teragen) using the 'yarn' or 'dsperf' user.",
                "5. Observe the failure due to excessive signals."
            ],
            "ExpectedBehavior": "The MapReduce application should run successfully without any container failure.",
            "ObservedBehavior": "The application fails with an error indicating 'Too many signals to container'.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1752.json",
        "creation_time": "2014-02-22T05:51:42.000+0000",
        "bug_report": {
            "BugID": "YARN-1752",
            "Title": "Invalid State Transition Error on Application Attempt Launch",
            "Description": "An error occurs when an application attempt is launched, resulting in an invalid state transition. The system cannot handle the 'UNREGISTERED' event at the 'LAUNCHED' state.",
            "StackTrace": [
                "2014-02-21 14:56:03,453 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED",
                "  at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "  at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "  at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)",
                "  at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "  at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "  at java.lang.Thread.run(Thread.java:695)"
            ],
            "StepsToReproduce": [
                "1. Launch an application attempt in the Hadoop YARN environment.",
                "2. Trigger an event that causes the application to transition to the 'UNREGISTERED' state.",
                "3. Observe the logs for any errors related to state transitions."
            ],
            "ExpectedBehavior": "The application attempt should handle state transitions correctly without throwing an error.",
            "ObservedBehavior": "An error is logged indicating that the system cannot handle the 'UNREGISTERED' event at the 'LAUNCHED' state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-6629.json",
        "creation_time": "2017-05-22T08:31:16.000+0000",
        "bug_report": {
            "BugID": "YARN-6629",
            "Title": "NullPointerException during Container Allocation Proposal Application",
            "Description": "A NullPointerException (NPE) occurs when a container allocation proposal is applied, but its resource requests are removed beforehand. This issue was discovered while writing a test case for another problem in branch-2.",
            "StackTrace": [
                "FATAL event.EventDispatcher (EventDispatcher.java:run(75)) - Error in handling event type NODE_UPDATE to the Event Dispatcher",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:446)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply(FiCaSchedulerApp.java:516)",
                "at org.apache.hadoop.yarn.client.TestNegativePendingResource$1.answer(TestNegativePendingResource.java:225)",
                "at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:31)",
                "at org.mockito.internal.MockHandler.handle(MockHandler.java:97)",
                "at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp$$EnhancerByMockitoWithCGLIB$$29eb8afc.apply(<generated>)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit(CapacityScheduler.java:2396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.submitResourceCommitRequest(CapacityScheduler.java:2281)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1247)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1236)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1325)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:987)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1367)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:143)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the Application Master (AM) and request 1 container with schedulerRequestKey#1.",
                "2. The scheduler allocates 1 container for this request and accepts the proposal.",
                "3. The AM removes this request.",
                "4. The scheduler applies this proposal, which leads to the NPE when calling schedulerKeyToPlacementSets.get(schedulerRequestKey).allocate(schedulerKey, type, node)."
            ],
            "ExpectedBehavior": "The container allocation proposal should be applied without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the application of the container allocation proposal.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-3493.json",
        "creation_time": "2015-04-15T22:03:19.000+0000",
        "bug_report": {
            "BugID": "YARN-3493",
            "Title": "ResourceManager Fails to Start with Invalid Memory Configuration",
            "Description": "The ResourceManager (RM) fails to start when memory settings are changed in the yarn-site.xml file. This occurs when the maximum allocation memory is set lower than the memory requested by a running job.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)"
            ],
            "StepsToReproduce": [
                "1. Change yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb to 4000 in yarn-site.xml.",
                "2. Start a random text writer job with mapreduce.map.memory.mb=4000 in the background and wait for the job to reach the running state.",
                "3. Restore yarn-site.xml to have yarn.scheduler.maximum-allocation-mb set to 2048 before the above job completes.",
                "4. Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without errors.",
            "ObservedBehavior": "The ResourceManager fails to start with an error indicating an invalid resource request due to memory configuration.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7645.json",
        "creation_time": "2017-12-12T21:19:53.000+0000",
        "bug_report": {
            "BugID": "YARN-7645",
            "Title": "Flaky Test: TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers Fails with FairScheduler",
            "Description": "The test {{TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers}} exhibits flakiness when using the {{FairScheduler}}. The test fails intermittently, leading to inconsistent results.",
            "StackTrace": [
                "java.lang.AssertionError: Attempt state is not correct (timeout). expected:<ALLOCATED> but was:<SCHEDULED>",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.amRestartTests(TestContainerResourceUsage.java:275)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.testUsageAfterAMRestartWithMultipleContainers(TestContainerResourceUsage.java:254)"
            ],
            "StepsToReproduce": [
                "1. Set up a Hadoop YARN environment with FairScheduler enabled.",
                "2. Run the test suite that includes TestContainerResourceUsage.",
                "3. Observe the results of the test TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers."
            ],
            "ExpectedBehavior": "The test should pass without any assertion errors, indicating that the attempt state is correctly set to ALLOCATED.",
            "ObservedBehavior": "The test fails intermittently with an AssertionError indicating that the expected state was ALLOCATED but the actual state was SCHEDULED.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6054.json",
        "creation_time": "2017-01-04T20:58:59.000+0000",
        "bug_report": {
            "BugID": "YARN-6054",
            "Title": "TimelineServer Fails to Start Due to Missing LevelDb State Files",
            "Description": "The TimelineServer fails to start when certain LevelDb state files are missing, leading to a service state exception. This issue results in the inability to initialize the ApplicationHistoryServer, which is critical for tracking application history in Hadoop YARN.",
            "StackTrace": [
                "2016-11-21 20:46:43,134 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer failed in state INITED; cause: org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:182)",
                "Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)",
                "at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)",
                "at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)",
                "at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:229)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "... 5 more",
                "2016-11-21 20:46:43,136 INFO org.apache.hadoop.util.ExitUtil: Exiting with status -1"
            ],
            "StepsToReproduce": [
                "1. Ensure that the TimelineServer is configured correctly.",
                "2. Delete or move some LevelDb state files from the expected directory.",
                "3. Attempt to start the TimelineServer."
            ],
            "ExpectedBehavior": "The TimelineServer should start successfully, even if some state files are missing, and should handle the situation gracefully.",
            "ObservedBehavior": "The TimelineServer fails to start and throws a ServiceStateException due to missing LevelDb state files.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-196.json",
        "creation_time": "2012-01-16T09:52:45.000+0000",
        "bug_report": {
            "BugID": "YARN-196",
            "Title": "NodeManager Shutdown on ResourceManager Connection Failure",
            "Description": "When the NodeManager (NM) is started before the ResourceManager (RM), it shuts down with an error indicating a failure to connect to the RM. This behavior needs to be addressed to improve the robustness of the NodeManager.",
            "StackTrace": [
                "ERROR org.apache.hadoop.yarn.service.CompositeService: Error starting services org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)",
                "... 3 more",
                "Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:131)",
                "at $Proxy23.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "... 5 more",
                "Caused by: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:857)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1141)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1100)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:128)",
                "... 7 more",
                "Caused by: java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)",
                "at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:659)",
                "at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:469)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:563)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:211)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1247)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1117)",
                "... 9 more"
            ],
            "StepsToReproduce": [
                "Start the NodeManager service.",
                "Attempt to start the ResourceManager service after the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should handle the connection failure gracefully without shutting down.",
            "ObservedBehavior": "The NodeManager shuts down with an error indicating it cannot connect to the ResourceManager.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-8508.json",
        "creation_time": "2018-07-09T23:37:49.000+0000",
        "bug_report": {
            "BugID": "YARN-8508",
            "Title": "GPU Resource Allocation Failure on NodeManager Cleanup",
            "Description": "The GPU fails to release even though the container using it is being killed, leading to resource allocation issues for new containers requesting GPUs.",
            "StackTrace": [
                "2018-07-06 05:22:39,048 ERROR nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:handleLaunchForLaunchType(550)) - ResourceHandlerChain.preStart() failed!",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.internalAssignGpus(GpuResourceAllocator.java:225)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.assignGpus(GpuResourceAllocator.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl.preStart(GpuResourceHandlerImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.preStart(ResourceHandlerChain.java:75)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:509)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:479)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:494)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:306)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:103)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Launch a container that requires GPU resources.",
                "2. Kill the container while it is running.",
                "3. Attempt to launch a new container that requests more GPUs than are available."
            ],
            "ExpectedBehavior": "The GPU resources should be released after the container is killed, allowing new containers to launch successfully.",
            "ObservedBehavior": "The GPU resources are not released, resulting in an error when trying to launch a new container that requests more GPUs than are available.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-2308.json",
        "creation_time": "2014-07-17T10:01:57.000+0000",
        "bug_report": {
            "BugID": "YARN-2308",
            "Title": "NullPointerException during ResourceManager restart after queue configuration change",
            "Description": "A NullPointerException (NPE) occurs when the ResourceManager (RM) is restarted after modifying the CapacityScheduler queue configuration. This issue arises when queues are removed or added, leading to failures in recovering historical applications associated with the removed queues.",
            "StackTrace": [
                "2014-07-16 07:22:46,957 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "    at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Modify the CapacityScheduler queue configuration by removing some queues and adding new ones.",
                "2. Restart the ResourceManager.",
                "3. Observe the logs for any NullPointerException errors."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully and recover all historical applications without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to restart and throws a NullPointerException due to missing queue configurations.",
            "Resolution": "Fixed in version 2.6.0"
        }
    },
    {
        "filename": "YARN-933.json",
        "creation_time": "2013-07-17T12:29:28.000+0000",
        "bug_report": {
            "BugID": "YARN-933",
            "Title": "InvalidStateTransitionException during Application Master retries",
            "Description": "When the Application Master (AM) attempts to retry after a connection loss, it encounters an InvalidStateTransitionException. This occurs despite the maximum retries being configured correctly on both the client and Resource Manager (RM) sides.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: LAUNCH_FAILED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:630)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:99)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:495)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:476)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Step 1: Install cluster with Node Managers (NM) on 2 machines.",
                "Step 2: Ping from the RM machine to NM1 machine using IP (should succeed), but using hostname should fail.",
                "Step 3: Execute a job.",
                "Step 4: After AppAttempt_1 allocation to NM1 machine is done, simulate a connection loss."
            ],
            "ExpectedBehavior": "The Application Master should handle the connection loss gracefully and not throw an InvalidStateTransitionException.",
            "ObservedBehavior": "After AppAttempt_1 fails, the RM attempts to launch AppAttempt_1 again, resulting in an InvalidStateTransitionException. The client exits even though the job is still running.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1374.json",
        "creation_time": "2013-10-30T11:49:49.000+0000",
        "bug_report": {
            "BugID": "YARN-1374",
            "Title": "Resource Manager Fails to Start Due to ConcurrentModificationException",
            "Description": "The Resource Manager fails to start, throwing a ConcurrentModificationException during initialization. This issue prevents the Resource Manager from functioning properly, impacting the overall operation of the Hadoop YARN system.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)",
                "at java.util.AbstractList$Itr.next(AbstractList.java:343)",
                "at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)"
            ],
            "StepsToReproduce": [
                "Attempt to start the Resource Manager service.",
                "Monitor the logs for any exceptions or errors during the startup process."
            ],
            "ExpectedBehavior": "The Resource Manager should start successfully without any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start and logs a ConcurrentModificationException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-174.json",
        "creation_time": "2012-10-19T17:25:40.000+0000",
        "bug_report": {
            "BugID": "YARN-174",
            "Title": "NodeManager Fails to Start Due to Invalid Log Directory Path",
            "Description": "The NodeManager fails to start with a fatal error indicating that the log directory path is invalid. This issue is causing unit tests to exit unexpectedly, making it difficult to track down the root cause of the failure.",
            "StackTrace": [
                "2012-10-19 12:18:23,941 FATAL [Node Status Updater] nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(277)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService.init(NodeHealthCheckerService.java:48)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged(NodeManager.java:256)",
                "at org.apache.hadoop.yarn.service.AbstractService.changeState(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:112)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.reboot(NodeStatusUpdaterImpl.java:157)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$900(NodeStatusUpdaterImpl.java:63)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:357)"
            ],
            "StepsToReproduce": [
                "1. Set the yarn.log.dir property to an invalid path.",
                "2. Start the NodeManager.",
                "3. Observe the logs for any fatal errors."
            ],
            "ExpectedBehavior": "NodeManager should start successfully without any fatal errors.",
            "ObservedBehavior": "NodeManager fails to start with a fatal error regarding the log directory path.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-6448.json",
        "creation_time": "2017-04-05T18:39:49.000+0000",
        "bug_report": {
            "BugID": "YARN-6448",
            "Title": "Continuous Scheduling Thread Crashes Due to Comparison Method Violation",
            "Description": "The continuous scheduling thread in YARN crashes when sorting nodes due to a violation of the comparison method's general contract. This issue arises when the lock is removed in continuous scheduling while sorting nodes, leading to inconsistent comparisons if nodes change during sorting.",
            "StackTrace": [
                "2017-04-04 23:42:26,123 FATAL org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler: Critical thread FairSchedulerContinuousScheduling crashed!",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:899)",
                "at java.util.TimSort.mergeAt(TimSort.java:516)",
                "at java.util.TimSort.mergeForceCollapse(TimSort.java:457)",
                "at java.util.TimSort.sort(TimSort.java:254)",
                "at java.util.Arrays.sort(Arrays.java:1512)",
                "at java.util.ArrayList.sort(ArrayList.java:1454)",
                "at java.util.Collections.sort(Collections.java:175)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN resource manager.",
                "2. Enable continuous scheduling.",
                "3. Modify the node list while the scheduler is sorting nodes.",
                "4. Observe the crash in the continuous scheduling thread."
            ],
            "ExpectedBehavior": "The continuous scheduling thread should sort nodes without crashing, maintaining the integrity of the comparison method.",
            "ObservedBehavior": "The continuous scheduling thread crashes with an IllegalArgumentException due to a violation of the comparison method's general contract.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-4530.json",
        "creation_time": "2015-12-30T15:19:19.000+0000",
        "bug_report": {
            "BugID": "YARN-4530",
            "Title": "NullPointerException Triggered by LocalizedResource Download Failure in NodeManager",
            "Description": "In our cluster, a failure in downloading LocalizedResource caused a NullPointerException (NPE), leading to the shutdown of the NodeManager. This issue occurs when the resource's expected state does not match the actual state on the source filesystem.",
            "StackTrace": [
                "java.lang.NullPointerException at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)"
            ],
            "StepsToReproduce": [
                "1. Attempt to download a LocalizedResource in the Hadoop YARN environment.",
                "2. Ensure that the resource's state on the source filesystem changes unexpectedly during the download process.",
                "3. Observe the NodeManager logs for errors."
            ],
            "ExpectedBehavior": "The NodeManager should handle resource download failures gracefully without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when a resource download fails due to a state mismatch.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7737.json",
        "creation_time": "2018-01-11T19:35:01.000+0000",
        "bug_report": {
            "BugID": "YARN-7737",
            "Title": "FileNotFoundException for prelaunch.err on Container Failure",
            "Description": "An exception occurs when a container fails due to the absence of the prelaunch error log file. The error message indicates that the file does not exist, which leads to a failure in retrieving the container's prelaunch error log.",
            "StackTrace": [
                "2018-01-11 19:04:08,036 ERROR org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Failed to get tail of the container's prelaunch error log file",
                "java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Launch a container using the YARN framework.",
                "2. Simulate a failure in the container.",
                "3. Check the logs for the container's prelaunch error log."
            ],
            "ExpectedBehavior": "The prelaunch error log file should be created and accessible, allowing the system to retrieve the error log without exceptions.",
            "ObservedBehavior": "The system throws a FileNotFoundException indicating that the prelaunch.err file does not exist, leading to a failure in logging the container's prelaunch errors.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-5136.json",
        "creation_time": "2016-05-24T15:34:28.000+0000",
        "bug_report": {
            "BugID": "YARN-5136",
            "Title": "IllegalStateException when handling APP_ATTEMPT_REMOVED event in ResourceManager",
            "Description": "An IllegalStateException occurs in the ResourceManager when attempting to handle the APP_ATTEMPT_REMOVED event. This issue arises after moving the application, leading to the ResourceManager exiting unexpectedly.",
            "StackTrace": [
                "2016-05-24 23:20:47,202 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-05-24 23:20:47,202 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_e04_1464073905025_15410_01_001759 Container Transitioned from ACQUIRED to RELEASED",
                "2016-05-24 23:20:47,202 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with the Fair Scheduler.",
                "2. Submit an application to the ResourceManager.",
                "3. Move the application to a different state that triggers the APP_ATTEMPT_REMOVED event.",
                "4. Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the APP_ATTEMPT_REMOVED event without throwing an exception and continue running.",
            "ObservedBehavior": "The ResourceManager throws an IllegalStateException and exits unexpectedly when handling the APP_ATTEMPT_REMOVED event.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8211.json",
        "creation_time": "2018-04-26T02:13:22.000+0000",
        "bug_report": {
            "BugID": "YARN-8211",
            "Title": "BufferUnderflowException in Yarn Registry DNS Server",
            "Description": "The Yarn registry DNS server is consistently encountering a BufferUnderflowException during operation, which affects its ability to process requests properly.",
            "StackTrace": [
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(50)) - Execution exception when running task in RegistryDNS 76",
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread RegistryDNS 76:",
                "java.nio.BufferUnderflowException",
                "    at java.nio.Buffer.nextGetIndex(Buffer.java:500)",
                "    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)",
                "    at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)",
                "    at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)",
                "    at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)",
                "    at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "    at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "Start the Yarn registry DNS server.",
                "Perform a port ping operation.",
                "Monitor the logs for any exceptions."
            ],
            "ExpectedBehavior": "The Yarn registry DNS server should handle port ping operations without throwing exceptions.",
            "ObservedBehavior": "The Yarn registry DNS server throws a BufferUnderflowException during port ping operations, leading to potential service disruptions.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2124.json",
        "creation_time": "2014-06-05T07:44:27.000+0000",
        "bug_report": {
            "BugID": "YARN-2124",
            "Title": "NullPointerException in ProportionalCapacityPreemptionPolicy Initialization",
            "Description": "When using the scheduler with preemption, the ProportionalCapacityPreemptionPolicy fails to work, resulting in a NullPointerException (NPE) when the ResourceManager starts. This issue occurs because the ProportionalCapacityPreemptionPolicy is initialized before the CapacityScheduler, leading to a null ResourceCalculator.",
            "StackTrace": [
                "2014-06-05 11:01:33,201 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[SchedulingMonitor (ProportionalCapacityPreemptionPolicy),5,main] threw an Exception.",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)",
                "    at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager with the scheduler configured for preemption.",
                "2. Observe the logs for any exceptions thrown during initialization."
            ],
            "ExpectedBehavior": "The ProportionalCapacityPreemptionPolicy should initialize correctly without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the initialization of the ProportionalCapacityPreemptionPolicy, causing it to fail.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7849.json",
        "creation_time": "2018-01-29T23:49:33.000+0000",
        "bug_report": {
            "BugID": "YARN-7849",
            "Title": "TestMiniYarnClusterNodeUtilization#testUpdateNodeUtilization Fails Due to Heartbeat Sync Error",
            "Description": "The test case `testUpdateNodeUtilization` in `TestMiniYarnClusterNodeUtilization` is failing due to an assertion error indicating that container utilization is not propagated to RMNode as expected.",
            "StackTrace": [
                "java.lang.AssertionError: Containers Utilization not propagated to RMNode expected:<<pmem:1024, vmem:2048, vCores:11.0>> but was:<null>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.verifySimulatedUtilization(TestMiniYarnClusterNodeUtilization.java:227)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:116)"
            ],
            "StepsToReproduce": [
                "Run the test suite for `TestMiniYarnClusterNodeUtilization`.",
                "Observe the failure in the `testUpdateNodeUtilization` test case."
            ],
            "ExpectedBehavior": "The container utilization should be correctly propagated to RMNode with expected values: <<pmem:1024, vmem:2048, vCores:11.0>>.",
            "ObservedBehavior": "The test fails with an assertion error indicating that the expected utilization values are not propagated, resulting in a null value.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-8591.json",
        "creation_time": "2018-07-27T05:56:26.000+0000",
        "bug_report": {
            "BugID": "YARN-8591",
            "Title": "[ATSv2] Null Pointer Exception when checking entity ACL in non-secure cluster",
            "Description": "A Null Pointer Exception occurs in the TimelineReaderWebServices when attempting to access entity ACLs in a non-secure cluster environment. This issue leads to an internal server error response.",
            "StackTrace": [
                "2018-07-27 05:32:03,468 WARN  webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter.doFilter(TimelineReaderWhitelistAuthorizationFilter.java:85)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)",
                "at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)",
                "at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
                "at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:534)",
                "at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)",
                "at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)",
                "at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)",
                "at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)",
                "at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)",
                "at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)",
                "at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccess(TimelineReaderWebServices.java:3536)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccessForGenericEntities(TimelineReaderWebServices.java:3513)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:622)"
            ],
            "StepsToReproduce": [
                "1. Set up a non-secure Hadoop YARN cluster.",
                "2. Send a GET request to the TimelineReaderWebServices endpoint for entity ACLs.",
                "3. Observe the server response."
            ],
            "ExpectedBehavior": "The server should return the entity ACLs without throwing an exception.",
            "ObservedBehavior": "The server throws a Null Pointer Exception, resulting in an internal server error.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6649.json",
        "creation_time": "2017-05-25T20:36:08.000+0000",
        "bug_report": {
            "BugID": "YARN-6649",
            "Title": "500 Internal Server Error in Tez UI due to Object Decoding Failure",
            "Description": "When using the Tez UI, which makes REST API calls to the Timeline Service REST API, some calls return a 500 Internal Server Error. The root cause is related to YARN-6654, which involves handling object decoding to prevent internal server errors and instead respond with a partial message.",
            "StackTrace": [
                "2017-05-30 12:47:10,670 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)",
                "at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:636)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:294)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:588)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:95)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1352)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.util.FSTUtil.rethrow(FSTUtil.java:122)",
                "at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:879)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:414)",
                "at org.apache.hadoop.yarn.server.timeline.EntityFileTimelineStore.getEntity(EntityFileTimelineStore.java:911)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.doGetEntity(TimelineDataManager.java:215)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getEntity(TimelineDataManager.java:202)",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:155)",
                "... 52 more",
                "Caused by: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:240)",
                "at org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:877)",
                "... 58 more",
                "Caused by: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.nustaq.serialization.FSTClazzNameRegistry.decodeClass(FSTClazzNameRegistry.java:173)",
                "at org.nustaq.serialization.coders.FSTStreamDecoder.readClass(FSTStreamDecoder.java:431)",
                "at org.nustaq.serialization.FSTObjectInput.readClass(FSTObjectInput.java:853)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:338)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)",
                "at org.nustaq.serialization.serializers.FSTArrayListSerializer.instantiate(FSTArrayListSerializer.java:63)",
                "at org.nustaq.serialization.FSTObjectInput.instantiateAndReadWithSer(FSTObjectInput.java:459)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:354)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)",
                "at org.nustaq.serialization.serializers.FSTMapSerializer.instantiate(FSTMapSerializer.java:78)",
                "at org.nustaq.serialization.FSTObjectInput.instantiateAndReadWithSer(FSTObjectInput.java:459)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:354)",
                "at org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:304)",
                "at org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:238)",
                "... 59 more"
            ],
            "StepsToReproduce": [
                "Open the Tez UI.",
                "Perform an action that triggers a REST API call to the Timeline Service.",
                "Observe the response for a 500 Internal Server Error."
            ],
            "ExpectedBehavior": "The API should return a valid response without errors, or provide a partial message instead of a 500 Internal Server Error.",
            "ObservedBehavior": "The API returns a 500 Internal Server Error due to an object decoding failure.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3742.json",
        "creation_time": "2015-05-29T06:00:38.000+0000",
        "bug_report": {
            "BugID": "YARN-3742",
            "Title": "ResourceManager Shutdown on ZKClient Creation Timeout",
            "Description": "The ResourceManager (RM) shuts down when the ZKClient connection fails to be created, instead of transitioning to StandBy mode. This behavior prevents the other ResourceManager from taking over, leading to potential service disruptions.",
            "StackTrace": [
                "2015-04-19 01:22:20,513  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED. Cause:",
                "java.io.IOException: Wait for ZKClient creation timed out",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Simulate a failure in ZKClient connection creation.",
                "3. Observe the behavior of the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should transition to StandBy mode instead of shutting down.",
            "ObservedBehavior": "The ResourceManager shuts down completely when the ZKClient creation times out.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4984.json",
        "creation_time": "2016-04-21T19:16:03.000+0000",
        "bug_report": {
            "BugID": "YARN-4984",
            "Title": "LogAggregationService Swallows Exception Leading to Thread Leak",
            "Description": "The LogAggregationService fails to handle exceptions properly when creating application directories, resulting in thread leaks. This issue arises due to stale applications persisting in the NodeManager state store after a restart, causing application initiation failures due to invalid tokens.",
            "StackTrace": [
                "2016-04-19 23:38:33,039 ERROR logaggregation.LogAggregationService (LogAggregationService.java:run(300)) - Failed to setup application log directory for application_1448060878692_11842",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be found in cache",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1427)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1358)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)",
                "at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)",
                "at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.access$100(LogAggregationService.java:67)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createAppDir(LogAggregationService.java:261)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator(LogAggregationService.java:367)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:320)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:447)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)"
            ],
            "StepsToReproduce": [
                "1. Start the NodeManager with stale applications present in the state store.",
                "2. Attempt to initiate an application that requires log aggregation.",
                "3. Observe the logs for errors related to log directory setup."
            ],
            "ExpectedBehavior": "The LogAggregationService should handle exceptions properly and not create aggregator threads for invalid applications.",
            "ObservedBehavior": "The LogAggregationService swallows exceptions and continues to create aggregator threads for applications that fail to initialize due to invalid tokens.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4584.json",
        "creation_time": "2016-01-12T09:08:31.000+0000",
        "bug_report": {
            "BugID": "YARN-4584",
            "Title": "ResourceManager Fails to Start After Multiple Application Master Preemptions",
            "Description": "When multiple Application Masters (AMs) are preempted due to resource limits in the default queue, the ResourceManager (RM) fails to restart after a restart. This issue occurs specifically when the AM assigned to the default queue is preempted multiple times, leading to a NullPointerException during the recovery process.",
            "StackTrace": [
                "2016-01-12 10:49:04,081 DEBUG org.apache.hadoop.service.AbstractService: noteFailure java.lang.NullPointerException",
                "2016-01-12 10:49:04,081 INFO org.apache.hadoop.service.AbstractService: Service RMActiveServices failed in state STARTED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover(RMAppAttemptImpl.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:953)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:946)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recoverApplication(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recover(RMAppManager.java:464)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1232)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:594)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1022)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1062)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1058)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1705)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1058)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:323)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:127)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:877)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "2016-01-12 10:49:04,082 DEBUG org.apache.hadoop.service.AbstractService: Service: RMActiveServices entered state STOPPED",
                "2016-01-12 10:49:04,082 DEBUG org.apache.hadoop.service.CompositeService: RMActiveServices: stopping services, size=16"
            ],
            "StepsToReproduce": [
                "Configure 3 queues in the cluster with the following resource allocations: queue1 40%, queue2 50%, default queue 10%.",
                "Submit applications to all 3 queues with a container size of 1024MB (e.g., sleep job with 50 containers on all queues).",
                "Ensure that the Application Master assigned to the default queue gets preempted multiple times (approximately 20 times).",
                "Restart the ResourceManager after the preemptions."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully without any errors.",
            "ObservedBehavior": "The ResourceManager fails to restart and logs a NullPointerException.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2846.json",
        "creation_time": "2014-11-11T15:30:08.000+0000",
        "bug_report": {
            "BugID": "YARN-2846",
            "Title": "Incorrect Exit Code Persistence for Running Containers During NodeManager Restart",
            "Description": "The NodeManager (NM) restart feature is causing running Application Master (AM) containers to be marked as LOST and killed during the NM daemon stop. This issue arises from the way exit codes are recorded when the container process is interrupted by the NM stop. The expected behavior is that running containers should not have their exit codes recorded as LOST if the process is interrupted.",
            "StackTrace": [
                "2014-11-11 00:48:35,214 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(408)) - Memory usage of ProcessTree 22140 for container-id container_1415666714233_0001_01_000084: 53.8 MB of 512 MB physical memory used; 931.3 MB of 1.0 GB virtual memory used",
                "2014-11-11 00:48:35,223 ERROR nodemanager.NodeManager (SignalLogger.java:handle(60)) - RECEIVED SIGNAL 15: SIGTERM",
                "2014-11-11 00:48:35,299 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50060",
                "2014-11-11 00:48:35,337 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:cleanUpApplicationsOnNMShutDown(512)) - Applications still running : [application_1415666714233_0001]",
                "2014-11-11 00:48:35,338 INFO  ipc.Server (Server.java:stop(2437)) - Stopping server on 45454",
                "2014-11-11 00:48:35,344 INFO  ipc.Server (Server.java:run(706)) - Stopping IPC Server listener on 45454",
                "2014-11-11 00:48:35,346 INFO  logaggregation.LogAggregationService (LogAggregationService.java:serviceStop(141)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit",
                "2014-11-11 00:48:35,347 INFO  ipc.Server (Server.java:run(832)) - Stopping IPC Server Responder",
                "2014-11-11 00:48:35,347 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(502)) - Aborting log aggregation for application_1415666714233_0001",
                "2014-11-11 00:48:35,348 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(382)) - Aggregation did not complete for application application_1415666714233_0001",
                "2014-11-11 00:48:35,358 WARN  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(476)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.",
                "2014-11-11 00:48:35,406 ERROR launcher.RecoveredContainerLaunch (RecoveredContainerLaunch.java:call(87)) - Unable to recover container container_1415666714233_0001_01_000001",
                "java.io.IOException: Interrupted while waiting for process 20001 to exit",
                "    at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:46)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "    at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.InterruptedException: sleep interrupted",
                "    at java.lang.Thread.sleep(Native Method)",
                "    at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)",
                "    ... 6 more"
            ],
            "StepsToReproduce": [
                "1. Start a NodeManager with a running Application Master (AM) container.",
                "2. Initiate a restart of the NodeManager.",
                "3. Observe the behavior of the running AM container during the restart."
            ],
            "ExpectedBehavior": "The running AM container should not be marked as LOST and should retain its exit code correctly.",
            "ObservedBehavior": "The running AM container is marked as LOST due to an IOException caused by the NM stop, leading to incorrect exit code persistence.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7890.json",
        "creation_time": "2018-02-03T21:10:43.000+0000",
        "bug_report": {
            "BugID": "YARN-7890",
            "Title": "NullPointerException during Container Relaunch in YARN",
            "Description": "While running a recent build of trunk, a NullPointerException occurred during the relaunch of a container in YARN. This issue prevents the container from starting successfully.",
            "StackTrace": [
                "2018-02-02 21:02:40,026 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_e02_1517604848419_0002_01_000004 transitioned from RELAUNCHING to RUNNING",
                "2018-02-02 21:02:40,026 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch: Failed to relaunch container.",
                "java.lang.NullPointerException",
                "    at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)",
                "    at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)",
                "    at java.util.Collections.unmodifiableList(Collections.java:1287)",
                "    at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)",
                "    at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)",
                "    at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:49)",
                "    at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "    at java.lang.Thread.run(Thread.java:748)"
            ],
            "StepsToReproduce": [
                "1. Build the latest trunk version of YARN.",
                "2. Attempt to relaunch a container that has previously failed."
            ],
            "ExpectedBehavior": "The container should relaunch successfully without throwing a NullPointerException.",
            "ObservedBehavior": "The container fails to relaunch and throws a NullPointerException in the logs.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-139.json",
        "creation_time": "2012-10-01T19:51:20.000+0000",
        "bug_report": {
            "BugID": "YARN-139",
            "Title": "InterruptedException during AsyncDispatcher shutdown causes user confusion",
            "Description": "During the shutdown of successful applications, an InterruptedException is logged, which is harmless but leads to user confusion. This issue needs to be addressed to improve user experience.",
            "StackTrace": [
                "2012-09-28 14:50:12,477 WARN [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Interrupted Exception while stopping",
                "java.lang.InterruptedException",
                "\tat java.lang.Object.wait(Native Method)",
                "\tat java.lang.Thread.join(Thread.java:1143)",
                "\tat java.lang.Thread.join(Thread.java:1196)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:105)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:437)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:402)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "\tat java.lang.Thread.run(Thread.java:619)",
                "2012-09-28 14:50:12,477 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.service.AbstractService: Service:Dispatcher is stopped.",
                "2012-09-28 14:50:12,477 INFO [AsyncDispatcher event handler] org.apache.hadoop.yarn.service.AbstractService: Service:org.apache.hadoop.mapreduce.v2.app.MRAppMaster is stopped.",
                "2012-09-28 14:50:12,477 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Exiting MR AppMaster..GoodBye"
            ],
            "StepsToReproduce": [
                "Start a successful application using Hadoop YARN.",
                "Allow the application to run until it reaches the shutdown phase.",
                "Observe the logs during the shutdown process."
            ],
            "ExpectedBehavior": "No warnings or errors should be logged during the shutdown of a successful application.",
            "ObservedBehavior": "An InterruptedException warning is logged, causing confusion among users.",
            "Resolution": "The issue has been marked as fixed."
        }
    },
    {
        "filename": "YARN-42.json",
        "creation_time": "2012-05-14T11:38:55.000+0000",
        "bug_report": {
            "BugID": "YARN-42",
            "Title": "Node Manager throws NullPointerException on startup due to permission issues",
            "Description": "The Node Manager fails to start and throws a NullPointerException (NPE) if it does not have the necessary permissions on the local directory. This results in a failure to initialize the LocalizationService, causing the Node Manager to crash.",
            "StackTrace": [
                "2012-05-14 16:32:13,468 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:166)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:268)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:284)",
                "Caused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)",
                "at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2325)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)",
                "... 6 more",
                "2012-05-14 16:32:13,472 INFO org.apache.hadoop.yarn.service.CompositeService: Error stopping org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop(NonAggregatingLogHandler.java:82)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stop(ContainerManagerImpl.java:266)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:182)",
                "at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "StepsToReproduce": [
                "1. Ensure that the Node Manager's local directory does not have the necessary permissions.",
                "2. Start the Node Manager.",
                "3. Observe the logs for errors."
            ],
            "ExpectedBehavior": "The Node Manager should start successfully without throwing any exceptions.",
            "ObservedBehavior": "The Node Manager fails to start and throws a NullPointerException due to permission issues on the local directory.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7453.json",
        "creation_time": "2017-11-07T09:46:28.000+0000",
        "bug_report": {
            "BugID": "YARN-7453",
            "Title": "ResourceManager Fails to Switch to ACTIVE State After Initial Start",
            "Description": "The ResourceManager fails to transition to the ACTIVE state after its first successful start. This issue causes the ResourceManager to enter a loop of transitioning between ACTIVE and STANDBY states, leading to operational instability.",
            "StackTrace": [
                "2017-11-07 15:08:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state",
                "2017-11-07 15:08:11,669 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started",
                "2017-11-07 15:08:11,669 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.5",
                "2017-11-07 15:08:11,670 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state",
                "org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth",
                "\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:113)",
                "\tat org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1006)",
                "\tat org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:910)",
                "\tat org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:159)",
                "\tat org.apache.curator.framework.imps.CuratorTransactionImpl.access$200(CuratorTransactionImpl.java:44)",
                "\tat org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:129)",
                "\tat org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:125)",
                "\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)",
                "\tat org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:122)",
                "\tat org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction.commit(ZKCuratorManager.java:403)",
                "\tat org.apache.hadoop.util.curator.ZKCuratorManager.safeSetData(ZKCuratorManager.java:372)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.getAndIncrementEpoch(ZKRMStateStore.java:493)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1162)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1202)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1198)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:422)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1198)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)",
                "\tat org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)",
                "\tat org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:473)",
                "\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:607)",
                "\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)"
            ],
            "StepsToReproduce": [
                "Start the ResourceManager.",
                "Observe the transition of the ResourceManager from ACTIVE to STANDBY and back to ACTIVE.",
                "Monitor the logs for any errors during the transition."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the ACTIVE state after the first start without errors.",
            "ObservedBehavior": "The ResourceManager fails to transition to ACTIVE and enters a loop of transitioning between ACTIVE and STANDBY states, logging a NoAuthException.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3369.json",
        "creation_time": "2015-03-18T23:29:06.000+0000",
        "bug_report": {
            "BugID": "YARN-3369",
            "Title": "NullPointerException in AppSchedulingInfo due to missing null check",
            "Description": "In the method checkForDeactivation() of AppSchedulingInfo.java, a null pointer exception occurs because the method getResourceRequest can return null, which is not checked before dereferencing. This leads to a fatal error in the ResourceManager.",
            "StackTrace": [
                "2015-03-17 14:14:04,757 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)",
                "    at java.lang.Thread.run(Thread.java:722)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Trigger a NODE_UPDATE event.",
                "3. Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle NODE_UPDATE events without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when a NODE_UPDATE event is processed.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-945.json",
        "creation_time": "2013-07-19T22:59:06.000+0000",
        "bug_report": {
            "BugID": "YARN-945",
            "Title": "AM Registration Fails Due to Missing AMRM Token",
            "Description": "The Application Master (AM) fails to register with the Resource Manager (RM) because the AMRM token is missing, leading to an AccessControlException.",
            "StackTrace": [
                "2013-07-19 15:53:55,569 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54313: readAndProcess from client 127.0.0.1 threw exception [org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]]",
                "org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]",
                "at org.apache.hadoop.ipc.Server$Connection.initializeAuthContext(Server.java:1531)",
                "at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1482)",
                "at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:788)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:587)",
                "at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:562)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop cluster.",
                "2. Attempt to launch an Application Master.",
                "3. Observe the logs for any authentication errors."
            ],
            "ExpectedBehavior": "The Application Master should successfully register with the Resource Manager without any authentication errors.",
            "ObservedBehavior": "The Application Master fails to register, and the logs show an AccessControlException indicating that SIMPLE authentication is not enabled.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6072.json",
        "creation_time": "2017-01-08T09:21:12.000+0000",
        "bug_report": {
            "BugID": "YARN-6072",
            "Title": "Resource Manager Fails to Start in Secure Mode Due to NullPointerException",
            "Description": "The Resource Manager is unable to start in secure mode, resulting in a NullPointerException during the initialization process. This issue occurs when the AdminService attempts to refresh service ACLs before it is fully initialized.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:552)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "org.apache.hadoop.ha.ServiceFailedException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Error on refreshAll during transition to Active",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:311)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "... 4 more",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "... 5 more"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop Resource Manager in secure mode.",
                "2. Monitor the logs for any initialization errors."
            ],
            "ExpectedBehavior": "The Resource Manager should start successfully in secure mode without any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start, throwing a NullPointerException during the initialization process.",
            "Resolution": "The issue has been fixed in the latest release. Ensure that the AdminService is fully initialized before calling refreshServiceAcls."
        }
    },
    {
        "filename": "YARN-7663.json",
        "creation_time": "2017-12-15T01:52:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7663",
            "Title": "InvalidStateTransitionException when sending kill to application in ResourceManager",
            "Description": "When sending a kill command to an application, the ResourceManager logs an InvalidStateTransitionException indicating an invalid event transition from KILLED to START. This issue can be reproduced by inserting a sleep before the START event is created.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: START at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:805)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:885)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "StepsToReproduce": [
                "1. Start an application in the ResourceManager.",
                "2. Send a kill command to the application.",
                "3. Observe the ResourceManager logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should be killed without any exceptions in the ResourceManager logs.",
            "ObservedBehavior": "The ResourceManager logs show an InvalidStateTransitionException indicating an invalid event transition from KILLED to START.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5873.json",
        "creation_time": "2016-11-12T09:54:20.000+0000",
        "bug_report": {
            "BugID": "YARN-5873",
            "Title": "ResourceManager Crashes with NullPointerException When Generic Application History is Enabled",
            "Description": "The ResourceManager crashes with a NullPointerException when the generic application history feature is enabled. This issue occurs during the allocation of containers, leading to a failure in the application history writer.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:210)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.doAllocation(RegularContainerAllocator.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:832)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:865)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.ContainerAllocator.assignContainers(ContainerAllocator.java:81)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:931)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:690)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:508)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1475)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1470)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1346)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1601)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:149)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-11-12 14:22:07,153 INFO SchedulerEventDispatcher:Event Processor org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:79) org.apache.hadoop.yarn.event.EventDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "Enable the generic application history feature in the ResourceManager configuration.",
                "Submit a job to the ResourceManager.",
                "Monitor the ResourceManager logs for any NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the application history without crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when the generic application history is enabled.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3227.json",
        "creation_time": "2015-02-19T16:58:01.000+0000",
        "bug_report": {
            "BugID": "YARN-3227",
            "Title": "Renew Delegation Token Fails When ResourceManager User's TGT is Expired",
            "Description": "When the ResourceManager (RM) user's Kerberos Ticket Granting Ticket (TGT) is expired, the RM fails to renew the delegation token during job submission. The expected behavior is for the RM to automatically re-login and obtain a new TGT.",
            "StackTrace": [
                "2015-02-06 18:54:05,617 [DelegationTokenRenewer #25954] WARN security.DelegationTokenRenewer: Unable to add the application to the delegation token renewer.",
                "java.io.IOException: Failed to renew token: Kind: TIMELINE_DELEGATION_TOKEN, Service: timelineserver.example.com:4080, Ident: (owner=user, renewer=rmuser, realUser=oozie, issueDate=1423248845528, maxDate=1423853645528, sequenceNumber=9716, masterKeyId=9)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:443)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:808)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:789)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: java.io.IOException: HTTP status [401], message [Unauthorized]",
                "at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:286)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.renewDelegationToken(DelegationTokenAuthenticator.java:211)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.renewDelegationToken(DelegationTokenAuthenticatedURL.java:414)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:374)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:360)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$4.run(TimelineClientImpl.java:429)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:161)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.operateDelegationToken(TimelineClientImpl.java:444)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.renewDelegationToken(TimelineClientImpl.java:378)",
                "at org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier$Renewer.renew(TimelineDelegationTokenIdentifier$Renewer.java:81)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:532)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:529)"
            ],
            "StepsToReproduce": [
                "1. Ensure the ResourceManager user's Kerberos TGT is expired.",
                "2. Submit a job that requires a delegation token renewal.",
                "3. Observe the logs for any warnings or errors related to delegation token renewal."
            ],
            "ExpectedBehavior": "The ResourceManager should automatically re-login to obtain a new TGT and successfully renew the delegation token.",
            "ObservedBehavior": "The ResourceManager fails to renew the delegation token and logs an error indicating that it is unable to add the application to the delegation token renewer.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4235.json",
        "creation_time": "2015-10-07T19:26:24.000+0000",
        "bug_report": {
            "BugID": "YARN-4235",
            "Title": "NPE in FairScheduler when handling empty user groups",
            "Description": "The FairScheduler encounters a NullPointerException (NPE) when it attempts to process an application with empty groups returned for a user. This results in a crash of the ResourceManager.",
            "StackTrace": [
                "2015-09-22 16:51:52,780  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ADDED to the scheduler",
                "java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-09-22 16:51:52,797  INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Submit an application to the FairScheduler with a user that has no associated groups.",
                "2. Monitor the ResourceManager logs for errors."
            ],
            "ExpectedBehavior": "The FairScheduler should handle empty user groups gracefully without crashing the ResourceManager.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when processing an application with empty user groups.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4833.json",
        "creation_time": "2016-03-17T13:22:23.000+0000",
        "bug_report": {
            "BugID": "YARN-4833",
            "Title": "Client Retries Excessively on AccessControlException During Application Submission",
            "Description": "When submitting an application to a queue with ACL enabled, if the submitting user does not have the necessary permissions, the client retries the submission multiple times, leading to unnecessary load and potential delays. This behavior occurs even after the maximum retry attempts have been reached.",
            "StackTrace": [
                "16/03/18 10:01:06 INFO retry.RetryInvocationHandler: Exception while invoking submitApplication of class ApplicationClientProtocolPBClientImpl over rm1. Trying to fail over immediately.",
                "org.apache.hadoop.security.AccessControlException: User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:483)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2360)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2356)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2356)",
                "Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:483)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2360)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2356)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2356)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to a queue where ACL is enabled.",
                "2. Ensure that the submitting user does not have the necessary permissions.",
                "3. Observe the client behavior as it retries the submission."
            ],
            "ExpectedBehavior": "The client should handle the AccessControlException gracefully and not retry the submission multiple times.",
            "ObservedBehavior": "The client retries the submission up to the maximum allowed attempts, causing unnecessary load and delays.",
            "Resolution": "As per discussion with [Provide additional details], handle the AccessControlException in RetryPolicy and switch to fallbackPolicy. Wrap AccessControlException to YarnException in RMAppManager#submitApplication."
        }
    },
    {
        "filename": "YARN-1689.json",
        "creation_time": "2014-02-05T19:16:00.000+0000",
        "bug_report": {
            "BugID": "YARN-1689",
            "Title": "ResourceManager becomes unresponsive during Hive on Tez jobs",
            "Description": "When running some Hive on Tez jobs, the ResourceManager (RM) becomes unresponsive after a while, resulting in no jobs being executed. The RM logs indicate a NullPointerException and an InvalidStateTransitionException.",
            "StackTrace": [
                "2014-02-04 20:28:08,553 WARN  ipc.Server (Server.java:run(1978)) - IPC Server handler 0 on 8030, call org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster from 172.18.145.156:40474 Call#0 Retry#0: error: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)",
                "    at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)",
                "    at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)",
                "    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:396)",
                "    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)",
                "2014-02-04 20:28:08,544 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(626)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:624)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:81)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:656)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:640)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "    at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Submit a Hive on Tez job to the ResourceManager.",
                "3. Monitor the ResourceManager logs for exceptions."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the job submissions and maintain its operational state without becoming unresponsive.",
            "ObservedBehavior": "The ResourceManager becomes unresponsive, and no jobs are executed after a certain period.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-5594.json",
        "creation_time": "2016-08-30T15:14:19.000+0000",
        "bug_report": {
            "BugID": "YARN-5594",
            "Title": "InvalidProtocolBufferException during ResourceManager recovery after upgrade from v2.5.1 to v2.7.0",
            "Description": "After upgrading the cluster from version 2.5.1 to 2.7.0, the ResourceManager fails to load/recover its state, resulting in an InvalidProtocolBufferException. This issue arises due to incompatible file formats used in different Hadoop versions.",
            "StackTrace": [
                "2016-08-25 17:20:33,293 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state",
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).",
                "at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4644)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4740)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4735)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:5075)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:4955)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:337)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:267)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:210)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadState(FileSystemRMStateStore.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1007)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1044)"
            ],
            "StepsToReproduce": [
                "Upgrade the Hadoop cluster from version 2.5.1 to 2.7.0.",
                "Start the ResourceManager.",
                "Observe the logs for any errors related to state recovery."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully load and recover its state without errors.",
            "ObservedBehavior": "The ResourceManager fails to load/recover its state, resulting in an InvalidProtocolBufferException.",
            "Resolution": "This fix handles the old data format during ResourceManager recovery if an InvalidProtocolBufferException occurs."
        }
    },
    {
        "filename": "YARN-7511.json",
        "creation_time": "2017-11-16T11:41:43.000+0000",
        "bug_report": {
            "BugID": "YARN-7511",
            "Title": "NullPointerException in ContainerLocalizer During Resource Localization Failure",
            "Description": "A NullPointerException (NPE) occurs in the ContainerLocalizer when resource localization fails for a running container. This issue arises when the ContainerManagerImpl#localize method is called, leading to a failure in the ResourceLocalizationService.",
            "StackTrace": [
                "2017-09-30 20:14:32,839 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "    at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)",
                "    at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)",
                "    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "    at java.lang.Thread.run(Thread.java:834)",
                "2017-09-30 20:14:32,842 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start a container and ensure it is running.",
                "2. Call ContainerManagerImpl#localize for the running container.",
                "3. Simulate a localization failure in ResourceLocalizationService$LocalizerRunner#run, which sends out a ContainerResourceFailedEvent with a null LocalResourceRequest.",
                "4. Observe the NullPointerException when transitioning in ResourceLocalizationFailedWhileRunningTransition."
            ],
            "ExpectedBehavior": "The system should handle resource localization failures gracefully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when the resource localization fails for a running container.",
            "Resolution": "Ensure that the LocalResourceRequest is not null before attempting to remove it from the resource set."
        }
    },
    {
        "filename": "YARN-3790.json",
        "creation_time": "2015-06-10T04:53:40.000+0000",
        "bug_report": {
            "BugID": "YARN-3790",
            "Title": "AssertionError in TestWorkPreservingRMRestart due to stale metrics after recovery",
            "Description": "The test 'testSchedulerRecovery' in the class 'TestWorkPreservingRMRestart' fails due to an AssertionError indicating that the expected resource usage metrics are not matching the actual values after recovering the container. This suggests that the usedResource metrics from the rootQueue may be stale.",
            "StackTrace": [
                "java.lang.AssertionError: expected:<6144> but was:<8192>",
                "\tat org.junit.Assert.fail(Assert.java:88)",
                "\tat org.junit.Assert.failNotEquals(Assert.java:743)",
                "\tat org.junit.Assert.assertEquals(Assert.java:118)",
                "\tat org.junit.Assert.assertEquals(Assert.java:555)",
                "\tat org.junit.Assert.assertEquals(Assert.java:542)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.assertMetrics(TestWorkPreservingRMRestart.java:853)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.checkFSQueue(TestWorkPreservingRMRestart.java:342)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.testSchedulerRecovery(TestWorkPreservingRMRestart.java:241)"
            ],
            "StepsToReproduce": [
                "Run the test suite for the Hadoop YARN project.",
                "Execute the test 'testSchedulerRecovery' in the 'TestWorkPreservingRMRestart' class.",
                "Observe the output for any assertion failures."
            ],
            "ExpectedBehavior": "The test 'testSchedulerRecovery' should pass without any assertion errors, indicating that the resource metrics are correctly updated after recovery.",
            "ObservedBehavior": "The test fails with an AssertionError indicating that the expected resource usage (6144) does not match the actual resource usage (8192).",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-6068.json",
        "creation_time": "2017-01-07T03:16:07.000+0000",
        "bug_report": {
            "BugID": "YARN-6068",
            "Title": "Log Aggregation Fails After NodeManager Restart Despite Recovery",
            "Description": "The log aggregation process fails when the NodeManager is restarted, even when recovery is expected. The following exception is logged during the failure.",
            "StackTrace": [
                "2017-01-05 19:16:36,352 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(527)) - Aborting log aggregation for application_1483640789847_0001",
                "2017-01-05 19:16:36,352 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(399)) - Aggregation did not complete for application application_1483640789847_0001",
                "2017-01-05 19:16:36,353 WARN  application.ApplicationImpl (ApplicationImpl.java:handle(461)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-05 19:16:36,355 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1483640789847_0001 transitioned from RUNNING to null"
            ],
            "StepsToReproduce": [
                "1. Start the NodeManager.",
                "2. Submit an application that requires log aggregation.",
                "3. Allow the application to run until it reaches a state where log aggregation is expected.",
                "4. Restart the NodeManager.",
                "5. Observe the logs for any errors related to log aggregation."
            ],
            "ExpectedBehavior": "Log aggregation should complete successfully even after a NodeManager restart.",
            "ObservedBehavior": "Log aggregation fails with an InvalidStateTransitonException, indicating that the application cannot handle the log aggregation event.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-903.json",
        "creation_time": "2013-07-07T08:35:30.000+0000",
        "bug_report": {
            "BugID": "YARN-903",
            "Title": "Errors Logged by DistributedShell After Successful Completion",
            "Description": "The DistributedShell application logs errors even after successful execution. This issue was observed while running the application through the ApplicationMaster. The logs from both the NodeManager and ApplicationMaster contain several error messages that need to be addressed.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000002 is not handled by this NodeManager",
                "\tat org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)",
                "\tat org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)",
                "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)",
                "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)",
                "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1868)",
                "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1864)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:396)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)",
                "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1862)"
            ],
            "StepsToReproduce": [
                "1. Start the DistributedShell application using the ApplicationMaster.",
                "2. Monitor the logs from both the NodeManager and ApplicationMaster during execution.",
                "3. Observe the error messages logged after the application completes successfully."
            ],
            "ExpectedBehavior": "The DistributedShell application should complete successfully without logging any errors.",
            "ObservedBehavior": "The application logs multiple error messages related to container management even after successful execution.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-8236.json",
        "creation_time": "2018-04-29T16:28:11.000+0000",
        "bug_report": {
            "BugID": "YARN-8236",
            "Title": "NullPointerException in ServiceClient due to Invalid Kerberos Principal File Name",
            "Description": "A NullPointerException occurs in the ServiceClient when an invalid Kerberos principal file name is provided. This issue affects the ability to submit applications using the YARN service client.",
            "StackTrace": [
                "2018-04-29 16:22:54,266 WARN webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269)"
            ],
            "StepsToReproduce": [
                "1. Configure the YARN service client with an invalid Kerberos principal file name.",
                "2. Attempt to submit an application using the service client.",
                "3. Observe the logs for any exceptions thrown."
            ],
            "ExpectedBehavior": "The application should be submitted successfully or an appropriate error message should be displayed without causing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the application submission to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-2857.json",
        "creation_time": "2014-10-24T20:47:51.000+0000",
        "bug_report": {
            "BugID": "YARN-2857",
            "Title": "ConcurrentModificationException in ContainerLogAppender during Pig script execution",
            "Description": "When submitting a Pig script via Oozie, a ConcurrentModificationException occurs in the ContainerLogAppender, leading to job failure. This issue is observed in the Hadoop 2.3.0 environment with Oozie 4.0.1 and Pig version 0.11.x.",
            "StackTrace": [
                "2014-10-24 20:37:29,317 WARN [Thread-5] org.apache.hadoop.util.ShutdownHookManager: ShutdownHook '' failed, java.util.ConcurrentModificationException",
                "java.util.ConcurrentModificationException",
                "\tat java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "\tat java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "\tat org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)",
                "\tat org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)",
                "\tat org.apache.log4j.Category.removeAllAppenders(Category.java:891)",
                "\tat org.apache.log4j.Hierarchy.shutdown(Hierarchy.java:471)",
                "\tat org.apache.log4j.LogManager.shutdown(LogManager.java:267)",
                "\tat org.apache.hadoop.mapred.TaskLog.syncLogsShutdown(TaskLog.java:286)",
                "\tat org.apache.hadoop.mapred.TaskLog$2.run(TaskLog.java:339)",
                "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)",
                "2014-10-24 20:37:29,395 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping MapTask metrics system...",
                "java.util.ConcurrentModificationException",
                "\tat java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "\tat java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "\tat org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)",
                "\tat org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)",
                "\tat org.apache.log4j.Category.removeAllAppenders(Category.java:891)",
                "\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:759)",
                "\tat org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)",
                "\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)",
                "\tat org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440)",
                "\tat org.apache.pig.Main.configureLog4J(Main.java:740)",
                "\tat org.apache.pig.Main.run(Main.java:384)",
                "\tat org.apache.pig.PigRunner.run(PigRunner.java:49)",
                "\tat org.apache.oozie.action.hadoop.PigMain.runPigJob(PigMain.java:283)",
                "\tat org.apache.oozie.action.hadoop.PigMain.run(PigMain.java:223)",
                "\tat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:37)",
                "\tat org.apache.oozie.action.hadoop.PigMain.main(PigMain.java:76)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "\tat java.lang.reflect.Method.invoke(Method.java:483)",
                "\tat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:226)",
                "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "\tat java.security.AccessController.doPrivileged(Native Method)",
                "\tat javax.security.auth.Subject.doAs(Subject.java:422)",
                "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "StepsToReproduce": [
                "1. Set up Hadoop 2.3.0 with Oozie 4.0.1 and Pig version 0.11.x.",
                "2. Submit a Pig script using Oozie.",
                "3. Monitor the MR task log for exceptions."
            ],
            "ExpectedBehavior": "The Pig script should execute successfully without any exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, causing the job to fail.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-2416.json",
        "creation_time": "2014-08-13T22:36:31.000+0000",
        "bug_report": {
            "BugID": "YARN-2416",
            "Title": "InvalidStateTransitionException in ResourceManager when AMLauncher delays response for startContainers() call",
            "Description": "AMLauncher calls startContainers(allRequests) to launch a container for the application master. Normally, the call returns immediately, allowing the RMAppAttempt to change its state from ALLOCATED to LAUNCHED. However, in some cases, the RPC call returns late while the AM container has already started. This causes the RMAppAttempt to remain in the ALLOCATED state, leading to an InvalidStateTransitionException when the resource manager receives the REGISTERED event from the application master.",
            "StackTrace": [
                "2014-07-05 08:59:05,021 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: REGISTERED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)",
                "2014-07-05 08:59:06,152 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)",
                "2014-07-05 08:59:07,779 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Launch an application using AMLauncher.",
                "2. Simulate a delay in the response for the startContainers() RPC call.",
                "3. Observe the state of the RMAppAttempt after the application master registers."
            ],
            "ExpectedBehavior": "The RMAppAttempt should transition from ALLOCATED to LAUNCHED without errors.",
            "ObservedBehavior": "The RMAppAttempt remains in the ALLOCATED state, resulting in InvalidStateTransitionException errors when events are received.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-345.json",
        "creation_time": "2013-01-17T12:57:46.000+0000",
        "bug_report": {
            "BugID": "YARN-345",
            "Title": "InvalidStateTransitonException Errors in Node Manager for ApplicationImpl",
            "Description": "The Node Manager is encountering multiple InvalidStateTransitonException errors when handling application events, leading to application failures.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-17 04:03:46,726 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at APPLICATION_RESOURCES_CLEANINGUP",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-17 00:01:11,006 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHING_CONTAINERS_WAIT",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-17 10:56:36,975 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1358385982671_1304_01_000001 transitioned from NEW to DONE",
                "2013-01-17 10:56:36,975 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_CONTAINER_FINISHED at FINISHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-17 10:56:36,026 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: INIT_CONTAINER at FINISHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "Deploy an application using the Node Manager.",
                "Trigger events that lead to application state transitions.",
                "Monitor the Node Manager logs for errors."
            ],
            "ExpectedBehavior": "The Node Manager should handle application events without throwing InvalidStateTransitonException errors.",
            "ObservedBehavior": "The Node Manager throws multiple InvalidStateTransitonException errors, indicating it cannot handle certain events in the current application state.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-3894.json",
        "creation_time": "2015-07-08T07:00:51.000+0000",
        "bug_report": {
            "BugID": "YARN-3894",
            "Title": "ResourceManager Startup Fails with Invalid Capacity Scheduler NodeLabel Configuration",
            "Description": "When the ResourceManager (RM) is configured with an incorrect capacity configuration for NodeLabels, it fails to start. This issue occurs specifically when the capacity configuration for an already added label is incorrect, leading to exceptions during the initialization of queues.",
            "StackTrace": [
                "2015-07-07 19:18:25,655 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=0.5, absoluteCapacity=0.5, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0",
                "2015-07-07 19:18:25,656 WARN org.apache.hadoop.yarn.server.resourcemanager.AdminService: Exception refresh queues.",
                "java.io.IOException: Failed to re-init queues",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:383)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:376)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:605)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "    at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)",
                "Caused by: java.lang.IllegalArgumentException: Illegal capacity of 0.5 for children of queue root for label=node2",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues(ParentQueue.java:159)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:639)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:503)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:379)",
                "    ... 8 more"
            ],
            "StepsToReproduce": [
                "Configure ResourceManager with capacity scheduler.",
                "Add one or two node labels using rmadmin.",
                "Configure the capacity XML with a node label but introduce an issue with the capacity configuration for the already added label.",
                "Restart both ResourceManager instances.",
                "Check the service initialization of the capacity scheduler to see if the node label list is populated."
            ],
            "ExpectedBehavior": "The ResourceManager should not start if there is an invalid capacity configuration for NodeLabels.",
            "ObservedBehavior": "The ResourceManager fails to start and logs an exception related to queue initialization.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1903.json",
        "creation_time": "2014-04-04T20:51:24.000+0000",
        "bug_report": {
            "BugID": "YARN-1903",
            "Title": "Container Status Incorrect After Stopping Container",
            "Description": "The container status after stopping a container is not as expected, leading to an assertion error in the test case.",
            "StackTrace": [
                "java.lang.AssertionError: 4:",
                "    at org.junit.Assert.fail(Assert.java:93)",
                "    at org.junit.Assert.assertTrue(Assert.java:43)",
                "    at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testGetContainerStatus(TestNMClient.java:382)",
                "    at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testContainerManagement(TestNMClient.java:346)",
                "    at org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient(TestNMClient.java:226)"
            ],
            "StepsToReproduce": [
                "1. Start a container in the YARN environment.",
                "2. Change the container state to NEW or LOCALIZING.",
                "3. Stop the container.",
                "4. Check the container status."
            ],
            "ExpectedBehavior": "The container status should reflect the correct exit code and diagnostics after stopping the container.",
            "ObservedBehavior": "The container status does not set the exit code and diagnostics, resulting in an assertion error during the test.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4347.json",
        "creation_time": "2015-11-11T22:32:59.000+0000",
        "bug_report": {
            "BugID": "YARN-4347",
            "Title": "Resource Manager Null Pointer Exception on Application Recovery",
            "Description": "The Resource Manager fails with a Null Pointer Exception (NPE) while attempting to load or recover a finished application. This issue occurs during the recovery process, leading to application failures.",
            "StackTrace": [
                "2015-11-11 17:53:22,351 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(597)) - Failed to load/recover state",
                "java.lang.NullPointerException",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:755)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:839)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:854)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:844)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:719)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:411)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1219)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:593)",
                "    at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1026)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1067)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1063)",
                "    at java.security.AccessController.doPrivileged(Native Method)",
                "    at javax.security.auth.Subject.doAs(Subject.java:415)"
            ],
            "StepsToReproduce": [
                "1. Start the Resource Manager.",
                "2. Submit an application and allow it to finish.",
                "3. Restart the Resource Manager.",
                "4. Observe the logs for any errors during the recovery process."
            ],
            "ExpectedBehavior": "The Resource Manager should successfully recover the state of finished applications without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager throws a Null Pointer Exception during the recovery process, causing it to fail.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1692.json",
        "creation_time": "2014-02-07T02:01:17.000+0000",
        "bug_report": {
            "BugID": "YARN-1692",
            "Title": "ConcurrentModificationException in Fair Scheduler during Resource Demand Update",
            "Description": "A ConcurrentModificationException is thrown in the Fair Scheduler when iterating over application priorities. This issue occurs due to improper synchronization while accessing the resource requests map returned by FSSchedulerApp.getResourceRequests().",
            "StackTrace": [
                "2014-02-07 01:40:01,978 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Exception in fair scheduler UpdateThread",
                "java.util.ConcurrentModificationException",
                "    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)",
                "    at java.util.HashMap$ValueIterator.next(HashMap.java:954)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)",
                "    at java.lang.Thread.run(Thread.java:724)"
            ],
            "StepsToReproduce": [
                "1. Start the Hadoop YARN resource manager with the Fair Scheduler configured.",
                "2. Submit multiple applications with varying resource requests.",
                "3. Monitor the logs for the Fair Scheduler during the resource demand update phase."
            ],
            "ExpectedBehavior": "The Fair Scheduler should update resource demands without throwing any exceptions.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, causing the scheduler to fail in updating resource demands.",
            "Resolution": "A fix for this issue has been checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7697.json",
        "creation_time": "2018-01-03T19:28:50.000+0000",
        "bug_report": {
            "BugID": "YARN-7697",
            "Title": "OutOfMemoryError in Log Aggregation Service Causes NodeManager to Crash",
            "Description": "The NodeManager crashes due to an OutOfMemoryError when handling log aggregation, leading to service disruption.",
            "StackTrace": [
                "2017-12-29 01:43:50,601 FATAL yarn.YarnUncaughtExceptionHandler (YarnUncaughtExceptionHandler.java:uncaughtException(51)) - Thread Thread[LogAggregationService #0,5,main] threw an Error.  Shutting down now...",
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:823)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:840)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling(LogAggregationIndexedFileController.java:293)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.access$600(LogAggregationIndexedFileController.java:98)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:216)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:205)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:312)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:284)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)",
                "2017-12-29 01:43:50,601 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application ap"
            ],
            "StepsToReproduce": [
                "Start the NodeManager with default memory settings.",
                "Submit a job that generates a large amount of log data.",
                "Monitor the NodeManager logs for OutOfMemoryError."
            ],
            "ExpectedBehavior": "The NodeManager should handle log aggregation without crashing.",
            "ObservedBehavior": "The NodeManager crashes with an OutOfMemoryError, causing service disruption.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7382.json",
        "creation_time": "2017-10-23T23:36:59.000+0000",
        "bug_report": {
            "BugID": "YARN-7382",
            "Title": "NoSuchElementException in FairScheduler after RM Failover Causes Crash",
            "Description": "While running a MapReduce job (e.g., sleep) and an RM (Resource Manager) failover occurs, once the maps reach 100%, the now active RM crashes due to a NoSuchElementException. This leaves the cluster with no active RMs.",
            "StackTrace": [
                "2017-10-18 15:02:05,349 FATAL org.apache.hadoop.yarn.event.EventDispatcher: Error in handling event type NODE_UPDATE to the Event Dispatcher",
                "java.util.NoSuchElementException",
                "    at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)",
                "    at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)",
                "    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:128)",
                "    at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "    at java.lang.Thread.run(Thread.java:748)",
                "2017-10-18 15:02:05,360 INFO org.apache.hadoop.yarn.event.EventDispatcher: Exiting, bbye.."
            ],
            "StepsToReproduce": [
                "1. Start a MapReduce job (e.g., a simple sleep job).",
                "2. Trigger a Resource Manager failover during the job execution.",
                "3. Wait until the job's map tasks reach 100% completion."
            ],
            "ExpectedBehavior": "The Resource Manager should handle the failover gracefully without crashing.",
            "ObservedBehavior": "The active Resource Manager crashes with a NoSuchElementException, leaving the cluster without any active Resource Managers.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-1094.json",
        "creation_time": "2013-08-23T19:06:17.000+0000",
        "bug_report": {
            "BugID": "YARN-1094",
            "Title": "Null Pointer Exception on Resource Manager Restart in Secure Environment",
            "Description": "When attempting to restart the Resource Manager while a job is running, a Null Pointer Exception occurs, preventing the Resource Manager from starting successfully.",
            "StackTrace": [
                "2013-08-23 17:57:40,705 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(370)) - Recovering application application_1377280618693_0001",
                "2013-08-23 17:57:40,763 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(617)) - Failed to load/recover state",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.setTimerForTokenRenewal(DelegationTokenRenewer.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplication(DelegationTokenRenewer.java:307)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:819)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:613)",
                "        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:832)",
                "2013-08-23 17:57:40,766 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1"
            ],
            "StepsToReproduce": [
                "1. Enable the rmrestart feature.",
                "2. Start a job in the Resource Manager.",
                "3. Attempt to restart the Resource Manager."
            ],
            "ExpectedBehavior": "The Resource Manager should restart successfully without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start and throws a Null Pointer Exception.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    },
    {
        "filename": "YARN-7269.json",
        "creation_time": "2017-09-28T23:56:42.000+0000",
        "bug_report": {
            "BugID": "YARN-7269",
            "Title": "Redirection Failure for Tracking URL in Application State",
            "Description": "The tracking URL in the application state does not redirect to the ApplicationMaster for running applications, resulting in a ServletException. This issue affects the ability to access application information through the tracking URL.",
            "StackTrace": [
                "org.mortbay.log: /ws/v1/mapreduce/info",
                "javax.servlet.ServletException: Could not determine the proxy server for redirection",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:199)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:141)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1426)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "StepsToReproduce": [
                "1. Start a running application in the Hadoop YARN environment.",
                "2. Access the tracking URL for the application.",
                "3. Observe the behavior when attempting to access the ApplicationMaster."
            ],
            "ExpectedBehavior": "The tracking URL should redirect to the ApplicationMaster for running applications, allowing users to view application details.",
            "ObservedBehavior": "The tracking URL does not redirect and throws a ServletException indicating that the proxy server for redirection could not be determined.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7249.json",
        "creation_time": "2017-09-25T16:49:46.000+0000",
        "bug_report": {
            "BugID": "YARN-7249",
            "Title": "NullPointerException in CapacityScheduler when preempting container during node removal",
            "Description": "This issue occurs under the following conditions:\n\n1) A node is being removed from the scheduler.\n2) A container running on the node is being preempted.\n3) A rare race condition causes the scheduler to pass a null node to the leaf queue.\n\nThe fix involves adding a null node check inside the CapacityScheduler.",
            "StackTrace": [
                "2017-08-31 02:51:24,748 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(714)) - Error in handling event type KILL_RESERVED_CONTAINER to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705)"
            ],
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager.",
                "2. Submit a job that allocates containers on a node.",
                "3. Initiate the removal of the node from the scheduler while the container is running.",
                "4. Observe the logs for NullPointerException."
            ],
            "ExpectedBehavior": "The CapacityScheduler should handle the preemption of containers without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when a container is preempted while the node is being removed.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-4598.json",
        "creation_time": "2016-01-15T06:48:48.000+0000",
        "bug_report": {
            "BugID": "YARN-4598",
            "Title": "Invalid State Transition for Container in YARN NodeManager",
            "Description": "In our cluster, I found that the container has some problems in state transition. The logs indicate that the container transitioned from 'CONTAINER_CLEANEDUP_AFTER_KILL' to 'DONE', but an invalid event 'RESOURCE_FAILED' was encountered during this process, leading to an exception.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:83)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1071)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "StepsToReproduce": [
                "1. Deploy a YARN cluster with multiple containers.",
                "2. Trigger a container to be killed.",
                "3. Observe the state transition of the container in the logs."
            ],
            "ExpectedBehavior": "The container should handle the 'RESOURCE_FAILED' event appropriately without throwing an exception.",
            "ObservedBehavior": "The container throws an 'InvalidStateTransitonException' when encountering the 'RESOURCE_FAILED' event after transitioning to 'CONTAINER_CLEANEDUP_AFTER_KILL'.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-1149.json",
        "creation_time": "2013-09-04T21:46:58.000+0000",
        "bug_report": {
            "BugID": "YARN-1149",
            "Title": "InvalidStateTransitionException Thrown When Application Log Handling Finishes",
            "Description": "When the NodeManager receives a kill signal after an application has finished execution but before log aggregation has started, an InvalidStateTransitionException is thrown. This occurs specifically when the application log handling is marked as finished while the application is still in the RUNNING state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:689)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "StepsToReproduce": [
                "1. Submit an application to the NodeManager.",
                "2. Allow the application to finish execution.",
                "3. Send a kill signal to the NodeManager before log aggregation has started.",
                "4. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The NodeManager should handle the application log handling completion without throwing an exception.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown indicating that the application log handling finished event cannot be processed while the application is still in the RUNNING state.",
            "Resolution": "Fixed"
        }
    },
    {
        "filename": "YARN-7818.json",
        "creation_time": "2018-01-25T18:42:55.000+0000",
        "bug_report": {
            "BugID": "YARN-7818",
            "Title": "Container Launch Fails with Exit Code 143 After NM Restart",
            "Description": "During the execution of a Dshell application, containers fail to launch with exit code 143 after restarting the Node Manager (NM) where the Application Master (AM) is running. This issue occurs when the application attempts to validate the state of the containers post-restart.",
            "StackTrace": [
                "2018-01-24 09:48:30,547 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1516787230461_0001_01_000003 transitioned from RUNNING to KILLING",
                "2018-01-24 09:48:30,547 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1516787230461_0001_01_000003",
                "2018-01-24 09:48:30,552 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 143. Privileged Execution Operation Stderr:",
                "2018-01-24 09:48:30,553 WARN  runtime.DefaultLinuxContainerRuntime (DefaultLinuxContainerRuntime.java:launchContainer(127)) - Launch container failed. Exception:",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:",
                "Caused by: ExitCodeException exitCode=143:",
                "2018-01-24 09:48:30,553 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:launchContainer(557)) - Exit code from container container_e04_1516787230461_0001_01_000003 is : 143"
            ],
            "StepsToReproduce": [
                "1) Run Dshell Application using the following command:",
                "   yarn org.apache.hadoop.yarn.applications.distributedshell.Client -jar /usr/hdp/3.0.0.0-751/hadoop-yarn/hadoop-yarn-applications-distributedshell-*.jar -keep_containers_across_application_attempts -timeout 900000 -shell_command 'sleep 110' -num_containers 4",
                "2) Find out the host where the Application Master (AM) is running.",
                "3) Identify the containers launched by the application.",
                "4) Restart the Node Manager (NM) where the AM is running.",
                "5) Validate that a new attempt is not started and that the containers launched before the restart are in RUNNING state."
            ],
            "ExpectedBehavior": "The containers should remain in the RUNNING state after the Node Manager is restarted, and a new attempt should not be initiated.",
            "ObservedBehavior": "The containers fail to launch with exit code 143 after the Node Manager restart.",
            "Resolution": "A fix for this issue is checked into the tree and tested."
        }
    }
]