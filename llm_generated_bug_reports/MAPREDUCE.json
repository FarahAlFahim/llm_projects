[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "bug_report": {
            "title": "ArrayIndexOutOfBoundsException during Hadoop MapReduce Shuffle",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a MapReduce job that involves LZO compressed data.",
                    "2. Ensure that the job has a significant amount of data to process.",
                    "3. Monitor the job execution until it reaches the reduce phase."
                ],
                "actualBehavior": "The job fails during the shuffle phase with an ArrayIndexOutOfBoundsException.",
                "possibleCause": "The issue may be related to the handling of LZO compressed data in the LzoDecompressor, possibly due to incorrect input size or corrupted data."
            },
            "stackTrace": "Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29\n\tat org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n\tat com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)\n\tat org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)\n\tat org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)\n\tat org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)\n\tat org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)\n\tat org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "bug_report": {
            "title": "RuntimeException: native lz4 library not available",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop environment with Lz4 compression enabled.",
                    "2. Attempt to run a MapReduce job that utilizes Lz4 compression.",
                    "3. Observe the job execution and check for any errors."
                ],
                "actualBehavior": "The job fails with a RuntimeException indicating that the native lz4 library is not available.",
                "possibleCause": "The native lz4 library may not be installed or properly configured in the Hadoop environment."
            },
            "stackTrace": "java.lang.RuntimeException: native lz4 library not available\n\tat org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)\n\tat org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)\n\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)\n\tat org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher$EventHandler.java:391)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher$EventHandler.java:309)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)\n\tat org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "bug_report": {
            "title": "NotFoundException when accessing MapReduce task URI",
            "description": {
                "stepsToReproduce": [
                    "1. Start the MapReduce job on the server.",
                    "2. Attempt to access the task URI: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000.",
                    "3. Observe the response from the server."
                ],
                "actualBehavior": "The server returns a NotFoundException indicating that the requested URI is not found.",
                "possibleCause": "The task may not exist or has already completed, leading to the URI being unavailable."
            },
            "stackTrace": "com.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:123)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1069)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)"
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "bug_report": {
            "title": "MetricsException: QueueMetrics already exists in ResourceManager",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop ResourceManager service.",
                    "Attempt to register a metrics source with the name 'Hadoop:service=ResourceManager,name=QueueMetrics,q0=default'.",
                    "Observe the logs for any exceptions or errors."
                ],
                "actualBehavior": "The ResourceManager fails to start and throws a MetricsException indicating that the QueueMetrics for the default queue already exists.",
                "possibleCause": "The issue may be caused by an attempt to register the same MBean multiple times without proper cleanup or deregistration of the previous instance."
            },
            "stackTrace": "org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)\n\tat org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)\n\tat org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:93)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)\n\tat $Proxy6.postStart(Unknown Source)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)\nCaused by: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:113)\n\t... 19 more\n\njavax.management.RuntimeOperationsException: Exception occurred trying to register the MBean\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:969)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)\n\tat org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:57)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)\n\tat org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:93)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)\n\tat $Proxy6.postStart(Unknown Source)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)\n\tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)\n\tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)\nCaused by: java.lang.IllegalArgumentException: No object name specified\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:967)"
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException: No enum constant for JobState",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop MapReduce application.",
                    "2. Attempt to retrieve job states from the JobHistory server.",
                    "3. Observe the logs for any exceptions thrown."
                ],
                "actualBehavior": "An IllegalArgumentException is thrown indicating that there is no enum constant for JobState with the value '0'.",
                "possibleCause": "The application is trying to parse a job state value that is not defined in the JobState enum, possibly due to a misconfiguration or an unexpected value being returned from the job history."
            },
            "stackTrace": "java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0\n\tat java.lang.Enum.valueOf(Enum.java:236)\n\tat org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)\n\tat org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)\n\tat org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)\n\tat org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)\n\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1223)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "bug_report": {
            "title": "OutOfMemoryError when starting container in Hadoop YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Deploy a Hadoop YARN application with a high number of containers.",
                    "2. Monitor the resource usage of the system, particularly memory and threads.",
                    "3. Attempt to start a new container while the system is under heavy load."
                ],
                "actualBehavior": "The application fails to start a new container, throwing an UndeclaredThrowableException due to an OutOfMemoryError.",
                "possibleCause": "The system may be hitting a limit on the number of threads that can be created, leading to an OutOfMemoryError when trying to set up IO streams for the new container."
            },
            "stackTrace": "java.lang.reflect.UndeclaredThrowableException\n        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)\n        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"\"gsbl91525.blue.ygrid.yahoo.com\":45450; \n        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)\n        at $Proxy20.startContainer(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)\n        ... 4 more\nCaused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"\"gsbl91525.blue.ygrid.yahoo.com\":45450; \n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1089)\n        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)\n        ... 6 more\nCaused by: java.io.IOException: Couldn't set up IO streams\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)\n        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1065)\n        ... 7 more\nCaused by: java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:597)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)"
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "bug_report": {
            "title": "RpcNoSuchMethodException: Unknown method setErasureCodingPolicy",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to set the erasure coding policy on a Hadoop Distributed File System (HDFS) instance.",
                    "2. Execute a job that requires the erasure coding policy to be set.",
                    "3. Observe the error that occurs during the job submission process."
                ],
                "actualBehavior": "The job fails with an RpcNoSuchMethodException indicating that the method setErasureCodingPolicy is unknown.",
                "possibleCause": "The method setErasureCodingPolicy may not be implemented in the ClientProtocol interface or there may be a version mismatch between the client and server."
            },
            "stackTrace": "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol.\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:180)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1437)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1347)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)\n\tat org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)\n\tat org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:304)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:218)"
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "bug_report": {
            "title": "DiskErrorException when initializing job in Hadoop TaskTracker",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop cluster with TaskTracker configured.",
                    "2. Submit a job to the TaskTracker.",
                    "3. Monitor the TaskTracker logs for job initialization."
                ],
                "actualBehavior": "The TaskTracker fails to initialize the job and throws a DiskErrorException indicating that it could not find the job.xml file in any of the configured local directories.",
                "possibleCause": "The job cache directory may not be correctly set up or the specified local directories may not have the necessary permissions or may not exist."
            },
            "stackTrace": "org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)\n\tat org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)\n\tat org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)\n\tat org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)"
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "bug_report": {
            "title": "IOException: Spill failed during MapReduce job execution",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop MapReduce environment.",
                    "2. Run the WordCount example job with a large dataset.",
                    "3. Monitor the job execution for any spill-related errors."
                ],
                "actualBehavior": "The job fails with an IOException indicating that the spill operation has failed, leading to an EOFException.",
                "possibleCause": "The issue may be caused by corrupted output data during the spill process, or insufficient disk space for temporary files."
            },
            "stackTrace": "java.io.IOException: Spill failed\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)\n\tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)\n\tat org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:375)\n\tat org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n\tat org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)\n\tat org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)\n\tat org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)"
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "bug_report": {
            "title": "OutOfMemoryError: Java heap space in Hadoop",
            "description": {
                "stepsToReproduce": [
                    "Run a Hadoop job that processes a large dataset.",
                    "Monitor the application for memory usage during execution.",
                    "Observe the logs for any OutOfMemoryError exceptions."
                ],
                "actualBehavior": "The application throws multiple OutOfMemoryError exceptions, causing it to fail during execution.",
                "possibleCause": "The application may be processing data that exceeds the allocated Java heap space, leading to memory exhaustion."
            },
            "stackTrace": "Exception in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space\n\tat com.google.protobuf.CodedInputStream.<init>(CodedInputStream.java:538)\n\tat com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)\n\tat com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)\n\tat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)\n\tat org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)\n\nException in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space\n\tat java.util.HashMap.resize(HashMap.java:462)\n\tat java.util.HashMap.addEntry(HashMap.java:755)\n\tat java.util.HashMap.put(HashMap.java:385)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)\n\tat org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)\n\tat java.lang.Thread.run(Thread.java:619)\n\nException in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space\n\nException in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space"
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException: unknown event type in JobBuilder.process",
            "description": {
                "stepsToReproduce": [
                    "Run the Hadoop Rumen tool with a job history file that contains an unrecognized event type.",
                    "Ensure that the job history file is properly formatted but includes an event type that is not handled by the JobBuilder.",
                    "Observe the output or logs for any exceptions thrown during the processing."
                ],
                "actualBehavior": "The application throws an IllegalArgumentException indicating an unknown event type when processing the job history.",
                "possibleCause": "The job history file may contain an event type that is not defined or supported by the current version of the JobBuilder."
            },
            "stackTrace": "java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type\n        at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)\n        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)"
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "bug_report": {
            "title": "IOException: Filesystem closed during MapTask execution",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop cluster with HDFS.",
                    "2. Submit a MapReduce job that reads from HDFS.",
                    "3. Ensure that the filesystem is closed or becomes unavailable during the execution of the MapTask."
                ],
                "actualBehavior": "The MapTask fails with an IOException indicating that the filesystem is closed.",
                "possibleCause": "The issue may occur if the HDFS is closed or becomes unavailable while the MapTask is trying to read data from it."
            },
            "stackTrace": "java.io.IOException: Filesystem closed\n\tat org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)\n\tat java.io.DataInputStream.readByte(DataInputStream.java:265)\n\tat org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)\n\tat org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)\n\tat org.apache.hadoop.io.Text.readString(Text.java:464)\n\tat org.apache.hadoop.io.Text.readString(Text.java:457)\n\tat org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "bug_report": {
            "title": "NullPointerException in LocalJobRunner during MapTask execution",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop environment with a MapReduce job that utilizes CryptoOutputStream.",
                    "2. Submit the job for execution using LocalJobRunner.",
                    "3. Monitor the job execution to observe the failure."
                ],
                "actualBehavior": "The job fails with a NullPointerException, preventing successful execution.",
                "possibleCause": "The NullPointerException may be caused by an uninitialized variable or resource in the CryptoOutputStream constructor."
            },
            "stackTrace": "java.lang.Exception: java.lang.NullPointerException\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)\n        at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)\n        at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)\n        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask$NewOutputCollector.java:723)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "bug_report": {
            "title": "Container Launch Failure in Hadoop YARN NodeManager",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN cluster.",
                    "2. Submit a job that requires container execution.",
                    "3. Monitor the NodeManager logs for any errors during container launch."
                ],
                "actualBehavior": "The container fails to launch, resulting in an ExitCodeException with exitCode=1.",
                "possibleCause": "The failure may be due to incorrect configuration of the NodeManager or insufficient permissions for executing the container command."
            },
            "stackTrace": "ExitCodeException exitCode=1: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:927)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:838)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "bug_report": {
            "title": "NullPointerException in AppController when processing bad requests",
            "description": {
                "stepsToReproduce": [
                    "Send a bad request to the AppController endpoint.",
                    "Ensure that the request contains null or invalid parameters.",
                    "Observe the server logs for any exceptions thrown."
                ],
                "actualBehavior": "The server throws a NullPointerException, causing the request to fail without a proper error response.",
                "possibleCause": "The Joiner class is attempting to join null values, which leads to a NullPointerException."
            },
            "stackTrace": "java.lang.reflect.InvocationTargetException\n        at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)\n        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)\n        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n        .......\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.lang.NullPointerException\n        at com.google.common.base.Joiner.toString(Joiner.java:317)\n        at com.google.common.base.Joiner.appendTo(Joiner.java:97)\n        at com.google.common.base.Joiner.appendTo(Joiner.java:127)\n        at com.google.common.base.Joiner.join(Joiner.java:158)\n        at com.google.common.base.Joiner.join(Joiner.java:166)\n        at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)\n        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)"
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "bug_report": {
            "title": "ClosedChannelException during IPC response processing in Hadoop",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop cluster with TaskTracker running.",
                    "2. Submit a job that requires communication between the client and the server.",
                    "3. Monitor the logs for any exceptions during the job execution."
                ],
                "actualBehavior": "The job fails with a ClosedChannelException, indicating that the channel used for communication is closed unexpectedly.",
                "possibleCause": "The SocketChannel may have been closed prematurely, possibly due to network issues or server-side errors."
            },
            "stackTrace": "java.nio.channels.ClosedChannelException\n\tat sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)\n\tat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)\n\tat org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)\n\tat org.apache.hadoop.ipc.Server.access$2000(Server.java:98)\n\tat org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)\n\tat org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)\n\norg.apache.hadoop.util.Shell$ExitCodeException: \n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:255)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:182)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)\n\tat org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)\n\tat org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)\n\tat org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)\n\tat org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)\n\tat org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)"
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "bug_report": {
            "title": "NullPointerException in TaskAttemptImpl StatusUpdater",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a MapReduce job to the Hadoop cluster.",
                    "2. Monitor the job execution through the YARN ResourceManager.",
                    "3. Observe the logs for any task attempts that fail."
                ],
                "actualBehavior": "The job fails with a NullPointerException during the status update of a task attempt.",
                "possibleCause": "It is possible that a required object or state is not initialized properly before the status update is attempted."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)\n        at org.apache.hadoop.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)\n        at org.apache.hadoop.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)\n        at java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "bug_report": {
            "title": "IOException: Broken pipe during Hadoop streaming job",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop streaming job with a PipeMapper.",
                    "2. Provide input data to the streaming job.",
                    "3. Execute the job and observe the output."
                ],
                "actualBehavior": "The job fails with a java.io.IOException: Broken pipe error.",
                "possibleCause": "This issue may occur if the output stream is closed unexpectedly, possibly due to the downstream process terminating before the data is fully written."
            },
            "stackTrace": "Error: java.io.IOException: Broken pipe\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:282)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:105)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)\n\tat org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)\n\tat org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:394)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:329)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.doAs.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "bug_report": {
            "title": "IOException: File already exists when writing output in Hadoop",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop job that writes output to a specified path in Azure Blob Storage.",
                    "2. Run the job for the first time, allowing it to create the output files.",
                    "3. Run the same job again with the same output path."
                ],
                "actualBehavior": "The job fails with an IOException indicating that the file already exists.",
                "possibleCause": "The output path specified for the Hadoop job already contains files from a previous run, and the job does not handle overwriting existing files."
            },
            "stackTrace": "java.io.IOException: File already exists:wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2\n       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)\n       at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n       at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)\n       at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)\n       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)\n       at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)\n       at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)\n       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)\n       at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)\n       at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\n       at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n       at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n       at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:415)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n       at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "bug_report": {
            "title": "ApplicationNotFoundException when retrieving application report",
            "description": {
                "stepsToReproduce": [
                    "Submit a job to the YARN ResourceManager.",
                    "Wait for the job to complete or terminate it.",
                    "Attempt to retrieve the application report using the application ID after the job has completed or been terminated."
                ],
                "actualBehavior": "An IOException is thrown indicating that the application with the specified ID does not exist in the ResourceManager.",
                "possibleCause": "The application ID may be valid only during the job's execution. Once the job is completed or terminated, the application report may no longer be accessible."
            },
            "stackTrace": "java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.\n\tat org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)\n\tat org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)\n\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)\n\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)\n\tat org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)\n\tat org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)\n\tat org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)\n\tat org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)\n\tat org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.checkRunningState:257)\n\tat org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.checkState:282)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)\n\tat org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)\n\tat java.lang.Thread.run(Thread.java:662)\n\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)"
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException: Illegal job state: ERROR in Hadoop MapReduce",
            "description": {
                "stepsToReproduce": [
                    "Submit a MapReduce job to the Hadoop cluster.",
                    "Ensure that the job encounters an error during execution.",
                    "Attempt to transition the job state after the error occurs."
                ],
                "actualBehavior": "The job fails with an IllegalArgumentException indicating an illegal job state: ERROR.",
                "possibleCause": "The job state machine is not handling the ERROR state correctly, leading to an attempt to transition from an invalid state."
            },
            "stackTrace": "java.lang.IllegalArgumentException: Illegal job state: ERROR\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "bug_report": {
            "title": "ClassCastException in LongSumReducer",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop MapReduce job that uses LongSumReducer.",
                    "2. Ensure that the input data contains IntWritable values instead of LongWritable.",
                    "3. Run the MapReduce job."
                ],
                "actualBehavior": "The job fails with a ClassCastException indicating that IntWritable cannot be cast to LongWritable.",
                "possibleCause": "The input data types do not match the expected types for LongSumReducer, which expects LongWritable values."
            },
            "stackTrace": "java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable\n\tat org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)\n\tat org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)"
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "bug_report": {
            "title": "NullPointerException during Shuffle in Hadoop MapReduce Reduce Task",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a MapReduce job that requires shuffling of data.",
                    "2. Ensure that the job has a significant amount of data to process.",
                    "3. Monitor the job execution and observe the logs for any errors."
                ],
                "actualBehavior": "The job fails with a NullPointerException during the shuffle phase, specifically in the ShuffleSchedulerImpl.copyFailed method.",
                "possibleCause": "The NullPointerException may be caused by an uninitialized variable or an unexpected null value in the ShuffleSchedulerImpl class."
            },
            "stackTrace": "Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25\n         at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)\n         at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)\n         at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n         at java.security.AccessController.doPrivileged(Native Method)\n         at javax.security.auth.Subject.doAs(Subject.java:415)\n         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n         at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.lang.NullPointerException\n         at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)\n         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)\n         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "bug_report": {
            "title": "ClassCastException in OutputCommitter during Task Recovery",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a MapReduce job using Hadoop framework.",
                    "2. Simulate a failure during task execution to trigger recovery.",
                    "3. Observe the logs for any exceptions thrown during the recovery process."
                ],
                "actualBehavior": "A ClassCastException is thrown indicating that org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext.",
                "possibleCause": "The issue may arise from a mismatch between the versions of the Hadoop libraries being used, leading to incompatible class definitions."
            },
            "stackTrace": "java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext\n\tat org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)\n\tat org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "bug_report": {
            "title": "Deadlock in Hadoop MapReduce Shuffle Process",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop MapReduce job with a large dataset.",
                    "2. Monitor the job execution, particularly the reduce phase.",
                    "3. Observe the thread states in the logs during the shuffle phase."
                ],
                "actualBehavior": "The reduce task hangs indefinitely in a waiting state, causing the job to fail.",
                "possibleCause": "The EventFetcher thread is sleeping while another thread is waiting on the EventFetcher object, leading to a potential deadlock situation."
            },
            "stackTrace": "java.lang.Thread.State: TIMED_WAITING (sleeping)\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)\n\njava.lang.Thread.State: WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        - waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)\n        at java.lang.Thread.join(Thread.java:1143)\n        - locked <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)\n        at java.lang.Thread.join(Thread.java:1196)\n        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "bug_report": {
            "title": "NodeManager Fails to Start Due to Duplicate Registration Error",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN cluster.",
                    "2. Attempt to start the NodeManager service on a node.",
                    "3. Observe the logs for any errors during the startup process."
                ],
                "actualBehavior": "The NodeManager fails to start with a YarnException indicating a duplicate registration from the node.",
                "possibleCause": "The NodeManager may be attempting to register with the ResourceManager while another instance is already registered, possibly due to a previous instance not shutting down properly."
            },
            "stackTrace": "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)\nCaused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n\t... 2 more\nCaused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)\n\tat $Proxy13.registerNodeManager(Unknown Source)\n\tat org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)"
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException during Task and Job Handling in Hadoop YARN",
            "description": {
                "stepsToReproduce": [
                    "Submit a job to the Hadoop YARN cluster.",
                    "Ensure that the job encounters a failure during task execution.",
                    "Monitor the logs for exceptions related to state transitions."
                ],
                "actualBehavior": "The application throws InvalidStateTransitionException for both task and job events, indicating that the system is trying to process events in an invalid state.",
                "possibleCause": "The state machine may not be handling certain events correctly when transitioning between states, particularly during error handling scenarios."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:862)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n\tat java.lang.Thread.run(Thread.java:619)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:657)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:111)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:848)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:844)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)\n\tat java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "bug_report": {
            "title": "YARN Application Manager Stuck in WAITING State",
            "description": {
                "stepsToReproduce": [
                    "Submit a YARN application that requires container allocation.",
                    "Monitor the application status through the ResourceManager UI.",
                    "Observe the application manager thread state in the logs."
                ],
                "actualBehavior": "The application manager thread is stuck in a WAITING state, preventing the allocation of containers and causing the application to hang indefinitely.",
                "possibleCause": "The application manager may be waiting for a response from the ContainerManager, which is not being received due to a potential deadlock or network issue."
            },
            "stackTrace": "java.lang.Thread.State: WAITING (on object monitor)\n    at java.lang.Object.wait(Native Method)\n    at java.lang.Object.wait(Object.java:485)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1076)\n    - locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)\n    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)\n    at $Proxy76.startContainer(Unknown Source)\n    at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)\n    at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)\n    at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n    at java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "bug_report": {
            "title": "NodeManager Fails to Start Due to Invalid Resource Tracker Address",
            "description": {
                "stepsToReproduce": [
                    "1. Configure the Yarn ResourceManager with an invalid 'yarn.resourcemanager.resource-tracker.address'.",
                    "2. Attempt to start the NodeManager service.",
                    "3. Observe the logs for any errors or exceptions."
                ],
                "actualBehavior": "The NodeManager fails to start and throws an exception indicating that the resource tracker address is not a valid host:port pair.",
                "possibleCause": "The configuration for 'yarn.resourcemanager.resource-tracker.address' is incorrectly set, leading to a failure in creating a socket address."
            },
            "stackTrace": "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)\nCaused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n\t... 2 more\nCaused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)\n\t... 3 more\n\njava.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED\n\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)\n\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:95)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:85)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)\n\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)\n\njava.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED\n\tat org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)\n\tat org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:87)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)\n\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)"
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "bug_report": {
            "title": "InterruptedException in AsyncDispatcher during Application Master allocation",
            "description": {
                "stepsToReproduce": [
                    "1. Start a YARN application using the Application Master.",
                    "2. Trigger resource allocation requests from the Application Master.",
                    "3. Monitor the logs for any exceptions or errors during the allocation process."
                ],
                "actualBehavior": "An InterruptedException is thrown during the allocation process, causing the application to fail.",
                "possibleCause": "The exception may be caused by an interruption in the thread handling the allocation, possibly due to external factors such as timeouts or manual interruptions."
            },
            "stackTrace": "java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)\n\t... 11 more\n\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)\n\t... 11 more\n\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.YarnRuntimeException): java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)"
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "bug_report": {
            "title": "File Not Found Error in Hadoop Task Log",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop job that generates task logs.",
                    "2. Ensure that the output directory for the task logs does not exist or is incorrectly specified.",
                    "3. Monitor the job execution to observe the error."
                ],
                "actualBehavior": "The job fails with an ENOENT error indicating that a file or directory could not be found.",
                "possibleCause": "The specified directory for writing task logs may not exist, or there may be permission issues preventing access to the directory."
            },
            "stackTrace": "ENOENT: No such file or directory\n        at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)\n        at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)\n        at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)\n        at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)\n        at org.apache.hadoop.mapred.Child.main(Child.java:229)"
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "bug_report": {
            "title": "FileNotFoundException when accessing application logs",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop application that generates logs.",
                    "2. Attempt to access the logs for the application using the LogDumper utility.",
                    "3. Observe the output for any errors."
                ],
                "actualBehavior": "The application throws a FileNotFoundException indicating that the specified log file does not exist.",
                "possibleCause": "The log file may not have been created due to an error in the application or it may have been deleted before the LogDumper attempted to access it."
            },
            "stackTrace": "Exception in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.\n\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)\n\tat org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)\n\tat org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)\n\tat org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)\n\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)\n\tat org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)\n\tat org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)\n\tat org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)\n\tat org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)"
        }
    },
    {
        "filename": "MAPREDUCE-6815.json",
        "creation_time": "2016-12-02T00:30:37.000+0000",
        "bug_report": {
            "title": "AssertionError in TestKill.testKillTask due to incorrect job state",
            "description": {
                "stepsToReproduce": [
                    "1. Set up the Hadoop MapReduce environment.",
                    "2. Execute the TestKill.testKillTask test case.",
                    "3. Observe the job state after the task is killed."
                ],
                "actualBehavior": "The test fails with an AssertionError indicating that the job state is ERROR instead of the expected SUCCEEDED.",
                "possibleCause": "The job may not be transitioning to the SUCCEEDED state due to an error in task execution or handling of the kill signal."
            },
            "stackTrace": "java.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>\nat org.junit.Assert.fail(Assert.java:88)\nat org.junit.Assert.failNotEquals(Assert.java:743)\nat org.junit.Assert.assertEquals(Assert.java:118)\nat org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)\nat org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)"
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "bug_report": {
            "title": "NullPointerException during Application Master registration in YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN ResourceManager.",
                    "2. Submit a Kafka application that requires YARN for resource management.",
                    "3. Monitor the logs for the Application Master during the registration process."
                ],
                "actualBehavior": "The application fails to register with the ResourceManager, resulting in a NullPointerException.",
                "possibleCause": "The issue may be caused by a missing or improperly initialized request prototype in the ClientRMProtocol, leading to a NullPointerException when attempting to register the Application Master."
            },
            "stackTrace": "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)\n\tat kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)\n\tat kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)\n\tat kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)\nCaused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:130)\n\tat $Proxy6.registerApplicationMaster(Unknown Source)\n\tat org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:101)\n\t... 3 more\nCaused by: java.lang.NullPointerException: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1084)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)\n\t... 5 more\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)"
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "bug_report": {
            "title": "AccessControlException when attempting to cancel delegation token",
            "description": {
                "stepsToReproduce": [
                    "1. Log in as <someuser>.",
                    "2. Attempt to cancel a delegation token using the HistoryClientService.",
                    "3. Observe the response from the system."
                ],
                "actualBehavior": "<someuser> receives an AccessControlException indicating they are not authorized to cancel the token.",
                "possibleCause": "The user <someuser> does not have the necessary permissions to cancel the delegation token, possibly due to misconfigured access control settings."
            },
            "stackTrace": "org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token\n        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)\n        at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)\n        at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)\n        at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)"
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "bug_report": {
            "title": "CircularRedirectException in WebAppProxyServlet",
            "description": {
                "stepsToReproduce": [
                    "1. Access the web application that utilizes the WebAppProxyServlet.",
                    "2. Trigger a request that leads to a redirect to a specific job attempt URL.",
                    "3. Observe the behavior when the redirect occurs."
                ],
                "actualBehavior": "The application throws a CircularRedirectException, indicating an infinite redirect loop.",
                "possibleCause": "The URL being redirected to may be incorrectly configured, causing it to redirect back to itself."
            },
            "stackTrace": "Caused by:\n\norg.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'\n        at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)\n        at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)\n        at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "bug_report": {
            "title": "UnsatisfiedLinkError in NativeIO on Windows",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop environment on a Windows machine.",
                    "2. Attempt to run a Hadoop job that requires access to local directories.",
                    "3. Observe the logs for any errors during the job execution."
                ],
                "actualBehavior": "The job fails with an UnsatisfiedLinkError indicating that the native method 'access0' could not be found.",
                "possibleCause": "The native library for Windows may not be properly loaded or is missing, leading to the UnsatisfiedLinkError."
            },
            "stackTrace": "java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)\n\tat org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)\n\tat org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)\n\tat org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)\n\tat org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)\n\tat org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)"
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException: Wrong FS when accessing HDFS path",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to copy a file from HDFS to the local file system using the Hadoop FileSystem API.",
                    "2. Ensure that the source path is an HDFS path (e.g., hdfs://10.18.52.146:9000/history/job_201104291518_0001_root).",
                    "3. Execute the copy operation."
                ],
                "actualBehavior": "An IllegalArgumentException is thrown indicating a mismatch between the expected file system and the provided path.",
                "possibleCause": "The code is trying to access an HDFS path while the local file system is expected, leading to a conflict in file system types."
            },
            "stackTrace": "java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///\\n\\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)\\n\\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)\\n\\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)\\n\\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)\\n\\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)\\n\\tat org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)\\n\\tat org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)\\n\\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)\\n\\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)\\n\\tat org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\\n\\tat java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException: Invalid key to HMAC computation in CapacityScheduler",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit a job that requires container allocation.",
                    "3. Monitor the ResourceManager logs for errors during container assignment."
                ],
                "actualBehavior": "The ResourceManager throws an IllegalArgumentException indicating an invalid key for HMAC computation, preventing container allocation.",
                "possibleCause": "The issue may be caused by an improperly configured secret key for HMAC computation, possibly due to missing or corrupted configuration settings."
            },
            "stackTrace": "java.lang.IllegalArgumentException: Invalid key to HMAC computation\n        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)\n        at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)\n        at java.lang.Thread.run(Thread.java:619)\nCaused by: java.security.InvalidKeyException: Secret key expected\n        at com.sun.crypto.provider.HmacCore.a(DashoA13*..)\n        at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)\n        at javax.crypto.Mac.init(DashoA13*..)\n        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)"
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "bug_report": {
            "title": "IOException due to Resource Change on HDFS",
            "description": {
                "stepsToReproduce": [
                    "Submit a job to the Hadoop YARN cluster that involves uploading a JAR file to HDFS.",
                    "Modify the JAR file after it has been uploaded but before the job starts executing.",
                    "Monitor the job execution and observe the logs for any errors."
                ],
                "actualBehavior": "The job fails with an IOException indicating that the resource has changed on the source filesystem.",
                "possibleCause": "The JAR file was modified after it was staged in HDFS, leading to a mismatch in expected and actual file sizes."
            },
            "stackTrace": "java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875\n       at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)\n       at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)\n       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)\n       at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:396)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)\n       at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)\n       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n       at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n       at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n       at java.lang.Thread.run(Thread.java:619)\norg.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875\n       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.convertFromProtoFormat(LocalResourceStatusPBImpl.java:217)\n       at org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl.getException(LocalResourceStatusPBImpl.java:147)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.update(ResourceLocalizationService.java:827)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker.processHeartbeat(ResourceLocalizationService.java:497)\n       at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.heartbeat(ResourceLocalizationService.java:222)\n       at org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.service.LocalizationProtocolPBServiceImpl.heartbeat(LocalizationProtocolPBServiceImpl.java:46)\n       at org.apache.hadoop.yarn.proto.LocalizationProtocol$LocalizationProtocolService$2.callBlockingMethod(LocalizationProtocol.java:57)\n       at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:342)\n       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1493)\n       at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:396)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1487)"
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "bug_report": {
            "title": "IllegalMonitorStateException in IndexCache during ShuffleHandler operation",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop MapReduce job that involves shuffling data.",
                    "2. Ensure that the job has a significant amount of data to process.",
                    "3. Monitor the logs for any exceptions during the shuffle phase."
                ],
                "actualBehavior": "The job fails with an IllegalMonitorStateException, indicating that a thread attempted to wait on an object without holding the object's monitor.",
                "possibleCause": "The issue may be caused by improper synchronization in the IndexCache class, where a thread is trying to call wait() without owning the monitor of the object."
            },
            "stackTrace": "java.lang.IllegalMonitorStateException\n\tat java.lang.Object.wait(Native Method)\n\tat org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)\n\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)\n\tat org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)\n\tat org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)\n\tat org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)\n\tat org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "bug_report": {
            "title": "YARN Job Failure Due to Invalid NodeId",
            "description": {
                "stepsToReproduce": [
                    "Submit a job to the YARN cluster.",
                    "Monitor the job's progress and completion events.",
                    "Encounter an error related to an unknown job and invalid NodeId."
                ],
                "actualBehavior": "The job fails with an IllegalArgumentException indicating an invalid NodeId format.",
                "possibleCause": "The NodeId provided does not conform to the expected host:port format, which may be due to misconfiguration or an issue in the job submission process."
            },
            "stackTrace": "RemoteTrace: \n at Local Trace: \n\torg.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1322040898409_0005\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:151)\n\tat $Proxy10.getTaskAttemptCompletionEvents(Unknown Source)\n\tat org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)\n\tat sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:273)\n\tat org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:320)\n\tat org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:438)\n\tat org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:621)\n\tat org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1231)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1179)\n\tat org.apache.hadoop.examples.Sort.run(Sort.java:181)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n\tat org.apache.hadoop.examples.Sort.main(Sort.java:192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)\n\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:189)\n\njava.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port\n        at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:985)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:128)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:845)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)\n        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "bug_report": {
            "title": "IOException due to ClosedByInterruptException in Hadoop IPC Client",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop cluster and ensure all services are running.",
                    "2. Submit a MapReduce job that requires communication with the Hadoop IPC Client.",
                    "3. Interrupt the job execution (e.g., by sending an interrupt signal or stopping the thread)."
                ],
                "actualBehavior": "The job fails with an IOException indicating a ClosedByInterruptException.",
                "possibleCause": "The issue may be caused by the job being interrupted while the IPC Client is attempting to write data to the socket, leading to the ClosedByInterruptException."
            },
            "stackTrace": "java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException\nat org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)\nat org.apache.hadoop.ipc.Client.call(Client.java:1062)\nat org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)\nat $Proxy0.statusUpdate(Unknown Source)\nat org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)\nat java.lang.Thread.run(Thread.java:662)\nCaused by: java.nio.channels.ClosedByInterruptException\nat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)\nat sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)\nat org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)\nat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\nat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)\nat org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)\nat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\nat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)\nat java.io.DataOutputStream.flush(DataOutputStream.java:106)\nat org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)\nat org.apache.hadoop.ipc.Client.call(Client.java:1040)"
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "bug_report": {
            "title": "NullPointerException in JvmManager during Task Execution",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop MapReduce job that requires JVM management.",
                    "2. Monitor the task execution process.",
                    "3. Observe the logs for any errors related to JVM management."
                ],
                "actualBehavior": "The task fails with a NullPointerException, causing the job to terminate unexpectedly.",
                "possibleCause": "The NullPointerException may be caused by an uninitialized object or resource in the JvmManager class, specifically in the getDetails method."
            },
            "stackTrace": "java.lang.Throwable: Child Error\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)\n\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)\n\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)\n\n-------\njava.lang.Throwable: Child Error\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)\n\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)\n\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)\n\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException when handling JOB_TASK_ATTEMPT_COMPLETED event",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop YARN cluster.",
                    "2. Monitor the job's task attempts and ensure that at least one task fails.",
                    "3. Observe the state transitions of the job and its task attempts."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown when the system attempts to handle a JOB_TASK_ATTEMPT_COMPLETED event while the job is in a FAILED state.",
                "possibleCause": "The system may not be correctly handling state transitions for job task attempts that complete after the job has already failed."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)\n        at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "bug_report": {
            "title": "NullPointerException in AppSchedulingInfo during container allocation",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application that requires resource allocation.",
                    "3. Monitor the ResourceManager logs for errors during the scheduling process."
                ],
                "actualBehavior": "A NullPointerException is thrown in the AppSchedulingInfo class, causing the application to fail during resource allocation.",
                "possibleCause": "It is possible that a required object or resource is not initialized or is null when the allocateNodeLocal method is called."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)\n        at java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "bug_report": {
            "title": "YarnRuntimeException due to ClosedChannelException in JobHistoryEventHandler",
            "description": {
                "stepsToReproduce": [
                    "Submit a MapReduce job to the Hadoop cluster.",
                    "Monitor the job's execution through the JobHistory server.",
                    "Stop the job or let it finish to trigger the JobHistoryEventHandler."
                ],
                "actualBehavior": "The application throws a YarnRuntimeException caused by a ClosedChannelException when attempting to handle job history events.",
                "possibleCause": "The issue may be related to the JobHistory server trying to write to a closed output stream, possibly due to premature termination of the job or network issues."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$1.run(MRAppMaster.java:605)\nCaused by: java.nio.channels.ClosedChannelException\n        at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)\n        at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)\n        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n        at java.io.DataOutputStream.write(DataOutputStream.java:107)\n        at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)\n        at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)\n        at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)\n        at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)"
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "bug_report": {
            "title": "Kerberos Authentication Failure in Hadoop Client",
            "description": {
                "stepsToReproduce": [
                    "1. Configure Hadoop to use Kerberos authentication.",
                    "2. Attempt to connect to the Hadoop cluster at 192.168.7.80:8020.",
                    "3. Execute a job that requires access to the Hadoop Distributed File System (HDFS)."
                ],
                "actualBehavior": "The job fails with an IOException indicating that no valid Kerberos credentials are provided.",
                "possibleCause": "The user may not have a valid Kerberos ticket-granting ticket (TGT) or the Kerberos configuration may be incorrect."
            },
            "stackTrace": "java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1097)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\n        at $Proxy7.getProtocolVersion(Unknown Source)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)\n        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)\n        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)\n        at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)\n        at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)\n        at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)\n        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)\n        at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)\n        at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:543)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:488)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:590)\n        at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:187)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1228)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1072)\n        ... 20 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:134)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:385)\n        at org.apache.hadoop.ipc.Client$Connection.access$1200(Client.java:187)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:583)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:580)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:579)\n        ... 23 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)"
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "bug_report": {
            "title": "NullPointerException in AppSchedulingInfo during container allocation",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application that requires resource allocation.",
                    "3. Monitor the ResourceManager logs for errors during the scheduling process."
                ],
                "actualBehavior": "A NullPointerException is thrown, causing the application to fail during the resource allocation process.",
                "possibleCause": "It is possible that a required object or resource is not initialized or is null when the allocateNodeLocal method is called."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)\n        at java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException in Hadoop YARN during Task Attempt",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop YARN cluster.",
                    "2. Monitor the task attempts for the submitted job.",
                    "3. Observe the state transitions of the task attempts."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown with the message 'Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING'.",
                "possibleCause": "The task attempt is trying to transition to a state that is not valid at the current state, possibly due to a race condition or incorrect event handling."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:722)"
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "bug_report": {
            "title": "UnknownServiceException: no content-type in JobEndNotifier",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop MapReduce framework.",
                    "2. Wait for the job to complete.",
                    "3. Observe the logs for any exceptions thrown during the job completion notification."
                ],
                "actualBehavior": "An UnknownServiceException is thrown indicating 'no content-type' when the job completion notification is attempted.",
                "possibleCause": "The URL being accessed during the job completion notification does not return a valid content-type, which may indicate a misconfiguration or an issue with the service being called."
            },
            "stackTrace": "java.net.UnknownServiceException: no content-type\n\tat java.net.URLConnection.getContentHandler(URLConnection.java:1192)\n\tat java.net.URLConnection.getContent(URLConnection.java:689)\n\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)\n\tat org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)"
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "bug_report": {
            "title": "FileNotFoundException due to Permission Denied on Local File System",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop YARN cluster.",
                    "2. Ensure that the job requires writing to the local file system at the specified path.",
                    "3. Monitor the job execution and check for any permission-related errors."
                ],
                "actualBehavior": "The job fails with a FileNotFoundException indicating 'Permission denied' when attempting to write to the specified local file path.",
                "possibleCause": "The user running the Hadoop job does not have the necessary permissions to write to the directory '/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/'."
            },
            "stackTrace": "java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)\n\n at java.io.FileOutputStream.open0(Native Method)\n\n at java.io.FileOutputStream.open(FileOutputStream.java:270)\n\n at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n\n at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)\n\n at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)\n\n at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)\n\n at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)\n\n at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)\n\n at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\n\n at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\n\n at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1026)\n\n at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:703)\n\n at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)\n\n at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)\n\n at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)\n\n at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)\n\n at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)\n\n at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)\n\n at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)\n\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n\n at java.security.AccessController.doPrivileged(Native Method)\n\n at javax.security.auth.Subject.doAs(Subject.java:422)\n\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)\n\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)"
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "bug_report": {
            "title": "NullPointerException in CountersBlock Constructor",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop MapReduce application.",
                    "2. Access the web interface for the application.",
                    "3. Trigger the CountersBlock to load."
                ],
                "actualBehavior": "The application throws a NullPointerException when trying to initialize the CountersBlock.",
                "possibleCause": "The CountersBlock constructor is attempting to access a null reference, possibly due to uninitialized counters."
            },
            "stackTrace": "Caused by: com.google.inject.ProvisionException: Guice provision errors:\n\njava.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n...\n..\n...\n\n1) Error injecting constructor, java.lang.NullPointerException\n  at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)\n  while locating org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock\n...\n..\n...\nCaused by: java.lang.NullPointerException    \n    at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)\n    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)\n    at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)"
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "bug_report": {
            "title": "ArithmeticException: Division by Zero in ResourceCalculatorUtils",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop MapReduce job that requires resource allocation.",
                    "2. Ensure that the resource manager is configured with zero available containers.",
                    "3. Monitor the logs for any exceptions during the resource allocation process."
                ],
                "actualBehavior": "The application throws an ArithmeticException due to division by zero when trying to compute available containers.",
                "possibleCause": "The ResourceCalculatorUtils.computeAvailableContainers method does not handle the case where the total available resources are zero, leading to a division by zero error."
            },
            "stackTrace": "java.lang.ArithmeticException: / by zero\nat org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)\nat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)\nat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "bug_report": {
            "title": "No handler registered for EventType: AM_STARTED in Hadoop YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop YARN application.",
                    "2. Monitor the job history server for events.",
                    "3. Observe the logs for any exceptions related to event handling."
                ],
                "actualBehavior": "An exception is thrown indicating that there is no handler registered for the EventType: AM_STARTED.",
                "possibleCause": "It is possible that the event handling mechanism for the AM_STARTED event type is not properly configured or implemented in the current version of the application."
            },
            "stackTrace": "java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n        at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "bug_report": {
            "title": "NullPointerException in TaskAttemptImpl during DeallocateContainerTransition",
            "description": {
                "stepsToReproduce": [
                    "1. Start a MapReduce job using Hadoop YARN.",
                    "2. Simulate a failure in one of the task attempts.",
                    "3. Observe the logs for any exceptions thrown during the deallocation of the container."
                ],
                "actualBehavior": "A NullPointerException is thrown in the TaskAttemptImpl class when attempting to send a Job History start event for a failed task.",
                "possibleCause": "The NullPointerException may be caused by an uninitialized variable or object that is expected to be present during the deallocation process."
            },
            "stackTrace": "java.lang.NullPointerException\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException: Comparison method violates its general contract",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop MapReduce job that requires sorting of data.",
                    "2. Ensure that the comparison method used for sorting does not adhere to the contract of the Comparator interface.",
                    "3. Monitor the job execution until it triggers the sorting process."
                ],
                "actualBehavior": "The job fails with an IllegalArgumentException indicating that the comparison method violates its general contract.",
                "possibleCause": "The Comparator used for sorting may not be consistent with equals, leading to unpredictable behavior during sorting operations."
            },
            "stackTrace": "java.lang.IllegalArgumentException: Comparison method violates its general contract!\n     at java.util.TimSort.mergeLo(TimSort.java:747)\n     at java.util.TimSort.mergeAt(TimSort.java:483)\n     at java.util.TimSort.mergeCollapse(TimSort.java:408)\n     at java.util.TimSort.sort(TimSort.java:214)\n     at java.util.TimSort.sort(TimSort.java:173)\n     at java.util.Arrays.sort(Arrays.java:659)\n     at java.util.Collections.sort(Collections.java:217)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)\n     at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)\n     at java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "bug_report": {
            "title": "NumberFormatException when parsing large input string in Hadoop",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop job that requires resource calculations.",
                    "2. Ensure that the process generates a large number that exceeds the Long.MAX_VALUE.",
                    "3. Monitor the logs for any exceptions thrown during the job execution."
                ],
                "actualBehavior": "The job fails with a NumberFormatException indicating that the input string is too large to be parsed as a Long.",
                "possibleCause": "The input string '18446743988060683582' exceeds the maximum value for a Long in Java, which is 9223372036854775807."
            },
            "stackTrace": "java.lang.NumberFormatException: For input string: \"18446743988060683582\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n\tat java.lang.Long.parseLong(Long.java:422)\n\tat java.lang.Long.parseLong(Long.java:468)\n\tat org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)\n\tat org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)\n\tat org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)\n\tat org.apache.hadoop.mapred.Task.initialize(Task.java:536)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)"
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "bug_report": {
            "title": "IOException during Subversion checkout in Hudson",
            "description": {
                "stepsToReproduce": [
                    "Trigger a build in Hudson that uses Subversion SCM.",
                    "Ensure that there are existing files in the workspace that need to be deleted.",
                    "Observe the build process and check for any errors."
                ],
                "actualBehavior": "The build fails with an IOException indicating that it is unable to delete a specific file in the workspace.",
                "possibleCause": "The file may be locked or in use by another process, preventing deletion."
            },
            "stackTrace": "hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7\n\tat hudson.FilePath.act(FilePath.java:749)\n\tat hudson.FilePath.act(FilePath.java:735)\n\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)\n\tat hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)\n\tat hudson.model.AbstractProject.checkout(AbstractProject.java:1116)\n\tat hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild$AbstractRunner.java:479)\n\tat hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild$AbstractRunner.java:411)\n\tat hudson.model.Run.run(Run.java:1324)\n\tat hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)\n\tat hudson.model.ResourceController.execute(ResourceController.java:88)\n\tat hudson.model.Executor.run(Executor.java:139)\nCaused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "bug_report": {
            "title": "Kerberos Authentication Failure in Hadoop",
            "description": {
                "stepsToReproduce": [
                    "1. Configure Hadoop to use Kerberos authentication.",
                    "2. Attempt to connect to the Hadoop cluster without valid Kerberos credentials.",
                    "3. Execute a command that requires access to HDFS, such as listing files."
                ],
                "actualBehavior": "The operation fails with an IOException indicating that no valid credentials were provided for Kerberos authentication.",
                "possibleCause": "The user may not have a valid Kerberos ticket-granting ticket (TGT) or the Kerberos configuration may be incorrect."
            },
            "stackTrace": "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)];\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1414)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1363)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n        at com.sun.proxy.$Proxy9.getListing(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)\n        at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy10.getListing(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)\n        at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)\n        at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)\n        at java.util.TimerThread.mainLoop(Timer.java:555)\n        at java.util.TimerThread.run(Timer.java:505)\nCaused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:677)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)\n        at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:640)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:724)\n        at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1462)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1381)\n        ... 21 more\nCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:411)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:550)\n        at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:367)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:716)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:712)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1641)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:711)\n        ... 24 more\nCaused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)\n        at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)\n        at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)\n        at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)\n        at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)\n        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)\n        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)"
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "bug_report": {
            "title": "ArrayIndexOutOfBoundsException in FileNameIndexUtils",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop MapReduce framework.",
                    "2. Wait for the job to complete.",
                    "3. Access the job history to retrieve done file names."
                ],
                "actualBehavior": "The application throws an ArrayIndexOutOfBoundsException when attempting to process done files.",
                "possibleCause": "The method trimURLEncodedString may be trying to access an index in an array that is out of bounds, possibly due to unexpected input or malformed data."
            },
            "stackTrace": "java.lang.ArrayIndexOutOfBoundsException: 50\n\tat org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)\n\tat org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)\n\tat org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException due to invalid DFS filename",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop environment with local file system configuration.",
                    "2. Submit a job that writes output to a specified path in the local file system.",
                    "3. Monitor the job execution and observe the output handling."
                ],
                "actualBehavior": "The job fails with an IllegalArgumentException indicating that the specified pathname is not a valid DFS filename.",
                "possibleCause": "The issue may arise from the use of a local file system path instead of a valid HDFS path, leading to the exception when the system attempts to resolve the file."
            },
            "stackTrace": "Exception running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.\n       at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)\n       at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)\n       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)\n       at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)\n       at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n       at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)\n       at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)\n       at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)\n       at org.apache.hadoop.mapred.Task.done(Task.java:1048)"
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "bug_report": {
            "title": "FileNotFoundException when accessing output file in Hadoop MapReduce",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop MapReduce job with the specified configuration.",
                    "2. Execute the job and ensure it runs locally.",
                    "3. Check the output directory for the expected output files after job completion."
                ],
                "actualBehavior": "The job fails with a FileNotFoundException indicating that the output file 'file.out.index' does not exist.",
                "possibleCause": "The output file may not have been created due to a failure in the job execution or incorrect output path configuration."
            },
            "stackTrace": "java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist\n  at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)\n  at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)\n  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)\n  at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n  at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)\n  at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)\n  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)\n  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)\n  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n  at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n  at java.lang.Thread.run(Thread.java:695)"
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "bug_report": {
            "title": "NoSuchElementException in Application Initialization Transition",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN NodeManager.",
                    "2. Submit an application to the NodeManager.",
                    "3. Monitor the application state transitions."
                ],
                "actualBehavior": "A NoSuchElementException is thrown during the application initialization transition, causing the application to fail.",
                "possibleCause": "The exception may be caused by an attempt to access an element in a HashMap that has been removed or is not present, indicating a potential issue with the state management of applications."
            },
            "stackTrace": "java.util.NoSuchElementException\n        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)\n        at java.util.HashMap$ValueIterator.next(HashMap.java:822)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ContainerManagerImpl.java:399)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "bug_report": {
            "title": "NullPointerException in MRAppMaster during Job History Parsing",
            "description": {
                "stepsToReproduce": [
                    "1. Start a MapReduce job that has a job history file.",
                    "2. Attempt to recover the job using the MRAppMaster.",
                    "3. Observe the logs for any exceptions thrown during the recovery process."
                ],
                "actualBehavior": "A NullPointerException is thrown, causing the MRAppMaster to fail during the job history parsing.",
                "possibleCause": "The job history file may contain a null value or an improperly formatted schema that leads to the StringReader initialization failure."
            },
            "stackTrace": "java.lang.NullPointerException\n        at java.io.StringReader.<init>(StringReader.java:50)\n        at org.apache.avro.Schema$Parser.parse(Schema.java:917)\n        at org.apache.avro.Schema.parse(Schema.java:966)\n        at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)\n        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)"
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException on Task Attempt Failure",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop YARN cluster.",
                    "2. Ensure that the job has tasks that are likely to fail due to fetch issues.",
                    "3. Monitor the job's task attempts and observe the state transitions."
                ],
                "actualBehavior": "The application throws an InvalidStateTransitionException with the message 'Invalid event: TA_TOO_MANY_FETCH_FAILURE at FAILED'.",
                "possibleCause": "The task attempt is trying to transition to an invalid state after exceeding the fetch failure limit, indicating a potential issue in state management within the task attempt lifecycle."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event:\nTA_TOO_MANY_FETCH_FAILURE at FAILED\n    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)\n    at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)\n    at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n    at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)\n    at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n    at java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException when creating Path from an empty string",
            "description": {
                "stepsToReproduce": [
                    "1. Initialize a JobConf object without providing a valid path.",
                    "2. Attempt to retrieve jobs using the Cluster.getJobs() method.",
                    "3. Observe the exception thrown during the process."
                ],
                "actualBehavior": "An IllegalArgumentException is thrown with the message 'Can not create a Path from an empty string'.",
                "possibleCause": "The JobConf constructor is being called with an empty string, which is not a valid argument for creating a Path object."
            },
            "stackTrace": "java.lang.IllegalArgumentException: Can not create a Path from an empty string\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:96)\n        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)\n        at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)\n        at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)\n        at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)\n        at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)\n        at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)\n        at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)\n        at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n        at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)\n        at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)\n        at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:192)"
        }
    },
    {
        "filename": "MAPREDUCE-6702.json",
        "creation_time": "2016-05-17T22:11:38.000+0000",
        "bug_report": {
            "title": "AssertionError in TestMiniMRChildTask due to environment checker job failure",
            "description": {
                "stepsToReproduce": [
                    "Run the TestMiniMRChildTask test suite.",
                    "Ensure that the environment is set up correctly for the MiniMRChildTask.",
                    "Execute the test cases that involve environment checking."
                ],
                "actualBehavior": "The test cases fail with an AssertionError indicating that the environment checker job failed.",
                "possibleCause": "The environment setup for the MiniMRChildTask may be incorrect or incomplete, leading to the failure of the environment checker job."
            },
            "stackTrace": "java.lang.AssertionError: The environment checker job failed.\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)\n\njava.lang.AssertionError: The environment checker job failed.\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)\n\tat org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)"
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException in Hadoop YARN during Task Completion",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a MapReduce job to the Hadoop YARN cluster.",
                    "2. Monitor the job execution until a task reaches the SUCCEEDED state.",
                    "3. Trigger an event that indicates the task has succeeded (e.g., T_ATTEMPT_SUCCEEDED)."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown, indicating that the event T_ATTEMPT_SUCCEEDED cannot be processed in the SUCCEEDED state.",
                "possibleCause": "The state machine for task transitions may not be correctly handling the T_ATTEMPT_SUCCEEDED event when the task is already in the SUCCEEDED state."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n        at java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "bug_report": {
            "title": "RuntimeException: Not a host:port pair in ResourceManager configuration",
            "description": {
                "stepsToReproduce": [
                    "1. Configure the ResourceManager with an invalid address for 'yarn.resourcemanager.admin.address'.",
                    "2. Start the ResourceManager service.",
                    "3. Observe the logs for any exceptions or errors."
                ],
                "actualBehavior": "The ResourceManager fails to start and throws a RuntimeException indicating that the provided address is not a valid host:port pair.",
                "possibleCause": "The configuration for 'yarn.resourcemanager.admin.address' is missing a valid port number or is incorrectly formatted."
            },
            "stackTrace": "java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)\n\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)\n\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)"
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "bug_report": {
            "title": "YarnRuntimeException: Error creating done directory due to Connection Refused",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop YARN Job History Server.",
                    "Attempt to access the Job History Server via the web interface or submit a job.",
                    "Observe the logs for any connection errors."
                ],
                "actualBehavior": "The Job History Server fails to start, throwing a YarnRuntimeException indicating that it cannot create the done directory due to a connection refusal.",
                "possibleCause": "The Hadoop NameNode may not be running or is not accessible at the specified address (localhost:8020)."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)\n\tat org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)\nCaused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1410)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1359)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:185)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:101)\n\tat com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:671)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1722)\n\tat org.apache.hadoop.fs.Hdfs.getFileStatus(Hdfs.java:124)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1106)\n\tat org.apache.hadoop.fs.FileContext$14.next(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.getFileStatus(FileContext.java:1102)\n\tat org.apache.hadoop.fs.FileContext$Util.exists(FileContext.java:1514)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.mkdir(HistoryFileManager.java:561)\n\tat org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:502)\n\t... 8 more\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:735)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1377)"
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "bug_report": {
            "title": "Invalid State Transition Exception in Hadoop YARN Job Management",
            "description": {
                "stepsToReproduce": [
                    "Submit a job to the Hadoop YARN cluster.",
                    "Ensure that the job completes successfully.",
                    "Trigger a task attempt completion or rescheduling event after the job has succeeded."
                ],
                "actualBehavior": "The system throws an InvalidStateTransitionException indicating that a JOB_TASK_ATTEMPT_COMPLETED or JOB_MAP_TASK_RESCHEDULED event cannot occur in the SUCCEEDED state.",
                "possibleCause": "The job state machine is not handling certain events correctly after the job has reached the SUCCEEDED state, leading to invalid state transitions."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)\n\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n\tat java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "bug_report": {
            "title": "NoClassDefFoundError: scala/Function1 in Hadoop MapReduce Job",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop MapReduce job that requires Scala dependencies.",
                    "2. Ensure that the job configuration is correct and all necessary libraries are included.",
                    "3. Submit the job to the Hadoop cluster."
                ],
                "actualBehavior": "The job fails to start with a NoClassDefFoundError indicating that scala.Function1 cannot be found.",
                "possibleCause": "The Scala library is not included in the classpath of the Hadoop job, leading to the ClassNotFoundException."
            },
            "stackTrace": "java.lang.NoClassDefFoundError: scala/Function1\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:190)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)\n        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)\n        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)\nCaused by: java.lang.ClassNotFoundException: scala.Function1\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)"
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "bug_report": {
            "title": "EOFException during HDFS Data Streaming",
            "description": {
                "stepsToReproduce": [
                    "1. Start an HDFS cluster with multiple DataNodes.",
                    "2. Attempt to write a large file to HDFS using an application that utilizes the DFSOutputStream.",
                    "3. Monitor the DataNodes for any connectivity issues or interruptions during the write process."
                ],
                "actualBehavior": "The application throws a java.io.EOFException indicating a premature end of file with no length prefix available.",
                "possibleCause": "This issue may be caused by network interruptions between the client and DataNodes, or a misconfiguration in the HDFS setup that leads to incomplete data transmission."
            },
            "stackTrace": "java.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)\n\njava.io.EOFException: Premature EOF: no length prefix available\n        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)"
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "bug_report": {
            "title": "UninitializedMessageException: Message missing required fields: renewer",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job using Oozie that requires delegation tokens.",
                    "2. Ensure that the job configuration is missing the 'renewer' field.",
                    "3. Monitor the Oozie logs for any exceptions thrown during job submission."
                ],
                "actualBehavior": "The job submission fails with an UninitializedMessageException indicating that the 'renewer' field is missing.",
                "possibleCause": "The job configuration does not include the required 'renewer' field, which is necessary for creating a valid GetDelegationTokenRequest."
            },
            "stackTrace": "org.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer\n\tat org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)\n\tat org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)\n\tat org.apache.oozie.command.XCommand.call(XCommand.java:277)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)\n\tat org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)\n\tat org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer\n\tat com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)\n\tat org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)\n\tat org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)\n\tat org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)\n\tat org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)\n\tat org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)\n\tat org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:581)\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:576)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:576)\n\tat org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)"
        }
    }
]