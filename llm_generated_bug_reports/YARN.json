[
    {
        "filename": "YARN-5918.json",
        "creation_time": "2016-11-20T14:19:00.000+0000",
        "bug_report": {
            "title": "NullPointerException in OpportunisticContainerAllocatorAMService",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application that requires resource allocation.",
                    "3. Monitor the ResourceManager logs for errors."
                ],
                "actualBehavior": "A NullPointerException is thrown during the resource allocation process, causing the application to fail.",
                "possibleCause": "The issue may be caused by an uninitialized variable or an unexpected null value in the resource allocation logic."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)\n        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)\n        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)\n        at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)\n        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:467)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:990)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1857)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2539)"
        }
    },
    {
        "filename": "YARN-8629.json",
        "creation_time": "2018-08-07T00:14:14.000+0000",
        "bug_report": {
            "title": "FileNotFoundException when accessing cgroup tasks file in Hadoop YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop YARN cluster.",
                    "2. Submit a job that utilizes container resources.",
                    "3. Monitor the container lifecycle and observe the cleanup process."
                ],
                "actualBehavior": "A FileNotFoundException is thrown indicating that the specified cgroup tasks file does not exist.",
                "possibleCause": "The cgroup tasks file may not be created or may have been deleted before the cleanup process attempts to access it."
            },
            "stackTrace": "java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)\n        at java.io.FileInputStream.open0(Native Method)\n        at java.io.FileInputStream.open(FileInputStream.java:195)\n        at java.io.FileInputStream.<init>(FileInputStream.java:138)\n        at java.io.FileInputStream.<init>(FileInputStream.java:93)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)\n        at java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "YARN-4431.json",
        "creation_time": "2015-12-07T18:31:36.000+0000",
        "bug_report": {
            "title": "Connection Refused Error in NodeManager Unregistration",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN NodeManager service.",
                    "2. Attempt to unregister the NodeManager from the ResourceManager.",
                    "3. Observe the logs for any connection errors."
                ],
                "actualBehavior": "The NodeManager fails to unregister from the ResourceManager with a 'Connection refused' error.",
                "possibleCause": "The ResourceManager may not be running or is not accessible at the expected address (0.0.0.0:8031)."
            },
            "stackTrace": "java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n        at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)\n        at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1452)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1385)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n        at com.sun.proxy.$Proxy74.unRegisterNodeManager(Unknown Source)\n        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)\n        at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:483)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:255)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy75.unRegisterNodeManager(Unknown Source)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop(NodeStatusUpdaterImpl.java:245)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:377)"
        }
    },
    {
        "filename": "YARN-2273.json",
        "creation_time": "2014-07-10T18:38:53.000+0000",
        "bug_report": {
            "title": "NullPointerException in FairScheduler NodeAvailableResourceComparator",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with a FairScheduler configuration.",
                    "2. Submit multiple applications to the ResourceManager.",
                    "3. Monitor the scheduling process and observe the logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown during the comparison of node resources, causing the scheduling process to fail.",
                "possibleCause": "It is possible that one of the nodes being compared has null resource values, leading to the NullPointerException in the comparator."
            },
            "stackTrace": "java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)\n\tat java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)\n\tat java.util.TimSort.sort(TimSort.java:203)\n\tat java.util.TimSort.sort(TimSort.java:173)\n\tat java.util.Arrays.sort(Arrays.java:659)\n\tat java.util.Collections.sort(Collections.java:217)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)\n\tat java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "YARN-2834.json",
        "creation_time": "2014-11-09T06:07:01.000+0000",
        "bug_report": {
            "title": "NullPointerException in CapacityScheduler during Application Attempt Recovery",
            "description": {
                "stepsToReproduce": [
                    "Start the ResourceManager service in a Hadoop YARN cluster.",
                    "Submit an application to the YARN ResourceManager.",
                    "Simulate a failure or restart of the ResourceManager to trigger application attempt recovery."
                ],
                "actualBehavior": "The ResourceManager throws a NullPointerException while trying to recover application attempts, causing the service to fail.",
                "possibleCause": "It is likely that a required object or reference is not properly initialized before being accessed in the addApplicationAttempt method."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1005)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:101)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:843)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:826)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:701)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:312)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1091)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1226)"
        }
    },
    {
        "filename": "YARN-370.json",
        "creation_time": "2013-02-01T04:02:58.000+0000",
        "bug_report": {
            "title": "Unauthorized Request to Start Container Due to Resource Mismatch",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a YARN application with a requested resource configuration of memory:2048 and vCores:1.",
                    "2. Monitor the application attempt as it tries to launch.",
                    "3. Observe the logs for any authorization errors related to resource allocation."
                ],
                "actualBehavior": "The application fails to launch with an error indicating an unauthorized request due to a resource mismatch, specifically requesting memory:2048 but only finding memory:1536.",
                "possibleCause": "The node manager may not have sufficient resources available to fulfill the requested memory allocation, or there may be a misconfiguration in the resource settings for the application."
            },
            "stackTrace": "Error launching appattempt_1359688216672_0001_000001. Got exception: RemoteTrace: at LocalTrace: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: RemoteTrace: at LocalTrace: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unauthorized request to start container. Expected resource <memory:2048, vCores:1> but found <memory:1536, vCores:1> at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39) at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47) at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:383) at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:400) at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:68) at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1735) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1731) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1729) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native Method) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57) at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:123) at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:109) at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:111) at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:255) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) at java.lang.Thread.run(Thread.java:722)"
        }
    },
    {
        "filename": "YARN-3675.json",
        "creation_time": "2015-05-18T22:38:39.000+0000",
        "bug_report": {
            "title": "NullPointerException in FairScheduler during unreserve operation",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop YARN ResourceManager.",
                    "2. Monitor the job's progress until it reaches a completed state.",
                    "3. Observe the ResourceManager logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown in the FSAppAttempt.unreserve method, causing the ResourceManager to fail to handle the completed container properly.",
                "possibleCause": "The issue may be caused by an attempt to unreserve resources for an application that has already been removed or is in an invalid state, leading to a null reference."
            },
            "stackTrace": "java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserve(FSAppAttempt.java:469)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:815)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:763)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1217)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-4763.json",
        "creation_time": "2016-03-04T10:03:56.000+0000",
        "bug_report": {
            "title": "NullPointerException in RMAppsBlock.renderData",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Access the ResourceManager web interface.",
                    "3. Navigate to the applications section to view running applications."
                ],
                "actualBehavior": "The application fails to render the applications list and throws a NullPointerException.",
                "possibleCause": "It is possible that the application data being rendered is null, leading to the NullPointerException in the renderData method."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)\n        at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)\n        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)\n        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)\n        at org.apache.hadoop.yarn.webapp.View.render(View.java:235)\n        at org.apache.hadoop.yarn.webapp.view.HtmlBlock$Block.subView(HtmlBlock.java:43)\n        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet._(Hamlet.java:30354)\n        at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppsBlockWithMetrics.render(AppsBlockWithMetrics.java:30)\n        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)\n        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)\n        at org.apache.hadoop.yarn.webapp.View.render(View.java:235)\n        at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)\n        at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)\n        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:848)\n        at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)\n        at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)\n        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:156)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)"
        }
    },
    {
        "filename": "YARN-8202.json",
        "creation_time": "2018-04-24T15:52:00.000+0000",
        "bug_report": {
            "title": "InvalidResourceRequestException when allocating resources in YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a resource allocation request to YARN with the following parameters: memory=200, vCores=1, resource1=500M.",
                    "2. Ensure that the maximum allowed allocation for resource1 is set to 5G.",
                    "3. Observe the response from the YARN ResourceManager."
                ],
                "actualBehavior": "The system throws an InvalidResourceRequestException indicating that the requested resource type for resource1 is invalid.",
                "possibleCause": "The requested resource for resource1 (500M) is less than the minimum required or exceeds the maximum allowed allocation set by the scheduler."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation. Requested resource=<memory:200, vCores:1, resource1: 500M>, maximum allowed allocation=<memory:6144, vCores:8, resource1: 5G>, please note that maximum allowed allocation is calculated by scheduler based on maximum resource of registered NodeManagers, which might be less than configured maximum allocation=<memory:8192, vCores:8192, resource1: 9223372036854775807G>\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)\n\tat org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)\n\tat org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.processor.DisabledPlacementProcessor.allocate(DisabledPlacementProcessor.java:75)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AMSProcessingChain.allocate(AMSProcessingChain.java:92)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:433)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)\n\tat org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)"
        }
    },
    {
        "filename": "YARN-7118.json",
        "creation_time": "2017-08-29T12:04:01.000+0000",
        "bug_report": {
            "title": "NullPointerException in WebServices.getApps",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN server.",
                    "2. Send a request to the AHSWebServices endpoint to retrieve applications.",
                    "3. Observe the server logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown when attempting to retrieve applications from the WebServices.",
                "possibleCause": "It is possible that a required object or parameter is not initialized before being accessed in the getApps method."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)"
        }
    },
    {
        "filename": "YARN-4743.json",
        "creation_time": "2016-02-27T09:12:28.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException in FairScheduler due to comparison method violation",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with a FairScheduler configuration.",
                    "2. Submit multiple applications that require resource allocation.",
                    "3. Monitor the ResourceManager logs for any exceptions during the scheduling process."
                ],
                "actualBehavior": "An IllegalArgumentException is thrown indicating that the comparison method violates its general contract, causing the scheduling process to fail.",
                "possibleCause": "The comparison method used in the sorting algorithm may not be consistent with the requirements of the Comparator interface, leading to unpredictable behavior during sorting."
            },
            "stackTrace": "java.lang.IllegalArgumentException: Comparison method violates its general contract!\n         at java.util.TimSort.mergeHi(TimSort.java:868)\n         at java.util.TimSort.mergeAt(TimSort.java:485)\n         at java.util.TimSort.mergeCollapse(TimSort.java:410)\n         at java.util.TimSort.sort(TimSort.java:214)\n         at java.util.TimSort.sort(TimSort.java:173)\n         at java.util.Arrays.sort(Arrays.java:659)\n         at java.util.Collections.sort(Collections.java:217)\n         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)\n         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)\n         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)\n         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)\n         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)\n         at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)\n         at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)\n         at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-2414.json",
        "creation_time": "2014-08-12T23:48:48.000+0000",
        "bug_report": {
            "title": "NullPointerException in AppBlock.render() during YARN ResourceManager web application rendering",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN ResourceManager.",
                    "2. Access the ResourceManager web interface.",
                    "3. Navigate to the application block section."
                ],
                "actualBehavior": "The web application throws a NullPointerException and fails to render the application block.",
                "possibleCause": "The NullPointerException may be caused by an uninitialized object or missing data in the AppBlock.render() method."
            },
            "stackTrace": "java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)\n\tat com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)\n\tat com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:84)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:460)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1191)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)\n\tat org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)\n\tat org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)\n\tat org.apache.hadoop.yarn.webapp.View.render(View.java:235)\n\tat org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)\n\tat org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)\n\tat org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)\n\tat org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)\n\tat org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)\n\tat org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:55)"
        }
    },
    {
        "filename": "YARN-3878.json",
        "creation_time": "2015-07-02T00:20:59.000+0000",
        "bug_report": {
            "title": "InterruptedException during AsyncDispatcher event handling in Hadoop YARN",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop YARN ResourceManager.",
                    "Submit an application that triggers multiple application attempts.",
                    "Stop the ResourceManager while the application attempts are still processing."
                ],
                "actualBehavior": "An InterruptedException is thrown, causing the AsyncDispatcher event handler to fail.",
                "possibleCause": "The interruption may be caused by a shutdown signal sent to the ResourceManager while it is processing events, leading to the failure in acquiring locks."
            },
            "stackTrace": "java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.access$3300(RMAppAttemptImpl.java:109)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1619)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:786)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:108)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:838)\n\n\"AsyncDispatcher event handler\" prio=10 tid=0x00007fb980222800 nid=0x4b1e waiting on condition [0x00007fb9654e9000]\n   java.lang.Thread.State: WAITING (parking)\n\tat sun.misc.Unsafe.park(Native Method)\n\t- parking to wait for  <0x0000000700b79250> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)\n\tat java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:113)\n\tat java.lang.Thread.run(Thread.java:744)\n\n\"main\" prio=10 tid=0x00007fb98000a800 nid=0x49c3 in Object.wait() [0x00007fb989851000]\n   java.lang.Thread.State: TIMED_WAITING (on object monitor)\n\tat java.lang.Object.wait(Native Method)\n\t- waiting on <0x0000000700b79430> (a java.lang.Object)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.serviceStop(AsyncDispatcher.java:156)\n\t- locked <0x0000000700b79430> (a java.lang.Object)\n\tat org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n\t- locked <0x0000000700b79420> (a java.lang.Object)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.serviceStop(RMStateStore.java:515)\n\tat org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n\t- locked <0x0000000700b79630> (a java.lang.Object)\n\tat org.apache.hadoop.service.AbstractService.close(AbstractService.java:250)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStop(ResourceManager.java:599)"
        }
    },
    {
        "filename": "YARN-6683.json",
        "creation_time": "2017-06-02T00:29:13.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException on COLLECTOR_UPDATE event",
            "description": {
                "stepsToReproduce": [
                    "1. Submit an application to the YARN ResourceManager.",
                    "2. Trigger a COLLECTOR_UPDATE event while the application is in the KILLED state.",
                    "3. Observe the logs for any exceptions thrown."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown indicating that the COLLECTOR_UPDATE event is invalid in the KILLED state.",
                "possibleCause": "The application state machine does not allow for a COLLECTOR_UPDATE event to be processed when the application is already in the KILLED state."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)"
        }
    },
    {
        "filename": "YARN-2910.json",
        "creation_time": "2014-11-27T06:19:00.000+0000",
        "bug_report": {
            "title": "ConcurrentModificationException in FairScheduler Allocation",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with a FairScheduler configuration.",
                    "2. Submit multiple applications that require resource allocation simultaneously.",
                    "3. Monitor the resource allocation process while making changes to the application queues."
                ],
                "actualBehavior": "A ConcurrentModificationException is thrown, causing the resource allocation process to fail.",
                "possibleCause": "The exception may be caused by concurrent modifications to the ArrayList while iterating over it in the FSLeafQueue class."
            },
            "stackTrace": "java.util.ConcurrentModificationException: java.util.ConcurrentModificationException\nat java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)\nat java.util.ArrayList$Itr.next(ArrayList.java:831)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)\nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)\nat org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)"
        }
    },
    {
        "filename": "YARN-192.json",
        "creation_time": "2012-11-01T05:00:41.000+0000",
        "bug_report": {
            "title": "NullPointerException in FSSchedulerApp.unreserve",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit a job that requires resource allocation.",
                    "3. Trigger a scenario where the unreserve method is called, such as cancelling the job or releasing resources."
                ],
                "actualBehavior": "The application throws a NullPointerException when attempting to unreserve resources.",
                "possibleCause": "It is possible that the application is trying to unreserve resources that have not been properly initialized or have already been released."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.unreserve(FSSchedulerApp.java:356)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.unreserve(AppSchedulable.java:214)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:266)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:330)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueSchedulable.assignContainer(FSQueueSchedulable.java:161)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:759)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:836)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:329)\n        at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "YARN-4581.json",
        "creation_time": "2016-01-12T03:37:40.000+0000",
        "bug_report": {
            "title": "IOException and OutOfMemoryError during application history writing in Hadoop",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop YARN application that generates a significant amount of application history data.",
                    "2. Monitor the application history service while the application is running.",
                    "3. Observe the logs for any IOException or OutOfMemoryError related to application history writing."
                ],
                "actualBehavior": "The application fails to write history data due to an IOException indicating that the output file is not at zero offset, followed by an OutOfMemoryError indicating that the system is unable to create new native threads.",
                "possibleCause": "The issue may be caused by resource exhaustion, leading to an inability to create new threads, which in turn prevents the application history service from writing data correctly."
            },
            "stackTrace": "java.io.IOException: Output file not at zero offset.\n        at org.apache.hadoop.io.file.tfile.BCFile$Writer.<init>(BCFile.java:288)\n        at org.apache.hadoop.io.file.tfile.TFile$Writer.<init>(TFile.java:288)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:728)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)\n        at java.lang.Thread.run(Thread.java:745)\n\njava.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:714)\n        at org.apache.hadoop.hdfs.DFSOutputStream.start(DFSOutputStream.java:2033)\n        at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForAppend(DFSOutputStream.java:1652)\n        at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1573)\n        at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1603)\n        at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1591)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:328)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:324)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:324)\n        at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:723)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)\n        at java.lang.Thread.run(Thread.java:745)\n\njava.lang.Thread.State: TIMED_WAITING (on object monitor)\n        at java.lang.Object.wait(Native Method)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:502)\n        - locked <0x0000000745f88b98> (a java.util.LinkedList)"
        }
    },
    {
        "filename": "YARN-7786.json",
        "creation_time": "2018-01-22T14:29:46.000+0000",
        "bug_report": {
            "title": "NullPointerException in AMLauncher.setupTokens",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN ResourceManager.",
                    "2. Submit an application that requires resource allocation.",
                    "3. Monitor the ResourceManager logs for errors."
                ],
                "actualBehavior": "The application fails to launch, and a NullPointerException is thrown in the AMLauncher.setupTokens method.",
                "possibleCause": "It is possible that a required token or configuration object is not being initialized properly before being accessed in the setupTokens method."
            },
            "stackTrace": "java.lang.NullPointerException\n at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)\n at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)\n at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)\n at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-8035.json",
        "creation_time": "2018-03-16T12:02:04.000+0000",
        "bug_report": {
            "title": "MetricsException: Tag ContainerPid already exists",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN NodeManager.",
                    "2. Launch a container that requires monitoring.",
                    "3. Observe the logs for any metrics-related exceptions."
                ],
                "actualBehavior": "An exception is thrown indicating that the tag 'ContainerPid' already exists, preventing the proper recording of container metrics.",
                "possibleCause": "The issue may be caused by an attempt to register the same tag multiple times within the MetricsRegistry, possibly due to a bug in the container monitoring logic."
            },
            "stackTrace": "org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!\nat org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)\nat org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)\nat org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)\nat org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448)"
        }
    },
    {
        "filename": "YARN-4152.json",
        "creation_time": "2015-09-12T15:02:22.000+0000",
        "bug_report": {
            "title": "NullPointerException in LogAggregationService during container stop",
            "description": {
                "stepsToReproduce": [
                    "1. Start a YARN application with log aggregation enabled.",
                    "2. Attempt to stop a container that is currently running.",
                    "3. Monitor the logs for any exceptions thrown."
                ],
                "actualBehavior": "A NullPointerException is thrown in the LogAggregationService when attempting to stop the container.",
                "possibleCause": "It is possible that a required object or resource is not initialized or is null when the stopContainer method is called."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-3697.json",
        "creation_time": "2015-05-21T18:05:38.000+0000",
        "bug_report": {
            "title": "InterruptedException during container scheduling in YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Start a YARN cluster with multiple applications running.",
                    "2. Trigger a scenario where resource allocation is high and contention occurs.",
                    "3. Monitor the logs for any InterruptedException errors."
                ],
                "actualBehavior": "An InterruptedException is thrown during the scheduling of containers, causing disruptions in resource allocation.",
                "possibleCause": "The exception may be caused by thread interruptions during the locking mechanism in the LinkedBlockingQueue, potentially due to external signals or application logic that interrupts threads."
            },
            "stackTrace": "java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)\n\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.lang.InterruptedException\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:249)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)\n\nCaused by: java.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)\n\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)\n\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)"
        }
    },
    {
        "filename": "YARN-2340.json",
        "creation_time": "2014-07-23T15:18:38.000+0000",
        "bug_report": {
            "title": "NullPointerException in CapacityScheduler when adding application attempt",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application to the ResourceManager.",
                    "3. Monitor the ResourceManager logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown in the CapacityScheduler when attempting to add an application attempt.",
                "possibleCause": "It is possible that the application attempt being added is not properly initialized or is missing required parameters, leading to a null reference."
            },
            "stackTrace": "java.lang.NullPointerException\n at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:568)\n at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:916)\n at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:101)\n at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:602)\n at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "YARN-8022.json",
        "creation_time": "2018-03-10T19:29:27.000+0000",
        "bug_report": {
            "title": "NullPointerException in AppBlock rendering",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application to the ResourceManager.",
                    "3. Access the application block page in the web interface."
                ],
                "actualBehavior": "The application block page fails to render and throws a NullPointerException.",
                "possibleCause": "It is possible that a required object is not initialized before being accessed in the AppBlock class."
            },
            "stackTrace": "java.lang.NullPointerException\n at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:283)\n at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:280)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:422)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)\n at org.apache.hadoop.yarn.server.webapp.AppBlock.render(AppBlock.java:279)\n at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock.render(RMAppBlock.java:71)\n at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)\n at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)\n at org.apache.hadoop.yarn.webapp.View.render(View.java:235)\n at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)\n at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)\n at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)\n at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)\n at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)\n at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)\n at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:54)"
        }
    },
    {
        "filename": "YARN-3793.json",
        "creation_time": "2015-06-10T20:52:38.000+0000",
        "bug_report": {
            "title": "NullPointerException in FileContext during file deletion",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to delete a file using the Hadoop FileContext API.",
                    "2. Ensure that the file path provided is relative and not properly initialized.",
                    "3. Execute the deletion operation."
                ],
                "actualBehavior": "The application throws a NullPointerException instead of successfully deleting the file.",
                "possibleCause": "The file path may not be properly initialized, leading to a null reference when attempting to fix the relative part of the path."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)\n        at org.apache.hadoop.fs.FileContext.delete(FileContext.java:755)\n        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)\n        at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)"
        }
    },
    {
        "filename": "YARN-6102.json",
        "creation_time": "2017-01-17T09:36:29.000+0000",
        "bug_report": {
            "title": "No handler registered for RMNodeEventType in AsyncDispatcher",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Trigger an event that should be handled by RMNodeEventType.",
                    "3. Observe the logs for any exceptions or errors."
                ],
                "actualBehavior": "An exception is thrown indicating that there is no handler registered for the class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType.",
                "possibleCause": "It is possible that the event handler for RMNodeEventType has not been properly registered in the AsyncDispatcher, leading to this exception."
            },
            "stackTrace": "java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-8409.json",
        "creation_time": "2018-06-08T20:36:32.000+0000",
        "bug_report": {
            "title": "Connection Refused Leading to NullPointerException in ActiveStandbyElector",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop ResourceManager service.",
                    "2. Ensure that the Zookeeper service is not running or is unreachable.",
                    "3. Observe the logs for any connection errors."
                ],
                "actualBehavior": "The ResourceManager fails to initialize due to a Connection refused error, leading to a NullPointerException in the ActiveStandbyElector.",
                "possibleCause": "The Zookeeper service is not available, causing the ActiveStandbyElector to attempt operations on a null reference."
            },
            "stackTrace": "java.net.ConnectException: Connection refused\n\nat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\nat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\nat org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)\n\nat org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)\n\njava.lang.NullPointerException\n\nat org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)\n\nat org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)\n\nat org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)\n\nat org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)\n\nat org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)\n\nat org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)\n\nat org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)\n\nat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\n\nat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)\n\nat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\n\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)"
        }
    },
    {
        "filename": "YARN-8223.json",
        "creation_time": "2018-04-27T11:49:02.000+0000",
        "bug_report": {
            "title": "ClassNotFoundException for org.apache.auxtest.AuxServiceFromLocal",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN NodeManager.",
                    "2. Configure an auxiliary service that requires the class org.apache.auxtest.AuxServiceFromLocal.",
                    "3. Monitor the NodeManager logs for any errors during initialization."
                ],
                "actualBehavior": "The NodeManager fails to start, throwing a ClassNotFoundException for org.apache.auxtest.AuxServiceFromLocal.",
                "possibleCause": "The class org.apache.auxtest.AuxServiceFromLocal is not included in the classpath or the required JAR file is missing."
            },
            "stackTrace": "java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)\n\tat org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)"
        }
    },
    {
        "filename": "YARN-8331.json",
        "creation_time": "2018-05-21T05:19:35.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException when handling CONTAINER_LAUNCHED event",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the YARN resource manager.",
                    "2. Monitor the container lifecycle events.",
                    "3. Observe the state transitions of the container."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown when the CONTAINER_LAUNCHED event is received while the container is in the DONE state.",
                "possibleCause": "The container may be incorrectly transitioning to the DONE state before the CONTAINER_LAUNCHED event is processed, indicating a potential issue in the state management logic."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)\n\tat java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "YARN-2931.json",
        "creation_time": "2014-12-08T21:09:13.000+0000",
        "bug_report": {
            "title": "FileNotFoundException when accessing Yarn file cache",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop Yarn service.",
                    "2. Attempt to run a job that requires file caching.",
                    "3. Monitor the logs for any file access errors."
                ],
                "actualBehavior": "The job fails with a FileNotFoundException indicating that the file /data/yarn/nm/filecache does not exist.",
                "possibleCause": "The specified file cache directory may not have been created or initialized properly, leading to the FileNotFoundException."
            },
            "stackTrace": "java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)\n\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)\n\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:720)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)\n\tat org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-6837.json",
        "creation_time": "2017-07-18T11:17:55.000+0000",
        "bug_report": {
            "title": "NullPointerException in ResourceSet.addResources",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN NodeManager.",
                    "2. Submit a job that requires resource localization.",
                    "3. Monitor the NodeManager logs for errors."
                ],
                "actualBehavior": "The NodeManager throws a NullPointerException when attempting to add resources, causing the job to fail.",
                "possibleCause": "It is possible that a required resource or configuration is not properly initialized before being accessed in the ResourceSet.addResources method."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.addResources(ResourceSet.java:84)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:868)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:819)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1684)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:96)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1418)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1411)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-4762.json",
        "creation_time": "2016-03-04T02:24:47.000+0000",
        "bug_report": {
            "title": "YarnRuntimeException: Failed to initialize container executor",
            "description": {
                "stepsToReproduce": [
                    "Start the NodeManager service in a Hadoop YARN environment.",
                    "Ensure that the Linux container runtime is configured correctly.",
                    "Monitor the logs for any initialization errors."
                ],
                "actualBehavior": "The NodeManager fails to start, throwing a YarnRuntimeException indicating that it could not initialize the container executor.",
                "possibleCause": "The Linux container runtime may not be properly configured or there may be insufficient permissions to initialize it."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)\nCaused by: java.io.IOException: Failed to initialize linux container runtime(s)!\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)\n        ... 3 more\n\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to initialize container executor\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:587)\nCaused by: java.io.IOException: Failed to initialize linux container runtime(s)!\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:238)"
        }
    },
    {
        "filename": "YARN-2823.json",
        "creation_time": "2014-11-06T21:38:47.000+0000",
        "bug_report": {
            "title": "NullPointerException in CapacityScheduler during Application Attempt State Transfer",
            "description": {
                "stepsToReproduce": [
                    "1. Submit an application to the Hadoop YARN ResourceManager.",
                    "2. Trigger a scenario where the application attempt needs to transfer state from a previous attempt.",
                    "3. Monitor the ResourceManager logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown, causing the application attempt to fail.",
                "possibleCause": "It is possible that the application attempt is trying to access a state or object that has not been initialized or is null."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)\n        at java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "YARN-5098.json",
        "creation_time": "2016-05-17T00:43:08.000+0000",
        "bug_report": {
            "title": "HDFS Delegation Token Not Found in Cache",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to connect to the HDFS cluster using a valid user account.",
                    "2. Perform an operation that requires fetching server defaults from the Namenode.",
                    "3. Observe the error that occurs during the connection setup."
                ],
                "actualBehavior": "The operation fails with a RemoteException indicating that the HDFS delegation token cannot be found in the cache.",
                "possibleCause": "The HDFS delegation token may have expired or was not properly cached during the authentication process."
            },
            "stackTrace": "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache\n        at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)\n        at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:583)\n        at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:398)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:752)\n        at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:748)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:747)\n        at org.apache.hadoop.ipc.Client$Connection.access$3100(Client.java:398)\n        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1597)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1439)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1386)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:240)\n        at com.sun.proxy.$Proxy83.getServerDefaults(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:282)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n        at com.sun.proxy.$Proxy84.getServerDefaults(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:1018)\n        at org.apache.hadoop.fs.Hdfs.getServerDefaults(Hdfs.java:156)\n        at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:550)\n        at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:687)"
        }
    },
    {
        "filename": "YARN-3971.json",
        "creation_time": "2015-07-24T10:17:05.000+0000",
        "bug_report": {
            "title": "IOException when removing node label in YARN ResourceManager",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN ResourceManager with a queue that has a label assigned (e.g., label=x on queue=a1).",
                    "2. Attempt to remove the label 'x' from the cluster node labels.",
                    "3. Observe the logs or console output for any exceptions thrown."
                ],
                "actualBehavior": "An IOException is thrown indicating that the label 'x' cannot be removed because it is still in use by queue 'a1'.",
                "possibleCause": "The ResourceManager is trying to remove a node label that is still associated with an active queue, which violates the expected behavior of the node label management."
            },
            "stackTrace": "java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label\n        at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue(RMNodeLabelsManager.java:104)\n        at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels(RMNodeLabelsManager.java:118)\n        at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.recover(FileSystemNodeLabelsStore.java:221)\n        at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:232)\n        at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:245)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:120)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:964)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1005)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1001)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1666)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1001)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:312)\n        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:832)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:422)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
        }
    },
    {
        "filename": "YARN-6948.json",
        "creation_time": "2017-08-04T08:23:46.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException when adding attempt in FINAL_SAVING state",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a YARN application that transitions to the FINAL_SAVING state.",
                    "2. Attempt to add a new application attempt while the application is in the FINAL_SAVING state.",
                    "3. Observe the system behavior."
                ],
                "actualBehavior": "The system throws an InvalidStateTransitionException indicating that an ATTEMPT_ADDED event cannot be processed in the FINAL_SAVING state.",
                "possibleCause": "The application state machine does not allow adding new attempts once it has reached the FINAL_SAVING state, which may indicate a flaw in the state management logic."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-1409.json",
        "creation_time": "2013-11-13T11:25:56.000+0000",
        "bug_report": {
            "title": "RejectedExecutionException in ScheduledThreadPoolExecutor during log handling",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN NodeManager.",
                    "2. Trigger log handling for a container that generates multiple log events.",
                    "3. Observe the NodeManager logs for any exceptions."
                ],
                "actualBehavior": "A RejectedExecutionException is thrown indicating that tasks are being rejected from the ScheduledThreadPoolExecutor due to it being in a shutting down state.",
                "possibleCause": "The ScheduledThreadPoolExecutor may be shutting down while there are still tasks queued, leading to the rejection of new tasks."
            },
            "stackTrace": "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]\n        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)\n        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)\n        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)\n        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)\n        at java.lang.Thread.run(Thread.java:724)"
        }
    },
    {
        "filename": "YARN-5545.json",
        "creation_time": "2016-08-21T12:57:35.000+0000",
        "bug_report": {
            "title": "YARN Application Submission Failure Due to AccessControlException",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to submit a job to YARN using the Hadoop MapReduce framework.",
                    "2. Ensure that the queue 'root.default' is configured to accept applications.",
                    "3. Monitor the submission process for any errors."
                ],
                "actualBehavior": "The application submission fails with an AccessControlException indicating that the queue 'root.default' already has 0 applications and cannot accept the submission.",
                "possibleCause": "The queue 'root.default' may be misconfigured or not properly set up to accept new applications, leading to the AccessControlException."
            },
            "stackTrace": "java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001\n\tat org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)\n\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)\n...\nCaused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001\n\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)\n\tat org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)\n\tat org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:301)"
        }
    },
    {
        "filename": "YARN-301.json",
        "creation_time": "2013-01-01T05:40:18.000+0000",
        "bug_report": {
            "title": "ConcurrentModificationException in FairScheduler during Container Assignment",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with a FairScheduler configuration.",
                    "2. Submit multiple applications that require container resources simultaneously.",
                    "3. Monitor the ResourceManager logs for any exceptions during the container assignment process."
                ],
                "actualBehavior": "A ConcurrentModificationException is thrown, causing the container assignment process to fail.",
                "possibleCause": "The exception may be caused by concurrent modifications to the TreeMap while iterating over its entries, likely due to multiple threads attempting to update the scheduling state simultaneously."
            },
            "stackTrace": "java.util.ConcurrentModificationException\n        at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)\n        at java.util.TreeMap$KeyIterator.next(TreeMap.java:1154)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:297)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:181)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:780)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:842)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:340)\n        at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "YARN-7942.json",
        "creation_time": "2018-02-16T19:09:39.000+0000",
        "bug_report": {
            "title": "NoPathPermissionsException when attempting to delete YARN service",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to delete a YARN service using the API.",
                    "2. Ensure that the service is registered under the path `/registry/users/hbase/services/yarn-service/hbase-app-test`.",
                    "3. Observe the response from the API."
                ],
                "actualBehavior": "The API call fails with a NoPathPermissionsException, indicating that the user is not authorized to access the specified path.",
                "possibleCause": "The user may not have the necessary permissions set in the Access Control Lists (ACLs) for the specified Zookeeper path."
            },
            "stackTrace": "org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test\n        at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:412)\n        at org.apache.hadoop.registry.client.impl.zk.CuratorService.operationFailure(CuratorService.java:390)\n        at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:722)\n        at org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService.delete(RegistryOperationsService.java:162)\n        at org.apache.hadoop.yarn.service.client.ServiceClient.actionDestroy(ServiceClient.java:462)\n        at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:253)\n        at org.apache.hadoop.yarn.service.webapp.ApiServer$4.run(ApiServer.java:243)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)\n        at org.apache.hadoop.yarn.service.webapp.ApiServer.stopService(ApiServer.java:243)\n        at org.apache.hadoop.yarn.service.webapp.ApiServer.deleteService(ApiServer.java:223)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:89)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:941)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:875)\n        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:178)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:829)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:119)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133)\n        at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130)\n        at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter$Context.java:203)\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.http.XFrameOptionsFilter.doFilter(XFrameOptionsFilter.java:57)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.AuthenticationWithProxyUserFilter.doFilter(AuthenticationWithProxyUserFilter.java:101)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1617)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n        at org.eclipse.jetty.handler.HandlerCollection.handle(HandlerCollection.java:119)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n        at org.eclipse.jetty.server.Server.handle(Server.java:534)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n        at java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)\n        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:250)\n        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:244)\n        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)\n        at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:241)\n        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:225)\n        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:35)\n        at org.apache.hadoop.registry.client.impl.zk.CuratorService.zkDelete(CuratorService.java:718)\n        ... 75 more"
        }
    },
    {
        "filename": "YARN-7692.json",
        "creation_time": "2017-12-29T06:00:34.000+0000",
        "bug_report": {
            "title": "AccessControlException when submitting/updating YARN application",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to submit or update YARN application with user 'hrt_qa'.",
                    "2. Ensure that the application ID is 'application_1514268754125_0001'.",
                    "3. Observe the error message returned."
                ],
                "actualBehavior": "The system throws an AccessControlException indicating that user 'hrt_qa' does not have permission to submit/update the specified application.",
                "possibleCause": "The user 'hrt_qa' may not have the necessary permissions configured in the YARN resource manager for submitting or updating applications."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.checkAndGetApplicationPriority(CapacityScheduler.java:2348)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:396)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:358)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:567)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1390)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1143)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1183)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1179)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1179)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)\n        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:611)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:510)\nCaused by: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0"
        }
    },
    {
        "filename": "YARN-3917.json",
        "creation_time": "2015-07-11T00:41:28.000+0000",
        "bug_report": {
            "title": "UnsupportedOperationException: Could not determine OS in Hadoop YARN NodeManager",
            "description": {
                "stepsToReproduce": [
                    "1. Install Hadoop YARN on the target machine.",
                    "2. Attempt to start the NodeManager service.",
                    "3. Observe the logs for any exceptions or errors."
                ],
                "actualBehavior": "The NodeManager fails to start and throws an UnsupportedOperationException indicating that the OS could not be determined.",
                "possibleCause": "The issue may be caused by an unsupported or unrecognized operating system environment, or a misconfiguration in the system that prevents the OS from being identified."
            },
            "stackTrace": "java.lang.UnsupportedOperationException: Could not determine OS\n        at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)\n        at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)\n        at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)"
        }
    },
    {
        "filename": "YARN-3537.json",
        "creation_time": "2015-04-23T11:34:23.000+0000",
        "bug_report": {
            "title": "NullPointerException in NodeManager during shutdown",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN NodeManager service.",
                    "2. Trigger a shutdown of the NodeManager service.",
                    "3. Observe the logs during the shutdown process."
                ],
                "actualBehavior": "A NullPointerException is thrown during the shutdown process of the NodeManager.",
                "possibleCause": "The NodeManager may be attempting to access a resource or variable that has not been initialized or has been set to null."
            },
            "stackTrace": "java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore(NodeManager.java:181)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:326)\n\tat org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n\tat org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:106)"
        }
    },
    {
        "filename": "YARN-7962.json",
        "creation_time": "2018-02-22T22:32:20.000+0000",
        "bug_report": {
            "title": "RejectedExecutionException in DelegationTokenRenewer",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit multiple applications to the ResourceManager.",
                    "3. Wait for the applications to finish and observe the logs."
                ],
                "actualBehavior": "The ResourceManager throws a RejectedExecutionException when trying to process the DelegationTokenRenewer event after an application finishes.",
                "possibleCause": "The ThreadPoolExecutor used for handling delegation token renewal tasks is terminated, leading to rejected tasks."
            },
            "stackTrace": "java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]\n\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)\n\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)\n\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)\n\tat org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)\n\tat org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)\n\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)\n\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)\n\tat org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-8357.json",
        "creation_time": "2018-05-24T16:46:57.000+0000",
        "bug_report": {
            "title": "NullPointerException in ServiceClient during service start",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN service using the ApiServer.",
                    "2. Attempt to start a service through the ServiceClient.",
                    "3. Observe the logs for any exceptions thrown."
                ],
                "actualBehavior": "A NullPointerException is thrown when attempting to start the service.",
                "possibleCause": "It is possible that a required object or configuration is not initialized before the actionStart method is called."
            },
            "stackTrace": "java.lang.NullPointerException\n\n        at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)\n\n        at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)\n\n        at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:644)\n\n        at java.security.AccessController.doPrivileged(Native Method)\n\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)\n\n        at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)\n\n        at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\n        at java.lang.reflect.Method.invoke(Method.java:498)\n\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$ResponseOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:205)\n\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\n\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)"
        }
    },
    {
        "filename": "YARN-6534.json",
        "creation_time": "2017-04-26T21:43:52.000+0000",
        "bug_report": {
            "title": "FileNotFoundException for SSL KeyStore in Hadoop YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager service.",
                    "2. Ensure that the configuration is set to use SSL for secure communication.",
                    "3. Check the logs for any errors related to SSL initialization."
                ],
                "actualBehavior": "The ResourceManager fails to start due to a FileNotFoundException indicating that the specified KeyStore file '/etc/security/clientKeys/all.jks' does not exist.",
                "possibleCause": "The specified KeyStore file path is incorrect or the file has not been created or placed in the expected directory."
            },
            "stackTrace": "org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)\n        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)\n        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n        at org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher.serviceInit(AbstractSystemMetricsPublisher.java:59)\n        at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.serviceInit(TimelineServiceV1Publisher.java:67)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1453)\nCaused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)\n        at java.io.FileInputStream.open0(Native Method)\n        at java.io.FileInputStream.open(FileInputStream.java:195)\n        at java.io.FileInputStream.<init>(FileInputStream.java:138)\n        at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:168)\n        at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:86)\n        at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:219)\n        at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:179)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.getSSLFactory(TimelineConnector.java:176)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit(TimelineConnector.java:106)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)"
        }
    },
    {
        "filename": "YARN-4227.json",
        "creation_time": "2015-10-06T04:59:10.000+0000",
        "bug_report": {
            "title": "NullPointerException in FairScheduler during completedContainer execution",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit a job that utilizes the FairScheduler.",
                    "3. Monitor the ResourceManager logs for any completed container events."
                ],
                "actualBehavior": "The ResourceManager throws a NullPointerException when handling completed containers, causing job scheduling to fail.",
                "possibleCause": "It is possible that the FairScheduler is attempting to access a property or method on a null object reference when processing the completed container event."
            },
            "stackTrace": "java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:849)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1273)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:122)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:585)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-2649.json",
        "creation_time": "2014-10-06T22:57:46.000+0000",
        "bug_report": {
            "title": "AppAttempt State Mismatch During ResourceManager Node Updates",
            "description": {
                "stepsToReproduce": [
                    "1. Start the ResourceManager and submit an application.",
                    "2. Simulate a scenario where nodes become unusable during the application lifecycle.",
                    "3. Observe the state transitions of the application attempt."
                ],
                "actualBehavior": "The application attempt state is reported as SCHEDULED instead of the expected ALLOCATED state, leading to an AssertionFailedError in the test.",
                "possibleCause": "The ResourceManager may not be correctly handling the state transitions of application attempts when nodes are marked as unusable."
            },
            "stackTrace": "junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>\n\tat junit.framework.Assert.fail(Assert.java:50)\n\tat junit.framework.Assert.failNotEquals(Assert.java:287)\n\tat junit.framework.Assert.assertEquals(Assert.java:67)\n\tat org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)\n\tat org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)\n\tat org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)\n\nattempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(670)) - appattempt_1412569506932_0001_000001 State change from NEW to SUBMITTED\nevent.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent.EventType: STATUS_UPDATE\nrmnode.RMNodeImpl (RMNodeImpl.java:handle(384)) - Processing 127.0.0.1:1234 of type STATUS_UPDATE\nevent.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent.EventType: APP_ATTEMPT_ADDED\nevent.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent.EventType: NODE_UPDATE\nevent.AsyncDispatcher (AsyncDispatcher.java:dispatch(164)) - Dispatching the event org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent.EventType: ATTEMPT_ADDED\nattempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(658)) - Processing event for appattempt_1412569506932_0001_000001 of type ATTEMPT_ADDED\nattempt.RMAppAttemptImpl (RMAppAttemptImpl.java:handle(670)) - appattempt_1412569506932_0001_000001 State change from SUBMITTED to SCHEDULED"
        }
    },
    {
        "filename": "YARN-4288.json",
        "creation_time": "2015-10-22T12:30:16.000+0000",
        "bug_report": {
            "title": "Connection Reset by Peer Error in NodeManager Registration",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop YARN NodeManager service on the local host.",
                    "Attempt to register the NodeManager with the ResourceManager located at 172.27.62.57:8025.",
                    "Monitor the logs for any connection-related errors."
                ],
                "actualBehavior": "The NodeManager fails to register with the ResourceManager, resulting in a 'Connection reset by peer' error.",
                "possibleCause": "The ResourceManager may be down, unreachable, or rejecting connections from the NodeManager due to network issues or configuration errors."
            },
            "stackTrace": "java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1473)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1400)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)\n        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethod)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)\nCaused by: java.io.IOException: Connection reset by peer\n        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n        at sun.nio.ch.IOUtil.read(IOUtil.java:197)\n        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at java.io.FilterInputStream.read(FilterInputStream.java:133)\n        at java.io.FilterInputStream.read(FilterInputStream.java:133)\n        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:514)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)\n        at java.io.DataInputStream.readInt(DataInputStream.java:387)\n        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1072)\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:967)\n\norg.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:223)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)\nCaused by: java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"ebdp-ch2-172.27.62.28\"; destination host is: \"172.27.62.57\":8025;\n        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1473)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1400)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)\n        at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)\n        ... 1 more\nCaused by: java.io.IOException: Connection reset by peer\n        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n        at sun.nio.ch.IOUtil.read(IOUtil.java:197)\n        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n        at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n        at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n        at java.io.FilterInputStream.read(FilterInputStream.java:133)\n        at java.io.FilterInputStream.read(FilterInputStream.java:133)\n        at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:514)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:254)\n        at java.io.DataInputStream.readInt(DataInputStream.java:387)\n        at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1072)\n        at org.apache.hadoop.ipc.Client$Connection.run(Client.java:967)"
        }
    },
    {
        "filename": "YARN-1032.json",
        "creation_time": "2013-08-05T21:10:46.000+0000",
        "bug_report": {
            "title": "NullPointerException in RackResolver during container allocation",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop YARN application with multiple containers.",
                    "2. Configure the application to use rack awareness for resource allocation.",
                    "3. Monitor the application logs for any errors during the container allocation process."
                ],
                "actualBehavior": "The application throws a NullPointerException in the RackResolver class, causing the container allocation process to fail.",
                "possibleCause": "The RackResolver may be attempting to access a null reference, possibly due to misconfigured rack information or missing node data."
            },
            "stackTrace": "java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)\n\tat org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)\n\tat org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)\n\tat java.lang.Thread.run(Thread.java:722)"
        }
    },
    {
        "filename": "YARN-5837.json",
        "creation_time": "2016-11-04T16:06:59.000+0000",
        "bug_report": {
            "title": "NullPointerException in NodeCLI.printNodeStatus",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN application.",
                    "2. Execute the NodeCLI command to print node status.",
                    "3. Observe the output."
                ],
                "actualBehavior": "The application throws a NullPointerException and fails to print the node status.",
                "possibleCause": "It is possible that a required object or variable is not initialized before being accessed in the printNodeStatus method."
            },
            "stackTrace": "Exception in thread \"main\" java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)\n\tat org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)\n\tat org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)"
        }
    },
    {
        "filename": "YARN-6827.json",
        "creation_time": "2017-07-15T05:14:25.000+0000",
        "bug_report": {
            "title": "NullPointerException in TimelineClientImpl while putting entities",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Attempt to publish metrics using the TimelineServiceV1Publisher.",
                    "3. Monitor the logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown when trying to put entities in the TimelineClientImpl.",
                "possibleCause": "It is possible that a required object or parameter is not initialized before being used in the putEntities method."
            },
            "stackTrace": "java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:178)\n\tat org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.putEntity(TimelineServiceV1Publisher.java:368)"
        }
    },
    {
        "filename": "YARN-3832.json",
        "creation_time": "2015-06-19T13:31:18.000+0000",
        "bug_report": {
            "title": "IOException when renaming directory in HDFS",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to rename a directory in HDFS that is not empty.",
                    "2. Ensure the destination directory exists and contains files.",
                    "3. Execute the rename operation."
                ],
                "actualBehavior": "An IOException is thrown indicating that the rename cannot overwrite a non-empty destination directory.",
                "possibleCause": "The rename operation in HDFS does not allow overwriting of non-empty directories, which leads to this exception."
            },
            "stackTrace": "java.io.IOException: Rename cannot overwrite non empty destination directory /opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39\nat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:735)\nat org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:244)\nat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:678)\nat org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)\nat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)\nat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-2409.json",
        "creation_time": "2014-08-12T10:53:06.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException during Application Attempt in YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a YARN application with a specific configuration.",
                    "2. Monitor the application attempts as they transition through various states.",
                    "3. Observe the logs for any state transition errors."
                ],
                "actualBehavior": "The application fails with an InvalidStateTransitionException when it receives STATUS_UPDATE and CONTAINER_ALLOCATED events while in the LAUNCHED state.",
                "possibleCause": "The application may be sending events that are not valid for the current state, indicating a potential issue in the event handling logic or state management."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n\tat java.lang.Thread.run(Thread.java:662)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at LAUNCHED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)\n\tat org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n\tat java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "YARN-8116.json",
        "creation_time": "2018-04-04T15:30:52.000+0000",
        "bug_report": {
            "title": "NumberFormatException when loading container state in NodeManager",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop YARN NodeManager service.",
                    "Ensure that there is an empty string in the container state data.",
                    "Observe the logs for any exceptions thrown during the initialization of the NodeManager."
                ],
                "actualBehavior": "The NodeManager fails to start and throws a NumberFormatException due to an empty string being parsed as a long.",
                "possibleCause": "The container state data may be corrupted or improperly formatted, leading to an empty string being encountered where a numeric value is expected."
            },
            "stackTrace": "java.lang.NumberFormatException: For input string: \"\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Long.parseLong(Long.java:601)\n\tat java.lang.Long.parseLong(Long.java:631)\n\tat org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)\n\tat org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)"
        }
    },
    {
        "filename": "YARN-8403.json",
        "creation_time": "2018-06-06T22:34:42.000+0000",
        "bug_report": {
            "title": "YarnException: Download and unpack failed due to Permission Denied",
            "description": {
                "stepsToReproduce": [
                    "Submit a job to the Hadoop YARN cluster that requires downloading files from HDFS.",
                    "Ensure that the input files are located in a directory with restricted permissions.",
                    "Monitor the YARN NodeManager logs for errors during the localization process."
                ],
                "actualBehavior": "The job fails with a YarnException indicating that the download and unpack process failed due to a FileNotFoundException caused by permission denied errors.",
                "possibleCause": "The user running the YARN application does not have the necessary permissions to access the specified input directory or file."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed\n        at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:306)\n        at org.apache.hadoop.yarn.util.FSDownload.verifyAndCopy(FSDownload.java:283)\n        at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:409)\n        at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:66)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)\n        at java.io.FileOutputStream.open0(Native Method)\n        at java.io.FileOutputStream.open(FileOutputStream.java:270)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:213)\n        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)\n        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)\n        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)\n        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:408)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:399)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:381)\n        at org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack(FSDownload.java:298)\n        ... 9 more\n\njava.io.InterruptedIOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1012)\n        at org.apache.hadoop.util.Shell.run(Shell.java:902)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:402)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1229)\nCaused by: java.lang.InterruptedException\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:502)\n        at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n        ... 5 more\n\norg.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: java.io.InterruptedIOException: java.lang.InterruptedException\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:183)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:402)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1229)\nCaused by: java.io.InterruptedIOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1012)\n        at org.apache.hadoop.util.Shell.run(Shell.java:902)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)\n        ... 2 more\nCaused by: java.lang.InterruptedException\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:502)\n        at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)\n        ... 5 more\n\njava.io.IOException: Application application_1528246317583_0048 initialization failed (exitCode=-1) with output: null\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:411)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1229)\nCaused by: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: java.io.InterruptedIOException: java.lang.InterruptedException\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:183)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer(LinuxContainerExecutor.java:402)\n        ... 1 more\nCaused by: java.io.InterruptedIOException: java.lang.InterruptedException\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1012)\n        at org.apache.hadoop.util.Shell.run(Shell.java:902)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)\n        ... 2 more\nCaused by: java.lang.InterruptedException\n        at java.lang.Object.wait(Native Method)\n        at java.lang.Object.wait(Object.java:502)\n        at java.lang.UNIXProcess.waitFor(UNIXProcess.java:395)\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1002)"
        }
    },
    {
        "filename": "YARN-1458.json",
        "creation_time": "2013-11-29T03:31:39.000+0000",
        "bug_report": {
            "title": "Thread Blocked in FairScheduler Leading to Resource Management Delays",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with FairScheduler enabled.",
                    "2. Submit multiple applications to the ResourceManager simultaneously.",
                    "3. Monitor the ResourceManager logs for thread states and application handling."
                ],
                "actualBehavior": "The FairScheduler encounters a BLOCKED state while trying to remove an application, causing delays in resource allocation and application management.",
                "possibleCause": "The issue may be caused by contention for the FairScheduler lock, where multiple threads are trying to access shared resources simultaneously, leading to a deadlock or significant delays."
            },
            "stackTrace": "java.lang.Thread.State: BLOCKED (on object monitor)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplication(FairScheduler.java:671)\n        - waiting to lock <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1023)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:440)\n        at java.lang.Thread.run(Thread.java:744)\n\njava.lang.Thread.State: RUNNABLE\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getAppWeight(FairScheduler.java:545)\n        - locked <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.getWeights(AppSchedulable.java:129)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShare(ComputeFairShares.java:143)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.resourceUsedWithWeightToResourceRatio(ComputeFairShares.java:131)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShares(ComputeFairShares.java:102)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy.computeShares(FairSharePolicy.java:119)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.recomputeShares(FSLeafQueue.java:100)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.recomputeShares(FSParentQueue.java:62)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:282)\n        - locked <0x000000070026b6e0> (a org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:255)\n        at java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "YARN-8209.json",
        "creation_time": "2018-04-26T00:22:23.000+0000",
        "bug_report": {
            "title": "NullPointerException in DockerClient while executing container deletion",
            "description": {
                "stepsToReproduce": [
                    "1. Start a YARN application that uses Docker containers.",
                    "2. Attempt to delete a Docker container managed by YARN.",
                    "3. Monitor the logs for any exceptions thrown during the deletion process."
                ],
                "actualBehavior": "A NullPointerException is thrown in the DockerClient class, causing the container deletion process to fail.",
                "possibleCause": "The NullPointerException may be caused by an uninitialized variable or resource in the DockerClient class, specifically in the writeCommandToTempFile method."
            },
            "stackTrace": "java.lang.NullPointerException\n\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile(DockerClient.java:109)\n\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand(DockerCommandExecutor.java:85)\n\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeStatusCommand(DockerCommandExecutor.java:192)\n\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.getContainerStatus(DockerCommandExecutor.java:128)\n\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer(LinuxContainerExecutor.java:935)\n\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask.run(DockerContainerDeletionTask.java:61)\n\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\n        at java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "YARN-3804.json",
        "creation_time": "2015-06-15T08:54:42.000+0000",
        "bug_report": {
            "title": "ResourceManager Fails to Transition to Active State Due to Permission Issue",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop cluster with ResourceManager configured for High Availability.",
                    "2. Attempt to transition the ResourceManager to the Active state.",
                    "3. Observe the logs for any errors or exceptions."
                ],
                "actualBehavior": "The ResourceManager fails to transition to the Active state, throwing a ServiceFailedException due to permission issues.",
                "possibleCause": "The user 'yarn' does not have the necessary permissions to call 'refreshAdminAcls', which is required for the transition to Active."
            },
            "stackTrace": "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active\n        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:645)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:518)\nCaused by: org.apache.hadoop.ha.ServiceFailedException: Can not execute refreshAdminAcls\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:297)\n        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)\n        ... 4 more\nCaused by: org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'\n        at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:230)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAdminAcls(AdminService.java:465)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:295)\n        ... 5 more\nCaused by: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'\n        at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:182)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:148)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAccess(AdminService.java:223)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:228)"
        }
    },
    {
        "filename": "YARN-1839.json",
        "creation_time": "2014-03-14T23:52:29.000+0000",
        "bug_report": {
            "title": "Invalid NMToken Error in Hadoop YARN Container Management",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop YARN cluster.",
                    "2. Submit a MapReduce job that requires container allocation.",
                    "3. Monitor the logs for the job execution."
                ],
                "actualBehavior": "The job fails with an error indicating 'No NMToken sent for <host>:45454'.",
                "possibleCause": "The issue may be caused by a misconfiguration in the YARN ResourceManager or NodeManager, leading to the absence of a valid NMToken for the container."
            },
            "stackTrace": "org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454\n\tat org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)\n\tat org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)\n\tat org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)\n\tat org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)\n\tat org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)\n\tat org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:722)"
        }
    },
    {
        "filename": "YARN-6714.json",
        "creation_time": "2017-06-15T09:56:15.000+0000",
        "bug_report": {
            "title": "IllegalStateException when unreserving resources in YARN Capacity Scheduler",
            "description": {
                "stepsToReproduce": [
                    "1. Submit an application to the YARN ResourceManager.",
                    "2. Reserve resources for the application.",
                    "3. Attempt to unreserve resources for a different application attempt while the original application is still reserved."
                ],
                "actualBehavior": "An IllegalStateException is thrown indicating that the system is trying to unreserve resources for a different application attempt than the one currently reserved.",
                "possibleCause": "The issue may be caused by a race condition or incorrect state management in the resource reservation logic of the YARN Capacity Scheduler."
            },
            "stackTrace": "java.lang.IllegalStateException: Trying to unreserve  for application appattempt_1495188831758_0121_000002 when currently reserved  for application application_1495188831758_0121 on node host: node1:45454 #containers=2 available=... used=...\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:152)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)\n        at java.lang.Thread.run(Thread.java:834)"
        }
    },
    {
        "filename": "YARN-3351.json",
        "creation_time": "2015-03-16T14:19:59.000+0000",
        "bug_report": {
            "title": "BindException: Cannot assign requested address during HTTP connection",
            "description": {
                "stepsToReproduce": [
                    "1. Start the application that utilizes the Apache Commons HttpClient.",
                    "2. Attempt to make an HTTP request to a specified server endpoint.",
                    "3. Observe the application logs for any exceptions thrown."
                ],
                "actualBehavior": "The application throws a java.net.BindException indicating that it cannot assign the requested address when trying to establish an HTTP connection.",
                "possibleCause": "This issue may be caused by the application attempting to bind to an IP address that is not available on the host machine, possibly due to network configuration issues or resource exhaustion."
            },
            "stackTrace": "java.net.BindException: Cannot assign requested address\n\tat java.net.PlainSocketImpl.socketBind(Native Method)\n\tat java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)\n\tat java.net.Socket.bind(Socket.java:631)\n\tat java.net.Socket.<init>(Socket.java:423)\n\tat java.net.Socket.<init>(Socket.java:280)\n\tat org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)\n\tat org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)\n\tat org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)\n\tat org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)\n\tat org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:707)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)"
        }
    },
    {
        "filename": "YARN-2813.json",
        "creation_time": "2014-11-05T22:29:46.000+0000",
        "bug_report": {
            "title": "NullPointerException in TimelineWebServices.getDomains",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN server.",
                    "2. Send a request to the TimelineWebServices endpoint that triggers the getDomains method.",
                    "3. Observe the server logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown when accessing the getDomains method in TimelineWebServices.",
                "possibleCause": "The MemoryTimelineStore may not be properly initialized or may be missing required data, leading to a NullPointerException when attempting to retrieve domains."
            },
            "stackTrace": "javax.ws.rs.WebApplicationException: java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n        at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:96)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:572)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:269)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:542)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1204)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:713)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)\n        at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)\n        at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:353)"
        }
    },
    {
        "filename": "YARN-1550.json",
        "creation_time": "2013-12-30T03:58:32.000+0000",
        "bug_report": {
            "title": "NullPointerException in FairSchedulerAppsBlock.render",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Access the FairScheduler web interface.",
                    "3. Trigger the rendering of the applications block."
                ],
                "actualBehavior": "The application fails to render the FairScheduler apps block, resulting in a NullPointerException.",
                "possibleCause": "It is possible that a required object or resource is not initialized or is null when the render method is called."
            },
            "stackTrace": "java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t....\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock.render(FairSchedulerAppsBlock.java:96)\n\tat org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:66)\n\tat org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:76)"
        }
    },
    {
        "filename": "YARN-5006.json",
        "creation_time": "2016-04-28T08:26:38.000+0000",
        "bug_report": {
            "title": "ConnectionLossException in ZKRMStateStore during application state storage",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Attempt to submit an application that requires state storage.",
                    "3. Monitor the logs for any connection issues with ZooKeeper."
                ],
                "actualBehavior": "The ResourceManager throws a ConnectionLossException when trying to store application state in ZooKeeper.",
                "possibleCause": "The ResourceManager may be experiencing intermittent connectivity issues with the ZooKeeper ensemble, leading to the ConnectionLossException."
            },
            "stackTrace": "org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n        at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)\n        at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:936)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:933)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1075)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1096)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:933)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:947)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.createWithRetries(ZKRMStateStore.java:956)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:626)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:138)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:123)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:860)\n        at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:855)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:724)"
        }
    },
    {
        "filename": "YARN-5728.json",
        "creation_time": "2016-10-13T05:16:28.000+0000",
        "bug_report": {
            "title": "Test Timeout in MiniYarnClusterNodeUtilization",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a MiniYarnCluster environment.",
                    "2. Execute the test case 'testUpdateNodeUtilization' in the TestMiniYarnClusterNodeUtilization class.",
                    "3. Observe the execution for timeout behavior."
                ],
                "actualBehavior": "The test 'testUpdateNodeUtilization' fails with a timeout exception after 60000 milliseconds.",
                "possibleCause": "The test may be waiting for a node heartbeat response that is not being received in a timely manner, possibly due to network issues or resource contention."
            },
            "stackTrace": "java.lang.Exception: test timed out after 60000 milliseconds\n\tat java.lang.Thread.sleep(Native Method)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.processWaitTimeAndRetryInfo(RetryInvocationHandler.java:130)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:107)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)\n\tat com.sun.proxy.$Proxy85.nodeHeartbeat(Unknown Source)\n\tat org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:113)"
        }
    },
    {
        "filename": "YARN-2805.json",
        "creation_time": "2014-11-04T20:37:09.000+0000",
        "bug_report": {
            "title": "YarnRuntimeException: Failed to login due to keytab issue",
            "description": {
                "stepsToReproduce": [
                    "Start the ResourceManager service in a Hadoop YARN cluster.",
                    "Ensure that the keytab file /etc/security/keytabs/rm.service.keytab is correctly configured.",
                    "Attempt to log in using the ResourceManager."
                ],
                "actualBehavior": "The ResourceManager fails to start and throws a YarnRuntimeException indicating a login failure.",
                "possibleCause": "The keytab file may be misconfigured, or the user does not have the necessary permissions to access the keytab."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)\nCaused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user\n\n\tat org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)"
        }
    },
    {
        "filename": "YARN-4744.json",
        "creation_time": "2016-02-29T10:08:57.000+0000",
        "bug_report": {
            "title": "PrivilegedOperationException with Exit Code 9 in YARN NodeManager",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN NodeManager service.",
                    "2. Submit a job that requires container execution.",
                    "3. Monitor the NodeManager logs for any errors during container cleanup."
                ],
                "actualBehavior": "The NodeManager throws a PrivilegedOperationException with exit code 9 during the container cleanup process.",
                "possibleCause": "The exit code 9 may indicate a failure in executing a privileged operation, possibly due to insufficient permissions or a misconfiguration in the container runtime."
            },
            "stackTrace": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=9:\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:173)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer(DefaultLinuxContainerRuntime.java:132)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.signalContainer(DelegatingLinuxContainerRuntime.java:109)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.signalContainer(LinuxContainerExecutor.java:513)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:520)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:139)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:55)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: ExitCodeException exitCode=9:\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)\n        at org.apache.hadoop.util.Shell.run(Shell.java:838)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:150)"
        }
    },
    {
        "filename": "YARN-1752.json",
        "creation_time": "2014-02-22T05:51:42.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException when handling UNREGISTERED event at LAUNCHED state",
            "description": {
                "stepsToReproduce": [
                    "1. Submit an application to the YARN ResourceManager.",
                    "2. Ensure the application reaches the LAUNCHED state.",
                    "3. Trigger an UNREGISTERED event for the application attempt."
                ],
                "actualBehavior": "The application fails to handle the UNREGISTERED event and throws an InvalidStateTransitionException.",
                "possibleCause": "The application attempt is not designed to handle the UNREGISTERED event when it is in the LAUNCHED state, indicating a potential flaw in the state transition logic."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED\n  at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n  at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n  at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)\n  at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)\n  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)\n  at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)\n  at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n  at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n  at java.lang.Thread.run(Thread.java:695)"
        }
    },
    {
        "filename": "YARN-6629.json",
        "creation_time": "2017-05-22T08:31:16.000+0000",
        "bug_report": {
            "title": "NullPointerException in AppSchedulingInfo.allocate",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop YARN cluster with the CapacityScheduler.",
                    "2. Submit a job that requires resource allocation.",
                    "3. Trigger a condition that leads to negative pending resources (e.g., by manipulating resource requests)."
                ],
                "actualBehavior": "The application throws a NullPointerException during the resource allocation process.",
                "possibleCause": "The issue may be caused by an uninitialized object or a missing resource allocation context in the AppSchedulingInfo class."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:446)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply(FiCaSchedulerApp.java:516)\n        at org.apache.hadoop.yarn.client.TestNegativePendingResource$1.answer(TestNegativePendingResource.java:225)\n        at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:31)\n        at org.mockito.internal.MockHandler.handle(MockHandler.java:97)\n        at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp$$EnhancerByMockitoWithCGLIB$$29eb8afc.apply(<generated>)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit(CapacityScheduler.java:2396)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.submitResourceCommitRequest(CapacityScheduler.java:2281)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1247)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1236)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1325)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1112)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:987)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1367)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:143)\n        at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-3493.json",
        "creation_time": "2015-04-15T22:03:19.000+0000",
        "bug_report": {
            "title": "Invalid Resource Request Exception in ResourceManager",
            "description": {
                "stepsToReproduce": [
                    "1. Configure the ResourceManager with a maximum memory limit of 2048 MB.",
                    "2. Submit a resource request for 3072 MB of memory.",
                    "3. Start the ResourceManager services."
                ],
                "actualBehavior": "The ResourceManager throws an InvalidResourceRequestException indicating that the requested memory exceeds the maximum configured limit.",
                "possibleCause": "The application is attempting to request more memory than the maximum allowed configuration, leading to validation failure in the ResourceManager."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\n\norg.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)\n\njava.lang.NullPointerException\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.serviceStop(AsyncDispatcher.java:142)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)\n        at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStop(ResourceManager.java:601)\n        at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)\n        at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)\n        at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:203)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1031)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1071)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1208)"
        }
    },
    {
        "filename": "YARN-7645.json",
        "creation_time": "2017-12-12T21:19:53.000+0000",
        "bug_report": {
            "title": "AssertionError in TestContainerResourceUsage during AM Restart Tests",
            "description": {
                "stepsToReproduce": [
                    "Run the TestContainerResourceUsage test suite.",
                    "Ensure that multiple containers are scheduled.",
                    "Trigger an Application Master (AM) restart during the test execution."
                ],
                "actualBehavior": "The test fails with an AssertionError indicating that the expected state 'ALLOCATED' does not match the actual state 'SCHEDULED'.",
                "possibleCause": "The state transition logic for containers during AM restart may not be handling the expected state correctly, leading to mismatches in state assertions."
            },
            "stackTrace": "java.lang.AssertionError: Attempt state is not correct (timeout). expected:<ALLOCATED> but was:<SCHEDULED>\n\tat org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.amRestartTests(TestContainerResourceUsage.java:275)\n\tat org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.testUsageAfterAMRestartWithMultipleContainers(TestContainerResourceUsage.java:254)"
        }
    },
    {
        "filename": "YARN-6054.json",
        "creation_time": "2017-01-04T20:58:59.000+0000",
        "bug_report": {
            "title": "ApplicationHistoryServer Fails to Initialize Due to LevelDB Corruption",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop YARN Application History Server.",
                    "Ensure that the LevelDB timeline store is configured correctly.",
                    "Check the logs for any initialization errors."
                ],
                "actualBehavior": "The ApplicationHistoryServer fails to start, throwing a ServiceStateException due to missing files in the LevelDB store.",
                "possibleCause": "The LevelDB store may have been corrupted, leading to missing SST files required for initialization."
            },
            "stackTrace": "org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst\n        at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)\n        at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)\n        at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:182)\nCaused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst\n        at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)\n        at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)\n        at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)\n        at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:229)\n        at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)"
        }
    },
    {
        "filename": "YARN-196.json",
        "creation_time": "2012-01-16T09:52:45.000+0000",
        "bug_report": {
            "title": "Connection Refused Error in NodeManager Registration",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop YARN NodeManager service on HOST-10-18-52-230.",
                    "Ensure that the ResourceManager is running on HOST-10-18-52-250.",
                    "Monitor the logs for any connection errors during the NodeManager startup."
                ],
                "actualBehavior": "The NodeManager fails to start and throws a connection refused error when attempting to register with the ResourceManager.",
                "possibleCause": "The ResourceManager may not be running, or there could be network issues preventing the NodeManager from connecting to the ResourceManager."
            },
            "stackTrace": "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)\n\tat org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)\nCaused by: java.lang.reflect.UndeclaredThrowableException\n\tat org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)\n\t... 3 more\nCaused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:131)\n\tat $Proxy23.registerNodeManager(Unknown Source)\n\tat org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)\n\t... 5 more\nCaused by: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:857)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1141)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1100)\n\tat org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:128)\n\t... 7 more\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:659)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:469)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:563)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1247)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1117)\n\t... 9 more\njava.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)\n\tat java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:76)\n\tat java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "YARN-8508.json",
        "creation_time": "2018-07-09T23:37:49.000+0000",
        "bug_report": {
            "title": "Insufficient GPU Resources for Container Launch",
            "description": {
                "stepsToReproduce": [
                    "Submit a container request that requires 2 GPUs.",
                    "Ensure that only 1 GPU is available on the node.",
                    "Monitor the container launch process."
                ],
                "actualBehavior": "The container fails to launch due to insufficient GPU resources, resulting in a ResourceHandlerException.",
                "possibleCause": "The node does not have enough available GPUs to satisfy the container's request."
            },
            "stackTrace": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.internalAssignGpus(GpuResourceAllocator.java:225)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.assignGpus(GpuResourceAllocator.java:173)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl.preStart(GpuResourceHandlerImpl.java:98)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.preStart(ResourceHandlerChain.java:75)\n\tat org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:509)\n\tat org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:479)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:494)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:306)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:103)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\njava.io.IOException: ResourceHandlerChain.preStart() failed!\n\tat org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:551)\n\tat org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:479)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:494)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:306)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:103)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.internalAssignGpus(GpuResourceAllocator.java:225)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator.assignGpus(GpuResourceAllocator.java:173)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceHandlerImpl.preStart(GpuResourceHandlerImpl.java:98)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.preStart(ResourceHandlerChain.java:75)\n\tat org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType(LinuxContainerExecutor.java:509)"
        }
    },
    {
        "filename": "YARN-2308.json",
        "creation_time": "2014-07-17T10:01:57.000+0000",
        "bug_report": {
            "title": "NullPointerException in CapacityScheduler when adding application attempt",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application to the ResourceManager.",
                    "3. Monitor the ResourceManager logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown in the CapacityScheduler when attempting to add an application attempt.",
                "possibleCause": "It is possible that an application attempt is being added without proper initialization of required objects, leading to a NullPointerException."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "YARN-933.json",
        "creation_time": "2013-07-17T12:29:28.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException on Application Launch Failure",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a YARN application to the ResourceManager.",
                    "2. Ensure that the application attempts to connect to a remote host that is unreachable.",
                    "3. Monitor the application state transitions in the ResourceManager logs."
                ],
                "actualBehavior": "The application fails to launch and throws an InvalidStateTransitionException indicating an invalid event: LAUNCH_FAILED at FAILED.",
                "possibleCause": "The application may be trying to transition to a launch state while it is already in a failed state due to a connection timeout."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: LAUNCH_FAILED at FAILED\n at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)\n at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:630)\n at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:99)\n at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:495)\n at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:476)\n at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)\n at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)\n at java.lang.Thread.run(Thread.java:662)\n\nCaused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=host-10-18-40-15/10.18.40.59:8020]\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:573)\n at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)"
        }
    },
    {
        "filename": "YARN-1374.json",
        "creation_time": "2013-10-30T11:49:49.000+0000",
        "bug_report": {
            "title": "ConcurrentModificationException in ResourceManager Initialization",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop ResourceManager service.",
                    "Ensure that there are multiple threads interacting with the service during initialization.",
                    "Observe the logs for any exceptions thrown during the service initialization."
                ],
                "actualBehavior": "The ResourceManager fails to initialize and throws a ConcurrentModificationException.",
                "possibleCause": "The exception may be caused by concurrent modifications to a collection while it is being iterated over, likely due to multiple threads accessing shared resources."
            },
            "stackTrace": "java.util.ConcurrentModificationException\n\tat java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)\n\tat java.util.AbstractList$Itr.next(AbstractList.java:343)\n\tat java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)\n\njava.util.ConcurrentModificationException\n\tat java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)\n\tat java.util.AbstractList$Itr.next(AbstractList.java:343)\n\tat java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)\n\tat org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)\n\tat org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)"
        }
    },
    {
        "filename": "YARN-174.json",
        "creation_time": "2012-10-19T17:25:40.000+0000",
        "bug_report": {
            "title": "YarnException: Invalid Path for User Logs",
            "description": {
                "stepsToReproduce": [
                    "1. Configure the Yarn log directory using an environment variable (e.g., set yarn.log.dir=${yarn.log.dir}).",
                    "2. Start the Yarn NodeManager service.",
                    "3. Observe the logs for any exceptions or errors."
                ],
                "actualBehavior": "The NodeManager fails to start and throws a YarnException indicating that the specified log directory path is invalid.",
                "possibleCause": "The environment variable 'yarn.log.dir' is not being resolved correctly, leading to an invalid path being passed to the LocalDirsHandlerService."
            },
            "stackTrace": "org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme\n        at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)\n        at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)\n        at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)\n        at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService.init(NodeHealthCheckerService.java:48)\n        at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged(NodeManager.java:256)\n        at org.apache.hadoop.yarn.service.AbstractService.changeState(AbstractService.java:163)\n        at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:112)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.reboot(NodeStatusUpdaterImpl.java:157)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$900(NodeStatusUpdaterImpl.java:63)\n        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:357)"
        }
    },
    {
        "filename": "YARN-6448.json",
        "creation_time": "2017-04-05T18:39:49.000+0000",
        "bug_report": {
            "title": "IllegalArgumentException: Comparison method violates its general contract",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit a job that requires scheduling resources.",
                    "3. Monitor the scheduling process to observe the behavior."
                ],
                "actualBehavior": "The ResourceManager throws an IllegalArgumentException during the sorting of nodes, causing scheduling to fail.",
                "possibleCause": "The comparison method used for sorting nodes may not adhere to the contract defined by the Comparator interface, leading to inconsistent results during sorting."
            },
            "stackTrace": "java.lang.IllegalArgumentException: Comparison method violates its general contract!\n        at java.util.TimSort.mergeHi(TimSort.java:899)\n        at java.util.TimSort.mergeAt(TimSort.java:516)\n        at java.util.TimSort.mergeForceCollapse(TimSort.java:457)\n        at java.util.TimSort.sort(TimSort.java:254)\n        at java.util.Arrays.sort(Arrays.java:1512)\n        at java.util.ArrayList.sort(ArrayList.java:1454)\n        at java.util.Collections.sort(Collections.java:175)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)"
        }
    },
    {
        "filename": "YARN-4530.json",
        "creation_time": "2015-12-30T15:19:19.000+0000",
        "bug_report": {
            "title": "IOException due to resource modification during download",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job that requires the unilever_support_udf-0.0.1-SNAPSHOT.jar from HDFS.",
                    "2. Ensure that the jar file is modified or updated on the source filesystem while the job is running.",
                    "3. Monitor the job execution to observe the error."
                ],
                "actualBehavior": "The job fails with an IOException indicating that the resource has changed on the source filesystem.",
                "possibleCause": "The jar file was modified after the job started, leading to a mismatch in expected and actual timestamps."
            },
            "stackTrace": "java.io.IOException: Resource hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar changed on src filesystem (expected 1451380519452, was 1451380611793\n\tat org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:176)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:276)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:50)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)"
        }
    },
    {
        "filename": "YARN-7737.json",
        "creation_time": "2018-01-11T19:35:01.000+0000",
        "bug_report": {
            "title": "FileNotFoundException when accessing prelaunch.err file in Hadoop YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop YARN application that requires container execution.",
                    "2. Monitor the application logs for any errors during the container launch process.",
                    "3. Observe the logs for any FileNotFoundException related to the prelaunch.err file."
                ],
                "actualBehavior": "The application fails to launch the container due to a FileNotFoundException indicating that the prelaunch.err file does not exist.",
                "possibleCause": "The prelaunch.err file may not have been created due to a failure in the container initialization process or incorrect file path configuration."
            },
            "stackTrace": "java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist\n        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-5136.json",
        "creation_time": "2016-05-24T15:34:28.000+0000",
        "bug_report": {
            "title": "IllegalStateException when removing application from FairScheduler",
            "description": {
                "stepsToReproduce": [
                    "1. Submit an application to the FairScheduler.",
                    "2. Attempt to remove the application from the scheduler.",
                    "3. Observe the logs for any exceptions thrown."
                ],
                "actualBehavior": "An IllegalStateException is thrown indicating that the application to remove does not exist in the specified queue.",
                "possibleCause": "The application may have already been removed or never existed in the queue, leading to the exception when trying to remove it again."
            },
            "stackTrace": "java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]\n    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)\n    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)\n    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)\n    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)\n    at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)\n    at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-8211.json",
        "creation_time": "2018-04-26T02:13:22.000+0000",
        "bug_report": {
            "title": "BufferUnderflowException in RegistryDNS during NIO TCP Client Operation",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop registry server.",
                    "2. Initiate a DNS request using the NIO TCP client.",
                    "3. Observe the server logs for any exceptions."
                ],
                "actualBehavior": "A BufferUnderflowException is thrown, indicating that the buffer does not contain enough data to fulfill the request.",
                "possibleCause": "The issue may be caused by an attempt to read more bytes from the buffer than are available, possibly due to incorrect handling of the buffer's position or limit."
            },
            "stackTrace": "java.nio.BufferUnderflowException\n\n        at java.nio.Buffer.nextGetIndex(Buffer.java:500)\n\n        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)\n\n        at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)\n\n        at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)\n\n        at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)\n\n        at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)\n\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\n        at java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "YARN-2124.json",
        "creation_time": "2014-06-05T07:44:27.000+0000",
        "bug_report": {
            "title": "NullPointerException in ProportionalCapacityPreemptionPolicy",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Configure the Proportional Capacity Preemption Policy.",
                    "3. Submit a job that triggers resource allocation and preemption."
                ],
                "actualBehavior": "The ResourceManager throws a NullPointerException, causing the preemption policy to fail.",
                "possibleCause": "It is likely that a resource object being passed to the Resources.greaterThan method is null, leading to the NullPointerException."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)\n        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)\n        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)\n        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)\n        at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)\n        at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)\n        at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)\n        at java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "YARN-7849.json",
        "creation_time": "2018-01-29T23:49:33.000+0000",
        "bug_report": {
            "title": "AssertionError in Node Utilization Test",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a MiniYarnCluster environment.",
                    "2. Run the test case TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization.",
                    "3. Observe the assertion failure related to container utilization."
                ],
                "actualBehavior": "The test fails with an AssertionError indicating that the expected container utilization was not propagated to RMNode, resulting in a null value.",
                "possibleCause": "The node utilization metrics may not be correctly updated or propagated within the MiniYarnCluster, leading to the expected values not being available during the assertion check."
            },
            "stackTrace": "java.lang.AssertionError: Containers Utillization not propagated to RMNode expected:<<pmem:1024, vmem:2048, vCores:11.0>> but was:<null>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.verifySimulatedUtilization(TestMiniYarnClusterNodeUtilization.java:227)\n\tat org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:116)"
        }
    },
    {
        "filename": "YARN-8591.json",
        "creation_time": "2018-07-27T05:56:26.000+0000",
        "bug_report": {
            "title": "NullPointerException in TimelineReaderWebServices",
            "description": {
                "stepsToReproduce": [
                    "1. Send a request to the TimelineReaderWebServices endpoint to retrieve entities.",
                    "2. Ensure that the request includes parameters that may lead to a null value being processed.",
                    "3. Observe the server response."
                ],
                "actualBehavior": "The server throws a NullPointerException, resulting in a 500 Internal Server Error response.",
                "possibleCause": "The NullPointerException may be caused by an uninitialized variable or a missing check for null values in the checkAccess method."
            },
            "stackTrace": "javax.ws.rs.WebApplicationException: java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)\n        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)\n        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)\n        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)\n        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:558)\n        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:733)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1772)\n        at org.apache.hadoop.yarn.server.timelineservice.reader.security.TimelineReaderWhitelistAuthorizationFilter.doFilter(TimelineReaderWhitelistAuthorizationFilter.java:85)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:644)\n        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:592)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:98)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1604)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1759)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n        at org.eclipse.jetty.server.Server.handle(Server.java:534)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)\n        at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n        at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\nCaused by: java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccess(TimelineReaderWebServices.java:3536)\n        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccessForGenericEntities(TimelineReaderWebServices.java:3513)\n        at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:622)"
        }
    },
    {
        "filename": "YARN-6649.json",
        "creation_time": "2017-05-25T20:36:08.000+0000",
        "bug_report": {
            "title": "RuntimeException: unable to encodeValue class from code 1000 in TimelineWebServices",
            "description": {
                "stepsToReproduce": [
                    "1. Send a request to the TimelineWebServices endpoint to retrieve an entity.",
                    "2. Ensure that the entity being requested has a specific class that may not be properly registered or encoded.",
                    "3. Observe the response from the server."
                ],
                "actualBehavior": "The server throws a RuntimeException indicating it is unable to encodeValue class from code 1000.",
                "possibleCause": "The issue may be related to the serialization process where the class corresponding to code 1000 is not properly registered in the FSTClazzNameRegistry."
            },
            "stackTrace": "javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000\n\tat org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)\n\tat sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)\n\tat com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)\n\tat com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)\n\tat com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:288)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)\n\tat com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)\n\tat com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)\n\tat com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)\n\tat com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n\tat com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n\tat com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n\tat com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n\tat com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n\tat com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:636)\n\tat org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:294)\n\tat org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:588)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.security.http.CrossOriginFilter.doFilter(CrossOriginFilter.java:95)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1352)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000\n\tat org.nustaq.serialization.util.FSTUtil.rethrow(FSTUtil.java:122)\n\tat org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:879)\n\tat org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)\n\tat org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:414)\n\tat org.apache.hadoop.yarn.server.timeline.EntityFileTimelineStore.getEntity(EntityFileTimelineStore.java:911)\n\tat org.apache.hadoop.yarn.server.timeline.TimelineDataManager.doGetEntity(TimelineDataManager.java:215)\n\tat org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getEntity(TimelineDataManager.java:202)\n\tat org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:155)\n\t... 52 more\nCaused by: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000\n\tat org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:240)\n\tat org.nustaq.serialization.FSTConfiguration.asObject(FSTConfiguration.java:877)\n\t... 58 more\nCaused by: java.lang.RuntimeException: unable to encodeValue class from code 1000\n\tat org.nustaq.serialization.FSTClazzNameRegistry.decodeClass(FSTClazzNameRegistry.java:173)\n\tat org.nustaq.serialization.coders.FSTStreamDecoder.readClass(FSTStreamDecoder.java:431)\n\tat org.nustaq.serialization.FSTObjectInput.readClass(FSTObjectInput.java:853)\n\tat org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:338)\n\tat org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)\n\tat org.nustaq.serialization.serializers.FSTArrayListSerializer.instantiate(FSTArrayListSerializer.java:63)\n\tat org.nustaq.serialization.FSTObjectInput.instantiateAndReadWithSer(FSTObjectInput.java:459)\n\tat org.nustaq.serialization.FSTObjectInput.readObjectWithHeader(FSTObjectInput.java:354)\n\tat org.nustaq.serialization.FSTObjectInput.readObjectInternal(FSTObjectInput.java:323)\n\tat org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:304)\n\tat org.nustaq.serialization.FSTObjectInput.readObject(FSTObjectInput.java:238)"
        }
    },
    {
        "filename": "YARN-3742.json",
        "creation_time": "2015-05-29T06:00:38.000+0000",
        "bug_report": {
            "title": "ZKClient Creation Timeout in YARN ResourceManager",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN ResourceManager with ZKRMStateStore configured.",
                    "2. Attempt to submit an application to the ResourceManager.",
                    "3. Monitor the logs for any timeout errors related to ZKClient creation."
                ],
                "actualBehavior": "The ResourceManager fails to create a ZKClient, resulting in a timeout error and preventing application submissions.",
                "possibleCause": "The Zookeeper service may be down or unreachable, causing the ResourceManager to time out while waiting for ZKClient creation."
            },
            "stackTrace": "java.io.IOException: Wait for ZKClient creation timed out\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-4984.json",
        "creation_time": "2016-04-21T19:16:03.000+0000",
        "bug_report": {
            "title": "HDFS Delegation Token Not Found in Cache",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop cluster with HDFS and YARN services running.",
                    "2. Submit a job that requires writing to HDFS.",
                    "3. Monitor the logs for the NodeManager during the job execution."
                ],
                "actualBehavior": "The job fails with an error indicating that the HDFS delegation token cannot be found in the cache.",
                "possibleCause": "The HDFS delegation token may have expired or not been properly cached, leading to the inability to retrieve it when needed."
            },
            "stackTrace": "Exception is:\norg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be found in cache\n    at org.apache.hadoop.ipc.Client.call(Client.java:1427)\n    at org.apache.hadoop.ipc.Client.call(Client.java:1358)\n    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n    at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)\n    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n    at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n    at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)\n    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)\n    at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)\n    at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)\n    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.access$100(LogAggregationService.java:67)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createAppDir(LogAggregationService.java:261)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator(LogAggregationService.java:367)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:320)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:447)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)"
        }
    },
    {
        "filename": "YARN-4584.json",
        "creation_time": "2016-01-12T09:08:31.000+0000",
        "bug_report": {
            "title": "NullPointerException in RMAppAttemptImpl during Recovery Process",
            "description": {
                "stepsToReproduce": [
                    "1. Start the ResourceManager in a Hadoop YARN cluster.",
                    "2. Trigger a failover to transition the ResourceManager to active state.",
                    "3. Observe the logs for any exceptions during the recovery process."
                ],
                "actualBehavior": "A NullPointerException is thrown in the RMAppAttemptImpl.recover method, causing the recovery process to fail.",
                "possibleCause": "The exception may be caused by an uninitialized object or a missing reference in the RMAppAttemptImpl class during the recovery phase."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover(RMAppAttemptImpl.java:887)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:826)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:953)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:946)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:786)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:328)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:464)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1232)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:594)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1022)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1062)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1058)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1705)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1058)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:323)\n        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:127)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:877)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
        }
    },
    {
        "filename": "YARN-2846.json",
        "creation_time": "2014-11-11T15:30:08.000+0000",
        "bug_report": {
            "title": "IOException: Interrupted while waiting for process to exit in YARN NodeManager",
            "description": {
                "stepsToReproduce": [
                    "1. Start a YARN application that launches containers.",
                    "2. Simulate a scenario where the NodeManager is under heavy load or resource contention.",
                    "3. Observe the logs for any interruptions or failures during container execution."
                ],
                "actualBehavior": "The application fails with an IOException indicating that the process was interrupted while waiting for it to exit.",
                "possibleCause": "The issue may be caused by resource contention or interruptions in the thread responsible for managing container execution."
            },
            "stackTrace": "java.io.IOException: Interrupted while waiting for process 20001 to exit\n        at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:46)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.InterruptedException: sleep interrupted\n        at java.lang.Thread.sleep(Native Method)\n        at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)"
        }
    },
    {
        "filename": "YARN-7890.json",
        "creation_time": "2018-02-03T21:10:43.000+0000",
        "bug_report": {
            "title": "NullPointerException in ContainerStartContext.getFilecacheDirs",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN NodeManager.",
                    "2. Submit a job that requires container execution.",
                    "3. Monitor the logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown during the execution of the container, causing the job to fail.",
                "possibleCause": "It is possible that the file cache directories are not properly initialized or are null when accessed in the getFilecacheDirs method."
            },
            "stackTrace": "java.lang.NullPointerException\n        at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)\n        at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)\n        at java.util.Collections.unmodifiableList(Collections.java:1287)\n        at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:49)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "YARN-139.json",
        "creation_time": "2012-10-01T19:51:20.000+0000",
        "bug_report": {
            "title": "InterruptedException during AsyncDispatcher stop in MRAppMaster",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop YARN cluster.",
                    "2. Wait for the job to finish.",
                    "3. Observe the logs during the job completion phase."
                ],
                "actualBehavior": "An InterruptedException is thrown when the AsyncDispatcher attempts to stop, causing potential job completion issues.",
                "possibleCause": "The thread handling the job completion may be interrupted unexpectedly, possibly due to external factors or mismanagement of thread states."
            },
            "stackTrace": "java.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Thread.join(Thread.java:1143)\n\tat java.lang.Thread.join(Thread.java:1196)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:105)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:437)\n\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:402)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:619)"
        }
    },
    {
        "filename": "YARN-42.json",
        "creation_time": "2012-05-14T11:38:55.000+0000",
        "bug_report": {
            "title": "YarnException: Failed to initialize LocalizationService due to IOException",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN NodeManager.",
                    "2. Ensure that the directory /mrv2/tmp/nm-local-dir/usercache is not accessible or does not exist.",
                    "3. Monitor the NodeManager logs for initialization errors."
                ],
                "actualBehavior": "The NodeManager fails to start, throwing a YarnException indicating that it could not initialize the LocalizationService due to an IOException when trying to create the usercache directory.",
                "possibleCause": "The issue may be caused by insufficient permissions to create the directory or the parent directory not existing."
            },
            "stackTrace": "org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)\n\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)\n\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:166)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:268)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:284)\nCaused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed\n\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)\n\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)\n\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)\n\tat org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2325)\n\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)\n\t... 6 more\n\njava.lang.NullPointerException\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop(NonAggregatingLogHandler.java:82)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stop(ContainerManagerImpl.java:266)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)\n\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)\n\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:182)\n\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
        }
    },
    {
        "filename": "YARN-7453.json",
        "creation_time": "2017-11-07T09:46:28.000+0000",
        "bug_report": {
            "title": "NoAuthException during ResourceManager transition to active state",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager in a high-availability setup.",
                    "2. Trigger a failover to transition the standby ResourceManager to active state.",
                    "3. Observe the logs for any authentication-related errors."
                ],
                "actualBehavior": "The ResourceManager fails to transition to the active state due to a NoAuthException, indicating that authentication is required but not provided.",
                "possibleCause": "The ResourceManager may not have the necessary authentication credentials configured for ZooKeeper, leading to the NoAuthException when attempting to perform operations."
            },
            "stackTrace": "org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:113)\n\tat org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1006)\n\tat org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:910)\n\tat org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:159)\n\tat org.apache.curator.framework.imps.CuratorTransactionImpl.access$200(CuratorTransactionImpl.java:44)\n\tat org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:129)\n\tat org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:125)\n\tat org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)\n\tat org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:122)\n\tat org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction.commit(ZKCuratorManager.java:403)\n\tat org.apache.hadoop.util.curator.ZKCuratorManager.safeSetData(ZKCuratorManager.java:372)\n\tat org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.getAndIncrementEpoch(ZKRMStateStore.java:493)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)\n\tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1162)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1202)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1198)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1198)\n\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)\n\tat org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:607)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)"
        }
    },
    {
        "filename": "YARN-3369.json",
        "creation_time": "2015-03-18T23:29:06.000+0000",
        "bug_report": {
            "title": "NullPointerException in AppSchedulingInfo during container allocation",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application that requires resource allocation.",
                    "3. Monitor the ResourceManager logs for any exceptions during the scheduling process."
                ],
                "actualBehavior": "A NullPointerException is thrown in the AppSchedulingInfo class, causing the application scheduling to fail.",
                "possibleCause": "It is possible that an expected object is not initialized before being accessed in the checkForDeactivation method."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)\n        at java.lang.Thread.run(Thread.java:722)"
        }
    },
    {
        "filename": "YARN-945.json",
        "creation_time": "2013-07-19T22:59:06.000+0000",
        "bug_report": {
            "title": "AccessControlException: SIMPLE authentication is not enabled",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop server with default configuration.",
                    "2. Attempt to connect to the server using a client that requires SIMPLE authentication.",
                    "3. Observe the server logs for any authentication errors."
                ],
                "actualBehavior": "The server throws an AccessControlException indicating that SIMPLE authentication is not enabled.",
                "possibleCause": "The server configuration may not have SIMPLE authentication enabled, or the client is attempting to use an unsupported authentication method."
            },
            "stackTrace": "org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN]\n   at org.apache.hadoop.ipc.Server$Connection.initializeAuthContext(Server.java:1531)\n   at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1482)\n   at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:788)\n   at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:587)\n   at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:562)"
        }
    },
    {
        "filename": "YARN-6072.json",
        "creation_time": "2017-01-08T09:21:12.000+0000",
        "bug_report": {
            "title": "NullPointerException during ResourceManager transition to Active",
            "description": {
                "stepsToReproduce": [
                    "Start the Hadoop YARN ResourceManager in a high availability (HA) setup.",
                    "Trigger a failover to transition the standby ResourceManager to active.",
                    "Observe the logs for any exceptions or errors during the transition."
                ],
                "actualBehavior": "A NullPointerException is thrown, preventing the ResourceManager from transitioning to Active state.",
                "possibleCause": "The NullPointerException may be caused by uninitialized or improperly configured service ACLs during the refresh process."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:552)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)\n        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\norg.apache.hadoop.ha.ServiceFailedException\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)\n        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\norg.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active\n        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\nCaused by: org.apache.hadoop.ha.ServiceFailedException: Error on refreshAll during transition to Active\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:311)\n        at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)\n        ... 4 more\n\nCaused by: org.apache.hadoop.ha.ServiceFailedException\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:712)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)\n        ... 5 more"
        }
    },
    {
        "filename": "YARN-7663.json",
        "creation_time": "2017-12-15T01:52:46.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException when starting a KILLED application in YARN",
            "description": {
                "stepsToReproduce": [
                    "Submit a YARN application and let it run until it reaches the KILLED state.",
                    "Attempt to start the application again after it has been killed.",
                    "Observe the system behavior and logs for any exceptions."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown indicating that a START event cannot be processed for an application in the KILLED state.",
                "possibleCause": "The application state machine does not allow transitioning from KILLED to START, which may indicate a flaw in the state management logic."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: START at KILLED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:805)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:116)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:901)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:885)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-5873.json",
        "creation_time": "2016-11-12T09:54:20.000+0000",
        "bug_report": {
            "title": "NullPointerException in WritingContainerStartEvent",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN ResourceManager.",
                    "2. Submit a job that requires container allocation.",
                    "3. Monitor the ResourceManager logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown during the handling of container start events, causing the ResourceManager to fail to allocate containers properly.",
                "possibleCause": "The issue may be caused by an uninitialized object or variable in the WritingContainerStartEvent class, specifically at line 38."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)\n        at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:217)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:210)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.doAllocation(RegularContainerAllocator.java:746)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:832)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:865)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:931)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1044)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:690)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:508)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1475)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1470)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1559)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1346)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1221)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1601)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:149)\n        at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-3227.json",
        "creation_time": "2015-02-19T16:58:01.000+0000",
        "bug_report": {
            "title": "IOException: Failed to renew token due to Unauthorized error",
            "description": {
                "stepsToReproduce": [
                    "1. Submit an application to the YARN ResourceManager.",
                    "2. Ensure that the application requires a delegation token for accessing the timeline server.",
                    "3. Monitor the logs for any token renewal attempts."
                ],
                "actualBehavior": "The application fails to renew the delegation token, resulting in an IOException with an HTTP 401 Unauthorized error.",
                "possibleCause": "The user or service account attempting to renew the token may not have the necessary permissions or the token may have expired."
            },
            "stackTrace": "java.io.IOException: Failed to renew token: Kind: TIMELINE_DELEGATION_TOKEN,\nService: timelineserver.example.com:4080, Ident: (owner=user,\nrenewer=rmuser, realUser=oozie, issueDate=1423248845528,\nmaxDate=1423853645528, sequenceNumber=9716, masterKeyId=9)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:443)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:77)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:808)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:789)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: java.io.IOException: HTTP status [401], message [Unauthorized]\n        at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:286)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.renewDelegationToken(DelegationTokenAuthenticator.java:211)\n        at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.renewDelegationToken(DelegationTokenAuthenticatedURL.java:414)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:374)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:360)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$4.run(TimelineClientImpl.java:429)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:161)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.operateDelegationToken(TimelineClientImpl.java:444)\n        at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.renewDelegationToken(TimelineClientImpl.java:378)\n        at org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier$Renewer.renew(TimelineDelegationTokenIdentifier.java:81)\n        at org.apache.hadoop.security.token.Token.renew(Token.java:377)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:532)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:529)"
        }
    },
    {
        "filename": "YARN-4235.json",
        "creation_time": "2015-10-07T19:26:24.000+0000",
        "bug_report": {
            "title": "IndexOutOfBoundsException in FairScheduler when accessing empty list",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with FairScheduler enabled.",
                    "2. Submit an application that requires queue placement based on primary group.",
                    "3. Ensure that the primary group does not have any associated queues configured."
                ],
                "actualBehavior": "The application submission fails with an IndexOutOfBoundsException, indicating that the code is trying to access an element from an empty list.",
                "possibleCause": "The issue may be caused by the FairScheduler attempting to retrieve a queue for an application based on a primary group that has no associated queues, leading to an attempt to access an index in an empty list."
            },
            "stackTrace": "java.lang.IndexOutOfBoundsException: Index: 0\n\tat java.util.Collections$EmptyList.get(Collections.java:3212)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)\n\tat org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)\n\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)\n\tat java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-4833.json",
        "creation_time": "2016-03-17T13:22:23.000+0000",
        "bug_report": {
            "title": "AccessControlException when submitting application to YARN queue",
            "description": {
                "stepsToReproduce": [
                    "1. Attempt to submit a YARN application using the user 'hdfs'.",
                    "2. Ensure that the application is targeting the 'default' queue.",
                    "3. Observe the response from the YARN ResourceManager."
                ],
                "actualBehavior": "The application submission fails with an AccessControlException indicating that the user 'hdfs' does not have permission to submit to the 'default' queue.",
                "possibleCause": "The user 'hdfs' may not have the necessary permissions configured in the YARN queue settings."
            },
            "stackTrace": "org.apache.hadoop.security.AccessControlException: User hdfs does not have permission to submit application_1458273884145_0001 to queue default\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)\n        at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.submitApplication:618)\n        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)\n        at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:483)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2360)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2356)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2356)\n\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Native ConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateIOException(RPCUtil.java:80)\n        at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:119)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.submitApplication(ApplicationClientProtocolPBClientImpl.java:272)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:257)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy23.submitApplication(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.submitApplication:261)\n        at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.submitApplication:295)\n        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.submitJob:301)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.submitJobInternal:244)\n        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)\n        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1338)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1338)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1359)\n        at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:306)\n        at org.apache.hadoop.examples.QuasiMonteCarlo.run(QuasiMonteCarlo.java:359)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n        at org.apache.hadoop.examples.QuasiMonteCarlo.main(QuasiMonteCarlo.java:367)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver$ProgramDescription.java:71)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.main:74)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.run:222)\n        at org.apache.hadoop.util.RunJar.main(RunJar.main:136)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): User hdfs does not have permission to submit application_1458273884145_0001 to queue default\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)\n        at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.submitApplication:618)\n        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)\n        at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:483)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:637)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2360)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2356)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2356)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1449)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1386)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine$Invoker.java:230)\n        at com.sun.proxy.$Proxy22.submitApplication(Unknown Source)\n        at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.submitApplication(ApplicationClientProtocolPBClientImpl.java:269)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:257)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)\n        at com.sun.proxy.$Proxy23.submitApplication(Unknown Source)\n        at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.submitApplication:261)\n        at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.submitApplication:295)\n        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.submitJob:301)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.submitJobInternal:244)\n        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)\n        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1338)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1742)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1338)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1359)\n        at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:306)\n        at org.apache.hadoop.examples.QuasiMonteCarlo.run(QuasiMonteCarlo.java:359)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\n        at org.apache.hadoop.examples.QuasiMonteCarlo.main(QuasiMonteCarlo.java:367)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver$ProgramDescription.java:71)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.main:74)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingConstructorAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:497)\n        at org.apache.hadoop.util.RunJar.run(RunJar.run:222)\n        at org.apache.hadoop.util.RunJar.main(RunJar.main:136)\nCaused by: java.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.connect:206)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client$Connection.java:634)\n        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client$Connection.java:733)\n        at org.apache.hadoop.ipc.Client$Connection.access$2900(Client$Connection.java:378)\n        at org.apache.hadoop.ipc.Client.getConnection(Client$Connection.java:1510)"
        }
    },
    {
        "filename": "YARN-1689.json",
        "creation_time": "2014-02-05T19:16:00.000+0000",
        "bug_report": {
            "title": "NullPointerException in AbstractYarnScheduler during Application Master Registration",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager.",
                    "2. Submit an application to the ResourceManager.",
                    "3. Ensure that the application is in a KILLED state before the Application Master attempts to register."
                ],
                "actualBehavior": "A NullPointerException is thrown when the Application Master tries to register, leading to application failure.",
                "possibleCause": "The application may be in an invalid state (KILLED) when the Application Master attempts to register, causing the scheduler to encounter a null reference."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)\n        at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)\n        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)\n        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:624)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:81)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:656)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:640)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "YARN-5594.json",
        "creation_time": "2016-08-30T15:14:19.000+0000",
        "bug_report": {
            "title": "InvalidProtocolBufferException due to zero tag in RMDelegationTokenIdentifierDataProto",
            "description": {
                "stepsToReproduce": [
                    "Start the ResourceManager service in Hadoop YARN.",
                    "Attempt to load the state from the FileSystemRMStateStore.",
                    "Observe the logs for any exceptions thrown during the loading process."
                ],
                "actualBehavior": "The ResourceManager fails to start and throws an InvalidProtocolBufferException indicating that the protocol message contained an invalid tag (zero).",
                "possibleCause": "The state file being loaded may be corrupted or improperly formatted, leading to an invalid tag being encountered during deserialization."
            },
            "stackTrace": "com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).\nat com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)\nat com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)\nat org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)\nat org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4644)\nat org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4740)\nat org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4735)\nat org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:5075)\nat org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:4955)\nat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:337)\nat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:267)\nat com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:210)\nat com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:904)\nat org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)\nat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)\nat org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadState(FileSystemRMStateStore.java:199)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)\nat org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1007)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1048)\nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1044)"
        }
    },
    {
        "filename": "YARN-7511.json",
        "creation_time": "2017-11-16T11:41:43.000+0000",
        "bug_report": {
            "title": "NullPointerException in Resource Localization during Container Management",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN NodeManager.",
                    "2. Submit a job that requires resource localization.",
                    "3. Monitor the NodeManager logs for any errors during the resource localization phase."
                ],
                "actualBehavior": "A NullPointerException is thrown during the resource localization process, causing the container to fail.",
                "possibleCause": "The issue may be caused by an attempt to remove a node from a ConcurrentHashMap that is null, indicating that the resource being localized may not have been properly initialized or is missing."
            },
            "stackTrace": "java.lang.NullPointerException\n        at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)\n        at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)\n        at java.lang.Thread.run(Thread.java:834)"
        }
    },
    {
        "filename": "YARN-3790.json",
        "creation_time": "2015-06-10T04:53:40.000+0000",
        "bug_report": {
            "title": "AssertionError in TestWorkPreservingRMRestart due to unexpected metric value",
            "description": {
                "stepsToReproduce": [
                    "1. Set up the Hadoop YARN ResourceManager environment.",
                    "2. Run the TestWorkPreservingRMRestart test suite.",
                    "3. Observe the output of the test case 'testSchedulerRecovery'."
                ],
                "actualBehavior": "The test fails with an AssertionError indicating that the expected metric value was 6144 but the actual value was 8192.",
                "possibleCause": "There may be a discrepancy in the expected metrics due to changes in the ResourceManager's configuration or behavior that were not accounted for in the test."
            },
            "stackTrace": "java.lang.AssertionError: expected:<6144> but was:<8192>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:555)\n\tat org.junit.Assert.assertEquals(Assert.java:542)\n\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.assertMetrics(TestWorkPreservingRMRestart.java:853)\n\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.checkFSQueue(TestWorkPreservingRMRestart.java:342)\n\tat org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart.testSchedulerRecovery(TestWorkPreservingRMRestart.java:241)"
        }
    },
    {
        "filename": "YARN-6068.json",
        "creation_time": "2017-01-07T03:16:07.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException during Application Log Handling",
            "description": {
                "stepsToReproduce": [
                    "1. Start a YARN application and ensure it is in the RUNNING state.",
                    "2. Trigger an event that leads to application log handling.",
                    "3. Observe the application state transition."
                ],
                "actualBehavior": "The application throws an InvalidStateTransitionException indicating that the event APPLICATION_LOG_HANDLING_FAILED is invalid in the RUNNING state.",
                "possibleCause": "The application may be attempting to handle log events in an invalid state, suggesting a flaw in the state management logic."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)\n        at java.lang.Thread.run(Thread.java:745)"
        }
    },
    {
        "filename": "YARN-903.json",
        "creation_time": "2013-07-07T08:35:30.000+0000",
        "bug_report": {
            "title": "YarnException: Container not handled by NodeManager",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the YARN cluster that requires multiple containers.",
                    "2. Monitor the NodeManager logs while the job is running.",
                    "3. Attempt to stop one of the containers from the ResourceManager or through the API."
                ],
                "actualBehavior": "The system throws a YarnException indicating that the specified container is not handled by the NodeManager.",
                "possibleCause": "This may occur if the container was already stopped or if there is a mismatch between the NodeManager and the ResourceManager regarding the container's state."
            },
            "stackTrace": "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000002 is not handled by this NodeManager\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)\n\tat org.apache.hadoop.yarn.proto.ContainerManagementProtocol$ContainerManagementProtocolService$2.callBlockingMethod(ContainerManagementProtocol.java:85)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1868)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1864)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1862)\n\norg.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000001 is not handled by this NodeManager\n\tat org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:45)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)\n\tat org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)\n\tat org.apache.hadoop.yarn.proto.ContainerManagementProtocol$ContainerManagementProtocolService$2.callBlockingMethod(ContainerManagementProtocol.java:85)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:605)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1033)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1868)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1864)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1489)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1862)\n\njava.lang.InterruptedException\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1899)\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1934)\n\tat java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)\n\tat org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:281)"
        }
    },
    {
        "filename": "YARN-8236.json",
        "creation_time": "2018-04-29T16:28:11.000+0000",
        "bug_report": {
            "title": "NullPointerException in ServiceClient when adding keytab resource",
            "description": {
                "stepsToReproduce": [
                    "1. Initialize the ServiceClient instance.",
                    "2. Call the submitApp method with appropriate parameters.",
                    "3. Ensure that the application is configured to use a keytab resource."
                ],
                "actualBehavior": "The application throws a NullPointerException when attempting to add a keytab resource.",
                "possibleCause": "It is possible that the keytab resource or a related configuration is not properly initialized or is null when the addKeytabResourceIfSecure method is called."
            },
            "stackTrace": "java.lang.NullPointerException\nat org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)\nat org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)\nat org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269)"
        }
    },
    {
        "filename": "YARN-2857.json",
        "creation_time": "2014-10-24T20:47:51.000+0000",
        "bug_report": {
            "title": "ConcurrentModificationException in Pig Job Execution",
            "description": {
                "stepsToReproduce": [
                    "1. Set up a Hadoop environment with Oozie and Pig.",
                    "2. Create a Pig script that utilizes logging.",
                    "3. Execute the Pig job through Oozie."
                ],
                "actualBehavior": "The Pig job fails with a ConcurrentModificationException during the logging configuration phase.",
                "possibleCause": "The exception may be caused by concurrent modifications to a LinkedList while iterating over it, likely due to multiple threads attempting to modify the logging configuration simultaneously."
            },
            "stackTrace": "java.util.ConcurrentModificationException\n\tat java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)\n\tat java.util.LinkedList$ListItr.next(LinkedList.java:888)\n\tat org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)\n\tat org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)\n\tat org.apache.log4j.Category.removeAllAppenders(Category.java:891)\n\tat org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:759)\n\tat org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)\n\tat org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)\n\tat org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440)\n\tat org.apache.pig.Main.configureLog4J(Main.java:740)\n\tat org.apache.pig.Main.run(Main.java:384)\n\tat org.apache.pig.PigRunner.run(PigRunner.java:49)\n\tat org.apache.oozie.action.hadoop.PigMain.runPigJob(PigMain.java:283)\n\tat org.apache.oozie.action.hadoop.PigMain.run(PigMain.java:223)\n\tat org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:37)\n\tat org.apache.oozie.action.hadoop.PigMain.main(PigMain.java:76)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:226)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
        }
    },
    {
        "filename": "YARN-2416.json",
        "creation_time": "2014-08-13T22:36:31.000+0000",
        "bug_report": {
            "title": "Invalid State Transition Exceptions in YARN ResourceManager",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a YARN application to the ResourceManager.",
                    "2. Monitor the application state transitions.",
                    "3. Observe the logs for any state transition events."
                ],
                "actualBehavior": "The application encounters multiple InvalidStateTransitionExceptions for events like REGISTERED, STATUS_UPDATE, and CONTAINER_ALLOCATED while in the ALLOCATED state.",
                "possibleCause": "The application may be sending events that are not valid for the current state, indicating a potential issue in the event handling logic or state management."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: REGISTERED at ALLOCATED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:744)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at ALLOCATED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:744)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: CONTAINER_ALLOCATED at ALLOCATED\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)\n        at java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "YARN-345.json",
        "creation_time": "2013-01-17T12:57:46.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException in YARN Application Lifecycle",
            "description": {
                "stepsToReproduce": [
                    "Submit a YARN application with multiple containers.",
                    "Trigger the application to finish or clean up resources.",
                    "Observe the application state transitions in the logs."
                ],
                "actualBehavior": "The application encounters multiple InvalidStateTransitionException errors during its lifecycle, indicating that certain events are not valid in the current state.",
                "possibleCause": "The application may be sending events that are not allowed in the current state of the application, possibly due to race conditions or incorrect event handling logic."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:662)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at APPLICATION_RESOURCES_CLEANINGUP\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:662)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHING_CONTAINERS_WAIT\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:662)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_CONTAINER_FINISHED at FINISHED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:662)\n\norg.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: INIT_CONTAINER at FINISHED\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)\n\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)\n\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)\n\tat java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "YARN-3894.json",
        "creation_time": "2015-07-08T07:00:51.000+0000",
        "bug_report": {
            "title": "ResourceManager Fails to Transition to Active State Due to Queue Initialization Error",
            "description": {
                "stepsToReproduce": [
                    "1. Configure the CapacityScheduler with a queue that has an illegal capacity setting (e.g., 0.5 for children of queue root for label=node2).",
                    "2. Start the ResourceManager in a High Availability (HA) setup.",
                    "3. Attempt to transition the ResourceManager to the Active state."
                ],
                "actualBehavior": "The ResourceManager fails to transition to the Active state, throwing an IOException related to queue reinitialization.",
                "possibleCause": "The issue may be caused by an invalid queue configuration that specifies an illegal capacity value, leading to a failure during the reinitialization of queues."
            },
            "stackTrace": "java.io.IOException: Failed to re-init queues\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:383)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:376)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:605)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(EmbeddedElectorService.java:126)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\nCaused by: java.lang.IllegalArgumentException: Illegal capacity of 0.5 for children of queue root for label=node2\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues(ParentQueue.java:159)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:639)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitializeQueues(CapacityScheduler.java:503)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:379)\n        ... 8 more\n\norg.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active\n        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)\n        at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)\n        at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\nCaused by: org.apache.hadoop.ha.ServiceFailedException: Error when transitioning to Active mode\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:321)\n        at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)\n        ... 4 more\nCaused by: org.apache.hadoop.ha.ServiceFailedException: java.io.IOException: Failed to re-init queues\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:617)\n        at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)"
        }
    },
    {
        "filename": "YARN-1903.json",
        "creation_time": "2014-04-04T20:51:24.000+0000",
        "bug_report": {
            "title": "AssertionError in TestNMClient.testGetContainerStatus",
            "description": {
                "stepsToReproduce": [
                    "Run the TestNMClient test suite.",
                    "Ensure that the test environment is properly set up with all necessary dependencies.",
                    "Execute the test case testGetContainerStatus."
                ],
                "actualBehavior": "The test case testGetContainerStatus fails with an AssertionError.",
                "possibleCause": "The expected condition in the assertion may not be met, possibly due to incorrect container status or a failure in the container management logic."
            },
            "stackTrace": "java.lang.AssertionError: 4: \n\tat org.junit.Assert.fail(Assert.java:93)\n\tat org.junit.Assert.assertTrue(Assert.java:43)\n\tat org.apache.hadoop.yarn.client.api.impl.TestNMClient.testGetContainerStatus(TestNMClient.java:382)\n\tat org.apache.hadoop.yarn.client.api.impl.TestNMClient.testContainerManagement(TestNMClient.java:346)\n\tat org.apache.hadoop.yarn.client.api.impl.TestNMClient.testNMClient(TestNMClient.java:226)"
        }
    },
    {
        "filename": "YARN-4347.json",
        "creation_time": "2015-11-11T22:32:59.000+0000",
        "bug_report": {
            "title": "NullPointerException in CapacityScheduler during Application Attempt Recovery",
            "description": {
                "stepsToReproduce": [
                    "1. Start the ResourceManager service in a Hadoop YARN cluster.",
                    "2. Submit an application to the YARN ResourceManager.",
                    "3. Simulate a failure or restart of the ResourceManager while the application is in progress.",
                    "4. Observe the logs during the recovery process."
                ],
                "actualBehavior": "A NullPointerException is thrown in the CapacityScheduler when attempting to recover application attempts.",
                "possibleCause": "The issue may be caused by an uninitialized object or a missing application attempt reference during the recovery process."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:755)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:839)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:854)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:844)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)\n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:719)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:411)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1219)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:593)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1026)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1067)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1063)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)"
        }
    },
    {
        "filename": "YARN-1692.json",
        "creation_time": "2014-02-07T02:01:17.000+0000",
        "bug_report": {
            "title": "ConcurrentModificationException in FairScheduler during demand update",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with FairScheduler configured.",
                    "2. Submit multiple applications that require resource allocation simultaneously.",
                    "3. Monitor the ResourceManager logs for any exceptions during the demand update process."
                ],
                "actualBehavior": "A ConcurrentModificationException is thrown, causing the FairScheduler to fail in updating the demand for resources.",
                "possibleCause": "The exception may be caused by concurrent modifications to a HashMap while iterating over its entries, likely due to multiple threads accessing and modifying the same data structure."
            },
            "stackTrace": "java.util.ConcurrentModificationException\n        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)\n        at java.util.HashMap$ValueIterator.next(HashMap.java:954)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)\n        at java.lang.Thread.run(Thread.java:724)"
        }
    },
    {
        "filename": "YARN-7697.json",
        "creation_time": "2018-01-03T19:28:50.000+0000",
        "bug_report": {
            "title": "OutOfMemoryError in LogAggregationIndexedFileController",
            "description": {
                "stepsToReproduce": [
                    "1. Start a Hadoop YARN application with a large number of containers.",
                    "2. Enable log aggregation for the application.",
                    "3. Monitor the application until it attempts to aggregate logs."
                ],
                "actualBehavior": "The application throws a java.lang.OutOfMemoryError: Java heap space, causing log aggregation to fail.",
                "possibleCause": "The log aggregation process may be attempting to load too much data into memory, exceeding the allocated Java heap space."
            },
            "stackTrace": "java.lang.OutOfMemoryError: Java heap space\n        at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:823)\n        at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:840)\n        at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling(LogAggregationIndexedFileController.java:293)\n        at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.access$600(LogAggregationIndexedFileController.java:98)\n        at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:216)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)\n        at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:197)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:205)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:312)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:284)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "YARN-7382.json",
        "creation_time": "2017-10-23T23:36:59.000+0000",
        "bug_report": {
            "title": "NoSuchElementException in FairScheduler during container assignment",
            "description": {
                "stepsToReproduce": [
                    "1. Start the Hadoop YARN ResourceManager with a FairScheduler configuration.",
                    "2. Submit multiple applications that require resource allocation.",
                    "3. Monitor the scheduling process and observe the logs for any exceptions."
                ],
                "actualBehavior": "The application fails to allocate containers, and a NoSuchElementException is thrown, indicating that the scheduler is trying to access an element that does not exist.",
                "possibleCause": "The exception may occur when the scheduler attempts to retrieve the first key from an empty ConcurrentSkipListMap, possibly due to a race condition or improper state management in the scheduling logic."
            },
            "stackTrace": "java.util.NoSuchElementException\n        at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)\n        at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)\n        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:128)\n        at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)\n        at java.lang.Thread.run(Thread.java:748)"
        }
    },
    {
        "filename": "YARN-1094.json",
        "creation_time": "2013-08-23T19:06:17.000+0000",
        "bug_report": {
            "title": "NullPointerException in DelegationTokenRenewer during Application Submission",
            "description": {
                "stepsToReproduce": [
                    "1. Start the ResourceManager service.",
                    "2. Submit an application to the ResourceManager.",
                    "3. Observe the logs for any exceptions."
                ],
                "actualBehavior": "A NullPointerException is thrown when attempting to set a timer for token renewal, causing the application submission to fail.",
                "possibleCause": "It is possible that a required object for token renewal is not initialized or is null when the application is submitted."
            },
            "stackTrace": "java.lang.NullPointerException\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.setTimerForTokenRenewal(DelegationTokenRenewer.java:371)\n        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplication(DelegationTokenRenewer.java:307)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)\n        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:371)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:819)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:613)\n        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)\n        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:832)"
        }
    },
    {
        "filename": "YARN-7269.json",
        "creation_time": "2017-09-28T23:56:42.000+0000",
        "bug_report": {
            "title": "Proxy Server Redirection Failure in YARN Web Proxy",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN ResourceManager and NodeManager services.",
                    "2. Attempt to access the YARN web interface through a browser.",
                    "3. Observe the behavior when trying to access application details that require redirection."
                ],
                "actualBehavior": "The application fails to redirect properly, resulting in a ServletException indicating that the proxy server could not be determined.",
                "possibleCause": "The issue may be related to misconfiguration of the proxy settings in the YARN configuration files, or the proxy server may not be running."
            },
            "stackTrace": "javax.servlet.ServletException: Could not determine the proxy server for redirection\n\tat org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:199)\n\tat org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:141)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1426)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
        }
    },
    {
        "filename": "YARN-7249.json",
        "creation_time": "2017-09-25T16:49:46.000+0000",
        "bug_report": {
            "title": "NullPointerException in CapacityScheduler during Container Completion",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a job to the Hadoop YARN ResourceManager.",
                    "2. Ensure that the job has reserved containers.",
                    "3. Trigger the completion of a container in the LeafQueue."
                ],
                "actualBehavior": "The application throws a NullPointerException when attempting to complete a container in the LeafQueue.",
                "possibleCause": "It is possible that a required object or reference is not initialized before the completedContainer method is called, leading to a NullPointerException."
            },
            "stackTrace": "java.lang.NullPointerException \nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308) \nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469) \nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497) \nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505) \nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341) \nat org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127) \nat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705)"
        }
    },
    {
        "filename": "YARN-4598.json",
        "creation_time": "2016-01-15T06:48:48.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException when handling RESOURCE_FAILED event",
            "description": {
                "stepsToReproduce": [
                    "1. Start a YARN application with multiple containers.",
                    "2. Forcefully kill one of the containers during execution.",
                    "3. Observe the logs for any exceptions related to state transitions."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown indicating an invalid event RESOURCE_FAILED at the state CONTAINER_CLEANEDUP_AFTER_KILL.",
                "possibleCause": "The state machine may not be correctly handling the transition from CONTAINER_CLEANEDUP_AFTER_KILL to a state that can process RESOURCE_FAILED events."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL\n    at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)\n    at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n    at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:83)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)\n    at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1071)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)\n    at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:108)\n    at java.lang.Thread.run(Thread.java:744)"
        }
    },
    {
        "filename": "YARN-1149.json",
        "creation_time": "2013-09-04T21:46:58.000+0000",
        "bug_report": {
            "title": "InvalidStateTransitionException during Application Log Handling in YARN",
            "description": {
                "stepsToReproduce": [
                    "1. Submit a YARN application that requires logging.",
                    "2. Monitor the application state until it reaches RUNNING.",
                    "3. Trigger the application log handling process."
                ],
                "actualBehavior": "An InvalidStateTransitionException is thrown indicating an invalid event at the RUNNING state.",
                "possibleCause": "The application may be attempting to handle logs in an invalid state, suggesting a potential issue in the state management logic of the YARN NodeManager."
            },
            "stackTrace": "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING\n        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305) \n        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)\n        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:59)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:689)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)\n        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)   \n        at java.lang.Thread.run(Thread.java:662)"
        }
    },
    {
        "filename": "YARN-7818.json",
        "creation_time": "2018-01-25T18:42:55.000+0000",
        "bug_report": {
            "title": "PrivilegedOperationException with Exit Code 143 during Container Launch",
            "description": {
                "stepsToReproduce": [
                    "1. Start the YARN NodeManager service.",
                    "2. Submit a job that requires launching a container.",
                    "3. Monitor the logs for any exceptions during the container launch process."
                ],
                "actualBehavior": "The container fails to launch, resulting in a PrivilegedOperationException with exit code 143.",
                "possibleCause": "The exit code 143 typically indicates that a process was terminated due to a SIGTERM signal, which may suggest resource constraints or misconfiguration in the NodeManager."
            },
            "stackTrace": "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:180)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.launchContainer(DefaultLinuxContainerRuntime.java:124)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.launchContainer(DelegatingLinuxContainerRuntime.java:152)\n        at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:549)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:285)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:95)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\nCaused by: ExitCodeException exitCode=143:\n        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009)\n        at org.apache.hadoop.util.Shell.run(Shell.java:902)\n        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227)\n        at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:152)"
        }
    }
]